<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 分类 - siwei.io</title>
        <link>https://siwei.io/categories/llm/</link>
        <description>LLM - 分类 - siwei.io</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>weyl.gu@gmail.com (Wey Gu)</managingEditor>
            <webMaster>weyl.gu@gmail.com (Wey Gu)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 15 Aug 2023 11:10:34 &#43;0800</lastBuildDate><atom:link href="https://siwei.io/categories/llm/" rel="self" type="application/rss+xml" /><item>
    <title>Graph RAG: 知识图谱结合 LLM 的检索增强</title>
    <link>https://siwei.io/graph-rag/</link>
    <pubDate>Tue, 15 Aug 2023 11:10:34 &#43;0800</pubDate><author>
        <name>Wey Gu</name>
    </author><guid>https://siwei.io/graph-rag/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/graph-rag/featured-image.webp" referrerpolicy="no-referrer">
            </div><blockquote>
<p>本文为大家揭示我们优先提出的 Graph RAG 方法，这种结合知识图谱、图数据库作为大模型结合私有知识系统最新的技术栈，作为之前的图上下文学习、text2cypher 文章的第三篇文章。</p>
</blockquote>
<p>本文为大家揭示我们优先提出的 Graph RAG 方法，这种结合知识图谱、图数据库作为大模型结合私有知识系统最新的技术栈，作为之前的图上下文学习、text2cypher 文章的第三篇文章。</p>
<h2 id="graph-rag" class="headerLink">
    <a href="#graph-rag" class="header-mark"></a>1 Graph RAG</h2><p>在<a href="https://siwei.io/en/graph-enabled-llama-index/" target="_blank" rel="noopener noreferrer">第一篇关于上下文学习的博客中</a>我们介绍过， RAG（Retrieval Argumented Generation）这种基于特定任务/问题的文档检索范式中，我们通常先收集必要的上下文，然后利用具有认知能力的机器学习模型进行上下文学习（in-context learning），来合成任务的答案。</p>
<p>借助 LLM 这个只需要”说话“就可以灵活处理复杂问题的感知层，只需要两步，就能搭建一个基于私有知识的智能应用：</p>
<ul>
<li>利用各种搜索方式（比如 Embedding 与向量数据库）从给定的文档中检索相关知识。</li>
<li>利用 LLM 理解并智能地合成答案。</li>
</ul>
<p>而这篇博客中，我们结合最新的探索进展和思考，尝试把 Graph RAG 和其他方法进行比较，说得更透一点。并且，我们决定开始用 Graph RAG 这个叫法来描述它。</p>
<blockquote>
<p>实际上，<a href="https://siwei.io/talks/graph-rag-with-jerry/" target="_blank" rel="noopener noreferrer">Graph RAG</a>，是最先又我<a href="https://www.youtube.com/watch?v=bPoNCkjDmco" target="_blank" rel="noopener noreferrer">和 Jerry Liu 的直播研讨会讨论</a>和<a href="https://twitter.com/wey_gu/status/1673362774930628608" target="_blank" rel="noopener noreferrer">相关的讨论的 Twitter Thread</a>中提到的，差不多的内容我在 <a href="https://www.bilibili.com/video/BV1Pp4y157nt" target="_blank" rel="noopener noreferrer">NebulaGraph 社区直播</a> 中也用中文介绍过。</p>
</blockquote>
<h2 id="在-rag-中知识图谱的价值" class="headerLink">
    <a href="#%e5%9c%a8-rag-%e4%b8%ad%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e7%9a%84%e4%bb%b7%e5%80%bc" class="header-mark"></a>2 在 RAG 中知识图谱的价值</h2><p>这部分内容我们在第一篇文章中阐述过，比如一个查询：“告诉我所有关于苹果和乔布斯的事”，基于乔布斯自传这本书进行问答，而这个问题涉及到的上下文分布在自传这本书的 30 页（分块）的时候，传统的“分割数据，Embedding 再向量搜索”方法在多个文档块里用 top-k 去搜索的方法很难得到这种分散，细粒的完整信息。而且，这种方法还很容易遗漏互相关联的文档块，从而导致信息检索不完整。</p>
<p>除此之外，在之后一次技术会议中，我有幸和 leadscloud.com 的徐旭讨论之后（他们因为有知识图谱的技术背景，也做了和我们类似的探索和尝试！），让我意识到知识图谱可以减少基于嵌入的语义搜索所导致的不准确性。徐旭给出的一个有趣的例子是“保温大棚”与“保温杯”，尽管在语义上两者是存在相关性的，但在大多数场景下，这种通用语义（Embedding）下的相关性常常是我们不希望产生的，进而作为错误的上下文而引入“幻觉”。</p>
<p>这时候，保有领域知识的知识图谱则是非常直接可以缓解、消除这种幻觉的手段。</p>
<h2 id="用-nebulagraph-实现-graph-rag" class="headerLink">
    <a href="#%e7%94%a8-nebulagraph-%e5%ae%9e%e7%8e%b0-graph-rag" class="header-mark"></a>3 用 NebulaGraph 实现 Graph RAG</h2><p>一个简单的 Graph RAG 可以如下去简单实现：</p>
<ol>
<li>使用LLM(或其他)模型从问题中提取关键实体。</li>
<li>根据这些实体检索子图，深入到一定的深度（例如，2）。</li>
<li>利用获得的上下文利用LLM产生答案。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 伪代码</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_get_key_entities</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="kc">None</span> <span class="p">,</span><span class="n">with_llm</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_expand_synonyms</span><span class="p">(</span><span class="n">entities</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_retrieve_subgraph_context</span><span class="p">(</span><span class="n">entities</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nebulagraph_store</span><span class="o">.</span><span class="n">get_relations</span><span class="p">(</span><span class="n">entities</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_synthesize_answer</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">llm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">PROMPT_SYNTHESIZE_AND_REFINE</span><span class="p">,</span> <span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">simple_graph_rag</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">nebulagraph_store</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">entities</span> <span class="o">=</span> <span class="n">_get_key_entities</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">graph_rag_context</span> <span class="o">=</span> <span class="n">_retrieve_subgraph_context</span><span class="p">(</span><span class="n">entities</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_synthesize_answer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然而，有了像 Llama Index 这样方便的 LLM 编排工具，开发者可以专注于 LLM 的编排逻辑和 pipeline 设计，而不用亲自处理很多细节的抽象与实现。</p>
<p>所以，用 Llama Index，我们可以轻松搭建 Graph RAG，甚至整合更复杂的 RAG 逻辑，比如 <a href="https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html" target="_blank" rel="noopener noreferrer">Graph+Vector RAG</a>。</p>
<p><figure><a class="lightgallery" href="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870" title="Graph_RAG_Entity_SubGraph" data-thumbnail="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870">
        <img
            
            loading="lazy"
            src="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870"
            srcset="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870, https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870 1.5x, https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870 2x"
            sizes="auto"
            alt="Graph_RAG_Entity_SubGraph">
    </a></figure></p>
<p>在 Llama Index 中，我们有两种方法实现 Graph RAG：</p>
<ul>
<li><code>KnowledgeGraphIndex</code> 用来从任何私有数据只是从零构建知识图谱（基于 LLM 或者其他语言模型），然后 4 行代码进行 Graph RAG。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">graph_store = NebulaGraphStore(
</span></span><span class="line"><span class="cl">    space_name=space_name,
</span></span><span class="line"><span class="cl">    edge_types=edge_types,
</span></span><span class="line"><span class="cl">    rel_prop_names=rel_prop_names,
</span></span><span class="line"><span class="cl">    tags=tags,
</span></span><span class="line"><span class="cl">)
</span></span><span class="line"><span class="cl">storage_context = StorageContext.from_defaults(graph_store=graph_store)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Build KG
</span></span><span class="line"><span class="cl">kg_index = KnowledgeGraphIndex.from_documents(
</span></span><span class="line"><span class="cl">    documents,
</span></span><span class="line"><span class="cl">    storage_context=storage_context,
</span></span><span class="line"><span class="cl">    max_triplets_per_chunk=10,
</span></span><span class="line"><span class="cl">    space_name=space_name,
</span></span><span class="line"><span class="cl">    edge_types=edge_types,
</span></span><span class="line"><span class="cl">    rel_prop_names=rel_prop_names,
</span></span><span class="line"><span class="cl">    tags=tags,
</span></span><span class="line"><span class="cl">)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kg_query_engine = kg_index.as_query_engine()
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>KnowledgeGraphRAGQueryEngine</code> 则可以在任何已经存在的知识图谱上进行 Graph RAG，不过我还没有完成这个 <a href="https://github.com/jerryjliu/llama_index/pull/7204" target="_blank" rel="noopener noreferrer">PR</a>。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">graph_store = NebulaGraphStore(
</span></span><span class="line"><span class="cl">    space_name=space_name,
</span></span><span class="line"><span class="cl">    edge_types=edge_types,
</span></span><span class="line"><span class="cl">    rel_prop_names=rel_prop_names,
</span></span><span class="line"><span class="cl">    tags=tags,
</span></span><span class="line"><span class="cl">)
</span></span><span class="line"><span class="cl">storage_context = StorageContext.from_defaults(graph_store=graph_store)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">graph_rag_query_engine = KnowledgeGraphRAGQueryEngine(
</span></span><span class="line"><span class="cl">    storage_context=storage_context,
</span></span><span class="line"><span class="cl">)
</span></span></code></pre></td></tr></table>
</div>
</div><p>最后，我做了<a href="https://www.siwei.io/demos/graph-rag/" target="_blank" rel="noopener noreferrer">一个 streamlit 的 demo</a>来比较 Graph RAG 与 Vector RAG，从中我们可以看到 Graph RAG 并没有取代 Embedding、向量搜索的方法，而是增强了/补充了它的不足。</p>
<p><figure><a class="lightgallery" href="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29" title="img" data-thumbnail="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29">
        <img
            
            loading="lazy"
            src="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29"
            srcset="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29, https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29 1.5x, https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29 2x"
            sizes="auto"
            alt="img">
    </a></figure></p>
<h2 id="text2cypher" class="headerLink">
    <a href="#text2cypher" class="header-mark"></a>4 text2cypher</h2><p>基于图谱的 LLM 的另一种有趣方法是text2cypher。这种方法不依赖于实体的子图检索，而是将任务/问题翻译成一个面向答案的特定图查询，和我们常说的 text2sql 方法本质是一样的。</p>
<h3 id="在-nebulagraph-上进行-text2cypher" class="headerLink">
    <a href="#%e5%9c%a8-nebulagraph-%e4%b8%8a%e8%bf%9b%e8%a1%8c-text2cypher" class="header-mark"></a>4.1 在 NebulaGraph 上进行 text2cypher</h3><p>在之前的文章中我们已经介绍过，得益于 LLM，实现 text2cypher 比传统的 ML 方法更为简单和便宜。</p>
<p>比如，<a href="https://python.langchain.com/docs/use_cases/more/graph/graph_nebula_qa" target="_blank" rel="noopener noreferrer">LangChain: NebulaGraphQAChain</a> 和 <a href="https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html" target="_blank" rel="noopener noreferrer">Llama Index: KnowledgeGraphQueryEngine</a> 让我们 3 行代码就能跑起来 text2cypher。</p>
<h3 id="比较-text2cypher-和-subgraph-rag" class="headerLink">
    <a href="#%e6%af%94%e8%be%83-text2cypher-%e5%92%8c-subgraph-rag" class="header-mark"></a>4.2 比较 text2cypher 和 (Sub)Graph RAG</h3><p>这两种方法主要在其检索机制上有所不同。text2cypher 根据 KG 的 Schema 和给定的任务生成图形模式查询，而SubGraph RAG获取相关的子图以提供上下文。</p>
<p>两者都有其优点，为了大家更直观理解他们的特点，我做了这个 demo 视频：</p>
<p>我们可以看到两者的图查询模式在可视化下是有非常清晰的差异的。</p>
<p><figure><a class="lightgallery" href="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29" title="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29" data-thumbnail="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29">
        <img
            
            loading="lazy"
            src="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29"
            srcset="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29, https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29 1.5x, https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29 2x"
            sizes="auto"
            alt="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29">
    </a></figure></p>
<h3 id="结合text2cypher的graph-rag" class="headerLink">
    <a href="#%e7%bb%93%e5%90%88text2cypher%e7%9a%84graph-rag" class="header-mark"></a>4.3 结合text2cypher的Graph RAG</h3><p>然而，两者并没有绝对的好与坏，不同场景下，它们各有优劣。</p>
<p>在现实世界中，我们可能并不总是知道哪种方法更有效（好帮助区分应该用哪一种），因此，我倾向于考虑同时利用两者，这样获取的两种检索结果作为上下文，一起来生成最终答案的效果可能是最好的。</p>
<p>具体的实现方法在<a href="https://github.com/jerryjliu/llama_index/pull/7204" target="_blank" rel="noopener noreferrer">这个 PR</a>中已经可以做到了，只需要设置<code>with_text2cypher=True</code>，Graph RAG 就会包含text2cypher 上下文，敬请期待它的合并。</p>
<h2 id="结论" class="headerLink">
    <a href="#%e7%bb%93%e8%ae%ba" class="header-mark"></a>5 结论</h2><p>通过将知识图谱、图存储集成到 LLM 技术栈中，Graph RAG 把 RAG 的上下文学习推向了一个新的高度。它能在 LLM 应用中，通过利用现有（或新建）的知识图谱，提取细粒度、精确调整、领域特定且互联的知识。</p>
<p>请继续关注图谱和LLM领域的更深入的探索和进一步的发展。</p>
<blockquote>
<p>题图 prompt： A vast open book serves as the backdrop, with intricately interwoven nodes and lines forming a Graph on its pages. At the center of this graph, there&rsquo;s a glowing brain symbolizing the Knowledge Graph. Rays of light emanate from the brain, reaching every corner of the graph, mirroring neural connections linking diverse information. On the right side of the illustration, a robotic arm with a pen is swiftly writing, representing the input and output of the AI large language model.</p>
</blockquote>]]></description>
</item><item>
    <title>Text2Cypher：大语言模型驱动的图谱查询生成</title>
    <link>https://siwei.io/llm-text-to-nebulagraph-query/</link>
    <pubDate>Mon, 17 Jul 2023 20:30:04 &#43;0800</pubDate><author>
        <name>Wey Gu</name>
    </author><guid>https://siwei.io/llm-text-to-nebulagraph-query/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/llm-text-to-nebulagraph-query/featured-image.webp" referrerpolicy="no-referrer">
            </div><p>从 GPT-3 开始展现出超出预期的”理解能力“开始，我们一直在做 Graph + LLM 技术组合、互补的研究、探索和阜分享，截止到现在 NebulaGraph 已经在 LlamaIndex 与 Langchain 项目做出了不少领先的贡献，从本文开始，我们就把其中一些阶段性的成功、方法单独分享给大家。</p>
<p>本文的主题是我们认为这个领域最低垂的果实，text2cypher：自然语言生成图查询。</p>
<h2 id="text2cypher" class="headerLink">
    <a href="#text2cypher" class="header-mark"></a>1 Text2Cypher</h2><p>顾名思义， Text2Cypher 做的就是把自然语言的文本转换成 Cypher 查询语句的这件事儿，和另一个大家可能已经比较熟悉的场景 Text2SQL：文本转换 SQL 在形式上没有什么区别。而本质上，大多数知识图谱、图数据库的应用都是在图上按照人类意愿进行查询，我们在图数据库上构造方便的可视化工具、封装方便的 API 的工作都是为这个目标服务的。</p>
<p>一直以来，阻碍图数据库、知识图谱被更广泛应用的主要因素可能就是查询图数据库的门槛了。那么，在没有大语言模型的时候，我们是怎么做的呢？</p>
<h2 id="传统的-text2cypher" class="headerLink">
    <a href="#%e4%bc%a0%e7%bb%9f%e7%9a%84-text2cypher" class="header-mark"></a>2 传统的 Text2Cypher</h2><p>文本到查询的这个领域在大语言模型之前就一直存在这样的需求，一直是知识图谱最常见的应用之一，比如 KBQA（基于知识库的问答系统）的系统内部本质上就是 text2cypher。</p>
<p>这里以我之前写的项目 <a href="https://www.siwei.io/siwi" target="_blank" rel="noopener noreferrer">Siwi</a> （发音：/ˈsɪwi/， 一个基于篮球运动员数据集的问答应用）为例，了解一下它的后端架构：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">┌─────────────┬───────────────────────────────────┐
</span></span><span class="line"><span class="cl">│      Speech │  Frontend                         │
</span></span><span class="line"><span class="cl">│  ┌──────────▼──────────┐ Siwi, /ˈsɪwi/          │
</span></span><span class="line"><span class="cl">│  │ Web_Speech_API      │ A PoC of Dialog System │
</span></span><span class="line"><span class="cl">│  │ Vue.JS              │ With Graph Database    │
</span></span><span class="line"><span class="cl">│  │                     │ Backed Knowledge Graph │
</span></span><span class="line"><span class="cl">│  └──────────┬──────────┘                        │
</span></span><span class="line"><span class="cl">│             │  Sentence  Backend                │
</span></span><span class="line"><span class="cl">│┌────────────┼────────────────────────────┐      │
</span></span><span class="line"><span class="cl">││ ┌──────────▼──────────┐                 │      │
</span></span><span class="line"><span class="cl">││ │ Web API, Flask      │ ./app/          │      │
</span></span><span class="line"><span class="cl">││ └──────────┬──────────┘                 │      │
</span></span><span class="line"><span class="cl">││            │  Sentence  ./bot/          │      │
</span></span><span class="line"><span class="cl">││ ┌──────────▼──────────┐                 │      │
</span></span><span class="line"><span class="cl">││ │ Intent Matching,    │ ./bot/classifier│      │
</span></span><span class="line"><span class="cl">││ │ Symentic Processing │                 │      │
</span></span><span class="line"><span class="cl">││ └──────────┬──────────┘                 │      │
</span></span><span class="line"><span class="cl">││            │  Intent, Enties            │      │
</span></span><span class="line"><span class="cl">││ ┌──────────▼──────────┐                 │      │
</span></span><span class="line"><span class="cl">││ │ Intent Actor        │ ./bot/actions   │      │
</span></span><span class="line"><span class="cl">│└─┴──────────┬──────────┴─────────────────┘      │
</span></span><span class="line"><span class="cl">│             │  Graph Query                      │
</span></span><span class="line"><span class="cl">│  ┌──────────▼──────────┐                        │
</span></span><span class="line"><span class="cl">│  │ Graph Database      │  NebulaGraph           │
</span></span><span class="line"><span class="cl">│  └─────────────────────┘                        │
</span></span><span class="line"><span class="cl">└─────────────────────────────────────────────────┘
</span></span></code></pre></td></tr></table>
</div>
</div><p>当一个问题语句发送过来之后，它首先要做意图识别（Intent）、实体识别（Entity），然后再利用 NLP 模型或者代码把相应的意图和实体构造成知识图谱的查询语句，最终查询图数据库，并根据返回结构构造答案。</p>
<p>可以想象，让程序能够：</p>
<ul>
<li>从自然语言中理解意图：对应到哪一类支持回答的问题</li>
<li>找出实体：问题中涉及到的主要个体</li>
<li>从意图和实体构造查询语句</li>
</ul>
<p>不可能是一个容易的开发工作，一个真正能够落地的实现要训练的模型或者实现的规则代码所考虑的边界条件可能非常多。</p>
<h2 id="用语言模型做-text2cypher" class="headerLink">
    <a href="#%e7%94%a8%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%81%9a-text2cypher" class="header-mark"></a>3 用语言模型做 Text2Cypher</h2><p>而在”后大语言模型“时代，这种从前需要专门训练或者写规则的”智能“应用场景成了通用模型+提示工程（Prompt Engineering）就能完成的任务。</p>
<blockquote>
<p>注：提示工程指通过自然语言描述，让生成模型、语言模型完成”智能“任务的方法。</p>
</blockquote>
<p>事实上，在 GPT-3 刚发布之后，我就开始利用它帮助我写很多非常复杂的 Cypher 查询语句了，我发现它可以写很多非常复杂的模式匹配、多步条件那种之前我需要一点点调试半天才能写出来的语句，通常在它的答案之上，我只需要稍微修改就可以了，而且往往我还能从它的答案里知道我之前没了解到的 Cypher 语法盲区。</p>
<p>后来，在今年二月份的时候，我就试着实现了一个基于 GPT-3 （因为那时候还没有 GPT-3.5）的项目：<a href="https://ngql-gpt.siwei.io/" target="_blank" rel="noopener noreferrer">ngql-GPT</a>（<a href="https://github.com/wey-gu/NebulaGraph-GPT" target="_blank" rel="noopener noreferrer">代码仓库</a>）。</p>
<iframe width="800" height="450" src="https://user-images.githubusercontent.com/1651790/218627408-995b81e1-9b01-423c-ba90-849faaad6f5d.mp4"> </iframe>
<p>它的工作原理非常简单，和 Text2SQL 没有区别，语言模型已经通过公共领域学习了 Cypher 的语法表达，我们在提出任务的时候，只需要让大模型知道我们要查询的图的 Schema 作为上下文就可以了。</p>
<p>所以，基本上 Prompt 就是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">你是一位 NebulaGraph Cypher 专家，请根据给定的图 Schema 和问题，写出查询语句。
</span></span><span class="line"><span class="cl">schema 如下：
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">{schema}
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">问题如下：
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">{question}
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">下面写出查询语句：
</span></span></code></pre></td></tr></table>
</div>
</div><p>然而，真实世界的 prompt 往往还需要增加额外的要求：</p>
<ul>
<li>只返回语句，不用给出解释，不用道歉</li>
<li>强调不要写超出 schema 之外的点、边类型</li>
</ul>
<p>感兴趣的同学可以参考我在 LlamaIndex 的 <a href="https://github.com/jerryjliu/llama_index/blob/71919f9dfa09e9628af8b3a59d497ad02a7a82f8/llama_index/query_engine/knowledge_graph_query_engine.py#L24" target="_blank" rel="noopener noreferrer">KnowlegeGraph Query Engine 中的实现</a>。</p>
<p>在真实场景中，我们想快速学习、构建大语言模型应用的时候，常常会用到 Langchain 或者 LlamaIndex 这样的编排（Orchestrator）工具，它们可以帮我们做很多合理的抽象，从而避免从头去实现很多通用的脚手架代码：</p>
<ul>
<li>和不同语言模型交互</li>
<li>和不同向量数据库交互</li>
<li>数据分割</li>
</ul>
<p>而且，这些编排工具还内置了很多工程方法的最佳实践，这样，我们常常调用一个方法就可以用到最新最好用的大语言模型研究论文的方法了，比如 <a href="https://github.com/jerryjliu/llama_index/tree/main/llama_index/query_engine/flare" target="_blank" rel="noopener noreferrer">FLARE</a>、<a href="https://github.com/jerryjliu/llama_index/blob/main/docs/community/integrations/guidance.md" target="_blank" rel="noopener noreferrer">Guidence</a>。</p>
<p>为此，我在 LlamaIndex 和 Langchain 中都贡献了可以方便进行 NebulaGraph 上 Text2Cypher 的工具，真正做到 3 行代码，Text2Cypher。</p>
<h2 id="nebulagraph-上的-text2cypher" class="headerLink">
    <a href="#nebulagraph-%e4%b8%8a%e7%9a%84-text2cypher" class="header-mark"></a>4 NebulaGraph 上的 Text2Cypher</h2><p>在 LlamaIndex 的 <code>KnowledgeQueryEngine</code> 和 LangChain 的 <code>NebulaGraphQAChain</code> 中：NebulaGraph 图数据库的 Schema 获取、Cypher 语句生成的 Prompt、各种 LLM 的调用、结果的处理、衔接我们可以全都不用关心，开箱即用！</p>
<h3 id="使用-llamaindex" class="headerLink">
    <a href="#%e4%bd%bf%e7%94%a8-llamaindex" class="header-mark"></a>4.1 使用 LlamaIndex</h3><p>用 LlamaIndex，我们只需要：</p>
<ul>
<li>创建一个 <code>NebulaGraphStore</code> 实例</li>
<li>创建一个 <code>KnowledgeQueryEngine</code></li>
</ul>
<p>就可以直接进行问答了，是不是超级简单？</p>
<blockquote>
<p>参考文档：https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.query_engine</span> <span class="kn">import</span> <span class="n">KnowledgeGraphQueryEngine</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.storage.storage_context</span> <span class="kn">import</span> <span class="n">StorageContext</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.graph_stores</span> <span class="kn">import</span> <span class="n">NebulaGraphStore</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">graph_store</span> <span class="o">=</span> <span class="n">NebulaGraphStore</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">space_name</span><span class="o">=</span><span class="n">space_name</span><span class="p">,</span> <span class="n">edge_types</span><span class="o">=</span><span class="n">edge_types</span><span class="p">,</span> <span class="n">rel_prop_names</span><span class="o">=</span><span class="n">rel_prop_names</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">graph_store</span><span class="o">=</span><span class="n">graph_store</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">nl2kg_query_engine</span> <span class="o">=</span> <span class="n">KnowledgeGraphQueryEngine</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">storage_context</span><span class="o">=</span><span class="n">storage_context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">service_context</span><span class="o">=</span><span class="n">service_context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 问答</span>
</span></span><span class="line"><span class="cl"><span class="n">response</span> <span class="o">=</span> <span class="n">nl2kg_query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Tell me about Peter Quill?&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 只生成语句</span>
</span></span><span class="line"><span class="cl"><span class="n">graph_query</span> <span class="o">=</span> <span class="n">nl2kg_query_engine</span><span class="o">.</span><span class="n">generate_query</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Tell me about Peter Quill?&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="使用-langchain" class="headerLink">
    <a href="#%e4%bd%bf%e7%94%a8-langchain" class="header-mark"></a>4.2 使用 Langchain</h3><p>类似的，在 Langchain 里，我们只需要：</p>
<ul>
<li>创建一个 <code>NebulaGraph</code>实例</li>
<li>创建一个 <code>NebulaGraphQAChain</code> 实例</li>
</ul>
<p>就可以直接提问了。</p>
<blockquote>
<p>参考文档：https://python.langchain.com/docs/modules/chains/additional/graph_nebula_qa</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">NebulaGraphQAChain</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.graphs</span> <span class="kn">import</span> <span class="n">NebulaGraph</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">graph</span> <span class="o">=</span> <span class="n">NebulaGraph</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">space</span><span class="o">=</span><span class="n">space_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">username</span><span class="o">=</span><span class="s2">&#34;root&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">password</span><span class="o">=</span><span class="s2">&#34;nebula&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">address</span><span class="o">=</span><span class="s2">&#34;127.0.0.1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">port</span><span class="o">=</span><span class="mi">9669</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">session_pool_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">NebulaGraphQAChain</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">llm</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Tell me about Peter Quill?&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="demo" class="headerLink">
    <a href="#demo" class="header-mark"></a>5 Demo</h2><p><a href="https://www.siwei.io/demos/text2cypher/" target="_blank" rel="noopener noreferrer">demo 地址</a></p>
<iframe width="800" height="857" src="https://user-images.githubusercontent.com/1651790/254521700-6de6aadf-4b62-495a-9276-ef866ebb4add.mp4"> </iframe>
<p>这个 Demo 展示了如何利用 LLM 从不同类型的信息源（以维基百科为例）中抽取知识三元组，并存储到图数据库 NebulaGraph 中。</p>
<p>本 Demo 中，我们先抽取了维基百科中关于《银河护卫队3》的信息，然后利用 LLM 生成的知识三元组，构建了一个图谱。 然后利用 Cypher 查询图谱，最后利用 LlamaIndex 和 Langchain 中的 Text2Cypher，实现了自然语言查询图谱的功能。</p>
<p>您可以点击其他标签亲自试玩图谱的可视化、Cypher 查询、自然语言查询（Text2Cypher）等功能。</p>
<p>这里可以<a href="https://www.siwei.io/demo-dumps/kg-llm/KG_Building.ipynb" target="_blank" rel="noopener noreferrer">下载</a> 完整的 Notebook。</p>
<h2 id="结论" class="headerLink">
    <a href="#%e7%bb%93%e8%ae%ba" class="header-mark"></a>6 结论</h2><p>有了 LLM，知识图谱、NebulaGraph 图数据库中的的数据中进行 Text2Cypher 从来没有这么简单过。</p>
<p>一个具有更强人机、机器接入的知识图谱可以代表了全新的时代，我们可能不需要从前那样高额成本去实现图库之上的后端服务，也不再需要培训才能让领域专家从图中获取重要的洞察了。</p>
<p>利用 LlamaIndex 或者 Langchain 中的生态集成，我们可以几乎没有开发成本地几行代码把自己的应用、图数据智能化。</p>
<p>然而，Text2Cypher 只是一个开始，请大家关注我们后续的文章，展现更多知识图谱、图数据库为大语言模型生态带来的变革。</p>
<blockquote>
<p>题图 <strong>prompt</strong>：</p>
<p><em>In an artful fusion of language and AI, this minimalist oil painting captures the essence of technological advancement. Delicate brushstrokes depict a harmony of binary code and flowing words, converging into a central point. With a refined color palette and clean composition, the artwork represents the symbiotic relationship between language and artificial intelligence, inviting contemplation and appreciation.</em></p>
</blockquote>]]></description>
</item><item>
    <title>图谱驱动的大语言模型 Llama Index</title>
    <link>https://siwei.io/graph-enabled-llama-index/</link>
    <pubDate>Thu, 01 Jun 2023 14:52:53 &#43;0800</pubDate><author>
        <name>Wey Gu</name>
    </author><guid>https://siwei.io/graph-enabled-llama-index/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/graph-enabled-llama-index/featured-image.webp" referrerpolicy="no-referrer">
            </div><blockquote>
<p>如何利用图谱构建更好的 In-context Learning 大语言模型应用。</p>
</blockquote>
<p><a href="https://www.siwei.io/en/graph-enabled-llama-index/" target="_blank" rel="noopener noreferrer">English version</a></p>
<blockquote>
<p>注：本文是我最初以英文撰写的，然后麻烦 ChatGPT 帮我翻译成了英文，翻译的 prompt 是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">In this thread, you are a Chinese Tech blogger to help translate my blog in markdown from English into Chinese, the blog style is clear, fun yet professional. I will paste chapters in markdown to you and you will send back the translated and polished version.
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="llm-应用的范式" class="headerLink">
    <a href="#llm-%e5%ba%94%e7%94%a8%e7%9a%84%e8%8c%83%e5%bc%8f" class="header-mark"></a>1 LLM 应用的范式</h2><p>作为认知智能的一大突破，LLM 已经改变了许多行业，以一种我们没有预料到的方式进行自动化、加速和启用。每天都会看到新的 LLN 应用被创建出来，我们仍然在探索如何利用这种魔力的新方法和用例。</p>
<p>将 LLM 引入流程的最典型模式之一是要求 LLM 根据专有的/特定领域的知识理解事物。目前，我们可以向 LLM 添加两种范式以获取这些知识：微调——fine-tune和<a href="https://en.wikipedia.org/wiki/In-context_learning_%28natural_language_processing%29" target="_blank" rel="noopener noreferrer">上下文学习</a>—— in-context learning。</p>
<p>微调是指对 LLM 模型进行附加训练，以增加额外的知识；而上下文学习是在查询提示中添加一些额外的知识。我们目前观察到，<a href="https://arxiv.org/abs/2305.16938" target="_blank" rel="noopener noreferrer">由于其简单性，上下文学习比微调更受欢迎</a>。</p>
<p>在本博客中，我将分享我们在上下文学习方法方面所做的工作。</p>
<h2 id="llama-index数据与-llm-之间的接口" class="headerLink">
    <a href="#llama-index%e6%95%b0%e6%8d%ae%e4%b8%8e-llm-%e4%b9%8b%e9%97%b4%e7%9a%84%e6%8e%a5%e5%8f%a3" class="header-mark"></a>2 Llama Index：数据与 LLM 之间的接口</h2><h3 id="上下文学习" class="headerLink">
    <a href="#%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0" class="header-mark"></a>2.1 上下文学习</h3><p>上下文学习的基本思想是使用现有的 LLM（未更新）来处理特定知识数据集的特殊任务。</p>
<p>例如，要构建一个可以回答关于某个人的任何问题，甚至扮演一个人的数字化化身的应用程序，我们可以将上下文学习应用于一本自传书籍和 LLM。在实践中，应用程序将使用用户的问题和从书中&quot;搜索&quot;到的一些信息构建提示，然后查询 LLM 来获取答案。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">┌───────┐         ┌─────────────────┐         ┌─────────┐
</span></span><span class="line"><span class="cl">│       │         │ Docs/Knowledge  │         │         │
</span></span><span class="line"><span class="cl">│       │         └─────────────────┘         │         │
</span></span><span class="line"><span class="cl">│ User  │─────────────────────────────────────▶   LLM   │
</span></span><span class="line"><span class="cl">│       │                                     │         │
</span></span><span class="line"><span class="cl">│       │                                     │         │
</span></span><span class="line"><span class="cl">└───────┘                                     └─────────┘
</span></span></code></pre></td></tr></table>
</div>
</div><p>在这种搜索方法中，实现从文档/知识（上述示例中的那本书）中获取与特定任务相关信息的最有效方式之一是利用嵌入（Embedding）。</p>
<h3 id="嵌入embedding" class="headerLink">
    <a href="#%e5%b5%8c%e5%85%a5embedding" class="header-mark"></a>2.2 嵌入（Embedding）</h3><p>嵌入通常指的是将现实世界的事物映射到多维空间中的向量的方法。例如，我们可以将图像映射到一个（64 x 64）维度的空间中，如果映射足够好，两个图像之间的距离可以反映它们的相似性。</p>
<p>嵌入的另一个例子是 word2vec 算法，它将每个单词都映射到一个向量中。例如，如果嵌入足够好，我们可以对它们进行加法和减法操作，可能会得到以下结果：</p>
<ul>
<li><code>vec(apple) + vec(pie) =~ vec(&quot;apple apie&quot;)</code></li>
</ul>
<p>或者向量测量值 <code>vec(apple) + vec(pie) - vec(&quot;apple apie&quot;)</code> 趋近于0：</p>
<ul>
<li><code>|vec(apple) + vec(pie) - vec(&quot;apple apie&quot;)| =~ 0</code></li>
</ul>
<p>类似地，&ldquo;pear&rdquo; 应该比 &ldquo;dinosaur&rdquo; 更接近 &ldquo;apple&rdquo;：</p>
<ul>
<li><code>|vec(apple) - vec(pear)| &lt; |vec(apple) - vec(dinosaur)|</code></li>
</ul>
<p>有了这个基础，理论上我们可以搜索与给定问题更相关的书籍片段。基本过程如下：</p>
<ul>
<li>将书籍分割为小片段，为每个片段创建嵌入并存储它们</li>
<li>当有一个问题时，计算问题的嵌入</li>
<li>通过计算距离找到与书籍片段最相似的前 K 个嵌入</li>
<li>使用问题和书籍片段构建提示</li>
<li>使用提示查询 LLM</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">                  ┌────┬────┬────┬────┐                  
</span></span><span class="line"><span class="cl">                  │ 1  │ 2  │ 3  │ 4  │                  
</span></span><span class="line"><span class="cl">                  ├────┴────┴────┴────┤                  
</span></span><span class="line"><span class="cl">                  │  Docs/Knowledge   │                  
</span></span><span class="line"><span class="cl">┌───────┐         │        ...        │       ┌─────────┐
</span></span><span class="line"><span class="cl">│       │         ├────┬────┬────┬────┤       │         │
</span></span><span class="line"><span class="cl">│       │         │ 95 │ 96 │    │    │       │         │
</span></span><span class="line"><span class="cl">│       │         └────┴────┴────┴────┘       │         │
</span></span><span class="line"><span class="cl">│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │
</span></span><span class="line"><span class="cl">│       │                                     │         │
</span></span><span class="line"><span class="cl">│       │                                     │         │
</span></span><span class="line"><span class="cl">└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘
</span></span><span class="line"><span class="cl">    │          ┌──────────────────────────┐        ▲     
</span></span><span class="line"><span class="cl">    └────────┼▶│  Tell me ....., please   │├───────┘     
</span></span><span class="line"><span class="cl">               └──────────────────────────┘              
</span></span><span class="line"><span class="cl">             │ ┌────┐ ┌────┐               │             
</span></span><span class="line"><span class="cl">               │ 3  │ │ 96 │                             
</span></span><span class="line"><span class="cl">             │ └────┘ └────┘               │             
</span></span><span class="line"><span class="cl">              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ 
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="llama-index" class="headerLink">
    <a href="#llama-index" class="header-mark"></a>2.3 Llama Index</h3><p>Llama Index 是一个开源工具包，它能帮助我们以最佳实践去做 in-context learning：</p>
<ul>
<li>它提供了各种数据加载器，以统一格式序列化文档/知识，例如 PDF、维基百科页面、Notion、Twitter 等等，这样我们可以无需自行处理预处理、将数据分割为片段等操作。</li>
<li>它还可以帮助我们创建嵌入（以及其他形式的索引），并以一行代码的方式存储嵌入（在内存中或<a href="https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb" target="_blank" rel="noopener noreferrer">向量数据库</a>中）。</li>
<li>它内置了提示和其他工程实现，因此我们无需从头开始创建和研究，例如，<a href="https://twitter.com/jerryjliu0/status/1663213212932902913" target="_blank" rel="noopener noreferrer">用4行代码在现有数据上创建一个聊天机器人</a>。</li>
</ul>
<h2 id="文档分割和嵌入的问题" class="headerLink">
    <a href="#%e6%96%87%e6%a1%a3%e5%88%86%e5%89%b2%e5%92%8c%e5%b5%8c%e5%85%a5%e7%9a%84%e9%97%ae%e9%a2%98" class="header-mark"></a>3 文档分割和嵌入的问题</h2><p>嵌入和向量搜索在许多情况下效果良好，但在某些情况下仍存在挑战，其中之一是可能丢失全局上下文/跨节点上下文。</p>
<p>想象一下，当查询&quot;请告诉我关于作者和 foo 的事情&quot;，在这本书中，假设编号为 1、3、6、19~25、30~44 和 96~99 的分段都涉及到 foo 这个主题。则在这种情况下，简单地搜索与书籍片段相关的前 k 个嵌入可能效果不尽人意，因为这时候只考虑与之最相关的几个片段（比如 k = 3），从而丢失了许多上下文信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">┌────┬────┬────┬────┐
</span></span><span class="line"><span class="cl">│ 1  │ 2  │ 3  │ 4  │
</span></span><span class="line"><span class="cl">├────┴────┴────┴────┤
</span></span><span class="line"><span class="cl">│  Docs/Knowledge   │
</span></span><span class="line"><span class="cl">│        ...        │
</span></span><span class="line"><span class="cl">├────┬────┬────┬────┤
</span></span><span class="line"><span class="cl">│ 95 │ 96 │    │    │
</span></span><span class="line"><span class="cl">└────┴────┴────┴────┘
</span></span></code></pre></td></tr></table>
</div>
</div><p>而解决、缓解这个问题的方法，在 Llama Index 工具的语境下，就是创建<a href="https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html" target="_blank" rel="noopener noreferrer">组合索引</a>和<a href="https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html" target="_blank" rel="noopener noreferrer">综合索引</a>。</p>
<p>其中，向量存储（VectorStore）只是其中的一部分。除此之外，我们可以定义一个摘要索引和/或树形索引等，以<a href="https://gpt-index.readthedocs.io/en/latest/guides/tutorials/unified_query.html" target="_blank" rel="noopener noreferrer">将不同类型的问题路由到不同的索引</a>，从而避免在需要全局上下文时丧失它。</p>
<p>然而，借助知识图谱，我们可以采取更有意思的方法：</p>
<h2 id="知识图谱" class="headerLink">
    <a href="#%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1" class="header-mark"></a>4 知识图谱</h2><p>知识图谱这个术语最初由<a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/" target="_blank" rel="noopener noreferrer">谷歌在2012年5月提出</a>，作为其增强搜索结果和向用户提供更多上下文信息的努力的一部分。知识图谱旨在理解实体之间的关系，并直接提供查询的答案，而不仅仅返回相关网页的列表。</p>
<p>知识图谱是一种以图形格式组织和连接信息的方式，其中节点表示实体，边表示实体之间的关系。图形结构允许高效地存储、检索和分析数据。</p>
<p>它的结构如下图所示：</p>
<iframe src="harry_potter_graph.html" style="height:500px;width:800px" title="Graph"></iframe>
<p>那么知识图谱到底能怎么帮到我们呢？</p>
<h2 id="嵌入和知识图谱的结合" class="headerLink">
    <a href="#%e5%b5%8c%e5%85%a5%e5%92%8c%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e7%9a%84%e7%bb%93%e5%90%88" class="header-mark"></a>5 嵌入和知识图谱的结合</h2><p>这里的基本思想是，作为信息的精炼格式，知识图谱可以以比我们对原始数据/文档进行的分割更小的粒度进行查询/搜索。因此，通过不替换大块的数据，而是将两者结合起来，我们可以更好地搜索需要全局/跨节点上下文的查询。</p>
<p>请看下面的图示，假设问题是关于 <code>x</code> 的，所有数据片段中有20个与它高度相关。现在，除了获取主要上下文的前3个文档片段（比如编号为 1、2 和 96 的文档片段），我们还从知识图谱中对 <code>x</code> 进行两次跳转查询，那么完整的上下文将包括：</p>
<ul>
<li>问题：&ldquo;Tell me things about the author and x&rdquo;</li>
<li>来自文档片段编号 1、2 和 96 的原始文档，在 Llama Index 中，它们被称为节点 1、节点 2 和节点 96。</li>
<li>包含 &ldquo;x&rdquo; 的知识图谱中的 10 个三元组，通过对 <code>x</code> 进行两层深度的图遍历得到：
<ul>
<li>x -&gt; y（来自节点 1）</li>
<li>x -&gt; a（来自节点 2）</li>
<li>x -&gt; m（来自<strong>节点 4</strong>）</li>
<li>x &lt;- b-&gt; c（来自<strong>节点 95</strong>）</li>
<li>x -&gt; d（来自节点 96）</li>
<li>n -&gt; x（来自<strong>节点 98</strong>）</li>
<li>x &lt;- z &lt;- i（来自<strong>节点 1 和节点 3</strong>）</li>
<li>x &lt;- z &lt;- b（来自<strong>节点 1 和节点 95</strong>）</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐
</span></span><span class="line"><span class="cl">│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │
</span></span><span class="line"><span class="cl">│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │
</span></span><span class="line"><span class="cl">│ `▲&#39;       `─&#39;    │  `─&#39;       `─&#39;   │            `─&#39;   │  `─&#39;       `─&#39;   │
</span></span><span class="line"><span class="cl">│  │     1         │        2         │        3    │    │        4         │
</span></span><span class="line"><span class="cl">│ .─.              │                  │            .▼.   │                  │
</span></span><span class="line"><span class="cl">│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │
</span></span><span class="line"><span class="cl">│ `◀────┐          │                  │            `─&#39;  ││                  │
</span></span><span class="line"><span class="cl">├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤
</span></span><span class="line"><span class="cl">│       │                      Docs/Knowledge           │                   │
</span></span><span class="line"><span class="cl">│       │                            ...                │                   │
</span></span><span class="line"><span class="cl">│       │                                               │                   │
</span></span><span class="line"><span class="cl">├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤
</span></span><span class="line"><span class="cl">│  .─.  └──────.   │  .─.             │                 ││  .─.             │
</span></span><span class="line"><span class="cl">│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │
</span></span><span class="line"><span class="cl">│  `─&#39;       `─&#39;   │  `─&#39;             │                  │  `─&#39;             │
</span></span><span class="line"><span class="cl">│        95   │    │   │    96        │                  │   │    98        │
</span></span><span class="line"><span class="cl">│            .▼.   │  .▼.             │                  │   ▼              │
</span></span><span class="line"><span class="cl">│           ( c )  │ ( d )            │                  │  .─.             │
</span></span><span class="line"><span class="cl">│            `─&#39;   │  `─&#39;             │                  │ ( x )            │
</span></span><span class="line"><span class="cl">└──────────────────┴──────────────────┴──────────────────┴──`─&#39;─────────────┘
</span></span></code></pre></td></tr></table>
</div>
</div><p>显然，那些（可能很宝贵的）涉及到主题 <code>x</code> 的精炼信息来自于其他节点以及跨节点的信息，都因为我们引入知识图谱的步骤，而能够被包含在 prompt 中，用于进行上下文学习，从而克服了前边提到的问题。</p>
<h2 id="llama-index-中的知识图谱进展" class="headerLink">
    <a href="#llama-index-%e4%b8%ad%e7%9a%84%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e8%bf%9b%e5%b1%95" class="header-mark"></a>6 Llama Index 中的知识图谱进展</h2><p>最初，<a href="https://github.com/jerryjliu/llama_index/pull/433" target="_blank" rel="noopener noreferrer">William F.H.</a>将知识图谱的抽象概念引入了 Llama Index，其中知识图谱中的三元组与关键词相关联，并存储在内存中的文档中，随后<a href="https://github.com/jerryjliu/llama_index/pull/487" target="_blank" rel="noopener noreferrer">Logan Markewich</a>还增加了每个三元组的嵌入。</p>
<p>最近的几周中，我一直在与社区合作，致力于<a href="https://github.com/jerryjliu/llama_index/pull/2581" target="_blank" rel="noopener noreferrer">将 &ldquo;GraphStore&rdquo; 存储上下文引入 Llama Index</a>，从而引入了知识图谱的外部存储。首个实现是使用我自从 2021 年以来一直在开发的开源分布式图数据库 NebulaGraph。</p>
<p>在实现过程中，还引入了遍历图的多个跳数选项以及在前 k 个节点中收集更多关键实体的选项（用于在知识图谱中搜索以获得更多全局上下文），我们仍在对这些变更进行完善。</p>
<p>引入 GraphStore 后，还可以从现有的知识图谱中进行上下文学习，并与其他索引结合使用，这也非常有前景，因为知识图谱被认为具有比其他结构化数据更高的信息密度。</p>
<p>在接下来的几周里，我将在本博客中更新有关 Llama Index 中的知识图谱相关工作的内容，然后在 <a href="https://github.com/jerryjliu/llama_index/pull/2581" target="_blank" rel="noopener noreferrer">PR</a> 合并后，分享端到端的演示项目和教程。请继续关注！</p>]]></description>
</item></channel>
</rss>
