# åŸºäº NebulaGraph å›¾æ•°æ®åº“çš„æ¬ºè¯ˆæ£€æµ‹æ–¹æ³•ä¸ä»£ç ç¤ºä¾‹




> æœ¬æ–‡æ˜¯ä¸€ä¸ªåŸºäº NebulaGraph ä¸Šå›¾ç®—æ³•ã€å›¾æ•°æ®åº“ã€æœºå™¨å­¦ä¹ ã€GNN çš„ Fraud Detection æ–¹æ³•ç»¼è¿°ï¼Œé™¤äº†åŸºæœ¬æ–¹æ³•æ€æƒ³çš„ä»‹ç»ä¹‹å¤–ï¼Œæˆ‘è¿˜ç»™å¤§å®¶å¼„äº†å¯ä»¥è·‘çš„ Playgroundã€‚

> å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™è¿˜æˆ‘ç¬¬ä¸€æ¬¡ç»™å¤§å®¶ä»‹ç» Nebula-DGL è¿™ä¸ªé¡¹ç›® ğŸ˜ã€‚

<!--more-->

<!--

[TOC]

-->

## åŸºäºå›¾æ•°æ®åº“çš„æ¬ºè¯ˆæ£€æµ‹æ–¹æ³•

### å»ºç«‹å›¾è°±

é¦–å…ˆï¼Œå¯¹ç°æœ‰çš„å†å²æ•°æ®ã€æ ‡æ³¨ä¿¡æ¯é¢å‘å…³è”å…³ç³»è¿›è¡Œå±æ€§å›¾å»ºæ¨¡ã€‚è¿™ç§åŸå§‹æ•°æ®æ˜¯å¤šä¸ªè¡¨ç»“æ„ä¸­çš„é“¶è¡Œã€ç”µå­å•†åŠ¡æˆ–è€…ä¿é™©è¡Œä¸šé‡Œçš„äº¤æ˜“äº‹ä»¶è®°å½•ã€ç”¨æˆ·æ•°æ®å’Œé£æ§æ ‡æ³¨ï¼Œè€Œå»ºæ¨¡è¿‡ç¨‹å°±æ˜¯æŠ½è±¡å‡ºæˆ‘ä»¬å…³å¿ƒçš„å®ä½“ã€å®ä½“é—´çš„å…³è”å…³ç³»ã€å’Œå…¶ä¸­æœ‰æ„ä¹‰çš„å±æ€§ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œè‡ªç„¶äººã€å…¬å¸å®ä½“ã€ç”µè¯å·ç ã€åœ°å€ã€è®¾å¤‡ï¼ˆæ¯”å¦‚ç»ˆç«¯è®¾å¤‡ã€ç½‘ç»œåœ°å€ã€ç»ˆç«¯è®¾å¤‡æ‰€è¿æ¥çš„ WiFi SSID ç­‰ï¼‰ã€è®¢å•éƒ½æ˜¯å®ä½“æœ¬èº«ï¼Œå…¶ä»–ä¿¡æ¯æ¯”å¦‚é£é™©æ ‡æ³¨ï¼ˆæ˜¯å¦é«˜é£é™©ã€é£é™©æè¿°ç­‰ï¼‰ã€è‡ªç„¶äººå’Œå…¬å¸å®ä½“çš„ä¿¡æ¯ï¼ˆèŒä¸šã€æ”¶å…¥ã€å­¦å†ç­‰ï¼‰éƒ½ä½œä¸ºå®ä½“çš„å±æ€§æ¥å»ºæ¨¡ã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªå¯ä»¥å‚è€ƒçš„è´·æ¬¾åæ¬ºè¯ˆçš„ç¤ºä¾‹å»ºæ¨¡ï¼Œå®ƒæ¥è‡ªä¸€ä»½ä½œè€…å¼€æºçš„å›¾ç»“æ„æ•°æ®ç”Ÿæˆé¡¹ç›®ã€‚

> æ³¨ï¼Œä½ å¯ä»¥è®¿é—® https://github.com/wey-gu/fraud-detection-datagen è·å–è¿™ä¸ªå¼€æºçš„æ•°æ®ç”Ÿæˆå™¨ä»£ç å’Œä¸€ä»½ç¤ºä¾‹çš„æ•°æ®ã€‚



![](https://github.com/wey-gu/fraud-detection-datagen/raw/main/images/fraud_detection_graph_model.svg)

### å›¾æ•°æ®åº“æŸ¥è¯¢è¯†åˆ«é£é™©

æœ‰äº†ä¸€å¼ å›Šæ‹¬äº†äººã€å…¬å¸ã€å†å²è´·æ¬¾ç”³è¯·è®°å½•ã€ç”µè¯ã€çº¿ä¸Šç”³è¯·ç½‘ç»œè®¾å¤‡çš„å›¾è°±ï¼Œæˆ‘ä»¬å¯ä»¥æŒ–æ˜ä¸€äº›æœ‰æ„æ€çš„ä¿¡æ¯ã€‚

äº‹å®ä¸Šï¼Œå¾ˆå¤šå€¼å¾—è¢«å‘ç°ã€å¹¶æœ‰æ•ˆé˜»æ­¢ä»è€Œæ­¢æŸçš„éª—ä¿è¡Œä¸ºæ˜¯å…·æœ‰ç¾¤ä½“èšé›†æ€§çš„ã€‚æ¯”å¦‚æ¬ºè¯ˆå›¢ä¼™å¯èƒ½æ˜¯ä¸€å°æ‰¹äººï¼ˆæ¯”å¦‚3åˆ°5äººï¼‰æœ‰ç»„ç»‡åœ°æ”¶é›†æ›´å¤§è§„æ¨¡çš„èº«ä»½è¯ä¿¡æ¯ï¼ˆæ¯”å¦‚30å¼ ï¼‰ï¼ŒåŒæ—¶å‘èµ·å¤šä¸ªé‡‘èæœºæ„å¤§é‡è´·æ¬¾ï¼Œç„¶ååœ¨æ”¾æ¬¾åé€‰æ‹©ä¸¢å¼ƒè¿™æ‰¹ç•™ä¸‹äº†è¿çº¦è®°å½•çš„èº«ä»½è¯ï¼Œå†è¿›ä¸€æ­¥é€‰æ‹©ä¸‹ä¸€æ‰¹èº«ä»½è¯ä¿¡æ¯å¦‚æ³•ç‚®åˆ¶ã€‚

è¿™ç§å›¢ä¼™ä½œæ¡ˆçš„æ–¹å¼å› ä¸ºåˆ©ç”¨äº†å¤§é‡æ–°çš„èº«ä»½ä¿¡æ¯ï¼Œå®Œå…¨åˆ©ç”¨å†å²è®°å½•å»é»‘åå•è§„é¿é£é™©çš„æ–¹å¼æ˜¯æ— æ•ˆçš„ã€‚ä¸è¿‡ï¼Œå€ŸåŠ©äºå…³è”å…³ç³»çš„è§†è§’ï¼Œè¿™äº›æ¨¡å¼æ˜¯ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥è¢«åŠæ—¶è¯†åˆ«å‡ºæ¥çš„ã€‚

è¿™äº›å¯ä»¥è¢«è¯†åˆ«å‡ºçš„è§„å¾‹æˆ‘æŠŠå®ƒåˆ†æˆä¸¤ç§ï¼š

ä¸€ç§æ˜¯é£æ§ä¸“å®¶å¯ä»¥ç›´æ¥ç”¨æŸç§æ¨¡å¼æ¥æè¿°çš„ï¼Œä¾‹å¦‚ï¼šå’Œå·²ç»è¢«æ ‡æ³¨ä¸ºé«˜é£é™©çš„å®ä½“æœ‰ç›´æ¥æˆ–è€…é—´æ¥çš„å…³è”å…³ç³»ï¼ˆæ–°è®¢å•ç”³è¯·äººä½¿ç”¨äº†å’Œè¿‡å¾€é«˜é£é™©è®°å½•ç›¸åŒçš„ç½‘ç»œè®¾å¤‡ï¼‰ï¼Œè¿™ç§æ¨¡å¼å¯¹åº”åˆ°å›¾è°±ä¸­ï¼Œé€šè¿‡ä¸€ä¸ªå›¾æŸ¥è¯¢å°±å¯ä»¥å®æ—¶ç»™å‡ºç»“æœã€‚

å¦ä¸€ç§æ˜¯éšå«åœ¨æ•°æ®çš„å…³è”å…³ç³»èƒŒåï¼Œéœ€è¦é€šè¿‡å›¾ç®—æ³•æŒ–æ˜å¾—å‡ºçš„ä¸€äº›é£é™©æç¤ºï¼Œä¾‹å¦‚ï¼šå°½ç®¡ç»™å®šçš„å®ä½“ä¸æœ‰é™çš„æ ‡æ³¨é«˜é£é™©å®ä½“æ²¡æœ‰åŒ¹é…çš„å…³è”ï¼Œä½†æ˜¯å®ƒåœ¨å›¾ä¸­å½¢æˆäº†èšé›†æ€§å¯èƒ½æç¤ºæˆ‘ä»¬è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå°šæœªå¾—æ‰‹çš„è¿›è¡Œä¸­çš„å›¢ä¼™è´·æ¬¾è¯ˆéª—çš„å…¶ä¸­ä¸€æ¬¡ç”³è¯·ï¼Œè¿™ç§æƒ…å†µå¯ä»¥é€šè¿‡å®šæœŸåœ¨å†å²æ•°æ®ä¸­æ‰¹é‡æ‰§è¡Œç¤¾åŒºå‘ç°ç®—æ³•å¾—å‡ºï¼Œå¹¶åœ¨é«˜èšé›†ç¤¾åŒºä¸­åˆ©ç”¨ä¸­å¿ƒæ€§ç®—æ³•ç»™å‡ºæ ¸å¿ƒå®ä½“ï¼Œä¸€å¹¶æç¤ºç»™é£é™©ä¸“å®¶è¿›è¡Œåç»­è¯„ä¼°å’Œé£é™©æ ‡æ³¨ã€‚

#### åŸºäºå›¾è°±ä¸ä¸“å®¶å›¾æ¨¡å¼åŒ¹é…çš„æ¬ºè¯ˆæ£€æµ‹ç¤ºä¾‹

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬åˆ©ç”¨ Nebula-UP æ¥ä¸€é”®éƒ¨ç½²ä¸€å¥— NebulaGraph å›¾æ•°æ®åº“ï¼š

> æ›´å¤šè¯·å‚è€ƒ https://github.com/wey-gu/nebula-up/

```bash
curl -fsSL nebula-up.siwei.io/install.sh | bash
```

é¦–å…ˆï¼Œæˆ‘ä»¬æŠŠå‰è¾¹å»ºæ¨¡çš„å›¾è°±åŠ è½½åˆ° NebulaGraph é‡Œï¼š

```bash
# å…‹éš†æ•°æ®é›†ä»£ç ä»“åº“
git clone https://github.com/wey-gu/fraud-detection-datagen.git
cp -r data_sample_numerical_vertex_id data
# å»æ‰è¡¨å¤´
sed -i '1d' data/*.csv
docker run --rm -ti \
    --network=nebula-net \
    -v ${PWD}:/root/ \
    -v ${PWD}/data/:/data \
    vesoft/nebula-importer:v3.1.0 \
    --config /root/nebula_graph_importer.yaml
```

æœ‰äº†è¿™æ ·ä¸€ä¸ªå›¾è°±ï¼Œé£æ§ä¸“å®¶å¯ä»¥åœ¨å¯è§†åŒ–æ¢ç´¢å·¥å…·ä¸­æŒ‰éœ€æ¢ç´¢å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œç»˜åˆ¶ç›¸åº”çš„é£é™©æ¨¡å¼ï¼š

![](viz_graph_query.webp)

åœ¨è¿™ä¸ªæ¢ç´¢æˆªå›¾é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ˜æ˜¾çœ‹åˆ°ä¸€ä¸ªç¾¤æ§è®¾å¤‡çš„é£é™©æ¨¡å¼ï¼Œè¿™ä¸ªæ¨¡å¼å¯ä»¥è¢«äº¤ç»™å›¾æ•°æ®åº“å¼€å‘è€…ï¼ŒæŠ½è±¡æˆå¯ä»¥è¢«é£æ§åº”ç”¨å®šæœŸã€å®æ—¶æŸ¥è¯¢çš„è¯­å¥ï¼š

```cypher
## é’ˆå¯¹ä¸€ç¬”äº¤æ˜“ç”³è¯·å…³è”æŸ¥è¯¢
MATCH (n) WHERE id(n) == "200000010265"
OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]->(d)<-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]->(pn:phone_num)<-[e:`with_phone_num`]-(:`applicant`)
RETURN p_shared_d
```

æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ¨æ­¤æ¨¡å‹ä¹‹ä¸Šï¼Œé€šè¿‡ä¿®æ”¹è¿”å›çš„å…³è”è®¾å¤‡è®¡æ•°ï¼Œä½œä¸ºæ„å‘æŒ‡æ ‡æŸ¥è¯¢çš„åˆ¤æ–­ APIï¼š

```cypher
## ç¾¤æ§æŒ‡æ ‡
MATCH (n) WHERE id(n) == "200000010265"
OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]->(d)<-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]->(pn:phone_num)<-[e:`with_phone_num`]-(:`applicant`)
RETURN count(e)
```

å¦‚æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªç›¸å¯¹æœ‰æ•ˆçš„é£æ§ç³»ç»Ÿï¼Œåˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®å’Œä¸“å®¶èµ„æºï¼Œå»æ›´é«˜æ•ˆæ§åˆ¶å›¢ä¼™æ¬ºè¯ˆä½œæ¡ˆé£é™©ã€‚

å¦ä¸€ä¸ªåˆ©ç”¨æ ‡æ³¨é£é™©èŠ‚ç‚¹çš„æŸ¥è¯¢æ˜¯æ‰¾åˆ°ç›¸å…³è”èŠ‚ç‚¹é«˜é£é™©å±æ€§çš„æ•°é‡ï¼š

```cypher
MATCH p_=(p:`applicant`)-[*1..2]-(p2:`applicant`) WHERE id(p)=="200000014810" AND p2.`applicant`.is_risky == "True" RETURN p_ LIMIT 100
```

å¯ä»¥ä»è¿™ä¸ªè·¯å¾„æŸ¥è¯¢çœ‹åˆ° `200000014810` çš„ç›¸è¿æ¥çš„ç”³è¯·äººä¸­æœ‰ä¸å°‘æ˜¯é«˜é£é™©çš„ï¼ˆä¹Ÿèƒ½çœ‹å‡ºèšé›†çš„ deviceï¼‰ã€‚

![](is_risky_label.webp)

å¦‚æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ç›¸è¿é«˜é£é™©ç‚¹æ•°é‡ä¸ºä¸€ä¸ªæŒ‡æ ‡ï¼š

```cypher
MATCH (p:`applicant`)-[*1..2]-(p2:`applicant`) WHERE id(p)=="200000014810" AND p2.`applicant`.is_risky == "True" RETURN count(p2)
```

ç„¶è€Œï¼Œåœ¨ç°å®æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„å¤§å¤šæ•°æ ‡æ³¨æ•°æ®çš„è·å–è¿˜æ˜¯è¿‡äºæ˜‚è´µï¼Œé‚£ä¹ˆæœ‰æ²¡æœ‰ä»€ä¹ˆæ–¹æ³•æ˜¯æ›´æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„é£é™©æ ‡æ³¨å’Œå›¾ç»“æ„ï¼Œæ¥é¢„æµ‹å‡ºé£é™©å‘¢ï¼Ÿ

### åˆ©ç”¨å›¾æ‰©å……æ ‡æ³¨

ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œ Xiaojin Z. å’Œ Zoubin G. åœ¨è®ºæ–‡ï¼š[Learning from Labeled and Unlabeled Data with Label Propagation](http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf) ï¼ˆCMU-CALD-02-107ï¼‰ä¸­ï¼Œåˆ©ç”¨æ ‡ç­¾ä¼ æ’­ï¼ˆLabel Propagationï¼‰ç®—æ³•æ¥æŠŠæœ‰é™çš„æ ‡æ³¨ä¿¡æ¯åœ¨å›¾ä¸Šé€šè¿‡å…³è”å…³ç³»ä¼ æ’­åˆ°æ›´å¤šå®ä½“ä¸­ã€‚

è¿™æ ·ï¼Œåœ¨æˆ‘ä»¬å»ºç«‹çš„å›¾è°±ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å€ŸåŠ©æœ‰é™çš„é«˜é£é™©æ ‡æ³¨ï¼Œå»â€œä¼ æ’­â€äº§ç”Ÿæ›´å¤šçš„æ ‡æ³¨ä¿¡æ¯ã€‚è¿™äº›æ‰©å±•å‡ºæ¥çš„æ ‡æ³¨ä¿¡æ¯ä¸€æ–¹é¢å¯ä»¥åœ¨å®æ—¶çš„å›¾æŸ¥è¯¢ä¸­ç»™å‡ºæ›´å¤šçš„ç»“æœï¼Œå¦ä¸€æ–¹é¢ï¼Œå®ƒè¿˜èƒ½ä½œä¸ºé£æ§ä¸“å®¶é‡è¦çš„è¾“å…¥ä¿¡æ¯ï¼Œå¸®åŠ©æ¨è¿›åæ¬ºè¯ˆè°ƒæŸ¥è¡ŒåŠ¨çš„å¼€å±•ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šæœŸç¦»çº¿åœ°å…¨å›¾æ‰«ææ•°æ®ï¼Œé€šè¿‡å›¾ç®—æ³•æ‰©å……ã€æ›´æ–°æ ‡æ³¨ï¼Œå†å°†æœ‰æ•ˆçš„æ›´æ–°æ ‡æ³¨å†™å›åˆ°å›¾è°±ä¹‹ä¸­ã€‚

> æ³¨ï¼Œç±»ä¼¼çš„æ–¹æ³•è¿˜æœ‰ SIGNDiffusionï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å»äº†è§£ä¸€ä¸‹ã€‚

#### å›¾ç®—æ³•æ‰©å……æ¬ºè¯ˆé£é™©æ ‡æ³¨çš„ç¤ºä¾‹

ä¸‹é¢ï¼Œæˆ‘ç»™å‡ºä¸€ä¸ªå¯ä»¥è·‘é€šçš„æ¡ˆä¾‹ï¼š

è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ç”¨åˆ°äº† Yelp è¿™ä¸ªæ¬ºè¯ˆè¯†åˆ«çš„ç»å…¸æ•°æ®ï¼Œè¿™ä»½æ•°æ®ä¸åªä¼šç”¨åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåè¾¹ GNN æ–¹æ³•ä¸­çš„æ¡ˆä¾‹æˆ‘ä¹Ÿä¼šç”¨åˆ°å®ƒï¼Œæ‰€ä»¥å¤§å®¶å¯ä»¥è€å¿ƒæŠŠæ•°æ®å¯¼å…¥ NebulaGraphã€‚

å¯¼å…¥æ•°æ®åˆ°å›¾åº“

> ç”Ÿæˆå¯¼å…¥çš„æ–¹æ³•åœ¨è¿™é‡Œï¼Œhttps://github.com/wey-gu/nebulagraph-yelp-frauddetection

```bash
cd ~
git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection
cd nebulagraph-yelp-frauddetection
python3 -m pip install -r requirements.txt
python3 data_download.py

# å¯¼å…¥å›¾åº“
docker run --rm -ti \
    --network=nebula-net \
    -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \
    -v ${PWD}/data:/root \
    vesoft/nebula-importer:v3.1.0 \
    --config /root/importer.yaml
```

ç»“æŸä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹å›¾ä¸Šçš„ç»Ÿè®¡ï¼š

```bash
~/.nebula-up/console.sh -e "USE yelp; SHOW STATS"
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š

```bash
(root@nebula) [(none)]> USE yelp; SHOW STATS
+---------+---------------------------------------+---------+
| Type    | Name                                  | Count   |
+---------+---------------------------------------+---------+
| "Tag"   | "review"                              | 45954   |
| "Edge"  | "shares_restaurant_in_one_month_with" | 1147232 |
| "Edge"  | "shares_restaurant_rating_with"       | 6805486 |
| "Edge"  | "shares_user_with"                    | 98630   |
| "Space" | "vertices"                            | 45954   |
| "Space" | "edges"                               | 8051348 |
+---------+---------------------------------------+---------+
Got 6 rows (time spent 1911/4488 us)
```

ç›®å‰ï¼Œå¸‚é¢ä¸Šçš„ LPA æ ‡ç­¾ä¼ æ’­ç®—æ³•éƒ½æ˜¯ç”¨æ¥åšç¤¾åŒºæ£€æµ‹çš„ï¼Œå¾ˆå°‘æœ‰å®ç°æ˜¯ç”¨æ¥åšæ ‡ç­¾æ‹“å±•çš„ï¼ˆåªæœ‰ SK-Learn ä¸­æœ‰è¿™ä¸ªå®ç°ï¼‰ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬å‚è€ƒ [Thibaud M](https://datascience.stackexchange.com/users/77683/thibaud-m) ç»™å‡ºæ¥çš„å®ç°ã€‚

> åŸå§‹çš„è®¨è®ºå‚è€ƒï¼šhttps://datascience.stackexchange.com/a/55720/138720

ä¸ºäº†è®©è¿™ä¸ªç®—æ³•è·‘çš„å¿«ä¸€ç‚¹ï¼Œä¼šä» NebulaGraph é‡Œå–ä¸€ä¸ªç‚¹çš„å­å›¾ï¼Œåœ¨è¿™ä¸ªå°çš„å­å›¾ä¸Šåšæ ‡æ³¨çš„æ‰©å……ï¼š

é¦–å…ˆï¼Œæˆ‘ä»¬å¯åŠ¨ä¸€ä¸ª Jupyter çš„ Playgroundï¼Œ

> å‚è€ƒ https://github.com/wey-gu/nebula-dgl ä¸­çš„ Playground è¿‡ç¨‹ï¼š

```bash
git clone https://github.com/wey-gu/nebula-dgl.git
cd nebula-dgl
# è¿è¡Œ Jupyter Notebook
docker run -it --name dgl -p 8888:8888 --network nebula-net \
    -v "$PWD":/home/jovyan/work jupyter/datascience-notebook \
    start-notebook.sh --NotebookApp.token='nebulagraph'
```

è®¿é—®ï¼šhttp://localhost:8888/lab/tree/work?token=nebulagraph

å®‰è£…ä¾èµ–ï¼ˆè¿™äº›ä¾èµ–åœ¨åè¾¹çš„ GNN ä¾‹å­ä¸­ä¹Ÿä¼šè¢«ç”¨åˆ°ï¼‰

```python
!python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8
!python3 -m pip install .
```

ç„¶åï¼Œæˆ‘ä»¬ä»å›¾ä¸­è¯»å–ä¸€ä¸ªå­å›¾ï¼Œä» `2048` è¿™ä¸ªç‚¹å¼€å§‹æ¢ç´¢ä¸¤æ­¥å†…çš„æ‰€æœ‰è¾¹ã€‚

```python
import torch
import json
from torch import tensor
from dgl import DGLHeteroGraph, heterograph

from nebula3.gclient.net import ConnectionPool
from nebula3.Config import Config

config = Config()
config.max_connection_pool_size = 2
connection_pool = ConnectionPool()
connection_pool.init([('graphd', 9669)], config)

vertex_id = 2048
client = connection_pool.get_session('root', 'nebula')
r = client.execute_json(
    "USE yelp;"
    f"GET SUBGRAPH WITH PROP 2 STEPS FROM {vertex_id} YIELD VERTICES AS nodes, EDGES AS relationships;")

r = json.loads(r)
data = r.get('results', [{}])[0].get('data')
columns = r.get('results', [{}])[0].get('columns')

# create node and nodedata
node_id_map = {} # key: vertex id in NebulaGraph, value: node id in dgl_graph
node_idx = 0
features = [[] for _ in range(32)] + [[]]
for i in range(len(data)):
    for index, node in enumerate(data[i]['meta'][0]):
        nodeid = data[i]['meta'][0][index]['id']
        if nodeid not in node_id_map:
            node_id_map[nodeid] = node_idx
            node_idx += 1
            for f in range(32):
                features[f].append(data[i]['row'][0][index][f"review.f{f}"])
            features[32].append(data[i]['row'][0][index]['review.is_fraud'])

rur_start, rur_end, rsr_start, rsr_end, rtr_start, rtr_end = [], [], [], [], [], []
for i in range(len(data)):
    for edge in data[i]['meta'][1]:
        edge = edge['id']
        if edge['name'] == 'shares_user_with':
            rur_start.append(node_id_map[edge['src']])
            rur_end.append(node_id_map[edge['dst']])
        elif edge['name'] == 'shares_restaurant_rating_with':
            rsr_start.append(node_id_map[edge['src']])
            rsr_end.append(node_id_map[edge['dst']])
        elif edge['name'] == 'shares_restaurant_in_one_month_with':
            rtr_start.append(node_id_map[edge['src']])
            rtr_end.append(node_id_map[edge['dst']])

data_dict = {}
if rur_start:
    data_dict[('review', 'shares_user_with', 'review')] = tensor(rur_start), tensor(rur_end)
if rsr_start:
    data_dict[('review', 'shares_restaurant_rating_with', 'review')] = tensor(rsr_start), tensor(rsr_end)
if rtr_start:
    data_dict[('review', 'shares_restaurant_in_one_month_with', 'review')] = tensor(rtr_start), tensor(rtr_end)
# construct a dgl_graph, ref: https://docs.dgl.ai/en/0.9.x/generated/dgl.heterograph.html

dgl_graph: DGLHeteroGraph = heterograph(data_dict)

# load node features to dgl_graph
dgl_graph.ndata['label'] = tensor(features[32])

# heterogeneous graph to heterogeneous graph, keep ndata and edata
import dgl
hg = dgl.to_homogeneous(dgl_graph, ndata=['label'])
```

ç„¶åï¼Œæˆ‘ä»¬ç”¨ä¸Šè¾¹æåˆ°çš„ Torch Label Spreading å®ç°ï¼Œåº”ç”¨åˆ°æˆ‘ä»¬çš„å›¾ä¸Šã€‚

```python
from abc import abstractmethod
import torch

class BaseLabelPropagation:
    """Base class for label propagation models.
    
    Parameters
    ----------
    adj_matrix: torch.FloatTensor
        Adjacency matrix of the graph.
    """
    def __init__(self, adj_matrix):
        self.norm_adj_matrix = self._normalize(adj_matrix)
        self.n_nodes = adj_matrix.size(0)
        self.one_hot_labels = None 
        self.n_classes = None
        self.labeled_mask = None
        self.predictions = None

    @staticmethod
    @abstractmethod
    def _normalize(adj_matrix):
        raise NotImplementedError("_normalize must be implemented")

    @abstractmethod
    def _propagate(self):
        raise NotImplementedError("_propagate must be implemented")

    def _one_hot_encode(self, labels):
        # Get the number of classes
        classes = torch.unique(labels)
        classes = classes[classes != -1]
        self.n_classes = classes.size(0)

        # One-hot encode labeled data instances and zero rows corresponding to unlabeled instances
        unlabeled_mask = (labels == -1)
        labels = labels.clone()  # defensive copying
        labels[unlabeled_mask] = 0
        self.one_hot_labels = torch.zeros((self.n_nodes, self.n_classes), dtype=torch.float)
        self.one_hot_labels = self.one_hot_labels.scatter(1, labels.unsqueeze(1), 1)
        self.one_hot_labels[unlabeled_mask, 0] = 0

        self.labeled_mask = ~unlabeled_mask

    def fit(self, labels, max_iter, tol):
        """Fits a semi-supervised learning label propagation model.
        
        labels: torch.LongTensor
            Tensor of size n_nodes indicating the class number of each node.
            Unlabeled nodes are denoted with -1.
        max_iter: int
            Maximum number of iterations allowed.
        tol: float
            Convergence tolerance: threshold to consider the system at steady state.
        """
        self._one_hot_encode(labels)

        self.predictions = self.one_hot_labels.clone()
        prev_predictions = torch.zeros((self.n_nodes, self.n_classes), dtype=torch.float)

        for i in range(max_iter):
            # Stop iterations if the system is considered at a steady state
            variation = torch.abs(self.predictions - prev_predictions).sum().item()
            
            if variation < tol:
                print(f"The method stopped after {i} iterations, variation={variation:.4f}.")
                break

            prev_predictions = self.predictions
            self._propagate()

    def predict(self):
        return self.predictions

    def predict_classes(self):
        return self.predictions.max(dim=1).indices

class LabelPropagation(BaseLabelPropagation):
    def __init__(self, adj_matrix):
        super().__init__(adj_matrix)

    @staticmethod
    def _normalize(adj_matrix):
        """Computes D^-1 * W"""
        degs = adj_matrix.sum(dim=1)
        degs[degs == 0] = 1  # avoid division by 0 error
        return adj_matrix / degs[:, None]

    def _propagate(self):
        self.predictions = torch.matmul(self.norm_adj_matrix, self.predictions)

        # Put back already known labels
        self.predictions[self.labeled_mask] = self.one_hot_labels[self.labeled_mask]

    def fit(self, labels, max_iter=1000, tol=1e-3):
        super().fit(labels, max_iter, tol)

class LabelSpreading(BaseLabelPropagation):
    def __init__(self, adj_matrix):
        super().__init__(adj_matrix)
        self.alpha = None

    @staticmethod
    def _normalize(adj_matrix):
        """Computes D^-1/2 * W * D^-1/2"""
        degs = adj_matrix.sum(dim=1)
        norm = torch.pow(degs, -0.5)
        norm[torch.isinf(norm)] = 1
        return adj_matrix * norm[:, None] * norm[None, :]

    def _propagate(self):
        self.predictions = (
            self.alpha * torch.matmul(self.norm_adj_matrix, self.predictions)
            + (1 - self.alpha) * self.one_hot_labels
        )
    
    def fit(self, labels, max_iter=1000, tol=1e-3, alpha=0.5):
        """
        Parameters
        ----------
        alpha: float
            Clamping factor.
        """
        self.alpha = alpha
        super().fit(labels, max_iter, tol)
        
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

nx_hg = hg.to_networkx()
adj_matrix = nx.adjacency_matrix(nx_hg).toarray()
labels = hg.ndata['label']
# Create input tensors
adj_matrix_t = torch.FloatTensor(adj_matrix)
labels_t = torch.LongTensor(labels)

# Learn with Label Propagation
label_propagation = LabelPropagation(adj_matrix_t)
print("Label Propagation: ", end="")
label_propagation.fit(labels_t)
label_propagation_output_labels = label_propagation.predict_classes()

# Learn with Label Spreading
label_spreading = LabelSpreading(adj_matrix_t)
print("Label Spreading: ", end="")
label_spreading.fit(labels_t, alpha=0.8)
label_spreading_output_labels = label_spreading.predict_classes()
```

ç°åœ¨å’±ä»¬çœ‹çœ‹æŸ“è‰²çš„ä¼ æ’­æ•ˆæœï¼š

```python
color_map = {0: "blue", 1: "green"}
input_labels_colors = [color_map[int(l)] for l in labels]
lprop_labels_colors = [color_map[int(l)] for l in label_propagation_output_labels.numpy()]
lspread_labels_colors = [color_map[int(l)] for l in label_spreading_output_labels.numpy()]

plt.figure(figsize=(14, 6))
ax1 = plt.subplot(1, 4, 1)
ax2 = plt.subplot(1, 4, 2)
ax3 = plt.subplot(1, 4, 3)

ax1.title.set_text("Raw data (2 classes)")
ax2.title.set_text("Label Propagation")
ax3.title.set_text("Label Spreading")

pos = nx.spring_layout(nx_hg)
nx.draw(nx_hg, ax=ax1, pos=pos, node_color=input_labels_colors, node_size=50)
nx.draw(nx_hg, ax=ax2, pos=pos, node_color=lprop_labels_colors, node_size=50)
nx.draw(nx_hg, ax=ax3, pos=pos, node_color=lspread_labels_colors, node_size=50)

# Legend
ax4 = plt.subplot(1, 4, 4)
ax4.axis("off")
legend_colors = ["orange", "blue", "green", "red", "cyan"]
legend_labels = ["unlabeled", "class 0", "class 1", "class 2", "class 3"]
dummy_legend = [ax4.plot([], [], ls='-', c=c)[0] for c in legend_colors]
plt.legend(dummy_legend, legend_labels)

plt.show()
```

å¯ä»¥çœ‹åˆ°æœ€åç”»å‡ºæ¥çš„ç»“æœï¼š

![](lpa_spread_notation_matplot.webp)

å¯ä»¥çœ‹åˆ°æœ‰ä¸€äº›è“è‰²æ ‡ç­¾è¢« Spread å¼€äº†ï¼Œå®é™…ä¸Šæˆ‘çš„è¿™ä¸ªä¾‹å­çš„æ•ˆæœå¹¶ä¸ç†æƒ³ï¼ˆå› ä¸ºè¿™ä¸ªä¾‹å­æï¼Œç»¿è‰²çš„æ‰æ˜¯é‡è¦çš„æ ‡ç­¾ï¼‰ï¼Œä¸è¿‡æˆ‘ç»™çš„å­å›¾å®åœ¨æ˜¯å¤ªå°äº†ï¼Œä¹Ÿæœ¬ä¸åº”è¯¥å¥¢æ±‚æœ‰å¥½çš„ç»“æœï¼Œåªæ˜¯ä¸ºäº†ä¸ªå¤§å®¶æ¼”ç¤ºä¸€ä¸‹è¿™ä¸ªæ–¹æ³•ã€‚



### å¸¦æœ‰å›¾ç‰¹å¾çš„æœºå™¨å­¦ä¹ 

åœ¨é£æ§é¢†åŸŸå¼€å§‹åˆ©ç”¨å›¾çš„æ€æƒ³å’Œèƒ½åŠ›ä¹‹å‰ï¼Œå·²ç»æœ‰å¾ˆå¤šåˆ©ç”¨æœºå™¨å­¦ä¹ çš„åˆ†ç±»ç®—æ³•åŸºäºå†å²æ•°æ®é¢„æµ‹é«˜é£é™©è¡Œä¸ºçš„æ–¹æ³•äº†ï¼Œè¿™äº›æ–¹æ³•æŠŠè®°å½•ä¸­é¢†åŸŸä¸“å®¶è®¤ä¸ºæœ‰å…³çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼šå¹´é¾„ã€å­¦å†ã€æ”¶å…¥ï¼‰ä½œä¸ºç‰¹å¾ï¼Œå†å²æ ‡æ³¨ä¿¡æ¯ä½œä¸ºæ ‡ç­¾å»è®­ç»ƒé£é™©é¢„æµ‹æ¨¡å‹ã€‚

é‚£ä¹ˆè¯»åˆ°çš„è¿™é‡Œï¼Œæˆ‘ä»¬æ˜¯å¦ä¼šæƒ³åˆ°åœ¨è¿™äº›æ–¹æ³•çš„åŸºç¡€ä¹‹ä¸Šï¼Œå¦‚æœæŠŠåŸºäºå›¾ç»“æ„çš„å±æ€§ä¹Ÿè€ƒè™‘è¿›æ¥ï¼Œä½œä¸ºç‰¹å¾å»è®­ç»ƒçš„æ¨¡å‹å¯èƒ½æ›´æœ‰æ•ˆå‘¢ï¼Ÿç­”æ¡ˆä¹Ÿæ˜¯è‚¯å®šçš„ï¼Œå·²ç»æœ‰å¾ˆå¤šè®ºæ–‡å’Œå·¥ç¨‹å®è·µæ­ç¤ºè¿™æ ·çš„æ¨¡å‹æ¯”æœªè€ƒè™‘å›¾ç‰¹å¾çš„ç®—æ³•æ›´åŠ æœ‰æ•ˆï¼šè¿™äº›è¢«å°è¯•æœ‰æ•ˆçš„å›¾ç»“æ„ç‰¹å¾å¯èƒ½æ˜¯å®ä½“çš„ PageRank å€¼ã€Degree å€¼æˆ–è€…æ˜¯æŸä¸€ä¸ªç¤¾åŒºå‘ç°ç®—æ³•å¾—å‡ºçš„ç¤¾åŒº idã€‚

åœ¨ç”Ÿäº§ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å®šæœŸä»å›¾è°±ä¸­è·å¾—å®æ—¶çš„å…¨å›¾ä¿¡æ¯ï¼Œåœ¨å›¾è®¡ç®—å¹³å°ä¸­åˆ†æè¿ç®—è·å¾—æ‰€éœ€ç‰¹å¾ï¼Œç»è¿‡é¢„å®šçš„æ•°æ®ç®¡é“ï¼Œå¯¼å…¥æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­å‘¨æœŸè·å¾—æ–°çš„é£é™©æç¤ºï¼Œå¹¶å°†éƒ¨åˆ†ç»“æœå†™å›å›¾è°±æ–¹ä¾¿å…¶ä»–ç³»ç»Ÿå’Œä¸“å®¶æŠ½å–ã€å‚è€ƒã€‚

#### å¸¦æœ‰å›¾ç‰¹å¾çš„æœºå™¨å­¦ä¹ æ¬ºè¯ˆæ£€æµ‹ç¤ºä¾‹

è¿™é‡Œï¼Œæœºå™¨å­¦ä¹ çš„æ–¹æ³•æˆ‘å°±ä¸æ¼”ç¤ºäº†ï¼Œå°±æ˜¯å¸¸è§çš„åˆ†ç±»æ–¹æ³•ï¼Œåœ¨æ­¤ä¹‹ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®ä¸­é€šè¿‡å›¾ç®—æ³•è·å¾—ä¸€äº›æ–°çš„å±æ€§ï¼Œè¿™äº›å±æ€§å†å¤„ç†ä¸€ä¸‹ä½œä¸ºæ–°çš„ç‰¹å¾ã€‚æˆ‘åªæ¼”ç¤ºä¸€ä¸ªç¤¾åŒºå‘ç°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¨å›¾è·‘ä¸€ä¸ª Louvainï¼Œå¾—å‡ºä¸åŒèŠ‚ç‚¹çš„ç¤¾åŒºå½’å±ï¼Œç„¶åæŠŠç¤¾åŒºçš„å€¼å½“åšä¸€ä¸ªåˆ†ç±»å¤„ç†æˆä¸ºæ•°å€¼çš„ç‰¹å¾ã€‚

è¿™ä¸ªä¾‹å­é‡Œæˆ‘ä»¬è¿˜ç”¨ https://github.com/wey-gu/fraud-detection-datagen è¿™ä¸ªæ•°æ®ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿™ä¸ªä¾‹å­æˆ‘ç”¨åˆ°äº† [Nebula-Algorithm](https://github.com/vesoft-inc/nebula-algorithm/) è¿™ä¸ªé¡¹ç›®ï¼Œå®ƒæ˜¯ä¸€ä¸ª Spark åº”ç”¨ï¼Œå¯ä»¥åœ¨ NebulaGraph å›¾åº“ä¸Šè¿è¡Œå¾ˆå¤šå¸¸ç”¨çš„å›¾ç®—æ³•ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éƒ¨ç½² Spark å’Œ Nebula Algorithmï¼Œè¿˜æ˜¯åˆ©ç”¨ Nebula-UPï¼Œä¸€é”®éƒ¨ç½²ï¼š

```bash
curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark
```

é›†ç¾¤èµ·æ¥ä¹‹åï¼Œå› ä¸ºéœ€è¦çš„é…ç½®æ–‡ä»¶æˆ‘å·²ç»æ”¾åœ¨äº† Nebula-UP å†…éƒ¨ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€è¡Œå°±å¯ä»¥è¿è¡Œç®—æ³•å•¦ï¼

```bash
cd ~/.nebula-up/nebula-up/spark && ls -l

docker exec -it sparkmaster /spark/bin/spark-submit \
    --master "local" --conf spark.rpc.askTimeout=6000s \
    --class com.vesoft.nebula.algorithm.Main \
    --driver-memory 4g /root/download/nebula-algo.jar \
    -p /root/louvain.conf
```

è€Œæœ€ç»ˆçš„ç»“æœå°±åœ¨ sparkmaster å®¹å™¨å†…çš„ `/output` é‡Œï¼š

```bash
# docker exec -it sparkmaster bash
ls -l /output
```

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸ª Louvain çš„å›¾ç‰¹å¾åšä¸€äº›å¤„ç†ï¼Œå¹¶å¼€å§‹ä¼ ç»Ÿçš„æ¨¡å‹è®­ç»ƒäº†ã€‚

### å›¾ç¥ç»ç½‘ç»œçš„æ–¹æ³•

ç„¶è€Œï¼Œè¿™äº›å›¾ç‰¹å¾çš„æ–¹æ³•çš„é—®é¢˜åœ¨äºï¼š

1. å›¾ç‰¹å¾å¹¶ä¸èƒ½æŠŠå…³è”å…³ç³»ï¼Œæ•°æ®çš„å±€éƒ¨æ€§å……åˆ†åæ˜ åˆ°æˆ‘ä»¬çš„æ¨¡å‹ã€æ–¹æ³•é‡Œï¼›
2. å›¾çš„ç‰¹å¾å·¥ç¨‹æ˜¯å¾ˆæ˜‚è´µã€ç¹ççš„ã€‚

åœ¨æœ€è¿‘å‡ å¹´çš„æˆæœä¸­ï¼ŒåŸºäº GNN çš„æ–¹æ³•é€šè¿‡å°†å›¾ç»“æ„ä¸å±æ€§ä¿¡æ¯è¿›è¡ŒåµŒå…¥è¡¨ç¤ºï¼Œä½¿å¾—æˆ‘ä»¬èƒ½åœ¨ä¸è¿›è¡Œå›¾ç‰¹å¾æŠ½å–ã€ç‰¹å¾å·¥ç¨‹ã€ä¸“å®¶ä¸å·¥ç¨‹æ–¹æ³•çš„æ•°æ®æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå¾—åˆ°ç›¸æ¯”äºåŸºäºä¼ ç»Ÿå›¾ç‰¹å¾çš„æœºå™¨å­¦ä¹ æ›´å¥½çš„æ•ˆæœã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œç°åœ¨æ­£æ˜¯è¿™äº›æ–¹æ³•å¿«é€Ÿè¢«å‘ç°ã€æ¼”è¿›çš„æ—¶æœŸï¼ŒåŸºäºå›¾çš„æ·±åº¦å­¦ä¹ æ˜¯ä¹‹å‰å‡ å¹´æœ€çƒ­é—¨çš„æœºå™¨å­¦ä¹ ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚

åŒæ—¶ï¼Œå›¾æ·±åº¦å­¦ä¹ çš„ä¸€äº›æ–¹æ³•å¯ä»¥åšåˆ° Inductive Learningâ€”â€”æ¨¡å‹å¯ä»¥åœ¨æ–°çš„ç‚¹ã€è¾¹ä¸Šè¿›è¡Œæ¨ç†ï¼Œè¿™æ ·ï¼Œé…åˆå›¾æ•°æ®åº“ä¸Šçº¿ä¸Šçš„å­å›¾æŸ¥è¯¢èƒ½åŠ›ï¼Œåœ¨çº¿å®æ—¶çš„é£é™©é¢„æµ‹ä¹Ÿå˜å¾—å¾ˆç®€å•å¯è¡Œäº†ã€‚

#### åŸºäºå›¾è¡¨ç¤ºçš„å›¾ç¥ç»ç½‘ç»œæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿç¤ºä¾‹

åˆ©ç”¨ GNN çš„æ–¹æ³•ä¸­ï¼Œå›¾æ•°æ®åº“å¹¶ä¸æ˜¯å¿…é¡»çš„ï¼Œæ•°æ®çš„å­˜å‚¨å¯ä»¥åœ¨å…¶ä»–å‡ ç§å¸¸è§çš„ä»‹è´¨ä¹‹ä¸­ï¼Œä½†æ˜¯å›¾åº“èƒ½å¤Ÿæœ€å¤§åŒ–åŠ©ç›Šæ¨¡å‹è®­ç»ƒã€æ¨¡å‹æ›´æ–°ã€çº¿ä¸Šç»“æœçš„æ›´æ–°ã€‚å½“æˆ‘ä»¬æŠŠå›¾æ•°æ®åº“ä½œä¸ºæ•°æ®çš„å•ä¸€æ•°æ®æ¥æºï¼ˆsingle source of truthï¼‰çš„æ—¶å€™ï¼Œæ‰€æœ‰çš„åŸºäºçº¿ä¸Šã€ç¦»çº¿ã€å›¾è°±çš„æ–¹æ³•å¯ä»¥å¾ˆå®¹æ˜“è¢«é›†æˆèµ·æ¥ï¼Œä»è€Œç»„åˆæ‰€æœ‰æ–¹æ³•çš„ä¼˜åŠ¿ä¸ç»“æœï¼Œåšå‡ºæ›´æœ‰æ•ˆçš„æ¬ºè¯ˆæ£€æµ‹å¤åˆç³»ç»Ÿã€‚

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­æˆ‘ä»¬ä¸€æ ·åˆ†ä¸ºï¼š**æ•°æ®å¤„ç†**ã€**æ¨¡å‹è®­ç»ƒ**ã€**æ„å»ºæ£€æµ‹ç³»ç»Ÿ**è¿™å‡ éƒ¨åˆ†ã€‚

> æ³¨ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨çš„çš„å·¥å…·ä¸º Deep Graph libraryï¼ˆDGLï¼‰ï¼ŒNebulaGraph å›¾æ•°æ®åº“å’Œä»–ä»¬ä¹‹é—´çš„æ¡¥æ¢ï¼ŒNebula-DGLã€‚
>
> - DGL: https://www.dgl.ai/
> - Nebula-DGL: https://github.com/wey-gu/nebula-dgl æˆ‘ä¹Ÿæ˜¯è¿™ä¸ªåº“çš„ä½œè€… ğŸ˜

##### æ•°æ®é›†

æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯ Yelp-Fraudï¼Œä»–ç›´æ¥æ¥è‡ªäºè®ºæ–‡ [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https://paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud)ã€‚

è¿™ä¸ªå›¾ä¸­æœ‰ä¸€ç§ç‚¹ï¼Œä¸‰ç§å…³ç³»ï¼š

- é¡¶ç‚¹ï¼šæ¥è‡ª Yelp ä¸­çš„é¤å…ã€é…’åº—çš„è¯„ä»·ï¼Œæœ‰ä¸¤ç±»å±æ€§ï¼š
  - æ¯ä¸€ä¸ªè¯„ä»·ä¸­æœ‰è¢«æ ‡æ³¨äº†çš„æ˜¯å¦æ˜¯è™šå‡ã€æ¬ºè¯ˆè¯„ä»·çš„æ ‡ç­¾
  - 32 ä¸ªå·²ç»è¢«å¤„ç†è¿‡çš„æ•°å­—å‹å±æ€§
- è¾¹ï¼šä¸‰ç±»è¯„ä»·ä¹‹é—´çš„å…³è”å…³ç³»
  - R-U-Rï¼šä¸¤ä¸ªè¯„ä»·ç”±åŒä¸€ä¸ªç”¨æˆ·å‘å‡º shares_user_with
  - R-S-Rï¼šä¸¤ä¸ªè¯„ä»·æ˜¯åŒé¤å…åŒè¯„åˆ†ï¼ˆè¯„åˆ†å¯ä»¥æ˜¯1åˆ°5ï¼‰ shares_restaurant_rating_with
  - R-T-Rï¼šä¸¤ä¸ªè¯„ä»·æ˜¯åŒé¤å…åŒæäº¤æœˆä»½ shares_restaurant_in_one_month_with

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬å‡è®¾è¿™ä¸ªå›¾å·²ç»åœ¨æˆ‘ä»¬çš„ NebulaGraph é‡Œè¾¹äº†ã€‚

> æ³¨ï¼Œæˆ‘å·²ç»å¸®å¤§å®¶æå‰åšå¥½äº†å°†è¿™å¼ å›¾å¯¼å…¥ NebulaGraph çš„å·¥ä½œï¼Œé•¿è¯çŸ­è¯´å°±æ˜¯ï¼š
>
> ```bash
> # éƒ¨ç½² NebulaGraph
> curl -fsSL nebula-up.siwei.io/install.sh | bash
> 
> # æ‹‰å–è¿™ä¸ªæ•°æ®çš„ Repo
> git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection && cd nebulagraph-yelp-frauddetection
> 
> # å®‰è£…ä¾èµ–ï¼Œæ‰§è¡Œæ•°æ®ä¸‹è½½ç”Ÿæˆ
> python3 -m pip install -r requirements.txt
> python3 data_download.py
> 
> # å¯¼å…¥åˆ° NebulaGraph
> docker run --rm -ti \
>  --network=nebula-net \
>  -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \
>  -v ${PWD}/data:/root \
>  vesoft/nebula-importer:v3.1.0 \
>  --config /root/importer.yaml
> ```
>
> è¯¦æƒ…å‚è€ƒï¼šhttps://github.com/wey-gu/nebulagraph-yelp-frauddetection

##### æ•°æ®å¤„ç†

è¿™éƒ¨åˆ†çš„ä»»åŠ¡æ˜¯å°†å›¾è°±ä¸­å’Œé£é™©ç›¸å…³å­å›¾çš„æ‹“æ‰‘ç»“æ„è¡¨ç¤ºå’Œå…¶ä¸­æœ‰å…³çš„ç‰¹å¾ï¼ˆå±æ€§ï¼‰è¿›è¡Œå·¥ç¨‹å¤„ç†ï¼Œåºåˆ—åŒ–æˆä¸º DGL çš„å›¾å¯¹è±¡ã€‚

DGL æœ¬èº«æ”¯æŒä»ç‚¹ã€è¾¹åˆ—è¡¨ï¼ˆedgelistï¼‰å½¢å¼ CSV æ–‡ä»¶ï¼Œæˆ–è€…ä» NetworkX å’Œ SciPy çš„åºåˆ—åŒ–ç¨€ç–çš„é‚»æ¥çŸ©é˜µï¼ˆadjacency matrixï¼‰çš„æ•°æ®æ¥æ„é€ å®ƒçš„å›¾å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠåŸå§‹çš„å›¾æ•°æ®æˆ–è€…å›¾åº“ä¸­çš„æ•°æ®å…¨é‡å¯¼å‡ºä¸ºè¿™äº›å½¢å¼ï¼Œä¸è¿‡åœ¨çœŸå®çš„ä¾‹å­ä¸­å›¾åº“ä¸­çš„æ•°æ®æ˜¯å®æ—¶å˜åŒ–çš„ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨ NebulaGraph ä¸­çš„å­å›¾ä¸Šåš GNN è®­ç»ƒä¸€èˆ¬æ¥è¯´æ˜¯æ›´ç†æƒ³ã€‚å¾—ç›Šäº Nebula-DGL è¿™ä¸ªåº“ï¼Œåšè¿™ä»¶äº‹å„¿æ˜¯å¾ˆè‡ªç„¶çš„ã€‚

> æ³¨ï¼ŒDGL å¤–éƒ¨æ•°æ®å‘˜å¯¼å…¥æ–‡æ¡£ï¼šhttps://docs.dgl.ai/guide/graph-external.html

ç°åœ¨æˆ‘ä»¬å¼€å§‹è¿™ä¸ªæ•°æ®çš„å¯¼å…¥ï¼Œåœ¨è¿™ä¹‹å‰ï¼Œæˆ‘å…ˆä»‹ç»ä¸€ä¸‹ Nebula-DGLã€‚

Nebula-DGL å¯ä»¥æ ¹æ®ç»™å®šçš„æ˜ å°„å’Œè½¬æ¢è§„åˆ™ï¼ˆYAML æ ¼å¼ï¼‰ï¼Œå°† NebulaGraph ä¸­çš„é¡¶ç‚¹ã€è¾¹ï¼Œå’Œå®ƒä»¬çš„å±æ€§æŒ‰ç…§è§„åˆ™å¤„ç†æˆä¸ºç‚¹ã€è¾¹ã€å’Œå…¶ä¸­çš„æ ‡æ³¨ï¼ˆLabelï¼‰ä¸ç‰¹å¾ï¼ˆFeatureï¼‰ï¼Œä»è€Œæ„é€ ä¸º DGL çš„å›¾å¯¹è±¡ã€‚è¿™å…¶ä¸­ï¼Œå€¼å¾—ä¸€æçš„æ˜¯å±æ€§åˆ°ç‰¹å¾çš„è½¬æ¢ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œç‰¹å¾å¯èƒ½æ˜¯æŸä¸€ä¸ªå±æ€§çš„å€¼ã€ä¸€ä¸ªæˆ–è€…å¤šä¸ªå±æ€§çš„å€¼åšä¸€å®šçš„æ•°å­¦å˜æ¢ã€äº¦æˆ–æ˜¯å­—ç¬¦å‹çš„å±æ€§æŒ‰ç…§æšä¸¾è§„åˆ™è¾“å‡ºä¸ºæ•°å­—ã€‚ç›¸åº”çš„ï¼ŒNebula-DGL åœ¨è§„åˆ™ä¸­ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥é’ˆå¯¹è¿™å‡ ç§æƒ…å†µåˆ©ç”¨ `filter` è¿›è¡Œè¡¨è¾¾ï¼š

- ç‰¹å¾ç›´æ¥é€‰å–å±æ€§çš„å€¼ï¼š

è¿™ä¸ªä¾‹å­é‡Œï¼ŒNebulaGraph å›¾ä¸­ `follow` è¿™ä¸ªè¾¹å°†è¢«æŠ½å–ï¼Œè¾¹ä¸Šçš„å±æ€§ `degree` çš„å€¼å°†ç›´æ¥è¢«ä½œä¸ºåä¸º `degree` çš„ç‰¹å¾ã€‚

```yaml
edge_types:
  - name: follow
    start_vertex_tag: player
    end_vertex_tag: player
    features:
      - name: degree
        properties:
          - name: degree
            type: int
            nullable: False
        filter:
          type: value
```

- ç‰¹å¾ä»å±æ€§ä¸­ç»è¿‡æ•°å­¦å˜æ¢

è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æŠŠ `serve` è¾¹ä¹‹ä¸­çš„ä¸¤ä¸ªå±æ€§è¿›è¡Œ `(end_year - start_year) / 30` çš„å¤„ç†ï¼Œå˜ä¸º `service_time` è¿™æ ·çš„ä¸€ä¸ªç‰¹å¾ã€‚

```yaml
edge_types:
  - name: serve
    start_vertex_tag: player
    end_vertex_tag: team
    features:
      - name: service_time
        properties:
          - name: start_year
            type: int
            nullable: False
          - name: end_year
            type: int
            nullable: False
        # The variable was mapped by order of properties
        filter:
          type: function
          function: "lambda start_year, end_year: (end_year - start_year) / 30"
```

- æšä¸¾å±æ€§å€¼ä¸ºæ•°å­—ç‰¹å¾

è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æŠŠ `team` é¡¶ç‚¹ä¸­çš„ `name` å±æ€§è¿›è¡Œæšä¸¾ï¼Œæ ¹æ®è¿™ä¸ªå¯¹äºæ˜¯è¥¿å²¸è¿˜æ˜¯ä¸œå²¸ï¼š

```yaml
vertex_tags:
  - name: team
    features:
      - name: coast
        properties:
          - name: name
            type: str
            nullable: False
        filter:
          # 0 stand for the east coast, 1 stand for the west coast
          type: enumeration
          enumeration:
            Celtics: 0
            Nets: 0
            Nuggets: 1
            Timberwolves: 1
            Thunder: 1
# ... not showing all teams here
```

å¯ä»¥çœ‹åˆ°è¿™ä¸ªè½¬æ¢è§„åˆ™éå¸¸ç®€å•ç›´æ¥ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥å‚è€ƒ Nebula-DGL çš„å®Œæ•´ä¾‹å­äº†è§£å…¨éƒ¨ç»†èŠ‚ https://github.com/wey-gu/nebula-dgl/tree/main/exampleã€‚è€Œæœ‰ä¸Šè¾¹æ•°æ®å¤„ç†è§„åˆ™çš„äº†è§£ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å¤„ç†è¿™ä¸ª Yelp å›¾æ•°æ®äº†ã€‚

é¦–å…ˆï¼Œå®šä¹‰å¦‚ä¸‹è§„åˆ™ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬æŠŠé¡¶ç‚¹ review å’Œä¸‰ç§è¾¹éƒ½å¯¹åº”è¿‡æ¥äº†ï¼ŒåŒæ—¶ï¼Œreview ä¸Šçš„å±æ€§ä¹ŸæŒ‰ç…§åŸæœ¬çš„å€¼å¯¹åº”äº†è¿‡æ¥ï¼š

`nebulagraph_yelp_dgl_mapper.yaml`

```yaml
---
# If vertex id is string-typed, remap_vertex_id must be true.
remap_vertex_id: True
space: yelp
# str or int
vertex_id_type: int
vertex_tags:
  - name: review
    label:
      name: is_fraud
      properties:
        - name: is_fraud
          type: int
          nullable: False
      filter:
        type: value
    features:
      - name: f0
        properties:
          - name: f0
            type: float
            nullable: False
        filter:
          type: value
      - name: f1
        properties:
          - name: f1
            type: float
            nullable: False
        filter:
          type: value
# ...
      - name: f31
        properties:
          - name: f31
            type: float
            nullable: False
        filter:
          type: value
edge_types:
  - name: shares_user_with
    start_vertex_tag: review
    end_vertex_tag: review
  - name: shares_restaurant_rating_with
    start_vertex_tag: review
    end_vertex_tag: review
  - name: shares_restaurant_in_one_month_with
    start_vertex_tag: review
    end_vertex_tag: review

```

ç„¶åï¼Œæˆ‘ä»¬åœ¨å®‰è£…å¥½ Nebula-DGL ä¹‹ååªéœ€è¦è¿™å‡ è¡Œä»£ç å°±å¯ä»¥å°† NebulaGraph ä¸­çš„è¿™å¼ å›¾æ„é€ ä¸º DGL çš„ `DGLHeteroGraph` å›¾å¯¹è±¡ï¼š

```python
from nebula_dgl import NebulaLoader


nebula_config = {
    "graph_hosts": [
                ('graphd', 9669),
                ('graphd1', 9669),
                ('graphd2', 9669)
            ],
    "nebula_user": "root",
    "nebula_password": "nebula",
}

# load feature_mapper from yaml file
with open('nebulagraph_yelp_dgl_mapper.yaml', 'r') as f:
    feature_mapper = yaml.safe_load(f)

nebula_loader = NebulaLoader(nebula_config, feature_mapper)
g = nebula_loader.load()

g = g.to('cpu')
device = torch.device('cpu')
```

##### æ¨¡å‹è®­ç»ƒ

è¿™é‡Œï¼Œæˆ‘ç”¨ [GraphSAGE](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf) ç®—æ³•çš„ç‚¹åˆ†ç±»ï¼ˆNode Classificationï¼‰æ–¹æ³•æ¥ä¸¾ä¾‹ï¼ŒGraphSAGE çš„åŸå§‹ç‰ˆæœ¬æ˜¯ä¸€ä¸ªå½’çº³å­¦ä¹ ï¼ˆInductive Learningï¼‰çš„ç®—æ³•ï¼Œè¿™é‡Œï¼Œå½’çº³å­¦ä¹ åŒºåˆ«äºå®ƒçš„åé¢ï¼š Transductive Learning ï¼Œå¯ä»¥æŠŠæ–°çš„æ•°æ®ç”¨åœ¨å®Œå…¨æ—§çš„å›¾ä¹‹ä¸Šä¹ å¾—çš„æ¨¡å‹ï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¯ä»¥è¿›è¡Œçº¿ä¸Šå¢é‡æ•°æ®çš„æ¬ºè¯ˆæ£€æµ‹ï¼ˆè€Œä¸æ˜¯éœ€è¦é‡æ–°åŠ è½½ä¸ºå…¨å›¾è®­ç»ƒæ‰å¯ä»¥ï¼‰ã€‚

![GraphSAGE_FraudDetection](https://user-images.githubusercontent.com/1651790/182301784-21850dac-0d47-4dd5-b66f-a28b87fe9d4d.svg)

æ¨¡å‹è®­ç»ƒç³»ç»Ÿï¼ˆå·¦è¾¹ï¼‰ï¼š

- è¾“å…¥ï¼šå¸¦æœ‰æ¬ºè¯ˆæ ‡æ³¨çš„å†å²äº¤æ˜“å›¾è°±
- è¾“å‡ºï¼šä¸€ä¸ª GraphSAGE çš„ DGL æ¨¡å‹ 

çº¿ä¸Šæ¨ç†ç³»ç»Ÿï¼ˆå³è¾¹ï¼‰ï¼š

- æ¨¡å‹ï¼šåŸºäºå¸¦æœ‰æ¬ºè¯ˆæ ‡æ³¨çš„å†å²äº¤æ˜“å›¾è°±åŸºäº GraphSAGE è®­ç»ƒ

- è¾“å…¥ï¼šä¸€ç¬”æ–°çš„äº¤æ˜“

- è¾“å‡ºï¼šè¿™ç¬”äº¤æ˜“æ˜¯å¦æ¶‰å«Œæ¬ºè¯ˆ

**åˆ†å‰²æ•°æ®é›†**

æœºå™¨å­¦ä¹ è®­ç»ƒçš„è¿‡ç¨‹éœ€è¦åœ¨å·²ç»æœ‰çš„æ•°æ®ã€ä¿¡æ¯ä¸­åˆ†å‰²å‡ºç”¨æ¥è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•çš„å­é›†ï¼Œä»–ä»¬å¯ä»¥æ˜¯ä¸ç›¸äº¤çš„æ•´ä½“æ•°æ®çš„çœŸå­é›†ä¹Ÿå¯ä»¥å½¼æ­¤æœ‰é‡å ï¼Œåœ¨å®é™…çš„æƒ…å†µä¸­ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬å¯¹æ•°æ®çš„æ ‡æ³¨å¸¸å¸¸æ˜¯ä¸å……åˆ†çš„ï¼Œæ‰€ä»¥æŒ‰ç…§æ ‡æ³¨çš„æ¯”ä¾‹å»åˆ†å‰²æ•°æ®å¯èƒ½æ›´æœ‰æ„ä¹‰ä¸€äº›ï¼Œä¸‹è¾¹çš„ä¾‹å­æ˜¯æˆ‘æŒ‰ç…§ç‚¹ä¸Šæ˜¯å¦æ ‡æ³¨æ¬ºè¯ˆä¸ºæ ‡å‡†å»åˆ†å‰²æ•°æ®é›†ã€‚

è¿™é‡Œè¾¹æœ‰ä¸¤ä¸ªåœ°æ–¹å€¼å¾—æ³¨æ„ï¼š

1. `train_test_split` ä¸­çš„ `stratify=g.ndata['is_fraud']` ä»£è¡¨ä¿æŒ `is_fraud` çš„å€¼çš„åˆ†å¸ƒå»åˆ†å‰²ï¼Œç¬¦åˆæˆ‘ä»¬å‰è¾¹æåˆ°çš„æ€æƒ³ã€‚
2. æˆ‘ä»¬åˆ†å‰²çš„æ˜¯ `idx` ç´¢å¼•ï¼Œè¿™æ ·ï¼Œå¯ä»¥æœ€ç»ˆè·å¾—ä¸‰ä¸ªé›†åˆçš„ç´¢å¼•ï¼Œä¾›è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ—¶å€™ä½¿ç”¨ã€‚åŒæ—¶æˆ‘ä»¬è¿˜æŠŠå¯¹åº”é›†åˆ mask æ”¾åˆ°å›¾å¯¹è±¡ `g` é‡Œè¾¹å»äº†ã€‚

```python
# Split the graph into train, validation, and test sets

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# features are g.ndata['f0'], g.ndata['f1'], g.ndata['f2'], ... g.ndata['f31']
# label is in g.ndata['is_fraud']

# concatenate all features
features = []
for i in range(32):
    features.append(g.ndata['f' + str(i)])

g.ndata['feat'] = torch.stack(features, dim=1)
g.ndata['label'] = g.ndata['is_fraud']
# numpy array as an index of range n

idx = torch.tensor(np.arange(g.number_of_nodes()), device=device, dtype=torch.int64)

# split based on value distribution of label: the property "is_fraud", which is a binary variable.
X_train_and_val_idx, X_test_idx, y_train_and_val, y_test = train_test_split(
    idx, g.ndata['is_fraud'], test_size=0.2, random_state=42, stratify=g.ndata['is_fraud'])

# split train and val
X_train_idx, X_val_idx, y_train, y_val = train_test_split(
    X_train_and_val_idx, y_train_and_val, test_size=0.2, random_state=42, stratify=y_train_and_val)

# list of index to mask
train_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
train_mask[X_train_idx] = True

val_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
val_mask[X_val_idx] = True

test_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
test_mask[X_test_idx] = True

g.ndata['train_mask'] = train_mask
g.ndata['val_mask'] = val_mask
g.ndata['test_mask'] = test_mask
```

**å¼‚æ„å›¾è½¬æ¢ä¸ºåŒæ„å›¾**

GraphSAGE æ˜¯é’ˆå¯¹åŒæ„å›¾ï¼Œä¸”è¾¹æ—  feature çš„ç®—æ³•ï¼Œè€Œæˆ‘ä»¬å½“ä¸‹çš„ Yelp å›¾è°±æ˜¯å¼‚æ„çš„ï¼šä¸€ç±»ç‚¹ã€ä¸‰ç±»è¾¹ã€‚é‚£ä¹ˆï¼Œå¦‚ä½•æ‰èƒ½ç”¨ GraphSAGE å»å»ºæ¨¡ Yelp å›¾è°±å‘¢ï¼Ÿ

æˆ‘ä»¬é™¤äº†é€‰æ‹©ç”¨å…¶ä»–é’ˆå¯¹å¼‚æ„å›¾çš„ Inductive Learning æ–¹æ³•ä¹‹å¤–ï¼Œè¿˜å¯æƒ³åŠæ³•æŠŠåŒæ„å›¾è½¬æ¢æˆå¼‚æ„å›¾ã€‚ä¸ºäº†åœ¨è½¬æ¢ä¸­ä¸ä¸¢å¤±é‡è¦çš„è¾¹ç±»å‹ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾¹ç±»å‹å˜æˆæ•°å€¼ã€‚

è¿™é‡Œæˆ‘ç»™äº†ä¸€ç»´çš„ edge featureï¼Œå½“ç„¶ï¼ˆ3-1ï¼‰äºŒç»´ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚

```python
# shares_restaurant_in_one_month_with: 1, b"001"
# shares_restaurant_rating_with: 2, b"010"
# shares_user_with: 4, b"100"
```

> æ³¨ï¼šå…¶å®å¦‚æœæƒ³ç”¨ 0, 1, 2 è¿™æ ·çš„åˆ†å¸ƒï¼Œè½¬æ¢åˆ°åŒæ„å›¾ä¹‹åçš„ `hg.edata['_TYPE']` ä¹Ÿæ˜¯å¯ä»¥ç›´æ¥æ‹¿æ¥ç”¨çš„ï¼Œè¯¦è§ https://docs.dgl.ai/en/0.9.x/generated/dgl.to_homogeneous.html ä¸­çš„ä¾‹å­ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
# three types of edges
In [1]: g.etypes
Out[1]:
['shares_restaurant_in_one_month_with',
 'shares_restaurant_rating_with',
 'shares_user_with']

In [2]:
g.edges['shares_restaurant_in_one_month_with'].data['he'] = torch.ones(
    g.number_of_edges('shares_restaurant_in_one_month_with'), dtype=torch.int64)
g.edges['shares_restaurant_rating_with'].data['he'] = torch.full(
    (g.number_of_edges('shares_restaurant_rating_with'),), 2, dtype=torch.int64)
g.edges['shares_user_with'].data['he'] = torch.full(
    (g.number_of_edges('shares_user_with'),), 4, dtype=torch.int64)

In [3]: g.edata['he']
Out[3]:
{('review',
  'shares_restaurant_in_one_month_with',
  'review'): tensor([1, 1, 1,  ..., 1, 1, 1]),
 ('review',
  'shares_restaurant_rating_with',
  'review'): tensor([2, 2, 2,  ..., 2, 2, 2]),
 ('review', 'shares_user_with', 'review'): tensor([4, 4, 4,  ..., 4, 4, 4])}

```

> å‚è€ƒï¼šhttps://discuss.dgl.ai/t/how-to-convert-from-a-heterogeneous-graph-to-a-homogeneous-graph-with-data/2764

ç„¶åå°†å®ƒè½¬æ¢ä¸ºåŒæ„å›¾ï¼ŒæŠŠ `he` ä½œä¸ºè¦ä¿ç•™çš„ edataï¼š

```python
hg = dgl.to_homogeneous(g, edata=['he'], ndata=['feat', 'label', 'train_mask', 'val_mask', 'test_mask'])
```

> å‚è€ƒï¼šhttps://docs.dgl.ai/en/latest/guide/graph-heterogeneous.html?highlight=to_homogeneous#converting-heterogeneous-graphs-to-homogeneous-graphs

é»˜è®¤çš„ GraphSAGE å®ç°æ˜¯æ²¡è€ƒè™‘ edge feature çš„ï¼Œæˆ‘ä»¬è¦ä¿®æ”¹æ¶ˆæ¯ä¼ é€’çš„æ­¥éª¤ï¼Œåœ¨åè¾¹ä¼šæ¶‰åŠåˆ°è¿™éƒ¨åˆ†çš„å®æ“ã€‚

> å‚è€ƒï¼š
>
> - https://discuss.dgl.ai/t/frequently-asked-questions-faq/1681 ï¼ˆé—®é¢˜13ï¼‰
> - https://discuss.dgl.ai/t/using-node-and-edge-features-in-message-passing/762

**æ¨¡å‹è®­ç»ƒä»£ç **

DGL å®˜æ–¹ç»™å‡ºäº†ä¾‹å­åœ¨ï¼šhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsageï¼Œæˆ‘åœ¨æµ‹è¯•çš„æ—¶å€™è¿˜ä¿®å¤äº†ä¸€ä¸ªå° bugã€‚

å› ä¸ºæˆ‘ä»¬å¤„ç†è¿‡çš„åŒæ„å›¾é‡Œæ˜¯å¸¦æœ‰ edge feature çš„ï¼Œä¸èƒ½ç…§æ¬å®˜æ–¹çš„ GraphSAGE ä¾‹å­ä»£ç ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•æ¥å¤„ç†å®ƒï¼š

a. å¯ä»¥ç¨å¾®æ”¹åŠ¨ä¸€ä¸‹ `SAGEConv`æ¶ˆæ¯ä¼ é€’çš„éƒ¨åˆ†ï¼Œä»¥ mean èšåˆçš„æ–¹å¼ä¸ºä¾‹ï¼š

```diff
  graph.update_all(msg_fn, fn.mean('m', 'neigh'))
+ graph.update_all(fn.copy_e('he', 'm'), fn.mean('m', 'neigh'))
- h_neigh = graph.dstdata['neigh']
+ h_neigh = torch.cat((graph.dstdata['neigh'], graph.dstdata['neigh_e'].reshape(-1, 1)), 1)
```

è¿™ä¸ªå¤„ç†ä¸­ï¼Œé™¤äº†ä¸Šè¾¹æ¶ˆæ¯ä¼ é€’éƒ¨åˆ†å¢åŠ  edge feature ä¹‹å¤–ï¼Œè¿˜éœ€è¦æ³¨æ„ feature ç»´åº¦çš„å¤„ç†ã€‚

b. æŠŠè¾¹å‚æ•°ä½œä¸ºè¾¹æƒé‡ ï¼Œä»¥ mean èšåˆä¸ºä¾‹ï¼š

```diff
- graph.update_all(msg_fn, fn.mean('m', 'neigh'))
+ # consdier datatype with different weight, g.edata['he'] as weight here
+ g.update_all(fn.u_mul_e('h', 'he', 'm'), fn.mean('m', 'h'))
```

ä¸‹è¾¹ï¼Œæˆ‘ä»¬ä»¥æŠŠè¾¹çš„ç±»å‹ä½œä¸ºæƒé‡çš„æ–¹å¼ï¼Œ`mean`ä½œä¸ºèšåˆçš„æƒ…å†µä¸ºä¾‹æ¥å®æ“ï¼š

æˆ‘ä»¬æ¥ç»§æ‰¿ï¼Œå¹¶è¦†ç›–`SAGEConv`ï¼š

> æˆ‘ä»¬å…¶å®åªæ˜¯ä¿®æ”¹äº† Message Passing çš„éƒ¨åˆ†ã€‚

```python
from dgl import function as fn
from dgl.utils import check_eq_shape, expand_as_pair

class SAGEConv(dglnn.SAGEConv):
    def forward(self, graph, feat, edge_weight=None):
        r"""

        Description
        -----------
        Compute GraphSAGE layer.

        Parameters
        ----------
        graph : DGLGraph
            The graph.
        feat : torch.Tensor or pair of torch.Tensor
            If a torch.Tensor is given, it represents the input feature of shape
            :math:`(N, D_{in})`
            where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.
            If a pair of torch.Tensor is given, the pair must contain two tensors of shape
            :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.
        edge_weight : torch.Tensor, optional
            Optional tensor on the edge. If given, the convolution will weight
            with regard to the message.

        Returns
        -------
        torch.Tensor
            The output feature of shape :math:`(N_{dst}, D_{out})`
            where :math:`N_{dst}` is the number of destination nodes in the input graph,
            :math:`D_{out}` is the size of the output feature.
        """
        self._compatibility_check()
        with graph.local_scope():
            if isinstance(feat, tuple):
                feat_src = self.feat_drop(feat[0])
                feat_dst = self.feat_drop(feat[1])
            else:
                feat_src = feat_dst = self.feat_drop(feat)
                if graph.is_block:
                    feat_dst = feat_src[:graph.number_of_dst_nodes()]
            msg_fn = fn.copy_src('h', 'm')
            if edge_weight is not None:
                assert edge_weight.shape[0] == graph.number_of_edges()
                graph.edata['_edge_weight'] = edge_weight
                msg_fn = fn.u_mul_e('h', '_edge_weight', 'm')

            h_self = feat_dst

            # Handle the case of graphs without edges
            if graph.number_of_edges() == 0:
                graph.dstdata['neigh'] = torch.zeros(
                    feat_dst.shape[0], self._in_src_feats).to(feat_dst)

            # Determine whether to apply linear transformation before message passing A(XW)
            lin_before_mp = self._in_src_feats > self._out_feats

            # Message Passing
            if self._aggre_type == 'mean':
                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src
                # graph.update_all(msg_fn, fn.mean('m', 'neigh'))
                #########################################################################
                # consdier datatype with different weight, g.edata['he'] as weight here
                g.update_all(fn.u_mul_e('h', 'he', 'm'), fn.mean('m', 'h'))
                #########################################################################
                h_neigh = graph.dstdata['neigh']
                if not lin_before_mp:
                    h_neigh = self.fc_neigh(h_neigh)
            elif self._aggre_type == 'gcn':
                check_eq_shape(feat)
                graph.srcdata['h'] = self.fc_neigh(feat_src) if lin_before_mp else feat_src
                if isinstance(feat, tuple):  # heterogeneous
                    graph.dstdata['h'] = self.fc_neigh(feat_dst) if lin_before_mp else feat_dst
                else:
                    if graph.is_block:
                        graph.dstdata['h'] = graph.srcdata['h'][:graph.num_dst_nodes()]
                    else:
                        graph.dstdata['h'] = graph.srcdata['h']
                graph.update_all(msg_fn, fn.sum('m', 'neigh'))
                graph.update_all(fn.copy_e('he', 'm'), fn.sum('m', 'neigh'))
                # divide in_degrees
                degs = graph.in_degrees().to(feat_dst)
                h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']) / (degs.unsqueeze(-1) + 1)
                if not lin_before_mp:
                    h_neigh = self.fc_neigh(h_neigh)
            elif self._aggre_type == 'pool':
                graph.srcdata['h'] = F.relu(self.fc_pool(feat_src))
                graph.update_all(msg_fn, fn.max('m', 'neigh'))
                graph.update_all(fn.copy_e('he', 'm'), fn.max('m', 'neigh'))
                h_neigh = self.fc_neigh(graph.dstdata['neigh'])
            elif self._aggre_type == 'lstm':
                graph.srcdata['h'] = feat_src
                graph.update_all(msg_fn, self._lstm_reducer)
                h_neigh = self.fc_neigh(graph.dstdata['neigh'])
            else:
                raise KeyError('Aggregator type {} not recognized.'.format(self._aggre_type))

            # GraphSAGE GCN does not require fc_self.
            if self._aggre_type == 'gcn':
                rst = h_neigh
            else:
                rst = self.fc_self(h_self) + h_neigh

            # bias term
            if self.bias is not None:
                rst = rst + self.bias

            # activation
            if self.activation is not None:
                rst = self.activation(rst)
            # normalization
            if self.norm is not None:
                rst = self.norm(rst)
            return rst
```

å®šä¹‰æ¨¡å‹

```python
class SAGE(nn.Module):
    def __init__(self, in_size, hid_size, out_size):
        super().__init__()
        self.layers = nn.ModuleList()
        # three-layer GraphSAGE-mean
        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))
        self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))
        self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))
        self.dropout = nn.Dropout(0.5)
        self.hid_size = hid_size
        self.out_size = out_size

    def forward(self, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(block, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
                h = self.dropout(h)
        return h

    def inference(self, g, device, batch_size):
        """Conduct layer-wise inference to get all the node embeddings."""
        feat = g.ndata['feat']
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
                g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
                batch_size=batch_size, shuffle=False, drop_last=False,
                num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)

        for l, layer in enumerate(self.layers):
            y = torch.empty(
                g.num_nodes(), self.hid_size if l != len(self.layers) - 1 else self.out_size,
                device=buffer_device, pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[input_nodes]
                h = layer(blocks[0], x) # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                    h = self.dropout(h)
                # by design, our output nodes are contiguous
                y[output_nodes[0]:output_nodes[-1]+1] = h.to(buffer_device)
            feat = y
        return y
```

å®šä¹‰è®­ç»ƒã€æ¨ç†çš„å‡½æ•°

```python
def evaluate(model, graph, dataloader):
    model.eval()
    ys = []
    y_hats = []
    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):
        with torch.no_grad():
            x = blocks[0].srcdata['feat']
            ys.append(blocks[-1].dstdata['label'])
            y_hats.append(model(blocks, x))
    return MF.accuracy(torch.cat(y_hats), torch.cat(ys))

def layerwise_infer(device, graph, nid, model, batch_size):
    model.eval()
    with torch.no_grad():
        pred = model.inference(graph, device, batch_size) # pred in buffer_device
        pred = pred[nid]
        label = graph.ndata['label'][nid].to(pred.device)
        return MF.accuracy(pred, label)

def train(device, g, model, train_idx, val_idx):
    # create sampler & dataloader
    sampler = NeighborSampler([10, 10, 10],  # fanout for [layer-0, layer-1, layer-2]
                              prefetch_node_feats=['feat'],
                              prefetch_labels=['label'])
    use_uva = False
    train_dataloader = DataLoader(g, train_idx, sampler, device=device,
                                  batch_size=1024, shuffle=True,
                                  drop_last=False, num_workers=0,
                                  use_uva=use_uva)

    val_dataloader = DataLoader(g, val_idx, sampler, device=device,
                                batch_size=1024, shuffle=True,
                                drop_last=False, num_workers=0,
                                use_uva=use_uva)

    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)
    
    for epoch in range(10):
        model.train()
        total_loss = 0
        for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):
            x = blocks[0].srcdata['feat']
            y = blocks[-1].dstdata['label']
            y_hat = model(blocks, x)
            loss = F.cross_entropy(y_hat, y)
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item()
        acc = evaluate(model, g, val_dataloader)
        print("Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} "
              .format(epoch, total_loss / (it+1), acc.item()))
```

ä» NebulaGraph ä¸­åŠ è½½å›¾åˆ° DGLï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ªå¼‚æ„å›¾ï¼ˆä¸€ç±»ç‚¹ã€ä¸‰ç±»è¾¹ï¼‰

```python
from nebula_dgl import NebulaLoader

nebula_config = {
    "graph_hosts": [
                ('graphd', 9669),
                ('graphd1', 9669),
                ('graphd2', 9669)
            ],
    "nebula_user": "root",
    "nebula_password": "nebula",
}

with open('nebulagraph_yelp_dgl_mapper.yaml', 'r') as f:
     feature_mapper = yaml.safe_load(f)

nebula_loader = NebulaLoader(nebula_config, feature_mapper)

g = nebula_loader.load() # This will take you some time

# ä½œä¸ºç©·äººï¼Œæˆ‘ä»¬ç”¨ CPU
g = g.to('cpu')
device = torch.device('cpu')
```

åˆ†å‡ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ï¼Œç„¶åè½¬æ¢æˆåŒæ„å›¾ã€‚

```python
# Split the graph into train, validation and test sets

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# features are g.ndata['f0'], g.ndata['f1'], g.ndata['f2'], ... g.ndata['f31']
# label is in g.ndata['is_fraud']

# concatenate all features
features = []
for i in range(32):
    features.append(g.ndata['f'+str(i)])

g.ndata['feat'] = torch.stack(features, dim=1)
g.ndata['label'] = g.ndata['is_fraud']
# numpy array as index of range n

idx = torch.tensor(np.arange(g.number_of_nodes()), device=device, dtype=torch.int64)
# features.append(idx)
# concatenate one dim with index of node
# feature_and_idx = torch.stack(features, dim=1)

# split based on value distribution of label: the property "is_fraud", which is a binary variable.
X_train_and_val_idx, X_test_idx, y_train_and_val, y_test = train_test_split(
    idx, g.ndata['is_fraud'], test_size=0.2, random_state=42, stratify=g.ndata['is_fraud'])

# split train and val
X_train_idx, X_val_idx, y_train, y_val = train_test_split(
    X_train_and_val_idx, y_train_and_val, test_size=0.2, random_state=42, stratify=y_train_and_val)

# list of index to mask
train_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
train_mask[X_train_idx] = True

val_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
val_mask[X_val_idx] = True

test_mask = torch.zeros(g.number_of_nodes(), dtype=torch.bool)
test_mask[X_test_idx] = True

g.ndata['train_mask'] = train_mask
g.ndata['val_mask'] = val_mask
g.ndata['test_mask'] = test_mask

# shares_restaurant_in_one_month_with: 1, b"001"
# shares_restaurant_rating_with: 2, b"010"
# shares_user_with: 4, b"100"
# set edata of shares_restaurant_in_one_month_with to n of 1 tensor array
g.edges['shares_restaurant_in_one_month_with'].data['he'] = torch.ones(
    g.number_of_edges('shares_restaurant_in_one_month_with'), dtype=torch.float32)
g.edges['shares_restaurant_rating_with'].data['he'] = torch.full(
    (g.number_of_edges('shares_restaurant_rating_with'),), 2, dtype=torch.float32)
g.edges['shares_user_with'].data['he'] = torch.full(
    (g.number_of_edges('shares_user_with'),), 4, dtype=torch.float32)

# heterogeneous graph to heterogeneous graph, keep ndata and edata
hg = dgl.to_homogeneous(g, edata=['he'], ndata=['feat', 'label', 'train_mask', 'val_mask', 'test_mask'])
```

è®­ç»ƒã€æµ‹è¯•æ¨¡å‹ï¼

```python
# create GraphSAGE model
in_size = hg.ndata['feat'].shape[1]
out_size = 2
model = SAGE(in_size, 256, out_size).to(device)

# model training
print('Training...')
train(device, hg, model, X_train_idx, X_val_idx)

# test the model
print('Testing...')

acc = layerwise_infer(device, hg, X_test_idx, model, batch_size=4096)
print("Test Accuracy {:.4f}".format(acc.item()))

# è¿è¡Œç»“æœ
# Test Accuracy 0.9996
```

æœ‰äº†æ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒåºåˆ—åŒ–æˆæ–‡ä»¶ï¼Œåœ¨éœ€è¦çš„æ—¶å€™ï¼Œåªéœ€è¦æŠŠæ¨¡å‹çš„å½¢å¼å’Œè¿™ä¸ªåºåˆ—åŒ–æ–‡ä»¶å†åŠ è½½æˆä¸€ä¸ª pyTorch å°±å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ã€‚

```python
# save model
torch.save(model.state_dict(), "fraud_d.model")

# load model
device = torch.device('cpu')
model = SAGE(32, 256, 2).to(device)
model.load_state_dict(torch.load("fraud_d.model"))
```

æœ€åï¼Œæˆ‘ä»¬å¦‚ä½•æŠŠæ¨¡å‹æ”¾åˆ°æˆ‘ä»¬çš„çº¿ä¸Šæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿé‡Œå‘¢ï¼Ÿ

##### æ¨ç†æ¥å£

å‰è¾¹æåˆ°è¿‡ï¼Œ GraphSAGE æ˜¯æœ€ç®€å•çš„æ”¯æŒ Inductive Learning çš„æ¨¡å‹ï¼Œè€Œä¸Šè¾¹æˆ‘ä»¬çš„è®­ç»ƒæ¨ç†è¿‡ç¨‹å®é™…ä¸Šè¿˜ä¸æ˜¯è¿™æ ·çš„æˆ‘ä»¬çš„æµ‹è¯•å’Œè®­ç»ƒçš„å›¾æ˜¯åŒä¸€å¼ ï¼Œè™½ç„¶æ ‡æ³¨äº†è®­ç»ƒçš„ç‚¹çš„ç´¢å¼•ï¼Œä½†å®é™…ä¸Šæ˜¯æ•´å¼ å›¾ä½œä¸ºè¾“å…¥çš„ã€‚ä¸ºäº†åšåˆ°Inductive Learning æˆ‘ä»¬åªéœ€è¦æŠŠè®­ç»ƒå’Œæµ‹è¯•åˆ†æˆä¸¤ä¸ªæ— äº¤é›†çš„å­å›¾æ¥åšè®­ç»ƒå’Œæœ€ç»ˆæµ‹è¯•ï¼š

```python
# Inductive Learning, our test dataset are new nodes and new edges
hg_train = hg.subgraph(torch.cat([X_train_idx, X_val_idx]))

# model training
print('Training...')
train(device, hg_train, model, torch.arange(X_train_idx.shape[0]), torch.arange(X_train_idx.shape[0], hg_train.num_nodes()))

# test the model
print('Testing...')

hg_test = hg.subgraph(torch.cat([X_test_idx]))

sg_X_test_idx = torch.arange(hg_test.num_nodes())

acc = layerwise_infer(device, hg_test, sg_X_test_idx, model, batch_size=4096)
print("Test Accuracy {:.4f}".format(acc.item()))

# è¿è¡Œç»“æœ
# Test Accuracy 0.9990
```

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ä¸Šè¾¹çš„ä»£ç é‡Œï¼Œæµ‹è¯•æ‰€ç”¨åˆ°çš„å›¾å’Œè®­ç»ƒçš„å›¾æ˜¯å®Œå…¨ä¸åŒçš„ä¸¤ç»„æ•°æ®ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„çº¿ä¸Šç³»ç»Ÿå¯ä»¥æ˜¯ä¹‹å‰å®Œå…¨æ²¡æœ‰é‡åˆ°çš„æ•°æ®ï¼Œæˆ‘ä»¬åªè¦æŠŠå¯¹æ–°æ¥çš„ä¸€ä¸ªäº¤æ˜“è¯·æ±‚æ•°æ®å†™è¿› NebulaGraphï¼Œç„¶åå†ä»è¿™ä¸ªç‚¹è·å–ä¸€ä¸ªçº¿ä¸Šç³»ç»Ÿå¯ä»¥è¿”å›çš„å°å­å›¾ï¼Œå°±å¯ä»¥æŠŠå®ƒä½œä¸ºæ¨¡å‹æ¨ç†çš„è¾“å…¥ï¼Œè·å¾—å­å›¾çš„æ ‡ç­¾äº†ï¼

**æ–°çš„äº¤æ˜“è¯·æ±‚**ï¼š

è¿˜è®°å¾—æˆ‘ä»¬å‰è¾¹ç”»çš„çº¿ä¸Šæ¨ç†ç³»ç»Ÿçš„æµç¨‹å›¾ä¹ˆï¼Ÿ

```asciiarmor
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      
      â”‚                     â”‚                          â”‚                 â”‚
â”€â”€â”€â”€â”€â–¶â”‚ Transaction Record  â”œâ”€â”€â”€â”€â”€â”€2. Fraud Risk â”€â”€â”€â”€â”€â–¶â”‚  Inference API  â”‚â—€â”€â”€â”€â”€â”
      â”‚                     â”‚â—€â”€â”€â”€â”€Prediction with â”€â”€â”€â”€â”€â”¤                 â”‚     â”‚
      â”‚                     â”‚        Sub Graph         â”‚                 â”‚     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
           â”‚           â–²                                        â”‚              â”‚
           â”‚           â”‚                                        â”‚              â”‚
       0. Insert   1. Get New                              3.req: Node         â”‚
         Record    Record Sub                            Classification        â”‚
           â”‚         Graph                                      â”‚              â”‚
           â–¼           â”‚                                        â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      3.resp: â”‚
â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚                          Predictedâ”‚
â”‚â”‚   Graph of Historical Transactions   â”‚â”‚ â”‚                             Risk  â”‚
â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚                                   â”‚
â”‚                   .â”€.              .   â”‚ â”‚                                   â”‚
â”‚                  (   )â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€( )  â”‚ â”‚                                   â”‚
â”‚                   `â”€'              '   â”‚ â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  .       .â”€.       â•²             â—     â”‚ â”‚      â”‚ GNN Model Î›          â”‚     â”‚
â”‚ ( )â—€â”€â”€â”€â”€(   )       â•²           â•±      â”‚ â”‚  â”Œâ”€â”€â”€â”´â”€â”        â•± â•²      â”Œâ”€â”€â”´â”€â”€â”  â”‚
â”‚  '       `â”€'         â•²       . â•±       â”‚ â”‚  â”œâ”€â”€â”€â”€â”€â”¤       â•±   â•²     â”œâ”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â•²       â—€            â•²     ( )        â”‚ â””â”€â–¶â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–¶â–•     â”€â”€â”€â”€â”€â”œâ”€â”€â”€â”€â”€â”¤â”€â”€â”˜
â”‚   â•²  .  â•±              â—     '         â”‚    â”œâ”€â”€â”€â”€â”€â”¤       â•²   â•±     â”œâ”€â”€â”€â”€â”€â”¤   
â”‚    â—€( )â•±               .â”€.         .â”€. â”‚    â””â”€â”€â”€â”¬â”€â”˜        â•² â•±      â””â”€â”€â”¬â”€â”€â”˜   
â”‚      '                (   )â—€â”€â”€â”€â”€â”€â”€(   )â”‚        â”‚           V          â”‚      
â”‚                        `â”€'         `â”€' â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        
```

ç°åœ¨ï¼Œæˆ‘ä»¬å‡è®¾è¿™ä¸ªæ–°çš„äº¤æ˜“è¯·æ±‚å·²ç»å‘èµ·äº†ï¼Œè¿™æ¡äº¤æ˜“è®°å½•å·²ç»è¢«æ›´æ–°åœ¨å›¾è°±é‡Œäº†ï¼Œå’±ä»¬éšä¾¿å–ä¸€ä¸ªç‚¹ä½œä¸ºè¿™æ ·çš„è¯·æ±‚å§

```cypher
MATCH (n:`review`) RETURN n LIMIT 1
```

```cypher
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| (2048 :review{f0: 0.0, f1: 0.08034700155258179, f10: 0.3952670097351074, f11: 0.18671999871730804, f12: 0.2836120128631592, f13: 0.2843089997768402, f14: 0.38148200511932373, f15: 0.3816460072994232, f16: 0.9999740123748779, f17: 0.6430919766426086, f18: 0.9999740123748779, f19: 0.5051100254058838, f2: 0.12382200360298157, f20: 0.4940490126609802, f21: 0.7766339778900146, f22: 0.7705119848251343, f23: 0.9480599761009216, f24: 0.4032529890537262, f25: 0.12437800318002701, f26: 0.3184080123901367, f27: 0.5223879814147949, f28: 0.4278610050678253, f29: 0.343284010887146, f3: 0.42868199944496155, f30: 0.37313398718833923, f31: 0.328357994556427, f4: 0.9999849796295166, f5: 0.9999849796295166, f6: 0.9999849796295166, f7: 0.4850809872150421, f8: 0.454602986574173, f9: 0.8863419890403748, is_fraud: 0}) |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

å¥½ï¼Œå®ƒæ˜¯ 2048 è¿™ä¸ªç‚¹ã€‚å®ƒçš„ä¸‹ä¸€æ­¥æ˜¯ `1. Get New Record Subgraph` æˆ‘ä»¬æ¥è·å–å®ƒçš„å­å›¾ï¼š

```SQL
GET SUBGRAPH WITH PROP FROM 2048 YIELD VERTICES AS nodes, EDGES AS relationships;
```

å¯ä»¥çœ‹åˆ°è¿”å›çš„ç»“æœå…¶å®è¿˜æ˜¯å¾ˆå¤šçš„ï¼Œä¸è¿‡å¯¹äº NebulaGraph æ¥è¯´ï¼Œè¿™ä¸ªå­å›¾ç»“æœè¿”å›æ˜¯åœ¨ 10 ms å·¦å³è·å–çš„ï¼Œè¿™é‡Œæˆ‘å°±ä¸è´´å‡ºæ¥äº†ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ NebulaGraph Studio æˆ–è€… Explorer ä¸­å¯ä»¥æŠŠç»“æœæ¸²æŸ“å‡ºæ¥ï¼ˆå¯è§†åŒ–å±•ç¤ºçš„ Query å¯ä»¥å»æ‰ `WITH PROP` ï¼Œå¯ä»¥ç»™æµè§ˆå™¨çœç‚¹å†…å­˜ï¼‰ï¼Œç»“æœå°±æ›´å®¹æ˜“è®©äººè„‘ç†è§£äº†ï¼š

![](subgraph_console_view.webp)

![](subgraph_viz.webp)

ç°åœ¨æˆ‘ä»¬å°±æ¥å®ç°è¿™ä¸€æ­¥çš„ä»£ç å§ï¼Œå®ƒçš„è¾“å…¥æ˜¯ç‚¹çš„ idï¼š`vertex_id`ã€‚è¾“å‡ºæ˜¯ä¸€ä¸ª `dgl_graph`ï¼Œç”¨æ¥ä¼ ç»™æ¨ç†æ¥å£ã€‚

```python
# get SUBGRAPH of one node

import json
from torch import tensor
from dgl import DGLHeteroGraph, heterograph

from nebula3.gclient.net import ConnectionPool
from nebula3.Config import Config

config = Config()
config.max_connection_pool_size = 2
connection_pool = ConnectionPool()
connection_pool.init([('graphd', 9669)], config)

vertex_id = 2048
client = connection_pool.get_session('root', 'nebula')
r = client.execute_json(
    "USE yelp;"
    f"GET SUBGRAPH WITH PROP 2 STEPS FROM {vertex_id} YIELD VERTICES AS nodes, EDGES AS relationships;")

r = json.loads(r)
data = r.get('results', [{}])[0].get('data')
```

è¿™é‡Œæˆ‘ç”¨åˆ°äº† Nebula-Python è¿™ä¸ª NebulaGraph çš„ Python SDK/Clientï¼Œé€šè¿‡ `execute_json` æ‰§è¡Œè·å¾—äº†è¿™ä¸ªäº¤æ˜“çš„å­å›¾ã€‚ä¸‹ä¸€æ­¥ï¼Œå’±ä»¬éœ€è¦æŠŠå®ƒæ„é€ æˆä¸€ä¸ª `dgl_graph`ï¼š

```python
# create node and nodedata
node_id_map = {} # key: vertex id in NebulaGraph, value: node id in dgl_graph
node_idx = 0
features = [[] for _ in range(32)] + [[]]
for i in range(len(data)):
    for index, node in enumerate(data[i]['meta'][0]):
        nodeid = data[i]['meta'][0][index]['id']
        if nodeid not in node_id_map:
            node_id_map[nodeid] = node_idx
            node_idx += 1
            for f in range(32):
                features[f].append(data[i]['row'][0][index][f"review.f{f}"])
            features[32].append(data[i]['row'][0][index]['review.is_fraud'])


"""
- R-U-Rï¼šä¸¤ä¸ªè¯„ä»·ç”±åŒä¸€ä¸ªç”¨æˆ·å‘å‡º shares_user_with
- R-S-Rï¼šä¸¤ä¸ªè¯„ä»·æ˜¯åŒé¤å…åŒè¯„åˆ†ï¼ˆè¯„åˆ†å¯ä»¥æ˜¯1åˆ°5ï¼‰ shares_restaurant_rating_with
- R-T-Rï¼šä¸¤ä¸ªè¯„ä»·æ˜¯åŒé¤å…åŒæäº¤æœˆä»½ shares_restaurant_in_one_month_with
"""
rur_start, rur_end, rsr_start, rsr_end, rtr_start, rtr_end = [], [], [], [], [], []
for i in range(len(data)):
    for edge in data[i]['meta'][1]:
        edge = edge['id']
        if edge['name'] == 'shares_user_with':
            rur_start.append(node_id_map[edge['src']])
            rur_end.append(node_id_map[edge['dst']])
        elif edge['name'] == 'shares_restaurant_rating_with':
            rsr_start.append(node_id_map[edge['src']])
            rsr_end.append(node_id_map[edge['dst']])
        elif edge['name'] == 'shares_restaurant_in_one_month_with':
            rtr_start.append(node_id_map[edge['src']])
            rtr_end.append(node_id_map[edge['dst']])

data_dict = {}
if rur_start:
    data_dict[('review', 'shares_user_with', 'review')] = tensor(rur_start), tensor(rur_end)
if rsr_start:
    data_dict[('review', 'shares_restaurant_rating_with', 'review')] = tensor(rsr_start), tensor(rsr_end)
if rtr_start:
    data_dict[('review', 'shares_restaurant_in_one_month_with', 'review')] = tensor(rtr_start), tensor(rtr_end)

# construct a dgl_graph
dgl_graph: DGLHeteroGraph = heterograph(data_dict)
```

å®é™…ä¸Šæˆ‘å°±æ˜¯æŒ‰ç…§ DGL æ–‡æ¡£ï¼šhttps://docs.dgl.ai/en/0.9.x/generated/dgl.heterograph.htmlï¼Œä¸­çš„æ–¹å¼å»æ„é€  `data_dict`ï¼Œç„¶åç”¨ `heterograph()` å°±æŠŠç»“æœè½¬æ¢ä¸ºæƒ³è¦çš„ dgl_graph äº†ï¼Œå…¶ä¸­ `node_id_map` æ˜¯ NebulaGraph ä¹‹ä¸­ Vertex_ID åˆ°è¿™ä¸ªå¯¹è±¡ä¸­ node_id çš„å­—å…¸ã€‚

æœ€åï¼Œæˆ‘ä»¬å†æŠŠ node feature ä¹ŸåŠ è½½è¿›å»ã€‚

```python
# load node features to dgl_graph
for i in range(32):
    dgl_graph.ndata[f"f{i}"] = tensor(features[i])
dgl_graph.ndata['label'] = tensor(features[32])
```

åœ¨å¼€å§‹æ¨ç†ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æŠŠå®ƒè½¬æ¢æˆåŒæ„å›¾ï¼Œå’Œå‰è¾¹å®Œå…¨ä¸€æ ·ï¼š

```python
import torch


# to homogeneous graph
features = []
for i in range(32):
    features.append(dgl_graph.ndata[f"f{i}"])

dgl_graph.ndata['feat'] = torch.stack(features, dim=1)

dgl_graph.edges['shares_restaurant_in_one_month_with'].data['he'] = torch.ones(
    dgl_graph.number_of_edges('shares_restaurant_in_one_month_with'), dtype=torch.float32)
dgl_graph.edges['shares_restaurant_rating_with'].data['he'] = torch.full(
    (dgl_graph.number_of_edges('shares_restaurant_rating_with'),), 2, dtype=torch.float32)
dgl_graph.edges['shares_user_with'].data['he'] = torch.full(
    (dgl_graph.number_of_edges('shares_user_with'),), 4, dtype=torch.float32)


# heterogeneous graph to heterogeneous graph, keep ndata and edata
import dgl
hg = dgl.to_homogeneous(dgl_graph, edata=['he'], ndata=['feat', 'label'])
```

æœ€åï¼Œæˆ‘ä»¬çš„æ¨ç†æ¥å£å°±æ˜¯ï¼š

```python
def do_inference(device, graph, node_idx, model, batch_size):
    model.eval()
    with torch.no_grad():
        pred = model.inference(graph, device, batch_size) # pred in buffer_device
        return pred[node_idx]
```

æˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸€ä¸ªä¸€ä¸‹è¯•è¯•æ¨ç†æˆ‘ä»¬è¿™ä¸ªæ–°çš„ç‚¹ï¼š

```python
node_idx = node_id_map[vertex_id]
batch_size = 4096

result = do_inference(device, hg, node_idx, model, batch_size)
```

å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½åœ¨è¿™ä¸ªå°å­å›¾ä¸Šè®¡ç®—ä»–çš„æ­£ç¡®ç‡ï¼š

```python
def test_inference(device, graph, nid, model, batch_size):
    model.eval()
    with torch.no_grad():
        pred = model.inference(graph, device, batch_size) # pred in buffer_device
        pred = pred[nid]
        label = graph.ndata['label'][nid].to(pred.device)
        return MF.accuracy(pred, label)

node_idx = torch.tensor(list(node_id_map.values()))
acc = test_inference(device, hg, node_idx, model, batch_size=4096)
print("Test Accuracy {:.4f}".format(acc.item()))
```

è¾“å‡ºç»“æœï¼š

```python
In [307]: def test_inference(device, graph, nid, model, batch_size):
     ...:     model.eval()
     ...:     with torch.no_grad():
     ...:         pred = model.inference(graph, device, batch_size) # pred in buffer
     ...: _device
     ...:         pred = pred[nid]
     ...:         label = graph.ndata['label'][nid].to(pred.device)
     ...:         return MF.accuracy(pred, label)
     ...:
     ...: node_idx = torch.tensor(list(node_id_map.values()))
     ...: acc = test_inference(device, hg, node_idx, model, batch_size=4096)
     ...: print("Test Accuracy {:.4f}".format(acc.item()))
     ...:
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 130.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 152.29it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 173.55it/s]
Test Accuracy 0.9688
```

è¿™ä¸ªç¤ºä¾‹é¡¹ç›®çš„ä»£ç åœ¨ï¼š[github.com/wey-gu/NebulaGraph-Fraud-Detection-GNN](https://github.com/wey-gu/NebulaGraph-Fraud-Detection-GNN) ï¼Œå¦‚æœ‰é—®é¢˜æ¬¢è¿ç•™è¨€ã€ISSUEã€‚

### æ€»ç»“

æ€»ç»“èµ·æ¥ï¼Œæ¬ºè¯ˆæ£€æµ‹çš„æ–¹æ³•æœ‰ï¼š

- åœ¨ä¸€ä¸ªäº¤æ˜“å†å²ã€é£æ§çš„å›¾è°±ä¸Šï¼Œé€šè¿‡å›¾æ¨¡å¼æŸ¥è¯¢ç›´æ¥è·å¾—é£é™©æç¤º
- å®šæœŸåˆ©ç”¨å›¾ç®—æ³•æ‰©å……é£é™©æ ‡æ³¨ï¼Œå†™å›å›¾åº“
- å®šæœŸè®¡ç®—å›¾è°±ä¸­çš„å›¾ç‰¹å¾ï¼Œå’Œå…¶ä»–ç‰¹å¾ä¸€èµ·ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ç¦»çº¿é¢„æµ‹é£é™©
- å°†å›¾è°±ä¸­çš„å±æ€§å¤„ç†æˆä¸ºç‚¹ã€è¾¹ç‰¹å¾ï¼Œç”¨å›¾ç¥ç»ç½‘ç»œæ–¹æ³•ç¦»çº¿é¢„æµ‹é£é™©ï¼Œéƒ¨åˆ†å¯ä»¥ Inductive Learning çš„æ–¹æ³•ç»“åˆå›¾åº“å¯ä»¥å®ç°åœ¨çº¿é£é™©é¢„æµ‹

> Feature Image credit goes to https://unsplash.com/photos/BW0vK-FA3eg

