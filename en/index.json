[{"categories":["Nebula Graph"],"content":"What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know.","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/"},{"categories":["Nebula Graph"],"content":" What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know. In this article, I am trying walk you through all three Spark projects of Nebula Graph with some runnable hands-on examples. Also, I managed to make PySpark usable with Nebula Graph Spark Connector, which will be contributed to the Docs later. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:0:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#"},{"categories":["Nebula Graph"],"content":"1 The three Spark projects for Nebula GraphI used to draw a sketch around all data importing methods of Nebula Graph here, where all three of the Spark based Nebula Graph projects were already briefly introduced. In this article, instead, a slightly deeper dive on all of them will be made based on my recent work on them. TL;DR Nebula Spark Connector is a Spark Lib to enable spark application reading from and writing to Nebula Graph in form of dataframe. Nebula Exchange, built on top of Nebula Spark Connector, is a Spark Lib and Application to exchange(for Open Source version, it‚Äôs one way: write, whereas for enterprise version it‚Äôs bidirectional) different data sources like(MySQL, Neo4j, PostgreSQL, Clickhouse, Hive etc.). Besides write directly to Nebula Graph, it could optionally generate SST files to be ingested into Nebula Graph to off load the storage computation outside of the Nebula Graph cluster. Nebula Algorithm, built on top of Nebula Spark Connector and GraphX, is an Spark Lib and Application to run de facto graph algorithms(pagerank, LPA etc‚Ä¶) on a graph from Nebula Graph. Then let‚Äôs have the long verson of those spark projects more on how-to perspectives. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:1:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#the-three-spark-projects-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"2 Spark-Connector Codebase: https://github.com/vesoft-inc/nebula-algorithm Documentation: https://docs.nebula-graph.io/3.0.2/nebula-algorithm/ (it‚Äôs versioned, as for now, I put the lastest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ Code Examples: example/src/main/scala/com/vesoft/nebula/algorithm ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#spark-connector"},{"categories":["Nebula Graph"],"content":"2.1 Nebula Graph Spark ReaderTo read data from Nebula Graph, i.e. vertex, Nebula Spark Connector will scan all storage instances who hold given label(TAG): withLabel(\"player\"), and we could optionally specify the properties of the vertex: withReturnCols(List(\"name\", \"age\")). With needed configuration being provided, a call of spark.read.nebula.loadVerticesToDF will return dataframe of the Vertex Scan call towards Nebula Graph: def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColumn(false) .withReturnCols(List(\"name\", \"age\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() vertex.printSchema() vertex.show(20) println(\"vertex count: \" + vertex.count()) } It‚Äôs similar for the writor part and one big difference here is the wrinting path is done via GraphD as underlying Spark Connector is shooting nGQL INSERT queries. Then let‚Äôs do a hands-on end to end practise. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#nebula-graph-spark-reader"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it‚Äôs assumed below procedure being run on a Linux Machine with internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let‚Äôs deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script being executed, let‚Äôs connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minitues to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it‚Äôs quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In above one line command, we created a container named spark-master-0 with a built-in hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access to the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn which /usr/lib/mvn/bin/mvn 2.2.3 Run spark connector exampleLet‚Äôs clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .w","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-spark-connector"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it‚Äôs assumed below procedure being run on a Linux Machine with internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let‚Äôs deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script being executed, let‚Äôs connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minitues to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it‚Äôs quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In above one line command, we created a container named spark-master-0 with a built-in hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access to the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn which /usr/lib/mvn/bin/mvn 2.2.3 Run spark connector exampleLet‚Äôs clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .w","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#bootstrap-a-nebula-graph-cluster"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it‚Äôs assumed below procedure being run on a Linux Machine with internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let‚Äôs deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script being executed, let‚Äôs connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minitues to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it‚Äôs quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In above one line command, we created a container named spark-master-0 with a built-in hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access to the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn which /usr/lib/mvn/bin/mvn 2.2.3 Run spark connector exampleLet‚Äôs clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .w","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#create-a-spark-playground"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it‚Äôs assumed below procedure being run on a Linux Machine with internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let‚Äôs deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script being executed, let‚Äôs connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minitues to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it‚Äôs quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In above one line command, we created a container named spark-master-0 with a built-in hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access to the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn which /usr/lib/mvn/bin/mvn 2.2.3 Run spark connector exampleLet‚Äôs clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .w","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#run-spark-connector-example"},{"categories":["Nebula Graph"],"content":"3 Exchange Codebase: https://github.com/vesoft-inc/nebula-exchange/ Documentation: https://docs.nebula-graph.io/3.0.2/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ (it‚Äôs versioned, as for now, I put the lastest released version 3.0.2 here) Jar Package: https://github.com/vesoft-inc/nebula-exchange/releases Configuration Examples: exchange-common/src/test/resources/application.conf Nebula Exchange is a Spark Lib/App to read data from multiple sources, then, write to either Nebula Graph directly or into Nebula Graph SST Files. The way to leverage Nebula Exchange is only to firstly create the configuration files to let exchange know how data should be fetched and written, then call the exchange package with the conf file specified. Now let‚Äôs do a hands on test with the same envrioment created in last chapter. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#exchange"},{"categories":["Nebula Graph"],"content":"3.1 Hands-on ExchangeHere, we are using Exchange to consume data source from a CSV file, where first column is Vertex ID, and second, third to be properties of ‚Äúname‚Äù and ‚Äúage‚Äù. player800,\"Foo Bar\",23 player801,\"Another Name\",21 Let‚Äôs get into the spark container created in last chapter, and download the Jar package of Nebula Exchange: docker exec -it spark-master bash cd /root/ wget https://github.com/vesoft-inc/nebula-exchange/releases/download/v3.0.0/nebula-exchange_spark_2.4-3.0.0.jar Create a conf file named exchange.conf in format HOCON, where: under .nebula, information regarding Nebula Graph Cluster were configured under .tags, information regarding Vertecies like how required fields are reflected to our data source(here it‚Äôs CSV file) were configured { # Spark relation config spark: { app: { name: Nebula Exchange } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory: 1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"graphd:9669\"] meta:[\"metad0:9559\", \"metad1:9559\", \"metad2:9559\"] } user: root pswd: nebula space: basketballplayer # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://localhost:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is client, just change type.sink to sst if you want to use client import mode. { name: player type: { source: csv sink: client } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } Finally, let‚Äôs create player.csv and exchange.conf, it should be listed as the following: # ls -l -rw-r--r-- 1 root root 1912 Apr 19 08:21 exchange.conf -rw-r--r-- 1 root root 157814140 Apr 19 08:17 nebula-exchange_spark_2.4-3.0.0.jar -rw-r--r-- 1 root root 52 Apr 19 08:06 player.csv And we could call the exchange as: /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange_spark_2.4-3.0.0.jar \\ -c exchange.conf And the result should be like ... 22/04/19 08:22:08 INFO Exchange$: import for tag player cost time: 1.32 s 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchFailure.player: 0 ... Please refer to documentation and conf examples for more datasources. For hands on Exchange writing to SST Files, you could refer to both Documentation and Nebula Exchange SST 2.x Hands-on Guide. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-exchange"},{"categories":["Nebula Graph"],"content":"4 Algorithm Codebase: https://github.com/vesoft-inc/nebula-exchange/ Documentation: https://docs.nebula-graph.io/3.0.2/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ (it‚Äôs versioned, as for now, I put the lastest released version 3.0.2 here) Jar Package: https://github.com/vesoft-inc/nebula-exchange/releases Calling Algorithm in code examples: exchange-common/src/test/resources/application.conf Conf exampls for calling with submit: nebula-algorithm/src/main/resources/application.conf ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#algorithm"},{"categories":["Nebula Graph"],"content":"4.1 Calling with spark submitWhen we call Nebula Algorithm with spark submit, on how to use perspective, it is quite similar to Exchange. This post already showed us how to do that in actions. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-with-spark-submit"},{"categories":["Nebula Graph"],"content":"4.2 Calling as a lib in codeOn the other hands, we could call Nebula Algorithm in spark as a Spark Lib, the gain will be: More control/customization on the output format of the algorithm Possbile to perform algorithm for non-numerical vertex ID cases, see here ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-as-a-lib-in-code"},{"categories":["Nebula Graph"],"content":"5 PySpark for Nebula GraphPySpark comes with capability to call java/scala packages inside python, thus it‚Äôs also quite easy to use Spark Connector with Python. Here I am doing this from the pyspark entrypoint in /spark/bin/pyspark, with connector‚Äôs Jar package specified with --driver-class-path and --jars docker exec -it spark-master-0 bash cd root wget https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/3.0.0/nebula-spark-connector-3.0.0.jar /spark/bin/pyspark --driver-class-path nebula-spark-connector-3.0.0.jar --jars nebula-spark-connector-3.0.0.jar Then, rather than pass NebulaConnectionConfig and ReadNebulaConfig to spark.read.nebula, we should instead call spark.read.format(\"com.vesoft.nebula.connector.NebulaDataSource\"). Voil√†! df = spark.read.format(\"com.vesoft.nebula.connector.NebulaDataSource\").option(\"type\", \"vertex\").option(\"spaceName\", \"basketballplayer\").option(\"label\", \"player\").option(\"returnCols\", \"name,age\").option(\"metaAddress\", \"metad0:9559\").option(\"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows Below are how I figured out how to make this work with almost zero scala knowledge :-P. How reader should be called def loadVerticesToDF(): DataFrame = { assert(connectionConfig != null \u0026\u0026 readConfig != null, \"nebula config is not set, please call nebula() before loadVerticesToDF\") val dfReader = reader .format(classOf[NebulaDataSource].getName) .option(NebulaOptions.TYPE, DataTypeEnum.VERTEX.toString) .option(NebulaOptions.SPACE_NAME, readConfig.getSpace) .option(NebulaOptions.LABEL, readConfig.getLabel) .option(NebulaOptions.PARTITION_NUMBER, readConfig.getPartitionNum) .option(NebulaOptions.RETURN_COLS, readConfig.getReturnCols.mkString(\",\")) .option(NebulaOptions.NO_COLUMN, readConfig.getNoColumn) .option(NebulaOptions.LIMIT, readConfig.getLimit) .option(NebulaOptions.META_ADDRESS, connectionConfig.getMetaAddress) .option(NebulaOptions.TIMEOUT, connectionConfig.getTimeout) .option(NebulaOptions.CONNECTION_RETRY, connectionConfig.getConnectionRetry) .option(NebulaOptions.EXECUTION_RETRY, connectionConfig.getExecRetry) .option(NebulaOptions.ENABLE_META_SSL, connectionConfig.getEnableMetaSSL) .option(NebulaOptions.ENABLE_STORAGE_SSL, connectionConfig.getEnableStorageSSL) if (connectionConfig.getEnableStorageSSL || connectionConfig.getEnableMetaSSL) { dfReader.option(NebulaOptions.SSL_SIGN_TYPE, connectionConfig.getSignType) SSLSignType.withName(connectionConfig.getSignType) match { case SSLSignType.CA =\u003e dfReader.option(NebulaOptions.CA_SIGN_PARAM, connectionConfig.getCaSignParam) case SSLSignType.SELF =\u003e dfReader.option(NebulaOptions.SELF_SIGN_PARAM, connectionConfig.getSelfSignParam) } } dfReader.load() } How Option String should be like # object NebulaOptions { /** nebula common config */ val SPACE_NAME: String = \"spaceName\" val META_ADDRESS: String = \"metaAddress\" val GRAPH_ADDRESS: String = \"graphAddress\" val TYPE: String = \"type\" val LABEL: String = \"label\" /** connection config */ val TIMEOUT: String = \"timeout\" val CONNECTION_RETRY: String = \"connectionRetry\" val EXECUTION_RETRY: String = \"executionRetry\" val RATE_TIME_OUT: String = \"reteTimeOut\" val USER_NAME: String = \"user\" val PASSWD: String = \"passwd\" val ENABLE_GRAPH_SSL: String = \"enableGraphSSL\" val ENABLE_META_SSL: String = \"enableMetaSSL\" val ENABLE_STORAGE_SSL: String = \"enableStorageSSL\" val SSL_SIGN_TYPE: String = \"sslSignType\" val CA_SIGN_PARAM: String = \"caSignParam\" val SELF_SIGN_PARAM: String = \"selfSignParam\" /** read config */ val RETURN_COLS: String = \"returnCols\" val NO_COLUMN: String = \"noColumn\" val PARTITION_NUMBER: String = \"partitionNumber\" val LIMIT: String = \"limit\" /** write config */ val RATE_LIMIT: String = \"rateLimit\" val VID_POLICY: String = \"vidPolicy\" val SRC_POLICY: String = \"srcPolicy\" val DST_POLICY: String = \"d","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:5:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#pyspark-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"Running Nebula Graph Database on ARM64 Single Board Computer/Respberry Pi","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/"},{"categories":["Nebula Graph"],"content":" With the ARM64 Docker Image of Nebula Graph, it‚Äôs actually quite easy to run it on SBC/Respberry Pi! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:0:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#"},{"categories":["Nebula Graph"],"content":"1 BackgroundRecently, after Yee from Nebula Graph Community fixed ARM build on nebula-third-party#37, we could play with Nebula Graph on M1 Chip Macbook. While, I didn‚Äôt get the chance to run it on a SBC/Pi. A couple of weeks before, in a twitter thread with @laixintao and @andelf I decided to purchase a Rock Pi 3A: And it looks nice!(Even come with a NPU inside) ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:1:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#background"},{"categories":["Nebula Graph"],"content":"2 The Guide of running Nebula Graph on a Pi SBC Actually, since v3.0.0, Nebula comes with a standalone version, which suits the deep edge deployment more, but today, I will only setup the cluster version as the Docker Image is out of box to be used. I will share more on standalone version in upcoming weeks. I put the Ubuntu Server installation steps in the appendix, and now I assumed we already have an ARM64 Linux up and running on a Pi SBC. ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#the-guide-of-running-nebula-graph-on-a-pi-sbc"},{"categories":["Nebula Graph"],"content":"2.1 Step 0, Install Docker-Compose on PiI am using debian/ubuntu here, while it should be the same for other distros, referring to https://docs.docker.com/engine/install/. sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release echo \\ \"deb [arch=$(dpkg --print-architecture)signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # follow https://docs.docker.com/engine/install/linux-postinstall/ sudo groupadd docker sudo usermod -aG docker $USER exit # login again newgrp docker After Docker being installed, we install compose here, there could be issues encounterred from the Docker website on Compose installation. While, due to compose is just a python package, let‚Äôs do it via python3-pip install: sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:1","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-0-install-docker-compose-on-pi"},{"categories":["Nebula Graph"],"content":"2.2 Step 1, bootstrap Nebula Graph Cluster on PiWe clone the compose file for nebula cluster first: git clone https://github.com/vesoft-inc/nebula-docker-compose.git \u0026\u0026 cd nebula-docker-compose docker-compose up -d Then, let‚Äôs download the client: nebula-console, and connect to the GraphD service: wget https://github.com/vesoft-inc/nebula-console/releases/download/v3.0.0/nebula-console-linux-arm64-v3.0.0 chmod +x nebula-console-linux-arm64-v3.0.0 ./nebula-console-linux-arm64-v3.0.0 -addr localhost -port 9669 -u root -p nebula Activate the storageD services: ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:2","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-1-bootstrap-nebula-graph-cluster-on-pi"},{"categories":["Nebula Graph"],"content":"2.3 Step 2, Play Nebula Graph on PiWIth the SHOW HOSTS we should see StorageD services are all ONLINE, then we could run this from the console session to load the test dataset. Referennce: https://docs.nebula-graph.io/3.0.1/nebula-console/#import_a_testing_dataset $:play basketballplayer; The test data will be loaded in around 1 minute. Then, we could query something like: USE basketballplayer; GO FROM \"player100\" OVER follow YIELD dst(edge); Check this out and‚Ä¶ Happy Graphing! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:3","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-2-play-nebula-graph-on-pi"},{"categories":["Nebula Graph"],"content":"3 Appendix: Installing Ubuntu Server on Rock Pi 3A SBC Get the image from https://wiki.radxa.com/Rock3/downloads decompressing the file into .img Write the image to a micro SD card with etcher Boot it! feature image credit: @_louisreed ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:3:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#appendix-installing-ubuntu-server-on-rock-pi-3a-sbc"},{"categories":["Nebula Graph"],"content":"Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph?","date":"2022-02-28","objectID":"/en/resolve-wordle/","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/"},{"categories":["Nebula Graph"],"content":" Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph? ","date":"2022-02-28","objectID":"/en/resolve-wordle/:0:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#"},{"categories":["Nebula Graph"],"content":"1 BackgroundYou may have seen tweets like this in past months, where the color dots in emoji was shared in SNS randomly. Feel free to Google Wordle first if you don‚Äôt know its meaning yet. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#background"},{"categories":["Nebula Graph"],"content":"1.1 Wordle SolverFor all magics being used to solve wordle, I am impressed by Grant Sanderson, who explained us the information theory when solving wordle, in an elegent and delightful way. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#wordle-solver"},{"categories":["Nebula Graph"],"content":"1.2 Chinese wordle: \u0026ldquo;handle\u0026rdquo;I am not going create yet another wordle-solver today, instead, it‚Äôs more about an intresting variant of wordle. To truly enjoy the fun of wordle, mostly we should be a native speaker, and it is not surprising that there is a Spanish wordle out there, and still tweets on wordle(es) are being shared literially every second now. While for non alphabetic languages like Chineses, do we have the luck to have fun with wordling? The answer is yes, while it‚Äôs a bit different. For the reason Chinese charactors, also called Hanzi or Han chactors are in from of Logogram, each charactor in Chinese is made up of radicals in quite different forms, each Chinese word can be 2/3/4 charactors. Most of the crossword games in Chinese are idiom, proverb or ancient poetry based: the slot to be filled are one Chinese. ref: Chinese idiom puzzle Thus, a wordle but in Chinese idiom will be quite strange, or even hilarious as Chee and JinGen discussed in this tweet thread, where you could see the candidate characters are counted in thousands: there is no chance to guess the idiom in 10 attempts! As one of the fun on playing wordle could be the feedback loop of guess --\u003e result in limited attempts, while the scale of Chinese Charactor had pushed the Chinese wordle variant creators leveraging other aspects of the charactor: pronunciation. Each charactor in Chinease is monosyllabic without exceptions, and when it comes to its pronunciation, they are all made up from two parts(initial and final, and they could be written in roman letters), which comes in tens level of possbilities. There are bounch of Chinese wordle varients asking player to guess idiom leveraging pinyin: https://pinyincaichengyu.com/ https://cheeaun.github.io/chengyu-wordle/ https://apps.apple.com/cn/app/id1606194420 While, to me, a native Chinese speaker, it‚Äôs either too hard to play with condtions of pronunciation parts(pinyin) or too easy to guess on given around 20 Chinese charactors. Then, the varient stands out here is the ‚Äúhandle/Ê±âÂÖú\"(Hanzi-Wordle) created by Antfu. ‚ÄúHandle‚Äù introduced the tones with genius to add an extra dimension of all charactors per each guess attempt, which helped player to have more information on filtering the knowledge in the brain. Note, for each Chinese charactor, there will be a tone in 1 of 4 tones in its pronunciation. Let‚Äôs see what it‚Äôs like to play the ‚ÄúHandle‚Äù: There will be 4 Chinese Charactors to be filled in 10 times of guess Not only the charactor self will be colored in result: For example in first line, the green ‚ÄúÈó®‚Äù in position 2 is correct whereas in second line, the orange ‚Äú‰ªì‚Äù is corret while the possition should be all but not the first slot. There will be extra hints on: Pinyin parts for both part1(initial) and part2(final) In third line of the boxes, the green ‚Äúqiao‚Äù refers to the first charactor is ponouced in ‚Äúqiao‚Äù with initial:‚Äúq‚Äù and final:‚Äúiao‚Äù, although we filled the wrong charactor in the writing dimension. In third line, the orange ‚Äúuo‚Äù refers to there is one chacarctor in other poisition with the final part of the pinyin as ‚Äúuo‚Äù. Tones of the charactor: In third line, the green ‚Äú-‚Äù stands for the third charactor is in tone-1. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-wordle-handle"},{"categories":["Nebula Graph"],"content":"1.3 The Chinese Wordle HelperAs a non-native English speaker, the way I was playing wordle is to cheating relying on helpers: After knowing on the third letter to be ‚ÄúO‚Äù, I googled and got this: 5-letter-words-with-o-in-the-middle and do the searching üòÅ. The way to play with helpers works for me to have fun yet not ruin it by an automated cheat resolver(it‚Äôs only simulating my brain as a native-speaker!), so that I could somehow experience the same as Millions of people out there without cheating. While for Chinese ‚ÄúHandle‚Äù players, from my perspective, it‚Äôs still a bit harder(to find answers in 10 guesses), and the way my wife and I were playing ‚ÄúHandle‚Äù when lining up at the restaurant door ended up googling: idiom list with word ‚Äòfoo‚Äô, yet still having a lot of fun. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-chinese-wordle-helper"},{"categories":["Nebula Graph"],"content":"2 Chinese idiom Knowledge GraphCould I create something between the human brain and the game-cheater/ruiner to make it more of fun? The answer is yes: a game extension as a secondary brain. For this helper/secondary brain, the solution for ‚Äúhandle‚Äù differenciates from the English wordle, unlike the auto-solver, similar algorithms could help on both cases: In wordle(English), player searches in their brain or from a helper like the web page: 5-letter-words-with-o-in-the-middle. In handle(Chinese), it‚Äôs harder to be searching based on hints like tones/initial parts of pinyin in fulltext webpage searching anymore, the reason hehind is that the multidimensional filter condtions are not indexed by normal webpages. As I mentioned, the key of the helper to be leveraged to (not ruining the game) is to be the extension of the brain, then the question is: how does our brain work on handling the knowledge of ‚Äúhandle‚Äù(yes, I was preparing for this pun for so long!)? Thus, why not do it in a graph/neural network way? And here we go, let‚Äôs create a knowledge graph of Chinese idiom and see how it goes with the ‚Äúhandle‚Äù game. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-idiom-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.1 TL;DRIt‚Äôs indeed entertaining to me, and I could write Graph Queries[*] by hand or via Visualization tools[**] to help explore things in this graph, because I can we‚Äôre doing the ‚Äúthinking‚Äù process the similar way in our own brain, but not so well-informed. # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH (char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC ** ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#tldr"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the ‚Äúhandle‚Äù game üëâüèª https://handle.antfu.me/. We could start with one guess i.e. ‚ÄúÁà±ÊÜéÂàÜÊòé‚Äù. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as ‚Äúai‚Äù, but is not ‚ÄúÁà±‚Äù There is one Character in tone-1 not in 2nd position There is one Character with final part as ‚Äúing‚Äù, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH (char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"ÊÉäÊÑöÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"ÊÉä‰∏ñÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"ÊÉäËßÅÈ™áÈóª\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"Ê≤ΩÂêçÂçñÁõ¥\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"ÊÉäÂøÉÈ™áÁ•û\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"ËçÜÊ£òËΩΩÈÄî\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"Âá∫ÂçñÁÅµÈ≠Ç\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be ‚ÄúÊÉä‰∏ñÈ™á‰øó‚Äù, and let‚Äôs give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH p0=(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click ‚ÄúView Subgraphs‚Äù to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-long-version-of-playing-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the ‚Äúhandle‚Äù game üëâüèª https://handle.antfu.me/. We could start with one guess i.e. ‚ÄúÁà±ÊÜéÂàÜÊòé‚Äù. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as ‚Äúai‚Äù, but is not ‚ÄúÁà±‚Äù There is one Character in tone-1 not in 2nd position There is one Character with final part as ‚Äúing‚Äù, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH (char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"ÊÉäÊÑöÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"ÊÉä‰∏ñÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"ÊÉäËßÅÈ™áÈóª\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"Ê≤ΩÂêçÂçñÁõ¥\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"ÊÉäÂøÉÈ™áÁ•û\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"ËçÜÊ£òËΩΩÈÄî\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"Âá∫ÂçñÁÅµÈ≠Ç\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be ‚ÄúÊÉä‰∏ñÈ™á‰øó‚Äù, and let‚Äôs give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH p0=(char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click ‚ÄúView Subgraphs‚Äù to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#play-handle-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the ‚Äúhandle‚Äù game üëâüèª https://handle.antfu.me/. We could start with one guess i.e. ‚ÄúÁà±ÊÜéÂàÜÊòé‚Äù. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as ‚Äúai‚Äù, but is not ‚ÄúÁà±‚Äù There is one Character in tone-1 not in 2nd position There is one Character with final part as ‚Äúing‚Äù, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH (char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"ÊÉäÊÑöÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"ÊÉä‰∏ñÈ™á‰øó\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"ÊÉäËßÅÈ™áÈóª\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"Ê≤ΩÂêçÂçñÁõ¥\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"ÊÉäÂøÉÈ™áÁ•û\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"ËçÜÊ£òËΩΩÈÄî\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"Âá∫ÂçñÁÅµÈ≠Ç\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be ‚ÄúÊÉä‰∏ñÈ™á‰øó‚Äù, and let‚Äôs give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"Áà±\" MATCH p0=(char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"Áà±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click ‚ÄúView Subgraphs‚Äù to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-visualization-of-the-query"},{"categories":["Nebula Graph"],"content":"3 What\u0026rsquo;s NextIf you happened to get hands dirty(or being intrested to) on Graph Database, you could checkout the Nebula Graph project now and its Docs to have more fun of it! Also, here are some only playgrounds if you prefer to try it without deployment on your own envrioment. If you are intrested in the MATCH query syntax and would like to actually do exersices with each daily handle challenge, check below Documents: MATCH https://docs.nebula-graph.io/3.0.1/3.ngql-guide/7.general-query-statements/2.match/ Graph Patterns https://docs.nebula-graph.io/3.0.1/3.ngql-guide/1.nGQL-overview/3.graph-patterns/ nGQL command cheatsheet https://docs.nebula-graph.io/3.0.1/2.quick-start/6.cheatsheet-for-ngql/ Happy Graphing! ","date":"2022-02-28","objectID":"/en/resolve-wordle/:3:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#whats-next"},{"categories":["Nebula Graph"],"content":"4 Appendix: Setting up the Knowledge GraphI put the code and process here: https://github.com/wey-gu/chinese-graph, feel free to check that out. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#appendix-setting-up-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.1 Build the Knowledge GraphThe process would be: Modeling the Knowledge Preprocessing the data ETL data to a Graph Database: Nebula Graph Have fun on Nebula Graph ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#build-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.2 Modeling the Knowledge GraphFor Graph Modeling, it‚Äôs actually quite straight forward, the mind model for me was to put the knowledge I cares as vertcies and connect them with their relationships first. You will come back to iterate or optimize the modeling when you are actually playing with the data afterwards, thus, if you could imagine how the graph will be queried in the first place, the graph modeling could be adopted accordingly. Otherwise, don‚Äôt over design it, just do it the intuitive way. Here, I put the vertices with properties as: idiom character pinyin tone pinyin_part type The edges with properteis as: with_character with_pinyin with_pinyin_part ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#modeling-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.3 Deploy Nebula Graph With Nebula-UP, it‚Äôs an onliner call curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#deploy-nebula-graph"},{"categories":["Nebula Graph"],"content":"4.4 Load data # clone the code for data genration and data loading git clone https://github.com/wey-gu/chinese-graph.git \u0026\u0026 cd chinese-graph python3 graph_data_generator.py # generate data # load data with Nebula-Importer docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:4","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#load-data"},{"categories":["Nebula Graph"],"content":"Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index?","date":"2022-02-20","objectID":"/en/nebula-index-explained/","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/"},{"categories":["Nebula Graph"],"content":" Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index? The term of Nebula Graph Index is quite similar to index in RDBMS, while, they are not the same. It‚Äôs noticed when getting started with Nebula Graph, the index confused some of the users in first glance on What exactly Nebula Graph Index is. When I should use it. How it impacts the performance. Today I‚Äôm gonna walk you through the index in Nebula Graph. Let‚Äôs get started! ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:0:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#"},{"categories":["Nebula Graph"],"content":"1 What exactly Nebula Graph Index isTL;DR, Nebula Graph Index is only to be used to enable pure-prop-condition queries Not for graph walking through edges. It‚Äôs an prerequisite for such query. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#what-exactly-nebula-graph-index-is"},{"categories":["Nebula Graph"],"content":"1.1 pure-prop-condition queriesWe know that in RDBMS, an INDEX is to create a duplicated sorted DATA to enable QUERY with condition filtering on the sorted data, to accelerate the query in read and involves extra writes during the write. Note: in RDBMS/Tabular DB, an INDEX on some columns means to create extra data that are sorted on those columns to make query with those columns' condition to be scanned faster, rather than scanning from the original table data sorted based on the key only. In Nebula Graph, the INDEX is to create a duplicated sorted Vertex/Edge PROP DATA to enable the part of QUERY(it‚Äôs a must rather than accelerate it) like The following to GET data from PROP Conditions: or I call it the pure-prop-condition queries: ####QueriesrelyingonNebulaGraphIndex#query0pure-prop-conditionqueryLOOKUPONtag1WHEREcol1\u003e1ANDcol2==\"foo\"\\YIELDtag1.col1ascol1,tag1.col3ascol3;#query1pure-prop-conditionqueryMATCH(v:player{name:'Tim Duncan'})--\u003e(v2:player) \\ RETURNv2.player.nameASName; The pure-prop-condition queries, like above nGQL lines are literally to ‚ÄúFind VID/EDGE only based on given the propertiy condtions‚Äù. On the contrary, the following, are not pure-prop-condition queries: ####QueriesnotbasedonNebulaGraphIndex#query2,walkquerystartingfromgivenvertexVID:\"player100\"GOFROM\"player100\"OVERfollowREVERSELY\\YIELDsrc(edge)ASid|\\GOFROM$-.idOVERserve\\WHEREproperties($^).age\u003e20\\YIELDproperties($^).nameASFriendOf,properties($$).nameASTeam;#query3,walkquerystartingfromgivenvertexVID:\"player101\"or\"player102\"MATCH(v:player{name:'Tim Duncan'})--(v2) \\ WHEREid(v2)IN[\"player101\",\"player102\"]\\RETURNv2.player.nameASName; If we look into query 1 and query 3, where condition on vertex on tag:player are both { name: 'Tim Duncan' } though: For query 3 , the index is not required as the query will be started from known vertex ID in [\"player101\", \"player102\"] and thus: It‚Äôll directly fetch vertex Data from v2‚Äôs vertex IDs then to GetNeighbors(): walk through edges of v2, GetVertices() for next hop: v and filter based on property: name For query 1 , the query has to start from v due to no known vertex IDs were provided: It‚Äôll do IndexScan() first to find all vertices only with property condtion of { name: 'Tim Duncan' } Then, GetNeighbors(): walk through edges of v, GetVertices() for next hop: v2 Now, we could know the whole point is on whether to know the vertexID(s). You could check their execution plan with PROFILE or EXPLAIN like the follow: query 1, requires/based on index(on tag: player), pure prop condition query query 3, no index required, query starting from known vertex IDs ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:1","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#pure-prop-condition-queries"},{"categories":["Nebula Graph"],"content":"1.2 Why Nebula Graph index is a must in pure-prop-condition queriesIt‚Äôs because Nebula Graph stores data in a distributed and graph-oriented way, the full scan of data was condiser too expensive to be allowed(index not found will occur when it‚Äôs not created in pure-prop-condition queries). Note: from v3.0, it‚Äôs possible to do TopN Scan without INDEX, where the LIMIT \u003cn\u003e is used, this is different from the fullscan case(INDEX is a must), which will be explained later. MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name LIMIT 3; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:2","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-nebula-graph-index-is-a-must-in-pure-prop-condition-queries"},{"categories":["Nebula Graph"],"content":"1.3 Why pure-prop-condition queries only(requiring index)graph-queries vs pure-prop-condition queries graph-queries, see query 2 and query 3, are to walk through edges all the way for the given vertices pure-prop-condition queries, see query 0 and query 1, are to find vertex only on given prop condtions In Nebula Graph, the data is structured in a way to enable fast graph-queries, and is already indexed/sorted on vertex ID(for both vertex and edge) in raw data, where GetNeighbors() of given vertex is cheap and fast due to the locality/stored continuously(pysically linked). So in summary: Nebula Graph Index is sorted prop data to find vertex or edge on given pure prop conditions. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:3","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-pure-prop-condition-queries-onlyrequiring-index"},{"categories":["Nebula Graph"],"content":"2 Facts on Nebula Graph IndexTo understand more details/limitations/cost of Nebula, let‚Äôs reveal more on its design and here are some facts: Index Data is stored and sharded together with Vertex Data Only Left Match: It‚Äôs RocksDB Prefix Scan under the hood Cost: Write Path: Extra Data + Extra Read Read Path: RBO, Fan Out, TopN Push Down Data Full Scan TopN Sample(not fullscan) is supported w/o Index LOOKUP ON t YIELD t.name | LIMIT 1 MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name LIMIT 3; The key info can be seen from one of my sketch notes: We should notice that only the left match is supported in pure-prop-condition queries. For queries like wildcard or reguler-expression, Full-text Index/Search is to be used, which leveraged an external elastic search integrated with nebula: please check Nebula Graph Full text index for more. With this sketch note, we could see Local Index Design The index is stored and shared locally together with the graph data. It‚Äôs sorting based on prop value, and the matching is underlying a rocksDB prefix scan, that‚Äôs why only left match is supported() Write path The index enables the RDBMS-like Prop Condition Query with cost in the write path including not only the extra write, but also, random read, to ensure the data consistency. Index Data write is done in a sync way Read path In pure-prop-condition queries, in GraphD, the index will be selected with Rule-based-optimization like this example, where, in a rule, the col2 to be sorted first is considered optimal with the condition: col2 equals ‚Äòfoo‚Äô. After the index was chosen, index-scan request will be fanout to storageD instances, and in the case of filters like LIMIT N, it will be pushed down to the storage side to reduce data payload. Note: not shown in the sketch but actually from v3.0, the nebula graph allows LIMIT N Sample Prop condition query like this w/o index, which is underlying pushing down the LIMIT filter to storage side. Take aways: Use index only when we have to, as it‚Äôs costly in write cases and if limit N sample is allowed and fast enough, we can use that instead, if not: use index. Index is left match composite index order matters, should be created carefully. for full-text search use case, use full-text index instead. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:2:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#facts-on-nebula-graph-index"},{"categories":["Nebula Graph"],"content":"3 How to use the indexWe should always refer to the documentation, and I just put some highlights on this here: To create an index on a tag or edge type to specify a list of props in the order that we need. CREATE INDEX If an index was created after existing data was inserted, we need to trigger an index async rebuild job, as the index data will be written in sync way only when index is created. REBUILD INDEX We can see the index status after REBUILD INDEX issued. SHOW INDEX STATUS Queries levering index could be LOOKUP, and with the pipeline, in most cases we will do follow-up graph-walk queries like: LOOKUPONplayer\\WHEREplayer.name==\"Kobe Bryant\"\\YIELDid(vertex)ASVertexID,properties(vertex).nameASname|\\GOFROM$-.VertexIDOVERserve\\YIELD$-.name,properties(edge).start_year,properties(edge).end_year,properties($$).name; Or in MATCH query like this, under the hood, v will be searched on index and v2 will be walked by default graph data structure without involving index. MATCH (v:player{name:\"Tim Duncan\"})--\u003e(v2:player) \\ RETURN v2.player.name AS Name; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:3:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#how-to-use-the-index"},{"categories":["Nebula Graph"],"content":"4 RecapFinally, Let‚Äôs Recap INDEX is sorting PROP DATA to find data on given PURE PROP CONDITION INDEX is not for Graph Walk INDEX is left match, not for full-text search INDEX has cost on WRITE Remember to REBUILD after CREATE INDEX on existing data Happy Graphing! Feture image credit to Alina ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:4:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#recap"},{"categories":["Nebula Graph"],"content":"How to parse nebula graph data in an interactive way and what are the best practices?","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/"},{"categories":["Nebula Graph"],"content":" How to parse nebula graph data in an interactive way and what are the best practices? I will show you an easier way in this article üòÅ. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:0:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#"},{"categories":["Nebula Graph"],"content":"1 Prepare for the Java REPLThanks to https://github.com/albertlatacz/java-repl/ we could play with/debug this in an interactive way, and all we need is to leverage its docker image to have all the envrioment in a clean and quick way: docker pull albertlatacz/java-repl docker run --rm -it \\ --network=nebula-docker-compose_nebula-net \\ -v ~:/root \\ albertlatacz/java-repl \\ bash apt update -y \u0026\u0026 apt install ca-certificates -y wget https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz --no-check-certificate tar xzvf apache-maven-3.8.4-bin.tar.gz wget https://github.com/vesoft-inc/nebula-java/archive/refs/tags/v2.6.1.tar.gz tar xzvf v2.6.1.tar.gz cd nebula-java-2.6.1/ ../apache-maven-3.8.4/bin/mvn dependency:copy-dependencies ../apache-maven-3.8.4/bin/mvn -B package -Dmaven.test.skip=true java -jar ../javarepl/javarepl.jar Now, after executing java -jar ../javarepl/javarepl.jar we are in a Java Shell(REPL), this enable us to execute Java code in an interactive way without wasting time and patience in the slow path(code ‚Äì\u003e build ‚Äì\u003e execute ‚Äì\u003e add print ‚Äì\u003e build), isn‚Äôt that cool? Like this: root@a2e26ba62bb6:/javarepl/nebula-java-2.6.1# java -jar ../javarepl/javarepl.jar Welcome to JavaREPL version 428 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111) Type expression to evaluate, :help for more options or press tab to auto-complete. Connected to local instance at http://localhost:43707 java\u003e System.out.println(\"Hello, World!\"); Hello, World! java\u003e Now we are in the java REPL, let‚Äôs introduce all the class path needed and do the imports in one go: :cp /javarepl/nebula-java-2.6.1/client/target/client-2.6.1.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/fastjson-1.2.78.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-api-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-log4j12-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/commons-pool2-2.2.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/log4j-1.2.17.jar import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.vesoft.nebula.ErrorCode; import com.vesoft.nebula.client.graph.NebulaPoolConfig; import com.vesoft.nebula.client.graph.data.CASignedSSLParam; import com.vesoft.nebula.client.graph.data.HostAddress; import com.vesoft.nebula.client.graph.data.ResultSet; import com.vesoft.nebula.client.graph.data.SelfSignedSSLParam; import com.vesoft.nebula.client.graph.data.ValueWrapper; import com.vesoft.nebula.client.graph.net.NebulaPool; import com.vesoft.nebula.client.graph.net.Session; import java.io.UnsupportedEncodingException; import java.util.Arrays; import java.util.List; import java.util.concurrent.TimeUnit; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.*; And let‚Äôs connect it to the nebula graph, please replace your graphD IP and Port here, and execute them under the propmt string of java\u003e: NebulaPoolConfig nebulaPoolConfig = new NebulaPoolConfig(); nebulaPoolConfig.setMaxConnSize(10); List\u003cHostAddress\u003e addresses = Arrays.asList(new HostAddress(\"192.168.8.127\", 9669)); NebulaPool pool = new NebulaPool(); pool.init(addresses, nebulaPoolConfig); Session session = pool.getSession(\"root\", \"nebula\", false); ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:1:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#prepare-for-the-java-repl"},{"categories":["Nebula Graph"],"content":"2 The \u003ccode\u003eexecute\u003c/code\u003e for ResultSetFirst let‚Äôs check what we can do with a simple query: ResultSet resp = session.execute(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); Now you could play with it: Reference: client/graph/data/ResultSet.java java\u003e resp.isSucceeded() java.lang.Boolean res9 = true java\u003e resp.rowsSize() java.lang.Integer res16 = 1 java\u003e rows = resp.getRows() java.util.ArrayList rows = [Row ( values : [ \u003cValue vVal:Vertex ( vid : \u003cValue sVal:70 6c 61 79 65 72 31 30 30\u003e, tags : [ Tag ( name : 70 6C 61 79 65 72, props : { [B@5264a468 : \u003cValue iVal:42\u003e [B@496b8e10 : \u003cValue sVal:54 69 6d 20 44 75 6e 63 61 6e\u003e } ) ] )\u003e ] )] java\u003e row0 = resp.rowValues(0) java.lang.Iterable\u003ccom.vesoft.nebula.client.graph.data.ValueWrapper\u003e res10 = ColumnName: [n], Values: [(\"player100\" :player {name: \"Tim Duncan\", age: 42})] Remember our item is actually a vertex? (root@nebula) [basketballplayer]\u003e match (n:player) WHERE n.name == \"Tim Duncan\" return n +----------------------------------------------------+ | n | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2116/44373 us) Let‚Äôs see what(methods) can be done towards a value? v = Class.forName(\"com.vesoft.nebula.Value\") v.getDeclaredMethods() We could tell it‚Äôs quite Primitive on what com.vesoft.nebula.Value provided, thus we should use the ValueWrapper(or use executeJson actually) instead. To get a row of the result via iteration(as its a java iterable), we just follow how the example looped the result: import java.util.ArrayList; import java.util.List; List\u003cValueWrapper\u003e wrappedValueList = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c resp.rowsSize(); i++) { ResultSet.Record record = resp.rowValues(i); for (ValueWrapper value : record.values()) { wrappedValueList.add(value); if (value.isLong()) { System.out.printf(\"%15s |\", value.asLong()); } if (value.isBoolean()) { System.out.printf(\"%15s |\", value.asBoolean()); } if (value.isDouble()) { System.out.printf(\"%15s |\", value.asDouble()); } if (value.isString()) { System.out.printf(\"%15s |\", value.asString()); } if (value.isTime()) { System.out.printf(\"%15s |\", value.asTime()); } if (value.isDate()) { System.out.printf(\"%15s |\", value.asDate()); } if (value.isDateTime()) { System.out.printf(\"%15s |\", value.asDateTime()); } if (value.isVertex()) { System.out.printf(\"%15s |\", value.asNode()); } if (value.isEdge()) { System.out.printf(\"%15s |\", value.asRelationship()); } if (value.isPath()) { System.out.printf(\"%15s |\", value.asPath()); } if (value.isList()) { System.out.printf(\"%15s |\", value.asList()); } if (value.isSet()) { System.out.printf(\"%15s |\", value.asSet()); } if (value.isMap()) { System.out.printf(\"%15s |\", value.asMap()); } } System.out.println(); } As shown in above, the result value/item could be in properties string/int etc‚Ä¶ or in graph semantic vertex, edge, path, we should use correspond asXxxx methods: java\u003e v = wrappedValueList.get(0) com.vesoft.nebula.client.graph.data.ValueWrapper v = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e v.asNode() com.vesoft.nebula.client.graph.data.Node res16 = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e node = v.asNode() com.vesoft.nebula.client.graph.data.Node node = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) Btw, it‚Äôs also possible to play with it with reflections(we imported already): Of courese we could also check via client/graph/data/ResultSet.java java\u003e rClass=Class.forName(\"com.vesoft.nebula.client.graph.data.ResultSet\") java.lang.Class r = class com.vesoft.nebula.client.graph.data.ResultSet java\u003e rClass.getDeclaredMethods() java.lang.reflect.Method[] res20 = [public java.util.List com.vesoft.nebula.client.graph.data.ResultSet.getColumnNames(), public int com.vesoft.nebula.client.graph.data.ResultSet.rowsSize(), public com.vesoft.nebula.client.graph.data.ResultSet$Record com.vesoft","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:2:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-execute-for-resultset"},{"categories":["Nebula Graph"],"content":"3 The \u003ccode\u003eexecuteJson\u003c/code\u003e methodSince 2.6, nebule finally supports json string response and we could do this: java\u003e String resp_json = session.executeJson(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); java.lang.String resp_json = \" { \"errors\":[ { \"code\":0 } ], \"results\":[ { \"spaceName\":\"basketballplayer\", \"data\":[ { \"meta\":[ { \"type\":\"vertex\", \"id\":\"player100\" } ], \"row\":[ { \"player.age\":42, \"player.name\":\"Tim Duncan\" } ] } ], \"columns\":[ \"n\" ], \"errors\":{ \"code\":0 }, \"latencyInUs\":4761 } ] } \" java\u003e And I believe you know much better than I do with it. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:3:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-executejson-method"},{"categories":["Nebula Graph"],"content":"4 Conclusion If we go with JSON response, it‚Äôll be easier for you to have everything parsed. If we have to deal with resultSet object, just use the ValueWrapper asNode() if the value is a vertex, use asRelationship if value is an edge and asPath() if the value is a path. With the REPL tool shown together with java reflection and source code, it‚Äôs possbile to inspect on how data could be parsed. Happy Graphing! Picture CreditÔºöleunesmedia ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:4:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#conclusion"},{"categories":["Nebula Graph"],"content":"Dialog System With Graph Database Backed Knowledge Graph. Âü∫‰∫éÂõæÊï∞ÊçÆÂ∫ìÁöÑÊô∫ËÉΩÈóÆÁ≠îÂä©Êâã","date":"2021-09-18","objectID":"/en/nebula-siwi/","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/"},{"categories":["Nebula Graph"],"content":" a PoC of Dialog System With Graph Database Backed Knowledge Graph. Related GitHub Repo: https://github.com/wey-gu/nebula-siwi/ I created the Katacoda Interactive Env for this project üëâüèª https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#"},{"categories":["Nebula Graph"],"content":"Siwi the voice assistantSiwi (/Ààs…™wi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. For now, it‚Äôs a demo for task-driven(not general purpose) dialog bots with KG(Knowledge Graph) leveraging Nebula Graph with the minimal/sample dataset from Nebula Graph Manual/ NG‰∏≠ÊñáÊâãÂÜå. Tips: Now you can play with the graph online without installing yourself! Nebula Playground | Nebula Playground - China Mainland Supported queries: relation: What is the relationship between Yao Ming and Lakers? How does Yao Ming and Lakers connected? serving: Which team had Yao Ming served? friendship: Whom does Tim Duncan follow? Who are Yao Ming‚Äôs friends? ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#siwi-the-voice-assistant"},{"categories":["Nebula Graph"],"content":"1 Deploy and TryTBD (leveraging docker and nebula-up) ","date":"2021-09-18","objectID":"/en/nebula-siwi/:1:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#deploy-and-try"},{"categories":["Nebula Graph"],"content":"2 How does it work?This is one of the most naive pipeline for a specific domain/ single purpose chat bot built on a Knowledge Graph. ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#how-does-it-work"},{"categories":["Nebula Graph"],"content":"2.1 BackendThe Backend(Siwi API) is a Flask based API server: Flask API server takes questions in HTTP POST, and calls the bot API. In bot API part there are classfier(Symentic Parsing, Intent Matching, Slot Filling), and question actors(Call corresponding actions to query Knowledge Graph with intents and slots). Knowledge Graph is built on an Open-Source Graph Database: Nebula Graph ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend"},{"categories":["Nebula Graph"],"content":"2.2 FrontendThe Frontend is a VueJS Single Page Applicaiton(SPA): I reused a Vue Bot UI to showcase a chat window in this human-agent interaction, typing is supported. In addtion, leverating Chrome‚Äôs Web Speech API, a button to listen to human voice is introduced ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend"},{"categories":["Nebula Graph"],"content":"2.3 A Query Flow ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Speech ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Frontend ‚îÇ Siwi, /Ààs…™wi/ ‚îÇ ‚îÇ ‚îÇ Web_Speech_API ‚îÇ A PoC of ‚îÇ ‚îÇ ‚îÇ ‚îÇ Dialog System ‚îÇ ‚îÇ ‚îÇ Vue.JS ‚îÇ With Graph Database ‚îÇ ‚îÇ ‚îÇ ‚îÇ Backed Knowledge Graph ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ Sentence ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Backend ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Web API, Flask ‚îÇ ./app/ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Sentence ./bot/ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Intent matching, ‚îÇ ./bot/classifier‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Symentic Processing ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Intent, Entities ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Intent Actor ‚îÇ ./bot/actions ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ Graph Query ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Graph Database ‚îÇ Nebula Graph ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:3","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#a-query-flow"},{"categories":["Nebula Graph"],"content":"2.4 Source Code Tree . ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ src ‚îÇ ‚îú‚îÄ‚îÄ siwi # Siwi-API Backend ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ app # Web Server, take HTTP requests and calls Bot API ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ bot # Bot API ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ actions # Take Intent, Slots, Query Knowledge Graph here ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ bot # Entrypoint of the Bot API ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ classifier # Symentic Parsing, Intent Matching, Slot Filling ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ test # Example Data Source as equivalent/mocked module ‚îÇ ‚îî‚îÄ‚îÄ siwi_frontend # Browser End ‚îÇ ‚îú‚îÄ‚îÄ README.md ‚îÇ ‚îú‚îÄ‚îÄ package.json ‚îÇ ‚îî‚îÄ‚îÄ src ‚îÇ ‚îú‚îÄ‚îÄ App.vue # Listening to user and pass Questions to Siwi-API ‚îÇ ‚îî‚îÄ‚îÄ main.js ‚îî‚îÄ‚îÄ wsgi.py ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:4","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#source-code-tree"},{"categories":["Nebula Graph"],"content":"3 Manually Run Components","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#manually-run-components"},{"categories":["Nebula Graph"],"content":"3.1 BackendInstall and run. # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 For OpenFunction/ KNative docker build -t weygu/siwi-api . docker run --rm --name siwi-api \\ --env=PORT=5000 \\ --env=NG_ENDPOINTS=127.0.0.1:9669 \\ --net=host \\ weygu/siwi-api Try it out Web API: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"question\": \"What is the relationship between Yao Ming and Lakers?\"}' \\ http://192.168.8.128:5000/query | jq { \"answer\": \"There are at least 23 relations between Yao Ming and Lakers, one relation path is: Yao Ming follows Shaquille O'Neal serves Lakers.\" } Call Bot Python API: from nebula2.gclient.net import ConnectionPool from nebula2.Config import Config # define a config config = Config() config.max_connection_pool_size = 10 # init connection pool connection_pool = ConnectionPool() # if the given servers are ok, return true, else return false ok = connection_pool.init([('127.0.0.1', 9669)], config) # import siwi bot from siwi.bot import bot # instantiate a bot b = bot.SiwiBot(connection_pool) # make the question query b.query(\"Which team had Jonathon Simmons served?\") Then a response will be like this: In [4]: b.query(\"Which team had Jonathon Simmons serv ...: ed?\") [DEBUG] ServeAction intent: {'entities': {'Jonathon Simmons': 'player'}, 'intents': ('serve',)} [DEBUG] query for RelationshipAction: USE basketballplayer; MATCH p=(v)-[e:serve*1]-\u003e(v1) WHERE id(v) == \"player112\" RETURN p LIMIT 100; [2021-07-02 02:59:36,392]:Get connection to ('127.0.0.1', 9669) Out[4]: 'Jonathon Simmons had served 3 teams. Spurs from 2015 to 2015; 76ers from 2019 to 2019; Magic from 2017 to 2017; ' ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-1"},{"categories":["Nebula Graph"],"content":"3.2 FrontendReferring to siwi_frontend ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-1"},{"categories":["Nebula Graph"],"content":"4 Further work Use NBA-API to fallback undefined pattern questions Wrap and manage sessions instead of get and release session per request, this is somehow costly actually. Use NLP methods to implement proper Symentic Parsing, Intent Matching, Slot Filling Build Graph to help with Intent Matching, especially for a general purpose bot Use larger Dataset i.e. from wyattowalsh/basketball ","date":"2021-09-18","objectID":"/en/nebula-siwi/:4:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#further-work"},{"categories":["Nebula Graph"],"content":"5 Thanks to Upstream Projects ‚ù§Ô∏è","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"5.1 Backend I learnt a lot from the KGQA on MedicalKG created by Huanyong Liu Flask pyahocorasick created by Wojciech Mu≈Ça PyYaml ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-2"},{"categories":["Nebula Graph"],"content":"5.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design Image credit goes to https://unsplash.com/photos/0E_vhMVqL9g ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","Êô∫ËÉΩÂä©Êâã"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-2"},{"categories":["Nebula Graph"],"content":"Setup Nebula Graph Dev Env with CLion and Docker Êê≠Âª∫Âü∫‰∫é Docker ÁöÑ Nebula Graph CLion ÂºÄÂèëÁéØÂ¢É","date":"2021-09-18","objectID":"/en/nebula-clion/","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/"},{"categories":["Nebula Graph"],"content":" ‰πãÂâçÂç°ÊØîÂêåÂ≠¶ÂêëÊàëÂí®ËØ¢Êê≠Âª∫ CLion ÁéØÂ¢ÉÔºåÂºÄÂèë Nebula ÁöÑ‰∏Ä‰∫õÈóÆÈ¢òÔºåÊàëÂÅö‰∫Ü‰∏Ä‰∫õÂ∑•‰ΩúÊñπ‰æøÂà©Áî® Docker Âú®Êú¨Âú∞Êê≠Âª∫ËøôÊ†∑‰∏Ä‰∏™ÁéØÂ¢ÉÔºåÁõ∏ÂÖ≥ÁöÑ‰∏úË•øÊîæÂú®Ôºöhttps://github.com/wey-gu/nebula-dev-CLion „ÄÇ Related GitHub Repo: https://github.com/wey-gu/nebula-dev-CLion ","date":"2021-09-18","objectID":"/en/nebula-clion/:0:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#"},{"categories":["Nebula Graph"],"content":"1 Run Docker Env for Nebula-Graph with CLionBuild Docker Image git clone https://github.com/wey-gu/nebula-dev-CLion.git cd nebula-dev-CLion docker build -t wey/nebula-dev-clion:v2.0 . Run Docker Container for Nebula-Dev with CLion Integration Readiness(actually mostly Rsync \u0026 SSH). cd \u003cnebula-graph-repo-you-worked-on\u003e export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker run --rm -d \\ --name nebula-dev \\ --security-opt seccomp=unconfined \\ -p 2222:22 -p 2873:873 --cap-add=ALL \\ -v $PWD:/home/nebula \\ -w /home/nebula \\ wey/nebula-dev-clion:v2.0 Verify cmake with SSH. The default password is password ssh -o StrictHostKeyChecking=no root@localhost -p 2222 # in docker cd /home/nebula mkdir build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. Access container w/o SSH. docker exec -it nebula-dev bash mkdir -p build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. ","date":"2021-09-18","objectID":"/en/nebula-clion/:1:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#run-docker-env-for-nebula-graph-with-clion"},{"categories":["Nebula Graph"],"content":"2 Configurations in CLion Ref: https://www.jetbrains.com/help/clion/clion-toolchains-in-docker.html#build-and-run Toolchains Add a remote host root@localhost:2222 password Put /opt/vesoft/toolset/cmake/bin/cmake as CMake CMake Toochain: Select the one created in last step Build directory: /home/nebula/build ","date":"2021-09-18","objectID":"/en/nebula-clion/:2:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#configurations-in-clion"},{"categories":["Nebula Graph"],"content":"3 The appendix","date":"2021-09-18","objectID":"/en/nebula-clion/:3:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#the-appendix"},{"categories":["Nebula Graph"],"content":"3.1 References of CMake output: [root@4c98e3f77ce8 build]# cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. \u003e\u003e\u003e\u003e Options of Nebula Graph \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_BUILD_STORAGE : OFF (Whether to build storage) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_MODULE_FORCE_CHECKOUT : ON (Whether checkout branch of module to same as graph.) -- ENABLE_MODULE_UPDATE : OFF (Automatically update module) -- ENABLE_PACK_ONE : ON (Whether to package into one) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_VERBOSE_BISON : OFF (Enable Bison to report state) -- ENABLE_WERROR : ON (Regard warnings as errors) -- CMAKE_BUILD_TYPE : Release (Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...) -- CMAKE_INSTALL_PREFIX : /usr/local/nebula (Install path prefix, prepended onto install directories.) -- CMAKE_CXX_STANDARD : 17 -- CMAKE_CXX_COMPILER : /opt/vesoft/toolset/clang/9.0.0/bin/c++ (CXX compiler) -- CMAKE_CXX_COMPILER_ID : GNU -- NEBULA_USE_LINKER : bfd -- CCACHE_DIR : /root/.ccache \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' \u003c\u003c\u003c\u003c -- NEBULA_THIRDPARTY_ROOT : /opt/vesoft/third-party/2.0 -- Build info of nebula third party: Package : Nebula Third Party Version : 2.0 Date : Mon Jun 28 15:07:38 UTC 2021 glibc : 2.17 Arch : x86_64 Compiler : GCC 9.2.0 C++ ABI : 11 Vendor : VEsoft Inc. -- CMAKE_INCLUDE_PATH : /opt/vesoft/third-party/2.0/include -- CMAKE_LIBRARY_PATH : /opt/vesoft/third-party/2.0/lib64;/opt/vesoft/third-party/2.0/lib -- CMAKE_PROGRAM_PATH : /opt/vesoft/third-party/2.0/bin -- GLIBC_VERSION : 2.17 -- found krb5-config here /opt/vesoft/third-party/2.0/bin/krb5-config -- Found kerberos 5 headers: /opt/vesoft/third-party/2.0/include -- Found kerberos 5 libs: /opt/vesoft/third-party/2.0/lib/libgssapi_krb5.a;/opt/vesoft/third-party/2.0/lib/libkrb5.a;/opt/vesoft/third-party/2.0/lib/libk5crypto.a;/opt/vesoft/third-party/2.0/lib/libcom_err.a;/opt/vesoft/third-party/2.0/lib/libkrb5support.a \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' done \u003c\u003c\u003c\u003c -- Create the pre-commit hook -- Creating pre-commit hook done \u003e\u003e\u003e\u003e Configuring Nebula Common \u003c\u003c\u003c\u003c \u003e\u003e\u003e\u003e Options of Nebula Common \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_WERROR : ON (Regard warnings as errors) -- Set D_GLIBCXX_USE_CXX11_ABI to 1 -- CMAKE_BUIL","date":"2021-09-18","objectID":"/en/nebula-clion/:3:1","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#references-of-cmake-output"},{"categories":["courses"],"content":"Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database.","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/"},{"categories":["courses"],"content":"Walk you through in actions to do below sections exercises! Bootstrap a Nebula Graph Cluster and Studio Web App Import a graph of dataset about shareholding Exploring the shareholding data with Nebula Importer Visually Exploring the shareholding data with Nebula Studio Run Graph Algorithm on Nebula Cluster Graph Data The dataset comes from https://github.com/wey-gu/nebula-shareholding-example/tree/main/data_sample The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/:0:0","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/#"},{"categories":["courses"],"content":"Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s.","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/"},{"categories":["courses"],"content":"A full solution walkthrough for a Knowledge Graph Dialog System. Boostrap a Nebula Cluster in K8s Scale out the Nebula Cluster in K8s way Import the basketballplayer Dataset Siwi, the Knowledge Graph Dialog System with Nebula Graph Siwi (/Ààs…™wi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. The code of Siwi is here: https://github.com/wey-gu/nebula-siwi. The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/:0:0","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/#"},{"categories":["Nebula Graph"],"content":"A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®Á§∫‰æãÔºöËÇ°ÊùÉÂÖ≥Á≥ªÁ©øÈÄè","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/"},{"categories":["Nebula Graph"],"content":" A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®Á§∫‰æãÔºöËÇ°ÊùÉÂÖ≥Á≥ªÁ©øÈÄè Related GitHub Repo: https://github.com/wey-gu/nebula-shareholding-example Êõ¥Êñ∞ÔºöÂú®Ëøô‰∏™Êï∞ÊçÆÈõÜÁîüÊàêÁöÑÂ∑•‰ΩúÂü∫Á°Ä‰∏äÔºåÊàëÂèàÂÅö‰∫Ü‰∏Ä‰∏™ÂÖ®Ê†àÁ§∫‰æãÈ°πÁõÆ üëâüèª https://siwei.io/corp-rel-graph/ I created the Katacoda Interactive Env for this project üëâüèª https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ This is a demo of Shareholding Relationship Analysis with Distributed open-source Graph Database: Nebula Graph. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:0:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#"},{"categories":["Nebula Graph"],"content":"1 Data","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data"},{"categories":["Nebula Graph"],"content":"1.1 Data ModelingThere are various kinds of relationships when we checking companies' shareholding breakthrough, here let‚Äôs simplify it with only two kind of entities: person and corp, and with following relationship types. person can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp Below is the lines to reflect this graph modele in Nebula Graph, it‚Äôs quite straightforward, right? CREATETAGperson(namestring);CREATETAGcorp(namestring);CREATEEDGErole_as(rolestring);CREATEEDGEis_branch_of();CREATEEDGEhold_share(sharefloat);CREATEEDGEreletive_with(degreeint); ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-modeling"},{"categories":["Nebula Graph"],"content":"1.2 Data GenerationWe just randomly generate some data to help with this demo, you can call data_generator.py directly to generate or reuse what‚Äôs already done under data_sample folder. The generated data are records to be fit in above data model from below .csv files. $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:2","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-generation"},{"categories":["Nebula Graph"],"content":"1.3 Data ImportWith those data in .csv files, we can easily import them into a Nebula Graph Cluster with the help of Nebula-Importer. nebula-importer.yaml in this repo describes rules and configurations on how this import will be done by the importer. For Nebula Graph Database, plesae refer to Doc , Doc-CN to deploy on any Linux Servers, for study and test, you can run it via Docker following the Quick Start Chapter of the documentation. For Nebula-Importer, if you already have Docker env, you can run it as the following without installing anything. Or, if you prefer to install it, it‚Äôs quite easy as it‚Äôs written in Golang and you can run its single file binary quite easily, go check both Documentation and Nebula-Importer Repo: https://github.com/vesoft-inc/nebula-importer. Let‚Äôs start! Below is the commands I used to import our data into a Nebula Graph Database. # put generated data \u0026 nebula-importor.yaml to nebula-importer server $ scp -r data nebula_graph_host:~ $ scp nebula-importer.yaml data nebula_graph_host:~/data $ ssh nebula_graph_host $ ls -l ${HOME}/data total 756 -rw-r--r--. 1 wei.gu wei.gu 23941 Jul 14 05:44 corp.csv -rw-r--r--. 1 wei.gu wei.gu 1277 Jul 14 05:44 corp_rel.csv -rw-r--r--. 1 wei.gu wei.gu 3048 Jul 14 05:44 corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 3893 Jul 14 05:44 nebula-importer.yaml -rw-r--r--. 1 wei.gu wei.gu 211661 Jul 14 05:44 person.csv -rw-r--r--. 1 wei.gu wei.gu 179770 Jul 14 05:44 person_corp_role.csv -rw-r--r--. 1 wei.gu wei.gu 322965 Jul 14 05:44 person_corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 17689 Jul 14 05:44 person_rel.csv # import data into our nebula graph database $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${HOME}/data/nebula-importer.yaml:/root/nebula-importer.yaml \\ -v ${HOME}/data:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-importer.yaml 2021/07/14 05:49:32 --- START OF NEBULA IMPORTER --- 2021/07/14 05:49:32 [WARN] config.go:491: Not set files[0].schema.vertex.vid.Type, reset to default value `string' ... 2021/07/14 05:49:43 [INFO] reader.go:180: Total lines of file(/root/person_corp_role.csv) is: 5000, error lines: 1287 2021/07/14 05:49:43 [INFO] statsmgr.go:61: Done(/root/person_corp_role.csv): Time(11.39s), Finished(12523), Failed(0), Latency AVG(1514us), Batches Req AVG(1824us), Rows AVG(1099.43/s) 2021/07/14 05:49:47 [INFO] statsmgr.go:61: Tick: Time(15.00s), Finished(25807), Failed(0), Latency AVG(1500us), Batches Req AVG(1805us), Rows AVG(1720.46/s) 2021/07/14 05:49:48 [INFO] reader.go:180: Total lines of file(/root/person.csv) is: 10000, error lines: 0 2021/07/14 05:49:48 [INFO] statsmgr.go:61: Done(/root/person.csv): Time(16.10s), Finished(29731), Failed(0), Latency AVG(1505us), Batches Req AVG(1810us), Rows AVG(1847.17/s) 2021/07/14 05:49:50 [INFO] reader.go:180: Total lines of file(/root/person_corp_share.csv) is: 20000, error lines: 0 2021/07/14 05:49:50 [INFO] statsmgr.go:61: Done(/root/person_corp_share.csv): Time(17.74s), Finished(36013), Failed(0), Latency AVG(1531us), Batches Req AVG(1844us), Rows AVG(2030.29/s) 2021/07/14 05:49:50 Finish import data, consume time: 18.25s 2021/07/14 05:49:51 --- END OF NEBULA IMPORTER --- ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:3","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-import"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the üëÅÔ∏è icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#corporation-sharehold-relationship-breakthrough"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the üëÅÔ∏è icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#query-in-ngql"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the üëÅÔ∏è icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#in-a-visual-way"},{"categories":["Nebula Graph"],"content":"3 Thanks to Upstream Projects ‚ù§Ô∏è Python Faker https://github.com/joke2k/faker/ pydbgen https://github.com/tirthajyoti/pydbgen Nebula Graph https://github.com/vesoft-inc/nebula-graph ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"3.1 Tips: You can deploy nebula graph in one line with: Nebula-UP, it helps install a nebula graph with Docker Nebula-operator-KIND , it helps setup all dependencies of Nebula-K8s-Operator including a K8s in Docker, PV Provider and then install a Nebula Graph with Nebula-Operator in K8s. Image Credit goes to https://unsplash.com/photos/3fPXt37X6UQ ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","ÂõæÊï∞ÊçÆÂ∫ìÂ∫îÁî®","ËÇ°ÊùÉÁ©øÈÄè"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#tips"},{"categories":null,"content":" Data on K8s Community 2021 GraphDB on Kubesphere Read more... K8s Community Day 2021 Openfunction + GraphDB Read more... COScon 2021 ÊàëÁöÑÂºÄÊ∫ê‰πãË∑Ø Read more... PyCon China 2021 ÂõæÊï∞ÊçÆÂ∫ìËß£Ë∞ú‰∏é Python ÁöÑÂõæÂ∫ìÂ∫îÁî®ÂÆûË∑µ Read more... nMeetup: Nebula Â∫îÁî®‰∏äÊâãÂÆûÊìç ‰ªéÂ§¥ÂÆûÊìç Nebula ÁöÑÈÉ®ÁΩ≤ÔºåËÇ°ÊùÉÁ©øÈÄèÔºåÂõæÁÆóÊ≥ïËøêÁÆóÔºåËØ≠Èü≥Êô∫ËÉΩÂä©Êâã„ÄÇ Read more... How to Train your Dragon Â¶Ç‰ΩïÊàê‰∏∫ÂºÄÊ∫êÂºÄÂèëËÄÖÔºàÂ∏ÉÈÅìÂ∏àÔºâ„ÄÇ Read more... ","date":"2021-08-26","objectID":"/en/talk/:0:0","series":null,"tags":null,"title":"My Talks","uri":"/en/talk/#"},{"categories":["Nebula Graph"],"content":"Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm ÂØºÂÖ• Livejournal Êï∞ÊçÆÈõÜÂà∞ Nebula Âπ∂ËøêË°å Nebula Algorithm ÂõæÁÆóÊ≥ï","date":"2021-08-24","objectID":"/en/nebula-livejournal/","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/"},{"categories":["Nebula Graph"],"content":" ‰∏Ä‰∏™ÂØºÂÖ• Livejournal Êï∞ÊçÆÈõÜÂà∞ Nebula Graph ÂõæÊï∞ÊçÆÂ∫ìÔºåÂπ∂ÊâßË°å Nebula Algorithm ÂõæÁÆóÊ≥ïÁöÑËøáÁ®ãÂàÜ‰∫´„ÄÇ Related GitHub Repo: https://github.com/wey-gu/nebula-LiveJournal ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#"},{"categories":["Nebula Graph"],"content":"nebula-LiveJournalLiveJournal Dataset is a Social Network Dataset in one file with two columns(FromNodeId, ToNodeId). $ head soc-LiveJournal1.txt # Directed graph (each unordered pair of nodes is saved once): soc-LiveJournal1.txt # Directed LiveJournal friednship social network # Nodes: 4847571 Edges: 68993773 # FromNodeId ToNodeId 0 1 0 2 0 3 0 4 0 5 0 6 It could be accessed in https://snap.stanford.edu/data/soc-LiveJournal1.html. Dataset statistics Nodes 4847571 Edges 68993773 Nodes in largest WCC 4843953 (0.999) Edges in largest WCC 68983820 (1.000) Nodes in largest SCC 3828682 (0.790) Edges in largest SCC 65825429 (0.954) Average clustering coefficient 0.2742 Number of triangles 285730264 Fraction of closed triangles 0.04266 Diameter (longest shortest path) 16 90-percentile effective diameter 6.5 ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#nebula-livejournal"},{"categories":["Nebula Graph"],"content":"1 Dataset Download and Preprocessing","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#dataset-download-and-preprocessing"},{"categories":["Nebula Graph"],"content":"1.1 DownloadIt is accesissiable from the official web page: $ cd nebula-livejournal/data $ wget https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz Comments in data file should be removed to make the data import tool happy. ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#download"},{"categories":["Nebula Graph"],"content":"1.2 Preprocessing $ gzip -d soc-LiveJournal1.txt.gz $ sed -i '1,4d' soc-LiveJournal1.txt ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#preprocessing"},{"categories":["Nebula Graph"],"content":"2 Import dataset to Nebula Graph","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#import-dataset-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"2.1 With Nebula ImporterNebula-Importer is a Golang Headless import tool for Nebula Graph. You may need to edit the config file under nebula-importer/importer.yaml on Nebula Graph‚Äôs address and credential„ÄÇ Then, Nebula-Importer could be called in Docker as follow: $ cd nebula-livejournal $ docker run --rm -ti \\ --network=nebula-net \\ -v nebula-importer/importer.yaml:/root/importer.yaml \\ -v data/:/root \\ vesoft/nebula-importer:v2 \\ --config /root/importer.yaml Or if you have the binary nebula-importer locally: $ cd data $ \u003cpath_to_nebula-importer_binary\u003e --config ../nebula-importer/importer.yaml ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-importer"},{"categories":["Nebula Graph"],"content":"2.2 With Nebula ExchangeNebula-Exchange is a Spark Application to enable batch and streaming data import from multiple data sources to Nebula Graph. To be done. (You can refer to https://siwei.io/nebula-exchange-sst-2.x/) ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-exchange"},{"categories":["Nebula Graph"],"content":"3 Run Algorithms with Nebula GraphNebula-Algorithm is a Spark/GraphX Application to run Graph Algorithms with data consumed from files or a Nebula Graph Cluster. Supported Algorithms for now: Name Use Case PageRank page ranking, important node digging Louvain community digging, hierarchical clustering KCore community detection, financial risk control LabelPropagation community detection, consultation propagation, advertising recommendation ConnectedComponent community detection, isolated island detection StronglyConnectedComponent community detection ShortestPath path plan, network plan TriangleCount network structure analysis BetweennessCentrality important node digging, node influence calculation DegreeStatic graph structure analysis ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms-with-nebula-graph"},{"categories":["Nebula Graph"],"content":"3.1 Ad-hoc Spark Env setupHere I assume the Nebula Graph was bootstraped with Nebula-Up, thus nebula is running in a Docker Network named nebula-docker-compose_nebula-net. Then let‚Äôs start a single server spark: docker run --name spark-master --network nebula-docker-compose_nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ -v nebula-algorithm/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 Thus we could make spark application submt inside this container: docker exec -it spark-master bash cd /root/ # download Nebula-Algorithm Jar Packagem, 2.0.0 for example, for other versions, refer to nebula-algorithm github repo and documentations. wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.0.0/nebula-algorithm-2.0.0.jar ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#ad-hoc-spark-env-setup"},{"categories":["Nebula Graph"],"content":"3.2 Run AlgorithmsThere are many altorithms supported by Nebula-Algorithm, here some of their configuration files were put under nebula-algorithm as an example. Before using them, please first edit and change Nebula Graph Cluster Addresses and credentials. vim nebula-altorithm/algo-pagerank.conf Then we could enter the spark container and call corresponding algorithms as follow. Please adjust your --driver-memeory accordingly, i.e. pagerank altorithm: /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 16g nebula-algorithm-2.0.0.jar \\ -p pagerank.conf After the algorithm finished, the output will be under the path insdie the container defined in conf file: write:{ resultPath:/output/ } È¢òÂõæÁâàÊùÉÔºö@sigmund ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms"},{"categories":["Nebula Graph"],"content":"ËøôÁØáÊñáÁ´†Â∏¶Â§ßÂÆ∂‰ª•ÊúÄÂ∞èÊñπÂºèÔºåÂø´ÈÄüË∂ü‰∏Ä‰∏ã Nebula Exchange ‰∏≠ SST ÂÜôÂÖ•ÊñπÂºèÁöÑÊ≠•È™§„ÄÇ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/"},{"categories":["Nebula Graph"],"content":"ËøôÁØáÊñáÁ´†Â∏¶Â§ßÂÆ∂‰ª•ÊúÄÂ∞èÊñπÂºèÔºåÂø´ÈÄüË∂ü‰∏Ä‰∏ã Nebula Exchange ‰∏≠ SST ÂÜôÂÖ•ÊñπÂºèÁöÑÊ≠•È™§„ÄÇ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:0:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#"},{"categories":["Nebula Graph"],"content":"1 ‰ªÄ‰πàÊòØ Nebula Exchange ?‰πãÂâçÊàëÂú® Nebula Data Import Options ‰πã‰∏≠‰ªãÁªçËøáÔºåNebula Exchange ÊòØ‰∏Ä‰∏™ Nebula Graph Á§æÂå∫ÂºÄÊ∫êÁöÑ Spark ApplicaitonÔºåÂÆÉ‰∏ìÈó®Áî®Êù•ÊîØÊåÅÊâπÈáèÊàñËÄÖÊµÅÂºèÂú∞ÊääÊï∞ÊçÆÂØºÂÖ• Nebula Graph Database ‰πã‰∏≠„ÄÇ Nebula Exchange ÊîØÊåÅÂ§öÁßçÂ§öÊ†∑ÁöÑÊï∞ÊçÆÊ∫êÔºà‰ªé Apache Parquet, ORC, JSON, CSV, HBase, Hive MaxCompute Âà∞ Neo4j, MySQL, ClickHouse, ÂÜçÊúâ Kafka, PulsarÔºåÊõ¥Â§öÁöÑÊï∞ÊçÆÊ∫ê‰πüÂú®‰∏çÊñ≠Â¢ûÂä†‰πã‰∏≠Ôºâ„ÄÇ Â¶Ç‰∏äÂõæÊâÄÁ§∫ÔºåÂú® Exchange ÂÜÖÈÉ®Ôºå‰ªéÈô§‰∫Ü‰∏çÂêå Reader ÂèØ‰ª•ËØªÂèñ‰∏çÂêåÊï∞ÊçÆÊ∫ê‰πãÂ§ñÔºåÂú®Êï∞ÊçÆÁªèËøá Processor Â§ÑÁêÜ‰πãÂêéÈÄöËøá WriterÂÜôÂÖ•ÔºàsinkÔºâ Nebula Graph ÂõæÊï∞ÊçÆÂ∫ìÁöÑÊó∂ÂÄôÔºåÈô§‰∫ÜËµ∞Ê≠£Â∏∏ÁöÑ ServerBaseWriter ÁöÑÂÜôÂÖ•ÊµÅÁ®ã‰πãÂ§ñÔºåÂÆÉËøòÂèØ‰ª•ÁªïËøáÊï¥‰∏™ÂÜôÂÖ•ÊµÅÁ®ãÔºåÂà©Áî® Spark ÁöÑËÆ°ÁÆóËÉΩÂäõÂπ∂Ë°åÁîüÊàêÂ∫ïÂ±Ç RocksDB ÁöÑ SST Êñá‰ª∂Ôºå‰ªéËÄåÂÆûÁé∞Ë∂ÖÈ´òÊÄßËÉΩÁöÑÊï∞ÊçÆÂØºÂÖ•ÔºåËøô‰∏™ SST Êñá‰ª∂ÂØºÂÖ•ÁöÑÂú∫ÊôØÂ∞±ÊòØÊú¨ÊñáÂ∏¶Â§ßÂÆ∂‰∏äÊâãÁÜüÊÇâÁöÑÈÉ®ÂàÜ„ÄÇ ËØ¶ÁªÜ‰ø°ÊÅØËØ∑ÂèÇÈòÖÔºöNebula Graph ÊâãÂÜå:‰ªÄ‰πàÊòØ Nebula Exchange Nebula Graph ÂÆòÊñπÂçöÂÆ¢‰πüÊúâÊõ¥Â§ö Nebula Exchange ÁöÑÂÆûË∑µÊñáÁ´† ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:1:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#‰ªÄ‰πàÊòØ-nebula-exchange-"},{"categories":["Nebula Graph"],"content":"2 Ê≠•È™§Ê¶ÇËßÇ ÂÆûÈ™åÁéØÂ¢É ÈÖçÁΩÆ Exchange ÁîüÊàê SST Êñá‰ª∂ ÂÜôÂÖ• SST Êñá‰ª∂Âà∞ Nebula Graph ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:2:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#Ê≠•È™§Ê¶ÇËßÇ"},{"categories":["Nebula Graph"],"content":"3 ÂÆûÈ™åÁéØÂ¢ÉÂáÜÂ§á‰∏∫‰∫ÜÊúÄÂ∞èÂåñ‰ΩøÁî® Nebula Exchange ÁöÑ SST ÂäüËÉΩÔºåÊàë‰ª¨ÈúÄË¶ÅÔºö Êê≠Âª∫‰∏Ä‰∏™ Nebula Graph ÈõÜÁæ§ÔºåÂàõÂª∫ÂØºÂÖ•Êï∞ÊçÆÁöÑ SchemaÔºåÊàë‰ª¨ÈÄâÊã©‰ΩøÁî® Docker-Compose ÊñπÂºè„ÄÅÂà©Áî® Nebula-Up Âø´ÈÄüÈÉ®ÁΩ≤ÔºåÂπ∂ÁÆÄÂçï‰øÆÊîπÂÖ∂ÁΩëÁªúÔºå‰ª•Êñπ‰æøÂêåÊ†∑ÂÆπÂô®ÂåñÁöÑ Exchange Á®ãÂ∫èÂØπÂÖ∂ËÆøÈóÆ„ÄÇ Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ Spark ËøêË°åÁéØÂ¢É Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ HDFS ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÂÆûÈ™åÁéØÂ¢ÉÂáÜÂ§á"},{"categories":["Nebula Graph"],"content":"3.1 Êê≠Âª∫ Nebula Graph ÈõÜÁæ§ÂÄüÂä©‰∫é Nebula-Up Êàë‰ª¨ÂèØ‰ª•Âú® Linux ÁéØÂ¢É‰∏ã‰∏ÄÈîÆÈÉ®ÁΩ≤‰∏ÄÂ•ó Nebula Graph ÈõÜÁæ§Ôºö curl -fsSL nebula-up.siwei.io/install.sh | bash ÂæÖÈÉ®ÁΩ≤ÊàêÂäü‰πãÂêéÔºåÊàë‰ª¨ÈúÄË¶ÅÂØπÁéØÂ¢ÉÂÅö‰∏Ä‰∫õ‰øÆÊîπÔºåËøôÈáåÊàëÂÅöÁöÑ‰øÆÊîπÂÖ∂ÂÆûÂ∞±ÊòØ‰∏§ÁÇπÔºö Âè™‰øùÁïô‰∏Ä‰∏™ metaD ÊúçÂä° Ëµ∑Áî® Docker ÁöÑÂ§ñÈÉ®ÁΩëÁªú ËØ¶ÁªÜ‰øÆÊîπÁöÑÈÉ®ÂàÜÂèÇËÄÉÈôÑÂΩï‰∏Ä Â∫îÁî® docker-compose ÁöÑ‰øÆÊîπÔºö cd ~/.nebula-up/nebula-docker-compose vim docker-compose.yaml # ÂèÇËÄÉÈôÑÂΩï‰∏Ä docker network create nebula-net # ÈúÄË¶ÅÂàõÂª∫Â§ñÈÉ®ÁΩëÁªú docker-compose up -d --remove-orphans ‰πãÂêéÔºåÊàë‰ª¨Êù•ÂàõÂª∫Ë¶ÅÊµãËØïÁöÑÂõæÁ©∫Èó¥ÔºåÂπ∂ÂàõÂª∫ÂõæÁöÑ SchemaÔºå‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Âà©Áî® nebula-console ÔºåÂêåÊ†∑ÔºåNebula-Up ÈáåËá™Â∏¶‰∫ÜÂÆπÂô®ÂåñÁöÑ nebula-console„ÄÇ ËøõÂÖ• Nebula-Console ÊâÄÂú®ÁöÑÂÆπÂô® ~/.nebula-up/console.sh / # Âú® console ÂÆπÂô®ÈáåÂèëËµ∑ÈìæÊé•Âà∞ÂõæÊï∞ÊçÆÂ∫ìÔºåÂÖ∂‰∏≠ 192.168.x.y ÊòØÊàëÊâÄÂú®ÁöÑ Linux VM ÁöÑÁ¨¨‰∏Ä‰∏™ÁΩëÂç°Âú∞ÂùÄÔºåËØ∑Êç¢ÊàêÊÇ®ÁöÑ / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! ÂàõÂª∫ÂõæÁ©∫Èó¥ÔºàÊàë‰ª¨Ëµ∑ÂêçÂ≠óÂè´ sst ÔºâÔºå‰ª•Âèä schema createspacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));:sleep20usesstcreatetagplayer(namestring,ageint); Á§∫‰æãËæìÂá∫ (root@nebula)[(none)]\u003ecreatespacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));Executionsucceeded(timespent1468/1918us)(root@nebula)[(none)]\u003e:sleep20(root@nebula)[(none)]\u003eusesstExecutionsucceeded(timespent1253/1566us)Wed,18Aug202108:18:13UTC(root@nebula)[sst]\u003ecreatetagplayer(namestring,ageint);Executionsucceeded(timespent1312/1735us)Wed,18Aug202108:18:23UTC ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#Êê≠Âª∫-nebula-graph-ÈõÜÁæ§"},{"categories":["Nebula Graph"],"content":"3.2 Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ Spark ÁéØÂ¢ÉÂà©Áî® big data europe ÂÅöÁöÑÂ∑•‰ΩúÔºåËøô‰∏™ËøáÁ®ãÈùûÂ∏∏ÂÆπÊòì„ÄÇ ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºö Áé∞Âú®ÁöÑ Nebula Exchange ÂØπ Spark ÁöÑÁâàÊú¨ÊúâË¶ÅÊ±ÇÔºåÂú®Áé∞Âú®ÁöÑ 2021 Âπ¥ 8 ÊúàÔºåÊàëÊòØÁî®‰∫Ü spark-2.4.5-hadoop-2.7 ÁöÑÁâàÊú¨„ÄÇ ‰∏∫‰∫ÜÊñπ‰æøÔºåÊàëËÆ© Spark ËøêË°åÂú® Nebula Graph Áõ∏ÂêåÁöÑÊú∫Âô®‰∏äÔºåÂπ∂‰∏îÊåáÂÆö‰∫ÜËøêË°åÂú®Âêå‰∏Ä‰∏™ Docker ÁΩëÁªú‰∏ã docker run --name spark-master --network nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ bde2020/spark-master:2.4.5-hadoop2.7 ÁÑ∂ÂêéÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•ËøõÂÖ•Âà∞ÁéØÂ¢É‰∏≠‰∫ÜÔºö docker exec -it spark-master bash ËøõÂà∞ Spark ÂÆπÂô®‰∏≠‰πãÂêéÔºåÂèØ‰ª•ÂÉèËøôÊ†∑ÂÆâË£Ö maven: export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn ËøòÂèØ‰ª•ËøôÊ†∑Âú®ÂÆπÂô®Èáå‰∏ãËΩΩ nebula-exchange ÁöÑ jar ÂåÖÔºö cd ~ wget https://repo1.maven.org/maven2/com/vesoft/nebula-exchange/2.1.0/nebula-exchange-2.1.0.jar ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ-spark-ÁéØÂ¢É"},{"categories":["Nebula Graph"],"content":"3.3 Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ HDFSÂêåÊ†∑ÂÄüÂä© big-data-euroupe ÁöÑÂ∑•‰ΩúÔºåËøôÈùûÂ∏∏ÁÆÄÂçïÔºå‰∏çËøáÊàë‰ª¨Ë¶ÅÂÅö‰∏ÄÁÇπ‰øÆÊîπÔºåËÆ©ÂÆÉÁöÑ docker-compose.yml Êñá‰ª∂Èáå‰ΩøÁî® nebula-net Ëøô‰∏™‰πãÂâçÂàõÂª∫ÁöÑ Docker ÁΩëÁªú„ÄÇ ËØ¶ÁªÜ‰øÆÊîπÁöÑÈÉ®ÂàÜÂèÇËÄÉÈôÑÂΩï‰∫å git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop vim docker-compose.yml docker-compose up -d ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#Êê≠Âª∫ÂÆπÂô®ÂåñÁöÑ-hdfs"},{"categories":["Nebula Graph"],"content":"4 ÈÖçÁΩÆExchangeËøô‰∏™ÈÖçÁΩÆ‰∏ªË¶ÅÂ°´ÂÖ•ÁöÑ‰ø°ÊÅØÂ∞±ÊòØ Nebula Graph ÈõÜÁæ§Êú¨Ë∫´ÂíåÂ∞ÜË¶ÅÂÜôÂÖ•Êï∞ÊçÆÁöÑ Space NameÔºå‰ª•ÂèäÊï∞ÊçÆÊ∫êÁõ∏ÂÖ≥ÁöÑÈÖçÁΩÆÔºàËøôÈáåÊàë‰ª¨Áî® csv ‰Ωú‰∏∫‰æãÂ≠êÔºâÔºåÊúÄÂêéÂÜçÈÖçÁΩÆËæìÂá∫ÔºàsinkÔºâ‰∏∫ sst Nebula Graph GraphD Âú∞ÂùÄ MetaD Âú∞ÂùÄ credential Space Name Êï∞ÊçÆÊ∫ê source: csv path fields etc. ink: sst ËØ¶ÁªÜÁöÑÈÖçÁΩÆÂèÇËÄÉÈôÑÂΩï‰∫å Ê≥®ÊÑèÔºåËøôÈáå metaD ÁöÑÂú∞ÂùÄÂèØ‰ª•ËøôÊ†∑Ëé∑ÂèñÔºåÂèØ‰ª•ÁúãÂà∞ 0.0.0.0:49377-\u003e9559 Ë°®Á§∫ 49377 ÊòØÂ§ñÈÉ®ÁöÑÂú∞ÂùÄ„ÄÇ $ docker ps | grep meta 887740c15750 vesoft/nebula-metad:v2.0.0 \"./bin/nebula-metad ‚Ä¶\" 6 hours ago Up 6 hours (healthy) 9560/tcp, 0.0.0.0:49377-\u003e9559/tcp, :::49377-\u003e9559/tcp, 0.0.0.0:49376-\u003e19559/tcp, :::49376-\u003e19559/tcp, 0.0.0.0:49375-\u003e19560/tcp, :::49375-\u003e19560/tcp nebula-docker-compose_metad0_1 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:4:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÈÖçÁΩÆexchange"},{"categories":["Nebula Graph"],"content":"5 ÁîüÊàêSSTÊñá‰ª∂","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÁîüÊàêsstÊñá‰ª∂"},{"categories":["Nebula Graph"],"content":"5.1 ÂáÜÂ§áÊ∫êÊñá‰ª∂„ÄÅÈÖçÁΩÆÊñá‰ª∂ docker cp exchange-sst.conf spark-master:/root/ docker cp player.csv spark-master:/root/ ÂÖ∂‰∏≠ player.csv ÁöÑ‰æãÂ≠êÔºö 1100,Tim Duncan,42 1101,Tony Parker,36 1102,LaMarcus Aldridge,33 1103,Rudy Gay,32 1104,Marco Belinelli,32 1105,Danny Green,31 1106,Kyle Anderson,25 1107,Aron Baynes,32 1108,Boris Diaw,36 1109,Tiago Splitter,34 1110,Cory Joseph,27 1111,David West,38 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÂáÜÂ§áÊ∫êÊñá‰ª∂ÈÖçÁΩÆÊñá‰ª∂"},{"categories":["Nebula Graph"],"content":"5.2 ÊâßË°å exchange Á®ãÂ∫èËøõÂÖ• spark-master ÂÆπÂô®ÔºåÊèê‰∫§ÊâßË°å exchange Â∫îÁî®„ÄÇ docker exec -it spark-master bash cd /root/ /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange-2.1.0.jar\\ -c exchange-sst.conf Ê£ÄÊü•ÊâßË°åÁªìÊûúÔºö spark-submit ËæìÂá∫Ôºö 21/08/17 03:37:43 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 1093 ms on localhost (executor driver) (32/32) 21/08/17 03:37:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 21/08/17 03:37:43 INFO DAGScheduler: ResultStage 2 (foreachPartition at VerticesProcessor.scala:179) finished in 22.336 s 21/08/17 03:37:43 INFO DAGScheduler: Job 1 finished: foreachPartition at VerticesProcessor.scala:179, took 22.500639 s 21/08/17 03:37:43 INFO Exchange$: SST-Import: failure.player: 0 21/08/17 03:37:43 WARN Exchange$: Edge is not defined 21/08/17 03:37:43 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040 21/08/17 03:37:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! È™åËØÅ HDFS ‰∏äÁîüÊàêÁöÑ SST Êñá‰ª∂Ôºö docker exec -it namenode /bin/bash root@2db58903fb53:/# hdfs dfs -ls /sst Found 10 items drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/1 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/10 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/2 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/3 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/4 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/5 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/6 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/7 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/8 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/9 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÊâßË°å-exchange-Á®ãÂ∫è"},{"categories":["Nebula Graph"],"content":"6 ÂÜôÂÖ•SSTÂà∞NebulaGraphËøôÈáåÁöÑÊìç‰ΩúÂÆûÈôÖ‰∏äÈÉΩÊòØÂèÇËÄÉÊñáÊ°£ÔºöSST ÂØºÂÖ•ÔºåÂæóÊù•„ÄÇÂÖ∂‰∏≠Â∞±ÊòØ‰ªé console ‰πã‰∏≠ÊâßË°å‰∫Ü‰∏§Ê≠•Êìç‰ΩúÔºö Download Ingest ÂÖ∂‰∏≠ Download ÂÆûÈôÖ‰∏äÊòØËß¶Âèë Nebula Graph ‰ªéÊúçÂä°Á´ØÂèëËµ∑ HDFS Client ÁöÑ downloadÔºåËé∑Âèñ HDFS ‰∏äÁöÑ SST Êñá‰ª∂ÔºåÁÑ∂ÂêéÊîæÂà∞ storageD ËÉΩËÆøÈóÆÁöÑÊú¨Âú∞Ë∑ØÂæÑ‰∏ãÔºåËøôÈáåÔºåÈúÄË¶ÅÊàë‰ª¨Âú®ÊúçÂä°Á´ØÈÉ®ÁΩ≤ HDFS ÁöÑ‰æùËµñ„ÄÇÂõ†‰∏∫Êàë‰ª¨ÊòØÊúÄÂ∞èÂÆûË∑µÔºåÊàëÂ∞±ÂÅ∑ÊáíÊâãÂä®ÂÅö‰∫ÜËøô‰∏™ Download ÁöÑÊìç‰Ωú„ÄÇ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÂÜôÂÖ•sstÂà∞nebulagraph"},{"categories":["Nebula Graph"],"content":"6.1 ÊâãÂä®‰∏ãËΩΩËøôÈáåËæπÊâãÂä®‰∏ãËΩΩÊàë‰ª¨Â∞±Ë¶ÅÁü•ÈÅì Nebula Graph ÊúçÂä°Á´Ø‰∏ãËΩΩÁöÑË∑ØÂæÑÔºåÂÆûÈôÖ‰∏äÊòØ /data/storage/nebula/\u003cspace_id\u003e/download/ÔºåËøôÈáåÁöÑ Space ID ÈúÄË¶ÅÊâãÂä®Ëé∑Âèñ‰∏Ä‰∏ãÔºö Ëøô‰∏™‰æãÂ≠êÈáåÔºåÊàë‰ª¨ÁöÑ Space Name ÊòØ sstÔºåËÄå Space ID ÊòØ 49„ÄÇ (root@nebula)[sst]\u003eDESCspacesst+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |ID|Name|PartitionNumber|ReplicaFactor|Charset|Collate|VidType|AtomicEdge|Group|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |49|\"sst\"|10|1|\"utf8\"|\"utf8_bin\"|\"FIXED_STRING(32)\"|\"false\"|\"default\"|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ ‰∫éÊòØÔºå‰∏ãËæπÁöÑÊìç‰ΩúÂ∞±ÊòØÊâãÂä®Êää SST Êñá‰ª∂‰ªé HDFS ‰πã‰∏≠ get ‰∏ãÊù•ÔºåÂÜçÊã∑Ë¥ùÂà∞ storageD ‰πã‰∏≠„ÄÇ docker exec -it namenode /bin/bash $ hdfs dfs -get /sst /sst exit docker cp namenode:/sst . docker exec -it nebula-docker-compose_storaged0_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged1_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged2_1 mkdir -p /data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged0_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged1_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged2_1:/data/storage/nebula/49/download/ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÊâãÂä®‰∏ãËΩΩ"},{"categories":["Nebula Graph"],"content":"6.2 SST Êñá‰ª∂ÂØºÂÖ• ËøõÂÖ• Nebula-Console ÊâÄÂú®ÁöÑÂÆπÂô® ~/.nebula-up/console.sh / # Âú® console ÂÆπÂô®ÈáåÂèëËµ∑ÈìæÊé•Âà∞ÂõæÊï∞ÊçÆÂ∫ìÔºåÂÖ∂‰∏≠ 192.168.x.y ÊòØÊàëÊâÄÂú®ÁöÑ Linux VM ÁöÑÁ¨¨‰∏Ä‰∏™ÁΩëÂç°Âú∞ÂùÄÔºåËØ∑Êç¢ÊàêÊÇ®ÁöÑ / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! ÊâßË°å INGEST ÂºÄÂßãËÆ© StorageD ËØªÂèñ SST Êñá‰ª∂ (root@nebula) [(none)]\u003e use sst (root@nebula) [sst]\u003e INGEST; Êàë‰ª¨ÂèØ‰ª•Áî®Â¶Ç‰∏ãÊñπÊ≥ïÂÆûÊó∂Êü•Áúã Nebula Graph ÊúçÂä°Á´ØÁöÑÊó•Âøó tail -f ~/.nebula-up/nebula-docker-compose/logs/*/* ÊàêÂäüÁöÑ INGEST Êó•ÂøóÔºö I0817 08:03:28.611877 169 EventListner.h:96] Ingest external SST file: column family default, the external file path /data/storage/nebula/49/download/8/8-6.sst, the internal file path /data/storage/nebula/49/data/000023.sst, the properties of the table: # data blocks=1; # entries=1; # deletions=0; # merge operands=0; # range deletions=0; raw key size=48; raw average key size=48.000000; raw value size=40; raw average value size=40.000000; data block size=75; index block size (user-key? 0, delta-value? 0)=66; filter block size=0; (estimated) table size=141; filter policy name=N/A; prefix extractor name=nullptr; column family ID=N/A; column family name=N/A; comparator name=leveldb.BytewiseComparator; merge operator name=nullptr; property collectors names=[]; SST file compression algo=Snappy; SST file compression options=window_bits=-14; level=32767; strategy=0; max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0; ; creation time=0; time stamp of earliest key=0; file creation time=0; E0817 08:03:28.611912 169 StorageHttpIngestHandler.cpp:63] SSTFile ingest successfully È¢òÂõæÁâàÊùÉÔºöPietro Jeng ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#sst-Êñá‰ª∂ÂØºÂÖ•"},{"categories":["Nebula Graph"],"content":"7 ÈôÑÂΩï","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÈôÑÂΩï"},{"categories":["Nebula Graph"],"content":"7.1 ÈôÑÂΩï‰∏Ädocker-compose.yaml diff --git a/docker-compose.yaml b/docker-compose.yaml index 48854de..cfeaedb 100644 --- a/docker-compose.yaml +++ b/docker-compose.yaml @@ -6,11 +6,13 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=metad0 - --ws_ip=metad0 - --port=9559 - --ws_http_port=19559 + - --ws_storage_http_port=19779 - --data_path=/data/meta - --log_dir=/logs - --v=0 @@ -34,81 +36,14 @@ services: cap_add: - SYS_PTRACE - metad1: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad1 - - --ws_ip=metad1 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad1:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta1:/data/meta - - ./logs/meta1:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - - metad2: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad2 - - --ws_ip=metad2 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad2:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta2:/data/meta - - ./logs/meta2:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - storaged0: image: vesoft/nebula-storaged:v2.0.0 environment: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged0 - --ws_ip=storaged0 - --port=9779 @@ -119,8 +54,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged0:19779/status\"] interval: 30s @@ -146,7 +81,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged1 - --ws_ip=storaged1 - --port=9779 @@ -157,8 +92,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged1:19779/status\"] interval: 30s @@ -184,7 +119,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged2 - --ws_ip=storaged2 - --port=9779 @@ -195,8 +130,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged2:19779/status\"] interval: 30s @@ -222,17 +157,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd:19669/status\"] interval: 30s @@ -257,17 +194,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd1 - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd1:19669/status\"] interval: 30s @@ -292,17 +231,21 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=met","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÈôÑÂΩï‰∏Ä"},{"categories":["Nebula Graph"],"content":"7.2 ÈôÑÂΩï‰∫åhttps://github.com/big-data-europe/docker-hadoop ÁöÑ docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index ed40dc6..66ff1f4 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -14,6 +14,8 @@ services: - CLUSTER_NAME=test env_file: - ./hadoop.env + networks: + - nebula-net datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 @@ -25,6 +27,8 @@ services: SERVICE_PRECONDITION: \"namenode:9870\" env_file: - ./hadoop.env + networks: + - nebula-net resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 @@ -34,6 +38,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\" env_file: - ./hadoop.env + networks: + - nebula-net nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 @@ -43,6 +49,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\" env_file: - ./hadoop.env + networks: + - nebula-net historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 @@ -54,8 +62,14 @@ services: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env + networks: + - nebula-net volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: + +networks: + nebula-net: + external: true ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÈôÑÂΩï‰∫å"},{"categories":["Nebula Graph"],"content":"7.3 ÈôÑÂΩï‰∏ânebula-exchange-sst.conf { # Spark relation config spark: { app: { name: Nebula Exchange 2.1 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"192.168.8.128:9669\"] meta:[\"192.168.8.128:49377\"] } user: root pswd: nebula space: sst # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://192.168.8.128:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is sst, just change type.sink to client if you want to use client import mode. { name: player type: { source: csv sink: sst } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ÈôÑÂΩï‰∏â"},{"categories":["sketches"],"content":"Nebula Operator Explained","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/"},{"categories":["sketches"],"content":" Nebula Graph operator explained This note explained nebula graph‚Äôs K8s Operator: Intro 00:00 Nebula K8s Operator Explained 0:25 How do we use Nebula Operator? 02:23 What is the difference between the Operator based Nebula Graph Cluster and the binary-based one? 03:50 How about the Performance impact when it comes to K8s-Operator deployment? 04:55 What is the easiest way to try out the nebula operator? 06:04 Outra 07:30 ref: https://github.com/vesoft-inc/nebula-operator ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:0:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:1:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:2:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Config Explained","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/"},{"categories":["sketches"],"content":" Nebula Graph config explained This note explained nebula graph configurations: Intro 00:00 Nebula Graph Config Explained 0:16 How about Configurations in Nebula Graph Deployed with Docker? 03:01 What about Nebula Graph in K8s Operator Deployment case? 03:55 Should we use Local-Config or Not?(spoiler: Yes!) 05:03 Outra 05:27 ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:0:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:1:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:2:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Index Demystified","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/"},{"categories":["sketches"],"content":" Nebula Graph Native Index Demystified(Chinese only now, English version will be soon uploaded) Index Demystified 0:33 When should we use index? 06:37 Index v.s. Fulltext Index 07:12 Index Performance Impact 08:03 ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:0:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:1:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:2:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Deployment Options","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/"},{"categories":["sketches"],"content":" Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:0:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:1:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:2:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Data Import Options","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/"},{"categories":["sketches"],"content":" Nebula Graph comes with multiple Data Import utils and options, how should we choose from them? ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:0:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:1:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:2:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#youtube"},{"categories":["Nebula Graph"],"content":"one liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker)","date":"2021-06-09","objectID":"/en/nebula-operator-kind/","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/"},{"categories":["Nebula Graph"],"content":" Nebula-Kind, an one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND (K8s in Docker) ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:0:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#"},{"categories":["Nebula Graph"],"content":"1 Nebula-Operator-KindAs a Cloud Native Distributed Database, Nebula Graph comes with an open-source K8s Operator to enable boostrap and maintain Nebula Graph Cluster from a K8s CRD. Normally it takes you some time to setup all the dependencies and control plane resources of the Nebula Operator. If you are as lazy as I am, this Nebula-Operator-Kind is made for you to quick start and play with Nebula Graph in KIND. Nebula-Operator-Kind is the one-liner for setup everything for you including: Docker K8s(KIND) PV Provider Nebula-Operator Nebula-Console nodePort for accessing the Cluster Kubectl for playing with KIND and Nebula Operator ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:1:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#nebula-operator-kind"},{"categories":["Nebula Graph"],"content":"2 How To UseInstall Nebula-Operator-Kind: curl -sL nebula-kind.siwei.io/install.sh | bash You will see this after it‚Äôs done You can connect to the cluster via ~/.nebula-kind/bin/console as below: ~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000 ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:2:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#how-to-use"},{"categories":["Nebula Graph"],"content":"3 MoreIt‚Äôs in GitHub with more information you may be intrested in ;-), please try and feedback there~ https://github.com/wey-gu/nebula-operator-kind Updated Sept. 2021 Install on KubeSphere all-in-on clusterÔºö curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash Install on existing K8s cluster: curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash Banner Picture Credit: Maik Hankemann ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:3:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#more"},{"categories":null,"content":"Hi, this is Wey :)I am a developer @vesoft working as Developer Advocate of Nebula Graph, the open source distributed Graph Database I create toolings and content for Nebula Graph Database to help Developers in the open source community. I am working in open source and consider it is a privilege 1. It took me a couple of my early career years to figure out that my passion lies in helping others with my thoughts \u0026 the tech/magic I have learned. ","date":"2021-06-04","objectID":"/en/about/:0:0","series":null,"tags":null,"title":"","uri":"/en/about/#hi-this-is-wey-"},{"categories":null,"content":"1 Recent Projects Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-06-04","objectID":"/en/about/:1:0","series":null,"tags":null,"title":"","uri":"/en/about/#recent-projects"},{"categories":null,"content":"2 Sketches Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option should I use? Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-06-04","objectID":"/en/about/:2:0","series":null,"tags":null,"title":"","uri":"/en/about/#sketches"},{"categories":null,"content":"3 Hands-on Cources How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-06-04","objectID":"/en/about/:3:0","series":null,"tags":null,"title":"","uri":"/en/about/#hands-on-cources"},{"categories":null,"content":"4 Talks Data on K8s Community 2021 GraphDB on Kubesphere Read more... ","date":"2021-06-04","objectID":"/en/about/:4:0","series":null,"tags":null,"title":"","uri":"/en/about/#talks"},{"categories":null,"content":"5 Previous workI worked at Ericsson for amost 10 years(2011 to 2021). As the System Manager 2 of Cloud Execution Envrioment (CEE) 3 PDU Cloud, member of CEE 10 core team and CEE System Management team. Helping evolve CEE was my main job: I studied, designed and implemented more than 20 features for CEE 6.6.2 and CEE 10, including area of compute, network, storage, lifecycle management and security. I am also responsible for Ericsson CEE evangelism (internal and external) in China. I used to share my notes and thoughts on note.siwei.info, while from 2021, I will leave more ideas on siwei.io instead. ","date":"2021-06-04","objectID":"/en/about/:5:0","series":null,"tags":null,"title":"","uri":"/en/about/#previous-work"},{"categories":null,"content":"6 ContactYou can DM me via twitter, or wey.gu@vesoft.com. I share the same idea with Ahmet Alp Balkan‚Äôs tweet: Working in open source (and getting paid for it) is a privilege. It‚Äôs a career boost, makes you lots of friends across the industry, and gives you a public brand. I am one of the ‚Äúlucky few‚Äù \u0026 thankful to Microsoft and Google who let me work on OSS nearly all my career. ‚Äî ahmetbÓ®Ä (@ahmetb) February 19, 2021 ¬†‚Ü©Ô∏é System Manager, PDU Cloud: Job Description¬†‚Ü©Ô∏é Ericsson‚Äôs Telco. Infrastructure as a Service product offerring: Cloud Execution Environment¬†‚Ü©Ô∏é ","date":"2021-06-04","objectID":"/en/about/:6:0","series":null,"tags":null,"title":"","uri":"/en/about/#contact"},{"categories":["Nebula Graph"],"content":"Êú¨ÊñáÂàÜÊûê‰∫Ü Chia Network ÁöÑÂÖ®ÈìæÊï∞ÊçÆÔºåÂπ∂ÂÅö‰∫ÜÂ∞ÜÂÖ®ÈìæÊï∞ÊçÆÂØºÂÖ•ÂõæÊï∞ÊçÆÂ∫ìÔºöNebula Graph ‰πã‰∏≠ÁöÑÂ∞ùËØïÔºå‰ªéËÄåÂèØËßÜÂåñÂú∞Êé¢Á¥¢‰∫Ü Chia Âõæ‰∏≠Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥ËÅîÂÖ≥Á≥ª„ÄÇ","date":"2021-05-26","objectID":"/en/nebula-chia/","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/"},{"categories":["Nebula Graph"],"content":" Êú¨ÊñáÂàÜÊûê‰∫Ü Chia Network ÁöÑÂÖ®ÈìæÊï∞ÊçÆÔºåÂπ∂ÂÅö‰∫ÜÂ∞ÜÂÖ®ÈìæÊï∞ÊçÆÂØºÂÖ•ÂõæÊï∞ÊçÆÂ∫ìÔºöNebula Graph ‰πã‰∏≠ÁöÑÂ∞ùËØïÔºå‰ªéËÄåÂèØËßÜÂåñÂú∞Êé¢Á¥¢‰∫Ü Chia Âõæ‰∏≠Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥ËÅîÂÖ≥Á≥ª„ÄÇ ÊàëÊääÊ∂âÂèäÁöÑ‰ª£Á†ÅÂºÄÊ∫êÂú®‰∫ÜËøôÈáåÔºöhttps://github.com/wey-gu/nebula-chia ","date":"2021-05-26","objectID":"/en/nebula-chia/:0:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#"},{"categories":["Nebula Graph"],"content":"1 What is Chia Network?Chia Network ÊòØÁî± BitTorrent ÁöÑ‰ΩúËÄÖ Bram Cohen ÁöÑÂõ¢ÈòüÂú® 2017 Âπ¥ÂàõÂª∫ÁöÑÂå∫ÂùóÈìæÈ°πÁõÆ„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#what-is-chia-network"},{"categories":["Nebula Graph"],"content":"1.1 Why yet another Blockchain? ‰∏∫‰ªÄ‰πàÂÜçÊêû‰∏Ä‰∏™Âå∫ÂùóÈìæ?Chia Áî®‰∫ÜÂÖ®Êñ∞ÁöÑ‰∏≠Êú¨ËÅ™ÂÖ±ËØÜÁÆóÊ≥ïÔºåËøô‰∏™ÁÆóÊ≥ïÈÄöËøá‰∏çÂÖÅËÆ∏Âπ∂Ë°åËÆ°ÁÆóÔºåËÆ©ÊåñÁüøÔºàProof of WorkÔºâÊâÄÈúÄÁÆóÂäõÂíåËÉΩËÄóÈôçÂà∞ÈùûÂ∏∏‰ΩéÔºåËøô‰ΩøÂæóË∂ÖÂ§ßÁªÑÁªá„ÄÅÁé©ÂÆ∂Ê≤°Ê≥ïÂÉèÂú®ÂÖ∂‰ªñÁöÑÂå∫ÂùóÈìæÈ°πÁõÆÈÇ£Ê†∑ÊúâÁÆóÂäõÁöÑÁªùÂØπ‰ºòÂäøÔºå‰πü‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äËßÑÈÅø‰∫ÜËÉΩÊ∫êÁöÑÊµ™Ë¥π„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#why-yet-another-blockchain-‰∏∫‰ªÄ‰πàÂÜçÊêû‰∏Ä‰∏™Âå∫ÂùóÈìæ"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? Â¶Ç‰ΩïËøûÊé•Chia?Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá Chia Network ÁöÑÂÆ¢Êà∑Á´ØÊù•ËÆøÈóÆÂÆÉÔºåËøô‰∏™ÂÆ¢Êà∑Á´ØÊòØ Electron + Python ÁöÑÁ®ãÂ∫èÔºåÂ§©ÁÑ∂Ë∑®Âπ≥Âè∞ÔºåÊó¢Êúâ GUI ÂèàÊúâ CLI ÁöÑÊñπÂºè„ÄÇ 1.2.1 ÂÆâË£ÖÂè™ÈúÄË¶ÅÊåâÁÖßÂÆòÊñπÁöÑ Guide Êù•‰∏ãËΩΩÂÆâË£ÖÂ∞±Â•ΩÔºå https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLÔºåÊàëÂú® M1 Mac ‰∏ãÂÆâË£ÖÁöÑÊó∂ÂÄôËÑöÊú¨Âá∫‰∫ÜÁÇπÂ∞èÈóÆÈ¢òÔºåÂ§ßÊ¶ÇÊòØÂõ†‰∏∫ÊãâÂèñ‰∫åËøõÂà∂ wheel Êñá‰ª∂ÁΩëÁªúÂá∫ÈóÆÈ¢òËµ∞Âà∞‰∫ÜÁºñËØë wheelÁöÑÈÄªËæëÔºåËÄåÈÇ£ÈáåÊòØ‰æùËµñ cargoÁöÑÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÈÅáÂà∞‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÂâçÊâãÂä®ÂÆâË£Ö‰∏Ä‰∏ã rustÔºåÊàñËÄÖ cherry-pick ÊàëÁöÑËøô‰∏™ PR „ÄÇ 1.2.2 ËøêË°å ÊåâÁÖßÂÆòÊñπ guideÔºåÊØîÂ¶Ç macOS Êù•ËØ¥ÔºåÊúÄÂêé‰∏ÄÊ≠•ÊâßË°ånpm run electron \u0026 Â∞±ÊòØËøêË°åÂÆÉÁöÑGUIÂÆ¢Êà∑Á´Ø„ÄÇ Â¶ÇÊûúÂ§ßÂÆ∂ÂÉèÊàë‰∏ÄÊ†∑ÂñúÊ¨¢ CLIÔºåÁõ¥Êé•Âú®ÊâßË°åÂÆå . ./activate ‰πãÂêéÂ∞±ÂèØ‰ª• chia --help‰∫ÜÂìà‚ò∫ÔºåÈáåËæπÊúâÂè™ÂêØÂä®ÈÉ®ÂàÜÊúçÂä°ÁöÑÊñπÂºèÔºàÁõ∏ÊØî GUI ÂêØÂä®ÊâÄÊúâÊù•ËØ¥)„ÄÇ Âú®ËøêË°å‰πãÂêéÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁΩëÁªú‰∏çÊòØÂ§öÂ±Ç NAT ÁöÑÈÇ£ÁßçÔºåÁêÜËÆ∫‰∏äÊÇ®ÂèØ‰ª•ËøûÂà∞ mainnet Âπ∂‰∏îËá™Âä®Âíå‰∏ªÈìæÂêåÊ≠•Êï∞ÊçÆ‰∫ÜÔºåÂ¶ÇÊûúÊÇ®ÊòØÁ¨¨‰∫åÊ¨°ËøêË°åÔºåËøûÊé•‰∏ªÈìæÔºå‰∏ÄÂºÄÂßãÂèØËÉΩÊúâ‰∏ÄÈòµÂ≠êÂêåÊ≠•ÁöÑblock Êï∞ÊòØ‰∏çÂèòÁöÑÔºå‰πüÊ≤°Êúâ peer ËøûËøáÊù•Ôºå‰∏çÂøÖÊÉäÊÖåÔºåÁ≠â‰∏Ä‰∏ãÂ∞±Â•Ω‰∫Ü„ÄÇ Tips: Á¨¨‰∏ÄÊ¨°ËøûÂà∞ Chia Network ÁöÑÂêåÂ≠¶‰ª¨ÔºåÂÆ¢Êà∑Á´Ø‰ºöËá™Âä®ÁîüÊàê‰∏Ä‰∏™Èí±ÂåÖÔºåÂèäÁöÑ‰øùÂ≠òÈÇ£‰∏Ä‰∏≤ËØçÔºåÂÆÉ‰ª¨Â∞±ÊòØ‰Ω†ÁöÑÁßÅÈí•Âì¶„ÄÇ ‰∏á‰∏ÄÔºåÂ¶ÇÊûúÁúüÁöÑËøû‰∏ç‰∏äÁöÑËØùÔºåÂèØËÉΩÈúÄË¶ÅÂú®Ë∑ØÁî±‰∏äÈÖçÁΩÆÔºåUPnPÔºåÈò≤ÁÅ´Â¢ôË¶ÅÂÖÅËÆ∏ 8444„ÄÇ 1.2.3 ËÆøÈóÆ Chia ÁöÑÊï∞ÊçÆChia ÁöÑÂÆ¢Êà∑Á´ØÊääÊï∞ÊçÆÂ≠òÂú®‰∫ÜÂá†‰∏™ SQLite Êï∞ÊçÆÂ∫ìÈáåÔºåÂÆÉ‰ª¨ÁöÑË∑ØÂæÑÊòØÊàë‰ª¨ÂÆâË£ÖÂÆ¢Êà∑Á´ØÁöÑÁî®Êà∑ÁöÑÂÆ∂ÁõÆÂΩïÔºö~/.chia/mainnet ‰∏ãËæπÂ∞±ÊòØËøêË°åËµ∑Êù• Chia ‰πãÂêéÁîüÊàêÁöÑ‰∏ªË¶ÅÁöÑ‰∏§‰∏™Êï∞ÊçÆÂ∫ìÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ôºö ~/.chia/mainnet/db ‚ùØ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ‚ùØ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ÁÑ∂ÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÖàÁî® SQLite BrowserÔºå‰∏Ä‰∏™ SQlite Êï∞ÊçÆÂ∫ìÔºàÊñá‰ª∂ÔºâÁöÑÊµèËßàÂô®Êù•ÁúãÁúãÂÆÉ„ÄÇ SQlite ÊµèËßàÂô®ÁöÑÂÆòÁΩëÊòØ https://sqlitebrowser.org/ „ÄÇÂú®‰∏ãËΩΩÔºåÂÆâË£Ö‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÁÇπÂáª Open Database/ÊâìÂºÄÊï∞ÊçÆÂ∫ì ÈÄâÊã©ÊµèËßà‰∏äËæπÂàóÂá∫Êù•ÁöÑ‰∏§‰∏™ .sqlite Êâ©Â±ïÂêçÁöÑÊï∞ÊçÆÂ∫ìÊñá‰ª∂„ÄÇ ÊâìÂºÄÊï∞ÊçÆÂ∫ì‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã©Á¨¨‰∏Ä‰∏™Ê†áÁ≠æ Database Schema Êù•ÁúãÁúãË°®ÁöÑÁªìÊûÑ„ÄÇ Êàë‰ª¨ËøòËÉΩÂÉèÁ±ª‰ºº‰∫é Excel ‰∏ÄÊ†∑ÂéªÁúãË°®ÁöÑÊï∞ÊçÆÔºåËøòÂèØ‰ª• Filter/ËøáÊª§„ÄÅSort/ÊéíÂ∫è‰ªªÊÑèÂàó„ÄÇ ‰∏ã‰∏ÄÈÉ®ÂàÜÔºåÊàë‰ª¨Êù•ÁÆÄÂçïÁúãÁúãË°®ÈáåÁöÑÊï∞ÊçÆ„ÄÇ Tips: ËøôÈáåËæπÔºå~/.chia/mainnet/wallet ÂíåË£∏ÁõÆÂΩï ~/.chia/mainnet ‰∏ãËæπÁöÑ db ÈáåÂàÜÂà´ÈÉΩÊúâË°®Êñá‰ª∂Ôºå‰ªñ‰ª¨ÁöÑ‰ø°ÊÅØÊòØÊúâÈáçÂ§çÁöÑÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂàÜÂà´ÊâìÂºÄÁúãÁúãÂì¶ÔºåÂç≥‰ΩøÊòØÁõ∏ÂêåÁöÑË°®ÁöÑÂêçÂ≠óÔºåÊØîÂ¶Ç block_record ÂÜÖÈáåÁöÑ‰ø°ÊÅØ‰πüÁï•ÊúâÂ∑ÆÂà´ÔºåÂ¶ÇÊûúÂ§ßÂÆ∂Áü•ÈÅì‰∏∫‰ªÄ‰πàÊúâËøôÊ†∑ÁöÑÂ∑ÆÂà´ÔºåÊ¨¢ËøéÊµèËßàÂëäËØâÂ§ßÂÆ∂ÂìàÔºåÂèØËÉΩË¶Å‰ªîÁªÜÁ†îÁ©∂‰∏Ä‰∏ãÂÆ¢Êà∑Á´Ø„ÄÅÈí±ÂåÖÁ≠â‰ª£Á†ÅÊâçË°åÔºåÂπ∏ËøêÁöÑÊòØÔºåÂÆÉ‰ª¨Áõ∏ÂØπÊØîËæÉÂ•ΩÈòÖËØªÔºåÊòØ Python ÂÜôÁöÑÔºö https://github.com/Chia-Network/chia-blockchain „ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-can-i-access-chia-network-Â¶Ç‰ΩïËøûÊé•chia"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? Â¶Ç‰ΩïËøûÊé•Chia?Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá Chia Network ÁöÑÂÆ¢Êà∑Á´ØÊù•ËÆøÈóÆÂÆÉÔºåËøô‰∏™ÂÆ¢Êà∑Á´ØÊòØ Electron + Python ÁöÑÁ®ãÂ∫èÔºåÂ§©ÁÑ∂Ë∑®Âπ≥Âè∞ÔºåÊó¢Êúâ GUI ÂèàÊúâ CLI ÁöÑÊñπÂºè„ÄÇ 1.2.1 ÂÆâË£ÖÂè™ÈúÄË¶ÅÊåâÁÖßÂÆòÊñπÁöÑ Guide Êù•‰∏ãËΩΩÂÆâË£ÖÂ∞±Â•ΩÔºå https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLÔºåÊàëÂú® M1 Mac ‰∏ãÂÆâË£ÖÁöÑÊó∂ÂÄôËÑöÊú¨Âá∫‰∫ÜÁÇπÂ∞èÈóÆÈ¢òÔºåÂ§ßÊ¶ÇÊòØÂõ†‰∏∫ÊãâÂèñ‰∫åËøõÂà∂ wheel Êñá‰ª∂ÁΩëÁªúÂá∫ÈóÆÈ¢òËµ∞Âà∞‰∫ÜÁºñËØë wheelÁöÑÈÄªËæëÔºåËÄåÈÇ£ÈáåÊòØ‰æùËµñ cargoÁöÑÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÈÅáÂà∞‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÂâçÊâãÂä®ÂÆâË£Ö‰∏Ä‰∏ã rustÔºåÊàñËÄÖ cherry-pick ÊàëÁöÑËøô‰∏™ PR „ÄÇ 1.2.2 ËøêË°å ÊåâÁÖßÂÆòÊñπ guideÔºåÊØîÂ¶Ç macOS Êù•ËØ¥ÔºåÊúÄÂêé‰∏ÄÊ≠•ÊâßË°ånpm run electron \u0026 Â∞±ÊòØËøêË°åÂÆÉÁöÑGUIÂÆ¢Êà∑Á´Ø„ÄÇ Â¶ÇÊûúÂ§ßÂÆ∂ÂÉèÊàë‰∏ÄÊ†∑ÂñúÊ¨¢ CLIÔºåÁõ¥Êé•Âú®ÊâßË°åÂÆå . ./activate ‰πãÂêéÂ∞±ÂèØ‰ª• chia --help‰∫ÜÂìà‚ò∫ÔºåÈáåËæπÊúâÂè™ÂêØÂä®ÈÉ®ÂàÜÊúçÂä°ÁöÑÊñπÂºèÔºàÁõ∏ÊØî GUI ÂêØÂä®ÊâÄÊúâÊù•ËØ¥)„ÄÇ Âú®ËøêË°å‰πãÂêéÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁΩëÁªú‰∏çÊòØÂ§öÂ±Ç NAT ÁöÑÈÇ£ÁßçÔºåÁêÜËÆ∫‰∏äÊÇ®ÂèØ‰ª•ËøûÂà∞ mainnet Âπ∂‰∏îËá™Âä®Âíå‰∏ªÈìæÂêåÊ≠•Êï∞ÊçÆ‰∫ÜÔºåÂ¶ÇÊûúÊÇ®ÊòØÁ¨¨‰∫åÊ¨°ËøêË°åÔºåËøûÊé•‰∏ªÈìæÔºå‰∏ÄÂºÄÂßãÂèØËÉΩÊúâ‰∏ÄÈòµÂ≠êÂêåÊ≠•ÁöÑblock Êï∞ÊòØ‰∏çÂèòÁöÑÔºå‰πüÊ≤°Êúâ peer ËøûËøáÊù•Ôºå‰∏çÂøÖÊÉäÊÖåÔºåÁ≠â‰∏Ä‰∏ãÂ∞±Â•Ω‰∫Ü„ÄÇ Tips: Á¨¨‰∏ÄÊ¨°ËøûÂà∞ Chia Network ÁöÑÂêåÂ≠¶‰ª¨ÔºåÂÆ¢Êà∑Á´Ø‰ºöËá™Âä®ÁîüÊàê‰∏Ä‰∏™Èí±ÂåÖÔºåÂèäÁöÑ‰øùÂ≠òÈÇ£‰∏Ä‰∏≤ËØçÔºåÂÆÉ‰ª¨Â∞±ÊòØ‰Ω†ÁöÑÁßÅÈí•Âì¶„ÄÇ ‰∏á‰∏ÄÔºåÂ¶ÇÊûúÁúüÁöÑËøû‰∏ç‰∏äÁöÑËØùÔºåÂèØËÉΩÈúÄË¶ÅÂú®Ë∑ØÁî±‰∏äÈÖçÁΩÆÔºåUPnPÔºåÈò≤ÁÅ´Â¢ôË¶ÅÂÖÅËÆ∏ 8444„ÄÇ 1.2.3 ËÆøÈóÆ Chia ÁöÑÊï∞ÊçÆChia ÁöÑÂÆ¢Êà∑Á´ØÊääÊï∞ÊçÆÂ≠òÂú®‰∫ÜÂá†‰∏™ SQLite Êï∞ÊçÆÂ∫ìÈáåÔºåÂÆÉ‰ª¨ÁöÑË∑ØÂæÑÊòØÊàë‰ª¨ÂÆâË£ÖÂÆ¢Êà∑Á´ØÁöÑÁî®Êà∑ÁöÑÂÆ∂ÁõÆÂΩïÔºö~/.chia/mainnet ‰∏ãËæπÂ∞±ÊòØËøêË°åËµ∑Êù• Chia ‰πãÂêéÁîüÊàêÁöÑ‰∏ªË¶ÅÁöÑ‰∏§‰∏™Êï∞ÊçÆÂ∫ìÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ôºö ~/.chia/mainnet/db ‚ùØ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ‚ùØ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ÁÑ∂ÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÖàÁî® SQLite BrowserÔºå‰∏Ä‰∏™ SQlite Êï∞ÊçÆÂ∫ìÔºàÊñá‰ª∂ÔºâÁöÑÊµèËßàÂô®Êù•ÁúãÁúãÂÆÉ„ÄÇ SQlite ÊµèËßàÂô®ÁöÑÂÆòÁΩëÊòØ https://sqlitebrowser.org/ „ÄÇÂú®‰∏ãËΩΩÔºåÂÆâË£Ö‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÁÇπÂáª Open Database/ÊâìÂºÄÊï∞ÊçÆÂ∫ì ÈÄâÊã©ÊµèËßà‰∏äËæπÂàóÂá∫Êù•ÁöÑ‰∏§‰∏™ .sqlite Êâ©Â±ïÂêçÁöÑÊï∞ÊçÆÂ∫ìÊñá‰ª∂„ÄÇ ÊâìÂºÄÊï∞ÊçÆÂ∫ì‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã©Á¨¨‰∏Ä‰∏™Ê†áÁ≠æ Database Schema Êù•ÁúãÁúãË°®ÁöÑÁªìÊûÑ„ÄÇ Êàë‰ª¨ËøòËÉΩÂÉèÁ±ª‰ºº‰∫é Excel ‰∏ÄÊ†∑ÂéªÁúãË°®ÁöÑÊï∞ÊçÆÔºåËøòÂèØ‰ª• Filter/ËøáÊª§„ÄÅSort/ÊéíÂ∫è‰ªªÊÑèÂàó„ÄÇ ‰∏ã‰∏ÄÈÉ®ÂàÜÔºåÊàë‰ª¨Êù•ÁÆÄÂçïÁúãÁúãË°®ÈáåÁöÑÊï∞ÊçÆ„ÄÇ Tips: ËøôÈáåËæπÔºå~/.chia/mainnet/wallet ÂíåË£∏ÁõÆÂΩï ~/.chia/mainnet ‰∏ãËæπÁöÑ db ÈáåÂàÜÂà´ÈÉΩÊúâË°®Êñá‰ª∂Ôºå‰ªñ‰ª¨ÁöÑ‰ø°ÊÅØÊòØÊúâÈáçÂ§çÁöÑÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂàÜÂà´ÊâìÂºÄÁúãÁúãÂì¶ÔºåÂç≥‰ΩøÊòØÁõ∏ÂêåÁöÑË°®ÁöÑÂêçÂ≠óÔºåÊØîÂ¶Ç block_record ÂÜÖÈáåÁöÑ‰ø°ÊÅØ‰πüÁï•ÊúâÂ∑ÆÂà´ÔºåÂ¶ÇÊûúÂ§ßÂÆ∂Áü•ÈÅì‰∏∫‰ªÄ‰πàÊúâËøôÊ†∑ÁöÑÂ∑ÆÂà´ÔºåÊ¨¢ËøéÊµèËßàÂëäËØâÂ§ßÂÆ∂ÂìàÔºåÂèØËÉΩË¶Å‰ªîÁªÜÁ†îÁ©∂‰∏Ä‰∏ãÂÆ¢Êà∑Á´Ø„ÄÅÈí±ÂåÖÁ≠â‰ª£Á†ÅÊâçË°åÔºåÂπ∏ËøêÁöÑÊòØÔºåÂÆÉ‰ª¨Áõ∏ÂØπÊØîËæÉÂ•ΩÈòÖËØªÔºåÊòØ Python ÂÜôÁöÑÔºö https://github.com/Chia-Network/chia-blockchain „ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#ÂÆâË£Ö"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? Â¶Ç‰ΩïËøûÊé•Chia?Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá Chia Network ÁöÑÂÆ¢Êà∑Á´ØÊù•ËÆøÈóÆÂÆÉÔºåËøô‰∏™ÂÆ¢Êà∑Á´ØÊòØ Electron + Python ÁöÑÁ®ãÂ∫èÔºåÂ§©ÁÑ∂Ë∑®Âπ≥Âè∞ÔºåÊó¢Êúâ GUI ÂèàÊúâ CLI ÁöÑÊñπÂºè„ÄÇ 1.2.1 ÂÆâË£ÖÂè™ÈúÄË¶ÅÊåâÁÖßÂÆòÊñπÁöÑ Guide Êù•‰∏ãËΩΩÂÆâË£ÖÂ∞±Â•ΩÔºå https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLÔºåÊàëÂú® M1 Mac ‰∏ãÂÆâË£ÖÁöÑÊó∂ÂÄôËÑöÊú¨Âá∫‰∫ÜÁÇπÂ∞èÈóÆÈ¢òÔºåÂ§ßÊ¶ÇÊòØÂõ†‰∏∫ÊãâÂèñ‰∫åËøõÂà∂ wheel Êñá‰ª∂ÁΩëÁªúÂá∫ÈóÆÈ¢òËµ∞Âà∞‰∫ÜÁºñËØë wheelÁöÑÈÄªËæëÔºåËÄåÈÇ£ÈáåÊòØ‰æùËµñ cargoÁöÑÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÈÅáÂà∞‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÂâçÊâãÂä®ÂÆâË£Ö‰∏Ä‰∏ã rustÔºåÊàñËÄÖ cherry-pick ÊàëÁöÑËøô‰∏™ PR „ÄÇ 1.2.2 ËøêË°å ÊåâÁÖßÂÆòÊñπ guideÔºåÊØîÂ¶Ç macOS Êù•ËØ¥ÔºåÊúÄÂêé‰∏ÄÊ≠•ÊâßË°ånpm run electron \u0026 Â∞±ÊòØËøêË°åÂÆÉÁöÑGUIÂÆ¢Êà∑Á´Ø„ÄÇ Â¶ÇÊûúÂ§ßÂÆ∂ÂÉèÊàë‰∏ÄÊ†∑ÂñúÊ¨¢ CLIÔºåÁõ¥Êé•Âú®ÊâßË°åÂÆå . ./activate ‰πãÂêéÂ∞±ÂèØ‰ª• chia --help‰∫ÜÂìà‚ò∫ÔºåÈáåËæπÊúâÂè™ÂêØÂä®ÈÉ®ÂàÜÊúçÂä°ÁöÑÊñπÂºèÔºàÁõ∏ÊØî GUI ÂêØÂä®ÊâÄÊúâÊù•ËØ¥)„ÄÇ Âú®ËøêË°å‰πãÂêéÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁΩëÁªú‰∏çÊòØÂ§öÂ±Ç NAT ÁöÑÈÇ£ÁßçÔºåÁêÜËÆ∫‰∏äÊÇ®ÂèØ‰ª•ËøûÂà∞ mainnet Âπ∂‰∏îËá™Âä®Âíå‰∏ªÈìæÂêåÊ≠•Êï∞ÊçÆ‰∫ÜÔºåÂ¶ÇÊûúÊÇ®ÊòØÁ¨¨‰∫åÊ¨°ËøêË°åÔºåËøûÊé•‰∏ªÈìæÔºå‰∏ÄÂºÄÂßãÂèØËÉΩÊúâ‰∏ÄÈòµÂ≠êÂêåÊ≠•ÁöÑblock Êï∞ÊòØ‰∏çÂèòÁöÑÔºå‰πüÊ≤°Êúâ peer ËøûËøáÊù•Ôºå‰∏çÂøÖÊÉäÊÖåÔºåÁ≠â‰∏Ä‰∏ãÂ∞±Â•Ω‰∫Ü„ÄÇ Tips: Á¨¨‰∏ÄÊ¨°ËøûÂà∞ Chia Network ÁöÑÂêåÂ≠¶‰ª¨ÔºåÂÆ¢Êà∑Á´Ø‰ºöËá™Âä®ÁîüÊàê‰∏Ä‰∏™Èí±ÂåÖÔºåÂèäÁöÑ‰øùÂ≠òÈÇ£‰∏Ä‰∏≤ËØçÔºåÂÆÉ‰ª¨Â∞±ÊòØ‰Ω†ÁöÑÁßÅÈí•Âì¶„ÄÇ ‰∏á‰∏ÄÔºåÂ¶ÇÊûúÁúüÁöÑËøû‰∏ç‰∏äÁöÑËØùÔºåÂèØËÉΩÈúÄË¶ÅÂú®Ë∑ØÁî±‰∏äÈÖçÁΩÆÔºåUPnPÔºåÈò≤ÁÅ´Â¢ôË¶ÅÂÖÅËÆ∏ 8444„ÄÇ 1.2.3 ËÆøÈóÆ Chia ÁöÑÊï∞ÊçÆChia ÁöÑÂÆ¢Êà∑Á´ØÊääÊï∞ÊçÆÂ≠òÂú®‰∫ÜÂá†‰∏™ SQLite Êï∞ÊçÆÂ∫ìÈáåÔºåÂÆÉ‰ª¨ÁöÑË∑ØÂæÑÊòØÊàë‰ª¨ÂÆâË£ÖÂÆ¢Êà∑Á´ØÁöÑÁî®Êà∑ÁöÑÂÆ∂ÁõÆÂΩïÔºö~/.chia/mainnet ‰∏ãËæπÂ∞±ÊòØËøêË°åËµ∑Êù• Chia ‰πãÂêéÁîüÊàêÁöÑ‰∏ªË¶ÅÁöÑ‰∏§‰∏™Êï∞ÊçÆÂ∫ìÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ôºö ~/.chia/mainnet/db ‚ùØ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ‚ùØ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ÁÑ∂ÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÖàÁî® SQLite BrowserÔºå‰∏Ä‰∏™ SQlite Êï∞ÊçÆÂ∫ìÔºàÊñá‰ª∂ÔºâÁöÑÊµèËßàÂô®Êù•ÁúãÁúãÂÆÉ„ÄÇ SQlite ÊµèËßàÂô®ÁöÑÂÆòÁΩëÊòØ https://sqlitebrowser.org/ „ÄÇÂú®‰∏ãËΩΩÔºåÂÆâË£Ö‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÁÇπÂáª Open Database/ÊâìÂºÄÊï∞ÊçÆÂ∫ì ÈÄâÊã©ÊµèËßà‰∏äËæπÂàóÂá∫Êù•ÁöÑ‰∏§‰∏™ .sqlite Êâ©Â±ïÂêçÁöÑÊï∞ÊçÆÂ∫ìÊñá‰ª∂„ÄÇ ÊâìÂºÄÊï∞ÊçÆÂ∫ì‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã©Á¨¨‰∏Ä‰∏™Ê†áÁ≠æ Database Schema Êù•ÁúãÁúãË°®ÁöÑÁªìÊûÑ„ÄÇ Êàë‰ª¨ËøòËÉΩÂÉèÁ±ª‰ºº‰∫é Excel ‰∏ÄÊ†∑ÂéªÁúãË°®ÁöÑÊï∞ÊçÆÔºåËøòÂèØ‰ª• Filter/ËøáÊª§„ÄÅSort/ÊéíÂ∫è‰ªªÊÑèÂàó„ÄÇ ‰∏ã‰∏ÄÈÉ®ÂàÜÔºåÊàë‰ª¨Êù•ÁÆÄÂçïÁúãÁúãË°®ÈáåÁöÑÊï∞ÊçÆ„ÄÇ Tips: ËøôÈáåËæπÔºå~/.chia/mainnet/wallet ÂíåË£∏ÁõÆÂΩï ~/.chia/mainnet ‰∏ãËæπÁöÑ db ÈáåÂàÜÂà´ÈÉΩÊúâË°®Êñá‰ª∂Ôºå‰ªñ‰ª¨ÁöÑ‰ø°ÊÅØÊòØÊúâÈáçÂ§çÁöÑÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂàÜÂà´ÊâìÂºÄÁúãÁúãÂì¶ÔºåÂç≥‰ΩøÊòØÁõ∏ÂêåÁöÑË°®ÁöÑÂêçÂ≠óÔºåÊØîÂ¶Ç block_record ÂÜÖÈáåÁöÑ‰ø°ÊÅØ‰πüÁï•ÊúâÂ∑ÆÂà´ÔºåÂ¶ÇÊûúÂ§ßÂÆ∂Áü•ÈÅì‰∏∫‰ªÄ‰πàÊúâËøôÊ†∑ÁöÑÂ∑ÆÂà´ÔºåÊ¨¢ËøéÊµèËßàÂëäËØâÂ§ßÂÆ∂ÂìàÔºåÂèØËÉΩË¶Å‰ªîÁªÜÁ†îÁ©∂‰∏Ä‰∏ãÂÆ¢Êà∑Á´Ø„ÄÅÈí±ÂåÖÁ≠â‰ª£Á†ÅÊâçË°åÔºåÂπ∏ËøêÁöÑÊòØÔºåÂÆÉ‰ª¨Áõ∏ÂØπÊØîËæÉÂ•ΩÈòÖËØªÔºåÊòØ Python ÂÜôÁöÑÔºö https://github.com/Chia-Network/chia-blockchain „ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#ËøêË°å"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? Â¶Ç‰ΩïËøûÊé•Chia?Êàë‰ª¨ÂèØ‰ª•ÈÄöËøá Chia Network ÁöÑÂÆ¢Êà∑Á´ØÊù•ËÆøÈóÆÂÆÉÔºåËøô‰∏™ÂÆ¢Êà∑Á´ØÊòØ Electron + Python ÁöÑÁ®ãÂ∫èÔºåÂ§©ÁÑ∂Ë∑®Âπ≥Âè∞ÔºåÊó¢Êúâ GUI ÂèàÊúâ CLI ÁöÑÊñπÂºè„ÄÇ 1.2.1 ÂÆâË£ÖÂè™ÈúÄË¶ÅÊåâÁÖßÂÆòÊñπÁöÑ Guide Êù•‰∏ãËΩΩÂÆâË£ÖÂ∞±Â•ΩÔºå https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLÔºåÊàëÂú® M1 Mac ‰∏ãÂÆâË£ÖÁöÑÊó∂ÂÄôËÑöÊú¨Âá∫‰∫ÜÁÇπÂ∞èÈóÆÈ¢òÔºåÂ§ßÊ¶ÇÊòØÂõ†‰∏∫ÊãâÂèñ‰∫åËøõÂà∂ wheel Êñá‰ª∂ÁΩëÁªúÂá∫ÈóÆÈ¢òËµ∞Âà∞‰∫ÜÁºñËØë wheelÁöÑÈÄªËæëÔºåËÄåÈÇ£ÈáåÊòØ‰æùËµñ cargoÁöÑÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÈÅáÂà∞‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊèêÂâçÊâãÂä®ÂÆâË£Ö‰∏Ä‰∏ã rustÔºåÊàñËÄÖ cherry-pick ÊàëÁöÑËøô‰∏™ PR „ÄÇ 1.2.2 ËøêË°å ÊåâÁÖßÂÆòÊñπ guideÔºåÊØîÂ¶Ç macOS Êù•ËØ¥ÔºåÊúÄÂêé‰∏ÄÊ≠•ÊâßË°ånpm run electron \u0026 Â∞±ÊòØËøêË°åÂÆÉÁöÑGUIÂÆ¢Êà∑Á´Ø„ÄÇ Â¶ÇÊûúÂ§ßÂÆ∂ÂÉèÊàë‰∏ÄÊ†∑ÂñúÊ¨¢ CLIÔºåÁõ¥Êé•Âú®ÊâßË°åÂÆå . ./activate ‰πãÂêéÂ∞±ÂèØ‰ª• chia --help‰∫ÜÂìà‚ò∫ÔºåÈáåËæπÊúâÂè™ÂêØÂä®ÈÉ®ÂàÜÊúçÂä°ÁöÑÊñπÂºèÔºàÁõ∏ÊØî GUI ÂêØÂä®ÊâÄÊúâÊù•ËØ¥)„ÄÇ Âú®ËøêË°å‰πãÂêéÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁΩëÁªú‰∏çÊòØÂ§öÂ±Ç NAT ÁöÑÈÇ£ÁßçÔºåÁêÜËÆ∫‰∏äÊÇ®ÂèØ‰ª•ËøûÂà∞ mainnet Âπ∂‰∏îËá™Âä®Âíå‰∏ªÈìæÂêåÊ≠•Êï∞ÊçÆ‰∫ÜÔºåÂ¶ÇÊûúÊÇ®ÊòØÁ¨¨‰∫åÊ¨°ËøêË°åÔºåËøûÊé•‰∏ªÈìæÔºå‰∏ÄÂºÄÂßãÂèØËÉΩÊúâ‰∏ÄÈòµÂ≠êÂêåÊ≠•ÁöÑblock Êï∞ÊòØ‰∏çÂèòÁöÑÔºå‰πüÊ≤°Êúâ peer ËøûËøáÊù•Ôºå‰∏çÂøÖÊÉäÊÖåÔºåÁ≠â‰∏Ä‰∏ãÂ∞±Â•Ω‰∫Ü„ÄÇ Tips: Á¨¨‰∏ÄÊ¨°ËøûÂà∞ Chia Network ÁöÑÂêåÂ≠¶‰ª¨ÔºåÂÆ¢Êà∑Á´Ø‰ºöËá™Âä®ÁîüÊàê‰∏Ä‰∏™Èí±ÂåÖÔºåÂèäÁöÑ‰øùÂ≠òÈÇ£‰∏Ä‰∏≤ËØçÔºåÂÆÉ‰ª¨Â∞±ÊòØ‰Ω†ÁöÑÁßÅÈí•Âì¶„ÄÇ ‰∏á‰∏ÄÔºåÂ¶ÇÊûúÁúüÁöÑËøû‰∏ç‰∏äÁöÑËØùÔºåÂèØËÉΩÈúÄË¶ÅÂú®Ë∑ØÁî±‰∏äÈÖçÁΩÆÔºåUPnPÔºåÈò≤ÁÅ´Â¢ôË¶ÅÂÖÅËÆ∏ 8444„ÄÇ 1.2.3 ËÆøÈóÆ Chia ÁöÑÊï∞ÊçÆChia ÁöÑÂÆ¢Êà∑Á´ØÊääÊï∞ÊçÆÂ≠òÂú®‰∫ÜÂá†‰∏™ SQLite Êï∞ÊçÆÂ∫ìÈáåÔºåÂÆÉ‰ª¨ÁöÑË∑ØÂæÑÊòØÊàë‰ª¨ÂÆâË£ÖÂÆ¢Êà∑Á´ØÁöÑÁî®Êà∑ÁöÑÂÆ∂ÁõÆÂΩïÔºö~/.chia/mainnet ‰∏ãËæπÂ∞±ÊòØËøêË°åËµ∑Êù• Chia ‰πãÂêéÁîüÊàêÁöÑ‰∏ªË¶ÅÁöÑ‰∏§‰∏™Êï∞ÊçÆÂ∫ìÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ôºö ~/.chia/mainnet/db ‚ùØ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ‚ùØ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ÁÑ∂ÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÖàÁî® SQLite BrowserÔºå‰∏Ä‰∏™ SQlite Êï∞ÊçÆÂ∫ìÔºàÊñá‰ª∂ÔºâÁöÑÊµèËßàÂô®Êù•ÁúãÁúãÂÆÉ„ÄÇ SQlite ÊµèËßàÂô®ÁöÑÂÆòÁΩëÊòØ https://sqlitebrowser.org/ „ÄÇÂú®‰∏ãËΩΩÔºåÂÆâË£Ö‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÁÇπÂáª Open Database/ÊâìÂºÄÊï∞ÊçÆÂ∫ì ÈÄâÊã©ÊµèËßà‰∏äËæπÂàóÂá∫Êù•ÁöÑ‰∏§‰∏™ .sqlite Êâ©Â±ïÂêçÁöÑÊï∞ÊçÆÂ∫ìÊñá‰ª∂„ÄÇ ÊâìÂºÄÊï∞ÊçÆÂ∫ì‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã©Á¨¨‰∏Ä‰∏™Ê†áÁ≠æ Database Schema Êù•ÁúãÁúãË°®ÁöÑÁªìÊûÑ„ÄÇ Êàë‰ª¨ËøòËÉΩÂÉèÁ±ª‰ºº‰∫é Excel ‰∏ÄÊ†∑ÂéªÁúãË°®ÁöÑÊï∞ÊçÆÔºåËøòÂèØ‰ª• Filter/ËøáÊª§„ÄÅSort/ÊéíÂ∫è‰ªªÊÑèÂàó„ÄÇ ‰∏ã‰∏ÄÈÉ®ÂàÜÔºåÊàë‰ª¨Êù•ÁÆÄÂçïÁúãÁúãË°®ÈáåÁöÑÊï∞ÊçÆ„ÄÇ Tips: ËøôÈáåËæπÔºå~/.chia/mainnet/wallet ÂíåË£∏ÁõÆÂΩï ~/.chia/mainnet ‰∏ãËæπÁöÑ db ÈáåÂàÜÂà´ÈÉΩÊúâË°®Êñá‰ª∂Ôºå‰ªñ‰ª¨ÁöÑ‰ø°ÊÅØÊòØÊúâÈáçÂ§çÁöÑÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂàÜÂà´ÊâìÂºÄÁúãÁúãÂì¶ÔºåÂç≥‰ΩøÊòØÁõ∏ÂêåÁöÑË°®ÁöÑÂêçÂ≠óÔºåÊØîÂ¶Ç block_record ÂÜÖÈáåÁöÑ‰ø°ÊÅØ‰πüÁï•ÊúâÂ∑ÆÂà´ÔºåÂ¶ÇÊûúÂ§ßÂÆ∂Áü•ÈÅì‰∏∫‰ªÄ‰πàÊúâËøôÊ†∑ÁöÑÂ∑ÆÂà´ÔºåÊ¨¢ËøéÊµèËßàÂëäËØâÂ§ßÂÆ∂ÂìàÔºåÂèØËÉΩË¶Å‰ªîÁªÜÁ†îÁ©∂‰∏Ä‰∏ãÂÆ¢Êà∑Á´Ø„ÄÅÈí±ÂåÖÁ≠â‰ª£Á†ÅÊâçË°åÔºåÂπ∏ËøêÁöÑÊòØÔºåÂÆÉ‰ª¨Áõ∏ÂØπÊØîËæÉÂ•ΩÈòÖËØªÔºåÊòØ Python ÂÜôÁöÑÔºö https://github.com/Chia-Network/chia-blockchain „ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#ËÆøÈóÆ-chia-ÁöÑÊï∞ÊçÆ"},{"categories":["Nebula Graph"],"content":"2 Inspect the Chia Network, ÂàÜÊûê Chia ÁöÑÊï∞ÊçÆÂ¶ÇÊûúÂ§ßÂÆ∂‰ªîÁªÜÁúã‰∫Ü‰∏äËæπË°®ÁªìÊûÑÂÆö‰πâÁöÑÊà™ÂõæÔºåÂ∞±ËÉΩÊ≥®ÊÑèÂà∞‰∏Ä‰∫õË°®ÁöÑ‰∏ªË¶Å‰ø°ÊÅØÊòØÂµåÂ•ó‰∫åËøõÂà∂ KV ByteÔºåÊâÄ‰ª•Âè™‰ªé SQLite Âπ∂‰∏çËÉΩÁúãÂà∞ÊâÄÊúâ Chia ÁöÑÊï∞ÊçÆÔºåÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶ÅÔºàÁî®‰∏Ä‰∏™ÁºñÁ®ãËØ≠Ë®ÄÊù•ÔºâËØªÂèñË°®ÈáåÁöÑ Byte„ÄÇ Âπ∏ËøêÁöÑÊòØÔºåËøô‰ª∂‰∫ãÂÑøÂõ†‰∏∫ Chia ÊòØÂºÄÊ∫êÁöÑÔºåËÄå‰∏îÊòØ Python ÁöÑ‰ª£Á†ÅÔºå‰ΩøÂæóÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•‰∫§‰∫íÂºèÁöÑÂÅö„ÄÇ ÊàëËä±‰∫Ü‰∏ÄÁÇπÁÇπÊó∂Èó¥Âú® Chia ÂÆ¢Êà∑Á´Ø‰ª£Á†ÅÈáåÊâæÂà∞‰∫ÜÈúÄË¶ÅÁöÑÂ∞ÅË£ÖÁ±ªÔºåÂÄüÂä©ÂÆÉÔºåÂèØ‰ª•ÊØîËæÉÊñπ‰æøÁöÑÂàÜÊûê Chia ÂÆ¢Êà∑Á´ØÂú®Êú¨Âú∞ÁöÑÂÖ®ÈìæÊï∞ÊçÆ„ÄÇ Â¶ÇÊûúÊÇ®‰∏çÊÑüÂÖ¥Ë∂£ÁªÜËäÇÔºåÂèØ‰ª•Áõ¥Êé•ÁúãÊàëÂàÜÊûêÁöÑÁªìËÆ∫„ÄÇ ÁªìËÆ∫‰πãÂêéÔºåÊàë‰πüÁªôÂ§ßÂÆ∂ÊºîÁ§∫‰∏Ä‰∏ãÊòØÊÄé‰πàËØªÂèñÂÆÉ‰ª¨ÁöÑ„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#inspect-the-chia-network-ÂàÜÊûê-chia-ÁöÑÊï∞ÊçÆ"},{"categories":["Nebula Graph"],"content":"2.1 TL;DR, ÁªìËÆ∫Êàë‰ª¨ÂèØ‰ª•‰ªéË°®‰∏≠ËØªÂèñÂà∞Âå∫ÂùóÈìæËÆ∞ÂΩïÔºàBlock Record ÔºâÔºåChia Â∏ÅËÆ∞ÂΩïÔºàCoin RecordÔºâ„ÄÇ ‰ªéÂå∫ÂùóËÆ∞ÂΩï‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÂÖ≥ÈîÆÁöÑÊ∂âÂèä‰∫§ÊòìÁöÑ‰ø°ÊÅØÔºö ÂÖ≥ËÅîÁöÑ Coin ÔºåÂÖ≥ËÅîÁöÑ PuzzleÔºàÂú∞ÂùÄÔºâÔºåCoin ÁöÑÂÄº(Amount) ‰ªéÂ∏ÅËÆ∞ÂΩï‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÂÖ≥ÈîÆÁöÑÊ∂âÂèäÂå∫ÂùóÁöÑ‰ø°ÊÅØÔºö ÁîüÊàêËøô‰∏™ Coin ÊâÄÂú®Âå∫ÂùóÈìæÈáåÁöÑÁ¥¢ÂºïÈ´òÂ∫¶ÔºàConfirmed IndexÔºâ Â¶ÇÊûúËøô‰∏™ËÆ∞ÂΩïÊòØËä±Ë¥π Coin ÁöÑÔºåËä±Ë¥πÂÆÉÁöÑÁ¥¢ÂºïÈ´òÂ∫¶ÔºàSpent IndexÔºâ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Coin Record ‚îÇ ‚îÇ Block Record ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Coin Name ‚îÇ ‚îÇ Height ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∫ Puzzle ‚îÇ ‚îÇ Header ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∫ Coin Parent ‚îÇ ‚îÇ Prev Header ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∫ Amount ‚îÇ ‚îÇ Block Body ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ farmer_puzzle_hash ‚îÇ ‚îÇ ‚îÇ ‚îÇ Time Stamp ‚îÇ ‚îÇ fees ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ pool_puzzle_hash ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚î¨‚îÄ Confirmed Index ‚îÇ ‚îÇ prev_transaction_block_hash ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ prev_transaction_block_height ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ Spent Index ‚îÇ ‚îÇ transactions_info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ is_transaction_block ‚îÇ ‚îÇ Coinbase ‚îÇ ‚îÇ ‚îÇ sub_epoch_summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ is Peak ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄis Block ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îº‚îÄ‚ñ∫ Sub Epoch Segment ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#tldr-ÁªìËÆ∫"},{"categories":["Nebula Graph"],"content":"2.2 Preperation, ÂáÜÂ§áÂõ†‰∏∫ÂÆâË£ÖÂÆ¢Êà∑Á´Ø‰πãÂêéÔºåÊàë‰ª¨Êú¨Âú∞ÂÆûÈôÖ‰∏äÂ∑≤ÁªèÊúâ‰∫ÜÁõ∏ÂÖ≥ÁöÑ Python ÁéØÂ¢ÉÂíå‰æùËµñÔºåÂè™ÈúÄË¶ÅÂú®ÈáåËæπË∑ëËµ∑Êù•Â∞±Â•Ω„ÄÇ # Ê≥®ÊÑèÔºåÊàë‰ª¨Ë¶Å cd Âà∞‰πãÂâçÂÆâË£ÖÂÆ¢Êà∑Á´ØÊó∂ÂÄôÂÖãÈöÜÁöÑ‰ªìÂ∫ì„ÄÇ cd chia-blockchain # source activate ËÑöÊú¨Êù•ÂàáÊç¢Âà∞‰ªìÂ∫ìÂÆâË£ÖÊó∂ÂÄôÂàõÂª∫ÁöÑ Python ËôöÊãüÁéØÂ¢ÉÔºåÂπ∂ËøõÂà∞ IPython Èáå„ÄÇ source venv/bin/activate \u0026\u0026 pip install ipython \u0026\u0026 ipython ÁÑ∂ÂêéËØïÁùÄÂØºÂÖ•ÂÆ¢Êà∑Á´ØÈáåËæπÂ∏¶ÊúâÁöÑ Python ÁöÑ Chia ÁöÑÂ∞ÅË£ÖÁ±ªËØïËØïÁúã„ÄÇ In [1]: import sqlite3 ...: from chia.consensus.block_record import BlockRecord # ÂØºÂÖ•ÊàêÂäüÔºåÊ≤°ÊúâÊä•Èîô In [2]: !pwd # ÊàëÁöÑÂÆâË£ÖÂÖãÈöÜÁõÆÂΩï /Users/weyl/chia-blockchain ÊÅ≠Âñú‰Ω†ÂÅöÂ•Ω‰∫ÜÂáÜÂ§áÔºåÊàë‰ª¨ÁúãÁúã Block Record ÈáåÈÉΩÊúâ‰ªÄ‰πà„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#preperation-ÂáÜÂ§á"},{"categories":["Nebula Graph"],"content":"2.3 Block Record ChainÔºåÂå∫ÂùóËÆ∞ÂΩïÂú®‰∏ä‰∏ÄÊ≠•ÁöÑ IPython Á™óÂè£‰∏ã„ÄÇ # Ê≥®ÊÑèÔºåËøôÈáåÁöÑË∑ØÂæÑÁöÑÂâçÁºÄÊòØÊàë‰ª¨Ëá™Â∑±ÁöÑÂÆ∂ÁõÆÂΩïÔºå‰∏çÂêåÊìç‰ΩúÁ≥ªÁªüÔºå‰∏çÂêåÁöÑÁî®Êà∑ÈÉΩ‰ºöÊúâÊâÄ‰∏çÂêå„ÄÇ chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # ËøôÈáåÊàë‰ª¨ÂèñÁ¨¨ 201645 È´òÁöÑÂå∫Âùó rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # ËøôÈáå 0 Ë°®Á§∫ SELECT ÁªìÊûúÁöÑÁ¨¨‰∏ÄË°åÔºå3 Ë°®Á§∫Âú® BlockRecord Ëøô‰∏™Ë°®ÈáåËæπÔºåBlock ÁöÑ‰∫åËøõÂà∂ BLOB ÊòØÁ¨¨ÂõõÂàóÔºåÂèÇËÄÉÊú¨Á´†Â∫ïÈÉ®ÁöÑË°®ÂÆö‰πâÈÉ®ÂàÜ block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # ÂèØ‰ª•Êü•Áúã‰∏Ä‰∫õÂ±ûÊÄß is_transaction_blockÔºåtimestampÔºåreward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # ÂèØ‰ª•Âø´ÈÄü print ÁúãÂ§ßÈÉ®ÂàÜ‰ø°ÊÅØ print(block_records_201645) block_records_201645 ÁöÑÊâìÂç∞ÁªìÊûúÂ¶Ç‰∏ã„ÄÇ ËøôÈáåÊàëÊà™Êñ≠‰∫Ü‰∏Ä‰∫õÊï∞ÊçÆ {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} Âè¶Â§ñÔºåÊàë‰ª¨ÂèñÁöÑËøô‰∏™Ë°®ÁöÑÂÆö‰πâÂ¶Ç‰∏ã„ÄÇ CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#\u003c---- sub_epoch_summaryblob,is_peaktinyint,is_blocktinyint) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#block-record-chainÂå∫ÂùóËÆ∞ÂΩï"},{"categories":["Nebula Graph"],"content":"2.4 Coin Record ChainÔºåChia Â∏ÅËÆ∞ÂΩïÁ±ª‰ººÁöÑÔºåÊàë‰ª¨ÂèØ‰ª•Ëé∑Âèñ‰∏Ä‰∏™ Coin ÁöÑËÆ∞ÂΩïÔºåËøôÈáåËæπÔºå‰ªéË°®ÁöÑÂÆö‰πâÂèØ‰ª•ÁúãÂà∞ÔºåÂîØ‰∏Ä‰∫åËøõÂà∂Ôºà‰∏çËÉΩÁõ¥Êé•‰ªéÊï∞ÊçÆÂ∫ìÊü•ËØ¢‰∏≠Ë¢´‰∫∫ËØªÊáÇÔºâÁöÑÂ≠óÊÆµÂ∞±ÊòØÊòØÂ∏ÅÂÄºÔºå‰∏çÂ≠òÂú®ÂµåÂ•óÁöÑÁªìÊûÑÔºåÊâÄ‰ª•‰πüÂπ∂‰∏çÈúÄË¶ÅÂ∞ÅË£ÖÁöÑÁ±ªÊâçËÉΩÁúãÊ∏ÖÊ•öÈáåËæπÁöÑ‰ø°ÊÅØ„ÄÇ CREATETABLEcoin_record(coin_nametextPRIMARYKEY,confirmed_indexbigint,spent_indexbigint,spentint,coinbaseint,puzzle_hashtext,coin_parenttext,amountblob,timestampbigint) ËøôÈáåÂÄºÂæóÊ≥®ÊÑèÁöÑ‰ø°ÊÅØ‰∏ªË¶ÅÊòØ spent_index Âíå confirmed_index„ÄÇ from chia.util.ints import uint64 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" con = sqlite3.connect(chia_db_path) cur = con.cursor() rows = list(cur.execute('SELECT * FROM coin_record WHERE confirmed_index = 201645')) coin_amount = uint64.from_bytes(rows[0][7]) In [201]: rows[0] Out[201]: ('cf35da0f595b49dde626d676b511ee62bce886f2216751aa51bb8ff851563d35', # coin_name 201645, # confirmed_index 0, # spent_indexÔºåËøôÈáåÊ≤°ÊúâspentÔºåÊâÄ‰ª•ÂÄºÊó†Êïà 0, # spentÔºåÂÖ∂ÂÆûÊòØ bool 1, # coinbaseÔºåbool 'bbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', # puzzle_hash ÂØπÂ∫îÂà∞Âú∞ÂùÄ 'ccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', b'\\x00\\x00\\x01\\x97t \\xdc\\x00', # uint64 1619662081) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:4","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#coin-record-chainchia-Â∏ÅËÆ∞ÂΩï"},{"categories":["Nebula Graph"],"content":"2.5 Puzzles/ AddressÔºåÂú∞ÂùÄÊàë‰ª¨ÂèØ‰ª•Êää Chia ‰∏≠ÁöÑ Puzzle ÁêÜËß£Êàê‰∏∫‰∫§Êòì‰∏≠ÁöÑÂú∞ÂùÄÔºå‰∏∫‰∫ÜÊñπ‰æø‰ΩøÁî®ÔºåÈÄöÂ∏∏‰ºöÊää Puzzle ÁöÑ hash Áî®bech32m ËΩ¨Êç¢ÊàêÂú∞ÂùÄ„ÄÇ Tips: ËøôÈáåÊúâ‰∏Ä‰∏™Âú®Á∫øÂèåÂêëËΩ¨Êç¢ÁöÑÂú®Á∫øÂ∑•ÂÖ∑Êé®Ëçê‰∏Ä‰∏ã: https://www.chiaexplorer.com/tools/address-puzzlehash-converter ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:5","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#puzzles-addressÂú∞ÂùÄ"},{"categories":["Nebula Graph"],"content":"3 How to explore Chia Network? Â¶Ç‰ΩïÊé¢Á¥¢ Chia ÈìæÈöèÁùÄÊàë‰ª¨‰πãÂâçÂàÜÊûêÁöÑ‰ø°ÊÅØÔºåËá™ÁÑ∂Âú∞ÔºåÊàë‰ª¨ÂèØ‰ª•Êää Chia Âå∫ÂùóÈìæ‰∏≠ÁöÑ‰ø°ÊÅØÂèñÂá∫Êù•ÔºåÁî®ÂõæÔºàGraphÔºâÊù•Ë°®Á§∫ÔºåËøôÈáåÁöÑÂõæÂπ∂‰∏çÊòØÔºàGraphicÔºâÂõæÂΩ¢„ÄÅÂõæÁîªÁöÑÊÑèÊÄùÔºåÊòØÊï∞Â≠¶„ÄÅÂõæËÆ∫‰∏≠ÁöÑÂõæ„ÄÇ Âú®ÂõæÁöÑËØ≠Â¢É‰∏ãÔºåÊúÄ‰∏ªË¶ÅÁöÑ‰∏§‰∏™ÂÖÉÁ¥†Â∞±ÊòØÈ°∂ÁÇπÔºàVertexÔºâÂíåËæπÔºàEdgeÔºâ„ÄÇ È°∂ÁÇπË°®Á§∫‰∏Ä‰∏™ÂÆû‰ΩìÔºåËÄåËæπË°®Á§∫ÂÆû‰Ωì‰πãÈó¥ÁöÑÊüêÁßçÂÖ≥Á≥ªÔºåËøôÁßçÂÖ≥Á≥ªÂèØ‰ª•ÊòØÂØπÁ≠âÁöÑÔºàÊó†ÊñπÂêëÁöÑÔºâ‰πüÂèØ‰ª•ÊòØÊúâÊñπÂêëÁöÑ„ÄÇ ËøôÈáåÊàë‰ª¨ÂèØ‰ª•ÊääËøôÈáåÁöÑ‰ø°ÊÅØÊäΩË±°Êò†Â∞ÑÂà∞Â¶ÇÂõæÁöÑÂõæÊ®°ÂûãÈáåÔºö Block È°∂ÁÇπ Coin È°∂ÁÇπ Puzzle È°∂ÁÇπ spends ËæπÔºàBlock Âà∞ CoinÔºâ confirms Ëæπ ÔºàBlock Âà∞ CoinÔºâ belongs_to ËæπÔºàCoin Âà∞ PuzzleÔºâ ËøôÈáåÔºåÊàë‰ª¨Â∫îÁî®ÁöÑÂõæÊòØ‰∏ÄÁßçÂè´ÂÅöÂ±ûÊÄßÂõæÁöÑÂΩ¢ÂºèÔºåÈô§‰∫ÜÁÇπÂíåËæπÁöÑÂÖ≥Á≥ª‰πãÂ§ñ„ÄÇËøô‰∏§ÁßçÂÆû‰ΩìÔºàÁÇπ„ÄÅËæπÔºâËøòÊúâÂÖ∂‰ªñ‰ø°ÊÅØÂè™ÂíåÂÆÉ‰ª¨ÁöÑ‰∏Ä‰∏™ÂÆû‰æãÁõ∏ÂÖ≥ÔºåÊâÄ‰ª•ÂÜçÂÆö‰πâ‰∏∫È°∂ÁÇπ„ÄÅËæπÂ∞±‰∏çÊòØÂæàÈÄÇÂêàÔºåËøô‰∫õ‰ø°ÊÅØÂ∞±‰Ωú‰∏∫ÁÇπ„ÄÅËæπÁöÑÂ±ûÊÄßÔºàprepertyÔºâÂ≠òÂú®„ÄÇ ËøôÁßç‰∏∫‰∫ÜÂ§ÑÁêÜÂÆû‰Ωì‰πãÈó¥ÂÖ≥ËÅî„ÄÅÊ∂âÂèäÂÆû‰Ωì„ÄÅÂÖ≥ËÅîÁöÑÂ±ûÊÄß‰ø°ÊÅØÁöÑÔºå‰πüÂ∞±ÊòØ\"Â±ûÊÄßÂõæ\"ÁöÑÂ≠òÂÇ®‰ø°ÊÅØÁöÑÊñπÂºèÂú®ËÆ°ÁÆóÊú∫È¢ÜÂüüË∂äÊù•Ë∂äÊµÅË°åÔºåÁîöËá≥Êúâ‰∏ìÈó®‰∏∫Ê≠§ÁªìÊûÑËÄåÂéüÁîüÂºÄÂèëÁöÑÊï∞ÊçÆÂ∫ì‚Äî‚ÄîÂõæÊï∞ÊçÆÂ∫ìÔºàGraph DatabaseÔºâ„ÄÇ ËøôÈáåÔºåÊàë‰ª¨Áî®ÁöÑÂ∞±ÊòØ‰∏Ä‰∏™Âè´ÂÅö Nebula Graph ÁöÑÂõæÊï∞ÊçÆÂ∫ìÔºåÂÆÉÊòØ‰∏Ä‰∏™Áé∞‰ª£ÁöÑ„ÄÅ‰∏∫Ë∂ÖÂ§ßËßÑÊ®°ÂàÜÈÉ®ÁΩ≤Êû∂ÊûÑËÆæËÆ°ÁöÑ„ÄÅÂéüÁîüÂ≠òÂÇ®„ÄÅÊü•ËØ¢„ÄÅËÆ°ÁÆóÂõæÊï∞ÊçÆÁöÑÈ°πÁõÆÔºåÊõ¥Ê£íÁöÑÊòØÔºåÂÆÉÊòØ‰∫ßÁîü‰∫éÁ§æÂå∫ÁöÑÂºÄÊ∫ê‰∫ßÂìÅ„ÄÇ Tips: ÂÆâË£Ö Nebula Graph ‰∏ÄËà¨Êù•ËØ¥ÔºåÈù¢ÂêëË∂ÖÂ§ßËßÑÊ®°Êï∞ÊçÆÁöÑÂàÜÂ∏ÉÂºèÁ≥ªÁªüÔºåÂ§©ÁÑ∂ÁöÑÈÉΩÊòØ‰∏çÂÆπÊòìËΩªÈáèÈÉ®ÁΩ≤ÁöÑÔºåÂ§ßÂÆ∂Â¶ÇÊûúÁ¨¨‰∏ÄÊ¨°‰ΩøÁî®ÁöÑËØùÂèØ‰ª•ËØïËØïÊàëÂÜôÁöÑ‰∏Ä‰∏™Âè´ÂÅö nebula-up ÁöÑÂ∞èÂ∑•ÂÖ∑ÔºåÂèØ‰ª•‰∏ÄË°åÊåá‰ª§ÈÉ®ÁΩ≤‰∏Ä‰∏™Áî®Êù•ËØïÁî®„ÄÅÂ≠¶‰π†ÁöÑ Nebula Graph ÈõÜÁæ§ÔºåÂú∞ÂùÄÂú®ËøôÈáåÔºö https://github.com/wey-gu/nebula-up/ „ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-to-explore-chia-network-Â¶Ç‰ΩïÊé¢Á¥¢-chia-Èìæ"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph ÂØºÂÖ• Chia Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ìÊàë‰ª¨ÂàÜ‰∏§Ê≠•Ëµ∞ÔºåÁ¨¨‰∏ÄÊ≠•ËøôÊää Chia Network Êï∞ÊçÆËΩ¨Êç¢Êàê CSV Êñá‰ª∂ÔºåÁ¨¨‰∫åÊ≠•‰ΩøÁî® Nebula ÁöÑ Nebula-Importer ÊääÊï∞ÊçÆÂØºÂÖ• Nebula Graph„ÄÇ 3.1.1 Data conversion Êï∞ÊçÆËΩ¨Êç¢ËøôÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÊàëÂºÄÊ∫êÂú®ËøôÈáå‰∫Ü: https://github.com/wey-gu/nebula-chia ‰ΩøÁî®ÂÆÉÂè™ÈúÄË¶ÅÂú® Chia Network ÁöÑ python venv ‰∏ãÂÆâË£ÖÂÆÉ: python3 -m pip install nebula-chia ÁÑ∂ÂêéË∞ÉÁî® ChaiBatchConvertor Â∞±ÂèØ‰ª•Âú®ÂΩìÂâçÁõÆÂΩï‰∏ãÁîüÊàê‰∏§‰∏™ CSV Êñá‰ª∂„ÄÇ ËøôÈáåËæπÊúâ‰∏Ä‰∫õÂèØ‰ª•ÈÖçÁΩÆÁöÑÂèÇÊï∞ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ÁîüÊàêÁöÑÊñá‰ª∂Ôºö $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv ËøôÈáåËæπÂ≠óÊÆµÁöÑÂê´‰πâÂíåÁ±ªÂûãÔºåÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å‰∏≠ block_record_row Âíå coin_record_row ÁöÑ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import Êï∞ÊçÆÂØºÂÖ•Êúâ‰∫Ü CSV Êñá‰ª∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÄüÂä© Nebula-Importer ÂØºÂÖ•Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ì‰∏≠„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÂÜôÂ•Ω‰∫Ü nebula-importer ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÊ∂µ‰∫ÜÂ¶Ç‰∏ã‰ø°ÊÅØ: Âú® Nebula Graph ‰∏≠ÂàõÂª∫ÈúÄË¶ÅÁöÑÊï∞ÊçÆÊ®°Âûã SchemaÔºåËøôÂíåÊàë‰ª¨ÂâçËæπÂÅöÁöÑÂõæÊò†Â∞ÑÁöÑ‰ø°ÊÅØÊòØÁ≠â‰ª∑ÁöÑ ÊèèËø∞ CSV Êñá‰ª∂‰πã‰∏≠ÁöÑ Column ÁöÑÊï∞ÊçÆÂà∞ÂõæÊ®°ÂûãÔºàÁÇπÔºåËæπÔºåÁÇπÊàñËæπÁöÑÂ±ûÊÄßÔºâÊò†Â∞ÑÂÖ≥Á≥ª # ËøôÈáåÔºåÊàëÁöÑ csv Êñá‰ª∂Âíå ÈÖçÁΩÆÊñá‰ª∂ÈÉΩÊîæÂú® /home/wei.gu/chia ‰πã‰∏ã # Êàë‰ΩøÁî® docker-compose ÈªòËÆ§ÈÖçÁΩÆÈÉ®ÁΩ≤ÁöÑ Nebula Graph, # ÂÆÉÂàõÂª∫‰∫ÜÂè´ nebula-docker-compose_nebula-net ÁöÑ docker ÁΩëÁªú docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml ËøôÈáåÊàëÂ±ïÁ§∫‰∏Ä‰∏™ÂØºÂÖ•ÁöÑÁªìÊûúÁ§∫‰æãÔºåÊàëÂú®ÂçïÊú∫ÈÉ®ÁΩ≤ÁöÑ Nebula Graph ÈáåÂØºÂÖ•‰∫ÜÊàë‰∏Ä‰∏§Âë®‰πãÂâçÂèñÁöÑÂÖ®Èáè Chia Network Êï∞ÊçÆÁöÑÁªìÊûú„ÄÇ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#import-the-chia-to-a-graph-database-nebula-graph-ÂØºÂÖ•-chia-Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ì"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph ÂØºÂÖ• Chia Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ìÊàë‰ª¨ÂàÜ‰∏§Ê≠•Ëµ∞ÔºåÁ¨¨‰∏ÄÊ≠•ËøôÊää Chia Network Êï∞ÊçÆËΩ¨Êç¢Êàê CSV Êñá‰ª∂ÔºåÁ¨¨‰∫åÊ≠•‰ΩøÁî® Nebula ÁöÑ Nebula-Importer ÊääÊï∞ÊçÆÂØºÂÖ• Nebula Graph„ÄÇ 3.1.1 Data conversion Êï∞ÊçÆËΩ¨Êç¢ËøôÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÊàëÂºÄÊ∫êÂú®ËøôÈáå‰∫Ü: https://github.com/wey-gu/nebula-chia ‰ΩøÁî®ÂÆÉÂè™ÈúÄË¶ÅÂú® Chia Network ÁöÑ python venv ‰∏ãÂÆâË£ÖÂÆÉ: python3 -m pip install nebula-chia ÁÑ∂ÂêéË∞ÉÁî® ChaiBatchConvertor Â∞±ÂèØ‰ª•Âú®ÂΩìÂâçÁõÆÂΩï‰∏ãÁîüÊàê‰∏§‰∏™ CSV Êñá‰ª∂„ÄÇ ËøôÈáåËæπÊúâ‰∏Ä‰∫õÂèØ‰ª•ÈÖçÁΩÆÁöÑÂèÇÊï∞ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ÁîüÊàêÁöÑÊñá‰ª∂Ôºö $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv ËøôÈáåËæπÂ≠óÊÆµÁöÑÂê´‰πâÂíåÁ±ªÂûãÔºåÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å‰∏≠ block_record_row Âíå coin_record_row ÁöÑ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import Êï∞ÊçÆÂØºÂÖ•Êúâ‰∫Ü CSV Êñá‰ª∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÄüÂä© Nebula-Importer ÂØºÂÖ•Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ì‰∏≠„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÂÜôÂ•Ω‰∫Ü nebula-importer ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÊ∂µ‰∫ÜÂ¶Ç‰∏ã‰ø°ÊÅØ: Âú® Nebula Graph ‰∏≠ÂàõÂª∫ÈúÄË¶ÅÁöÑÊï∞ÊçÆÊ®°Âûã SchemaÔºåËøôÂíåÊàë‰ª¨ÂâçËæπÂÅöÁöÑÂõæÊò†Â∞ÑÁöÑ‰ø°ÊÅØÊòØÁ≠â‰ª∑ÁöÑ ÊèèËø∞ CSV Êñá‰ª∂‰πã‰∏≠ÁöÑ Column ÁöÑÊï∞ÊçÆÂà∞ÂõæÊ®°ÂûãÔºàÁÇπÔºåËæπÔºåÁÇπÊàñËæπÁöÑÂ±ûÊÄßÔºâÊò†Â∞ÑÂÖ≥Á≥ª # ËøôÈáåÔºåÊàëÁöÑ csv Êñá‰ª∂Âíå ÈÖçÁΩÆÊñá‰ª∂ÈÉΩÊîæÂú® /home/wei.gu/chia ‰πã‰∏ã # Êàë‰ΩøÁî® docker-compose ÈªòËÆ§ÈÖçÁΩÆÈÉ®ÁΩ≤ÁöÑ Nebula Graph, # ÂÆÉÂàõÂª∫‰∫ÜÂè´ nebula-docker-compose_nebula-net ÁöÑ docker ÁΩëÁªú docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml ËøôÈáåÊàëÂ±ïÁ§∫‰∏Ä‰∏™ÂØºÂÖ•ÁöÑÁªìÊûúÁ§∫‰æãÔºåÊàëÂú®ÂçïÊú∫ÈÉ®ÁΩ≤ÁöÑ Nebula Graph ÈáåÂØºÂÖ•‰∫ÜÊàë‰∏Ä‰∏§Âë®‰πãÂâçÂèñÁöÑÂÖ®Èáè Chia Network Êï∞ÊçÆÁöÑÁªìÊûú„ÄÇ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-conversion-Êï∞ÊçÆËΩ¨Êç¢"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph ÂØºÂÖ• Chia Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ìÊàë‰ª¨ÂàÜ‰∏§Ê≠•Ëµ∞ÔºåÁ¨¨‰∏ÄÊ≠•ËøôÊää Chia Network Êï∞ÊçÆËΩ¨Êç¢Êàê CSV Êñá‰ª∂ÔºåÁ¨¨‰∫åÊ≠•‰ΩøÁî® Nebula ÁöÑ Nebula-Importer ÊääÊï∞ÊçÆÂØºÂÖ• Nebula Graph„ÄÇ 3.1.1 Data conversion Êï∞ÊçÆËΩ¨Êç¢ËøôÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÊàëÂºÄÊ∫êÂú®ËøôÈáå‰∫Ü: https://github.com/wey-gu/nebula-chia ‰ΩøÁî®ÂÆÉÂè™ÈúÄË¶ÅÂú® Chia Network ÁöÑ python venv ‰∏ãÂÆâË£ÖÂÆÉ: python3 -m pip install nebula-chia ÁÑ∂ÂêéË∞ÉÁî® ChaiBatchConvertor Â∞±ÂèØ‰ª•Âú®ÂΩìÂâçÁõÆÂΩï‰∏ãÁîüÊàê‰∏§‰∏™ CSV Êñá‰ª∂„ÄÇ ËøôÈáåËæπÊúâ‰∏Ä‰∫õÂèØ‰ª•ÈÖçÁΩÆÁöÑÂèÇÊï∞ÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ÁîüÊàêÁöÑÊñá‰ª∂Ôºö $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv ËøôÈáåËæπÂ≠óÊÆµÁöÑÂê´‰πâÂíåÁ±ªÂûãÔºåÂèØ‰ª•ÂèÇËÄÉ‰ª£Á†Å‰∏≠ block_record_row Âíå coin_record_row ÁöÑ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import Êï∞ÊçÆÂØºÂÖ•Êúâ‰∫Ü CSV Êñá‰ª∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÄüÂä© Nebula-Importer ÂØºÂÖ•Êï∞ÊçÆÂà∞ÂõæÊï∞ÊçÆÂ∫ì‰∏≠„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÂÜôÂ•Ω‰∫Ü nebula-importer ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÊ∂µ‰∫ÜÂ¶Ç‰∏ã‰ø°ÊÅØ: Âú® Nebula Graph ‰∏≠ÂàõÂª∫ÈúÄË¶ÅÁöÑÊï∞ÊçÆÊ®°Âûã SchemaÔºåËøôÂíåÊàë‰ª¨ÂâçËæπÂÅöÁöÑÂõæÊò†Â∞ÑÁöÑ‰ø°ÊÅØÊòØÁ≠â‰ª∑ÁöÑ ÊèèËø∞ CSV Êñá‰ª∂‰πã‰∏≠ÁöÑ Column ÁöÑÊï∞ÊçÆÂà∞ÂõæÊ®°ÂûãÔºàÁÇπÔºåËæπÔºåÁÇπÊàñËæπÁöÑÂ±ûÊÄßÔºâÊò†Â∞ÑÂÖ≥Á≥ª # ËøôÈáåÔºåÊàëÁöÑ csv Êñá‰ª∂Âíå ÈÖçÁΩÆÊñá‰ª∂ÈÉΩÊîæÂú® /home/wei.gu/chia ‰πã‰∏ã # Êàë‰ΩøÁî® docker-compose ÈªòËÆ§ÈÖçÁΩÆÈÉ®ÁΩ≤ÁöÑ Nebula Graph, # ÂÆÉÂàõÂª∫‰∫ÜÂè´ nebula-docker-compose_nebula-net ÁöÑ docker ÁΩëÁªú docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml ËøôÈáåÊàëÂ±ïÁ§∫‰∏Ä‰∏™ÂØºÂÖ•ÁöÑÁªìÊûúÁ§∫‰æãÔºåÊàëÂú®ÂçïÊú∫ÈÉ®ÁΩ≤ÁöÑ Nebula Graph ÈáåÂØºÂÖ•‰∫ÜÊàë‰∏Ä‰∏§Âë®‰πãÂâçÂèñÁöÑÂÖ®Èáè Chia Network Êï∞ÊçÆÁöÑÁªìÊûú„ÄÇ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-import-Êï∞ÊçÆÂØºÂÖ•"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph Êé¢Á¥¢ Chia ÁöÑÊï∞ÊçÆ3.2.1 Graph DB QueriesÂØºÂÖ• Chia ÈìæÁöÑÁΩëÁªúÂà∞ Nebula Graph ‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âú®ÈáåËæπÂø´ÈÄüÊü•ËØ¢Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ ÊØîÂ¶ÇËøô‰∏™Êü•ËØ¢Ë°®Á§∫‰ªéÂå∫Âùó 524aa2013781ff4cd9d2b5dc... Êü•Ëµ∑ÔºåÁªèËøá‰∏âÁßçËæπ farmer_puzzle, spends, confirms ÂèåÂêëÈÅçÂéÜÁöÑÁªìÊûú„ÄÇ GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC ÂÜçÊØîÂ¶ÇÔºåËÆ°ÁÆó‰∏Ä‰∏™ Puzzle Âú∞ÂùÄ‰∏äÁöÑ‰ΩôÈ¢ùÔºàÊâÄÊúâ coin ÁöÑÊÄª‰ª∑ÂÄºÔºâÊØîÂ¶ÇËøô‰∏™puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫ÜÂõæÂΩ¢ÂåñÁïåÈù¢ÔºåÊúâ‰∫ÜÂÆÉÔºåÊàë‰ª¨ÂèØ‰ª•Áî®Êõ¥Á¨¶Âêà‰∫∫ËÑëÁöÑÊñπÂºèÂú∞Êü•Áúã Chia Network ‰∏≠ÁöÑÊï∞ÊçÆ„ÄÇ ÊØîÂ¶ÇÔºåÊàë‰ª¨ËøòÊòØÂõûÂà∞‰∏äËæπÁöÑÈÇ£‰∏™Âå∫ÂùóÔºå‰ªéËøôÈáåÊü•ËØ¢„ÄÇ Êàë‰ª¨Â∞±Ëé∑Âæó‰∫ÜËøô‰∏™ block Á±ªÂûãÁöÑ‰∏Ä‰∏™ÁÇπ/ vertex„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ªé‰ªñÂºÄÂßãËøõ‰∏ÄÊ≠•Êé¢Á¥¢ÔºåÂÖàÈº†Ê†áÂçïÂáªËøô‰∏™ÁÇπÔºåÂú®ÊãìÂ±ïÊù°‰ª∂ÈáåÊääÊñπÂêëÈÄâÊã©ÂèåÂêëÔºåÈªòËÆ§ÁöÑËæπÁ±ªÂûãÊòØÊâÄÊúâÁöÑËæπÁ±ªÂûãÔºåËøôÊ†∑Êàë‰ª¨Â∞±ÂèØ‰ª•ÊääÊâÄÊúâ Ê≠•Êï∞ÂÜÖÁõ∏ÂÖ≥ËÅîÁöÑÊï∞ÊçÆ‰∏Ä‰∏ãÂ≠êÂÖ®ÈÉΩÊâæÂá∫Êù•„ÄÇ ÈÄâÊã©Â•ΩÊãìÂ±ïÊù°‰ª∂‰πãÂêéÔºåÁÇπÂáªÊãìÂ±ïÂ∞±ÂèØ‰ª•„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÈÄâÊã©‰∫ÜÊ≠•Êï∞‰∏∫ 1ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºå‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âø´ÈÄüÂèåÂáªÂÖ∂‰ªñÁöÑÁÇπÁªßÁª≠ÊãìÂ±ïÔºåËøôÊòØÊàëÈº†Ê†áÁÇπ‰∫ÜÂá†Ê¨°‰πãÂêéÁúãÂà∞ÁöÑÊ†∑Â≠êÔºö Êàë‰ª¨Êé•‰∏ãÊù•ÂÜçËØïËØïÊãìÂ±ïÁöÑÊ≠•Êï∞‰∏∫ 2ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºåÁúãËµ∑Êù•ÊâæÂà∞‰∫ÜÊúâÊÑèÊÄùÁöÑ‰ø°ÊÅØ„ÄÇ Êàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∏™ÊúâÂæàÂ§öËæπÁöÑÈªëËâ≤ÁöÑÁÇπ„ÄÇ ÈÄöËøáÊü•ÁúãËøô‰∏™ÁÇπÂíåÊàë‰ª¨ÂºÄÂßãÊü•ÁúãÁöÑ block ‰πãÈó¥ÁöÑËæπÔºåÊàë‰ª¨Áü•ÈÅìËøô‰∏™ÁÇπÊ≠£ÊòØ farm Ëøô‰∏™ block ÁöÑÂú∞ÂùÄÔºåËøô‰∏™Âú∞ÂùÄ‰∏ãËæπÊúâÈùûÂ∏∏Â§öÁöÑ coin„ÄÇ ËøôÂè™ÊòØ‰∏Ä‰∏™ÂºÄÂßãÔºåÊúâ‰∫ÜËøô‰∏™ÂØºÂÖ•Âà∞ Nebula Graph ÂõæÊï∞ÊçÆÁöÑÂü∫Á°ÄÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅöÂæàÂ§öÊúâÊÑèÊÄùÁöÑÂàÜÊûêÂíåÊ¥ûÂØüÔºåÂ§ßÂÆ∂ÂèØ‰ª•Ëá™Â∑±ËØïËØïÁúãÔºåÂæóÂà∞Êõ¥ÊúâÊÑèÊÄùÁöÑÁªìÊûúÂàÜ‰∫´ÁªôÂÖ∂‰ªñÂêåÂ≠¶„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#explore-the-chia-graph-Êé¢Á¥¢-chia-ÁöÑÊï∞ÊçÆ"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph Êé¢Á¥¢ Chia ÁöÑÊï∞ÊçÆ3.2.1 Graph DB QueriesÂØºÂÖ• Chia ÈìæÁöÑÁΩëÁªúÂà∞ Nebula Graph ‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âú®ÈáåËæπÂø´ÈÄüÊü•ËØ¢Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ ÊØîÂ¶ÇËøô‰∏™Êü•ËØ¢Ë°®Á§∫‰ªéÂå∫Âùó 524aa2013781ff4cd9d2b5dc... Êü•Ëµ∑ÔºåÁªèËøá‰∏âÁßçËæπ farmer_puzzle, spends, confirms ÂèåÂêëÈÅçÂéÜÁöÑÁªìÊûú„ÄÇ GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC ÂÜçÊØîÂ¶ÇÔºåËÆ°ÁÆó‰∏Ä‰∏™ Puzzle Âú∞ÂùÄ‰∏äÁöÑ‰ΩôÈ¢ùÔºàÊâÄÊúâ coin ÁöÑÊÄª‰ª∑ÂÄºÔºâÊØîÂ¶ÇËøô‰∏™puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫ÜÂõæÂΩ¢ÂåñÁïåÈù¢ÔºåÊúâ‰∫ÜÂÆÉÔºåÊàë‰ª¨ÂèØ‰ª•Áî®Êõ¥Á¨¶Âêà‰∫∫ËÑëÁöÑÊñπÂºèÂú∞Êü•Áúã Chia Network ‰∏≠ÁöÑÊï∞ÊçÆ„ÄÇ ÊØîÂ¶ÇÔºåÊàë‰ª¨ËøòÊòØÂõûÂà∞‰∏äËæπÁöÑÈÇ£‰∏™Âå∫ÂùóÔºå‰ªéËøôÈáåÊü•ËØ¢„ÄÇ Êàë‰ª¨Â∞±Ëé∑Âæó‰∫ÜËøô‰∏™ block Á±ªÂûãÁöÑ‰∏Ä‰∏™ÁÇπ/ vertex„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ªé‰ªñÂºÄÂßãËøõ‰∏ÄÊ≠•Êé¢Á¥¢ÔºåÂÖàÈº†Ê†áÂçïÂáªËøô‰∏™ÁÇπÔºåÂú®ÊãìÂ±ïÊù°‰ª∂ÈáåÊääÊñπÂêëÈÄâÊã©ÂèåÂêëÔºåÈªòËÆ§ÁöÑËæπÁ±ªÂûãÊòØÊâÄÊúâÁöÑËæπÁ±ªÂûãÔºåËøôÊ†∑Êàë‰ª¨Â∞±ÂèØ‰ª•ÊääÊâÄÊúâ Ê≠•Êï∞ÂÜÖÁõ∏ÂÖ≥ËÅîÁöÑÊï∞ÊçÆ‰∏Ä‰∏ãÂ≠êÂÖ®ÈÉΩÊâæÂá∫Êù•„ÄÇ ÈÄâÊã©Â•ΩÊãìÂ±ïÊù°‰ª∂‰πãÂêéÔºåÁÇπÂáªÊãìÂ±ïÂ∞±ÂèØ‰ª•„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÈÄâÊã©‰∫ÜÊ≠•Êï∞‰∏∫ 1ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºå‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âø´ÈÄüÂèåÂáªÂÖ∂‰ªñÁöÑÁÇπÁªßÁª≠ÊãìÂ±ïÔºåËøôÊòØÊàëÈº†Ê†áÁÇπ‰∫ÜÂá†Ê¨°‰πãÂêéÁúãÂà∞ÁöÑÊ†∑Â≠êÔºö Êàë‰ª¨Êé•‰∏ãÊù•ÂÜçËØïËØïÊãìÂ±ïÁöÑÊ≠•Êï∞‰∏∫ 2ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºåÁúãËµ∑Êù•ÊâæÂà∞‰∫ÜÊúâÊÑèÊÄùÁöÑ‰ø°ÊÅØ„ÄÇ Êàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∏™ÊúâÂæàÂ§öËæπÁöÑÈªëËâ≤ÁöÑÁÇπ„ÄÇ ÈÄöËøáÊü•ÁúãËøô‰∏™ÁÇπÂíåÊàë‰ª¨ÂºÄÂßãÊü•ÁúãÁöÑ block ‰πãÈó¥ÁöÑËæπÔºåÊàë‰ª¨Áü•ÈÅìËøô‰∏™ÁÇπÊ≠£ÊòØ farm Ëøô‰∏™ block ÁöÑÂú∞ÂùÄÔºåËøô‰∏™Âú∞ÂùÄ‰∏ãËæπÊúâÈùûÂ∏∏Â§öÁöÑ coin„ÄÇ ËøôÂè™ÊòØ‰∏Ä‰∏™ÂºÄÂßãÔºåÊúâ‰∫ÜËøô‰∏™ÂØºÂÖ•Âà∞ Nebula Graph ÂõæÊï∞ÊçÆÁöÑÂü∫Á°ÄÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅöÂæàÂ§öÊúâÊÑèÊÄùÁöÑÂàÜÊûêÂíåÊ¥ûÂØüÔºåÂ§ßÂÆ∂ÂèØ‰ª•Ëá™Â∑±ËØïËØïÁúãÔºåÂæóÂà∞Êõ¥ÊúâÊÑèÊÄùÁöÑÁªìÊûúÂàÜ‰∫´ÁªôÂÖ∂‰ªñÂêåÂ≠¶„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#graph-db-queries"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph Êé¢Á¥¢ Chia ÁöÑÊï∞ÊçÆ3.2.1 Graph DB QueriesÂØºÂÖ• Chia ÈìæÁöÑÁΩëÁªúÂà∞ Nebula Graph ‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âú®ÈáåËæπÂø´ÈÄüÊü•ËØ¢Êï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ ÊØîÂ¶ÇËøô‰∏™Êü•ËØ¢Ë°®Á§∫‰ªéÂå∫Âùó 524aa2013781ff4cd9d2b5dc... Êü•Ëµ∑ÔºåÁªèËøá‰∏âÁßçËæπ farmer_puzzle, spends, confirms ÂèåÂêëÈÅçÂéÜÁöÑÁªìÊûú„ÄÇ GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC ÂÜçÊØîÂ¶ÇÔºåËÆ°ÁÆó‰∏Ä‰∏™ Puzzle Âú∞ÂùÄ‰∏äÁöÑ‰ΩôÈ¢ùÔºàÊâÄÊúâ coin ÁöÑÊÄª‰ª∑ÂÄºÔºâÊØîÂ¶ÇËøô‰∏™puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫ÜÂõæÂΩ¢ÂåñÁïåÈù¢ÔºåÊúâ‰∫ÜÂÆÉÔºåÊàë‰ª¨ÂèØ‰ª•Áî®Êõ¥Á¨¶Âêà‰∫∫ËÑëÁöÑÊñπÂºèÂú∞Êü•Áúã Chia Network ‰∏≠ÁöÑÊï∞ÊçÆ„ÄÇ ÊØîÂ¶ÇÔºåÊàë‰ª¨ËøòÊòØÂõûÂà∞‰∏äËæπÁöÑÈÇ£‰∏™Âå∫ÂùóÔºå‰ªéËøôÈáåÊü•ËØ¢„ÄÇ Êàë‰ª¨Â∞±Ëé∑Âæó‰∫ÜËøô‰∏™ block Á±ªÂûãÁöÑ‰∏Ä‰∏™ÁÇπ/ vertex„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ªé‰ªñÂºÄÂßãËøõ‰∏ÄÊ≠•Êé¢Á¥¢ÔºåÂÖàÈº†Ê†áÂçïÂáªËøô‰∏™ÁÇπÔºåÂú®ÊãìÂ±ïÊù°‰ª∂ÈáåÊääÊñπÂêëÈÄâÊã©ÂèåÂêëÔºåÈªòËÆ§ÁöÑËæπÁ±ªÂûãÊòØÊâÄÊúâÁöÑËæπÁ±ªÂûãÔºåËøôÊ†∑Êàë‰ª¨Â∞±ÂèØ‰ª•ÊääÊâÄÊúâ Ê≠•Êï∞ÂÜÖÁõ∏ÂÖ≥ËÅîÁöÑÊï∞ÊçÆ‰∏Ä‰∏ãÂ≠êÂÖ®ÈÉΩÊâæÂá∫Êù•„ÄÇ ÈÄâÊã©Â•ΩÊãìÂ±ïÊù°‰ª∂‰πãÂêéÔºåÁÇπÂáªÊãìÂ±ïÂ∞±ÂèØ‰ª•„ÄÇ ËøôÈáåÔºåÊàë‰ª¨ÈÄâÊã©‰∫ÜÊ≠•Êï∞‰∏∫ 1ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºå‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•Âø´ÈÄüÂèåÂáªÂÖ∂‰ªñÁöÑÁÇπÁªßÁª≠ÊãìÂ±ïÔºåËøôÊòØÊàëÈº†Ê†áÁÇπ‰∫ÜÂá†Ê¨°‰πãÂêéÁúãÂà∞ÁöÑÊ†∑Â≠êÔºö Êàë‰ª¨Êé•‰∏ãÊù•ÂÜçËØïËØïÊãìÂ±ïÁöÑÊ≠•Êï∞‰∏∫ 2ÔºåÁÇπÂáªÊãìÂ±ïÔºàÊàñËÄÖÂèåÂáªË¶ÅÊãìÂ±ïÁöÑÁÇπÔºâÔºåÁúãËµ∑Êù•ÊâæÂà∞‰∫ÜÊúâÊÑèÊÄùÁöÑ‰ø°ÊÅØ„ÄÇ Êàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∏™ÊúâÂæàÂ§öËæπÁöÑÈªëËâ≤ÁöÑÁÇπ„ÄÇ ÈÄöËøáÊü•ÁúãËøô‰∏™ÁÇπÂíåÊàë‰ª¨ÂºÄÂßãÊü•ÁúãÁöÑ block ‰πãÈó¥ÁöÑËæπÔºåÊàë‰ª¨Áü•ÈÅìËøô‰∏™ÁÇπÊ≠£ÊòØ farm Ëøô‰∏™ block ÁöÑÂú∞ÂùÄÔºåËøô‰∏™Âú∞ÂùÄ‰∏ãËæπÊúâÈùûÂ∏∏Â§öÁöÑ coin„ÄÇ ËøôÂè™ÊòØ‰∏Ä‰∏™ÂºÄÂßãÔºåÊúâ‰∫ÜËøô‰∏™ÂØºÂÖ•Âà∞ Nebula Graph ÂõæÊï∞ÊçÆÁöÑÂü∫Á°ÄÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅöÂæàÂ§öÊúâÊÑèÊÄùÁöÑÂàÜÊûêÂíåÊ¥ûÂØüÔºåÂ§ßÂÆ∂ÂèØ‰ª•Ëá™Â∑±ËØïËØïÁúãÔºåÂæóÂà∞Êõ¥ÊúâÊÑèÊÄùÁöÑÁªìÊûúÂàÜ‰∫´ÁªôÂÖ∂‰ªñÂêåÂ≠¶„ÄÇ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#the-visulized-way-with-nebula-studio"},{"categories":["Nebula Graph"],"content":"4 Summary ÊÄªÁªìËøôÁØáÊñáÁ´†ÈáåÔºåÂú®Êàë‰ª¨ÁÆÄÂçï‰ªãÁªç‰∫Ü Chia Network ‰πãÂêéÔºåÊàë‰ª¨È¶ñÊ¨°ÁöÑÂ∏¶Â§ßÂÆ∂‰∏ÄËµ∑‰ªéÂÆâË£Ö‰∏Ä‰∏™ Chia ÁªàÁ´ØÔºåÂà∞ÂàÜÊûêÁªàÁ´ØÂêåÊ≠•Âà∞Êú¨Âú∞ÁöÑ Chia ÂÖ®ÁΩëÊï∞ÊçÆÔºåÂÄüÂä©‰∫é Chia ÁªàÁ´ØÂºÄÊ∫êÁöÑ Python ‰ª£Á†ÅÂ∫ìÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂÖ®ÁΩëÊï∞ÊçÆÈáåÁöÑÈáçË¶Å‰ø°ÊÅØ„ÄÇ ‰πãÂêéÔºåÊàë‰ª¨ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™Â∞èÂ∑•ÂÖ∑ Nebula-ChiaÔºåÊúâ‰∫ÜÂÆÉÔºåÂ∞±ÂèØ‰ª•Êää Chia ÁöÑÂÖ®ÁΩëÊï∞ÊçÆËΩ¨Êç¢Êàê CSV Ê†ºÂºèÔºåËøôÊ†∑ÔºåÂ∞±ÂèØ‰ª•ÂÄüÂä© nebula-importer ÊääÊâÄÊúâÁöÑÊï∞ÊçÆÂØºÂÖ•Âà∞‰∏Ä‰∏™ÂÖàËøõÁöÑÂõæÊï∞ÊçÆÂ∫ìÔºàNebula GraphÔºâ‰∏≠„ÄÇ Nebula Graph ÁöÑÈ°πÁõÆÂú∞ÂùÄÊòØ https://github.com/vesoft-inc/nebula-graph Nebula-Chia Êàë‰πüÂºÄÊ∫êÂú® https://github.com/wey-gu/nebula-chia Âú®ÂõæÊï∞ÊçÆÂ∫ì‰∏≠ÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂÅöÂü∫Êú¨ Query ÁöÑ‰æãÂ≠êÂíåÂÄüÂä©ÂõæÊï∞ÊçÆÂ∫ìËá™Â∏¶ÁöÑÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºåÊàë‰ª¨ÂèØ‰ª•ËΩªÊòìÂú∞Ëé∑Âèñ Chia ÂÖ®ÁΩëÊï∞ÊçÆ‰πãÈó¥ÂÖ≥ËÅîÂÖ≥Á≥ªÔºåÊúâ‰∫ÜËøô‰∏™‰Ωú‰∏∫Âü∫Á°ÄÔºåËøô‰∫õÊï∞ÊçÆ‰∏≠Ê¥ûÂØüÁöÑÊΩúÂäõÂíåÂèØ‰ª•Â∞ùËØïÁöÑÊúâÊÑèÊÄù‰∫ãÊÉÖÂèØ‰ª•ÊØîËæÉÁõ¥ËßÇÂíåÈ´òÊïàÂú∞Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢‰∫ÜÔºÅ ÊòØ‰∏çÊòØÂæàÈÖ∑Ôºü ","date":"2021-05-26","objectID":"/en/nebula-chia/:4:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#summary-ÊÄªÁªì"},{"categories":["Nebula Graph"],"content":"5 References https://www.chia.net/faq/ https://chialisp.com/docs/ https://www.chiaexplorer.com/chia-coins https://docs.google.com/document/d/1tmRIb7lgi4QfKkNaxuKOBHRmwbVlGL4f7EsBDr_5xZE https://github.com/sipa/bech32/tree/master/ref/python https://github.com/Chia-Network/chia-blockchain/blob/main/README.md https://www.chia.net/assets/ChiaGreenPaper.pdf https://docs.nebula-graph.com.cn Banner Picture Credit: Icons8 Team ","date":"2021-05-26","objectID":"/en/nebula-chia/:5:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#references"},{"categories":null,"content":" How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-05-26","objectID":"/en/cources/:0:0","series":null,"tags":null,"title":"Hands on Courses","uri":"/en/cources/#"},{"categories":null,"content":" Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... Nebula-Chia A exploration(and open-source utility) on extracting and loading Chia Network Blockchain into Nebula Graph. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-05-26","objectID":"/en/projects/:0:0","series":null,"tags":null,"title":"Side Projects","uri":"/en/projects/#"},{"categories":null,"content":" Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-05-26","objectID":"/en/sketch-notes/:0:0","series":null,"tags":null,"title":"Sketch Notes","uri":"/en/sketch-notes/#"},{"categories":["Nebula Graph"],"content":"nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience.","date":"2021-05-05","objectID":"/en/vscode-ngql/","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/"},{"categories":["Nebula Graph"],"content":" nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#"},{"categories":["Nebula Graph"],"content":"VS Code nGQL Syntax Highlight","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#vs-code-ngql-syntax-highlight"},{"categories":["Nebula Graph"],"content":"1 DownloadSearch ngql from the market or click here. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:1:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#download"},{"categories":["Nebula Graph"],"content":"2 Features Highlighting all Keywords, Functions of a given .ngql file ","date":"2021-05-05","objectID":"/en/vscode-ngql/:2:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#features"},{"categories":["Nebula Graph"],"content":"3 Release Notes","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#release-notes"},{"categories":["Nebula Graph"],"content":"3.1 0.0.1Initial release, only .ngql Syntax is supported. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:1","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#001"},{"categories":["Nebula Graph"],"content":"3.2 0.0.2Lower supported vscode version till ^1.50.1 ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:2","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#002"},{"categories":["Nebula Graph"],"content":"4 Reference https://docs.nebula-graph.io/ https://github.com/vesoft-inc/nebula-graph/blob/master/src/parser/scanner.lex ","date":"2021-05-05","objectID":"/en/vscode-ngql/:4:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#reference"},{"categories":["Big Data","Cloud"],"content":"How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub","date":"2021-05-03","objectID":"/en/nebula-insights/","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/"},{"categories":["Big Data","Cloud"],"content":" How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub ËøôÊòØÊàëÈ¶ñÂèëÂú® Datawhale ÁöÑÊñáÁ´†Ôºå‰ªãÁªçÊàë‰ª¨Â¶Ç‰ΩïÁî®ÂÖ¨Êúâ‰∫ë Serverless ÊäÄÊúØÔºöGoogle Cloud SchedulerÔºåGoogle Cloud Functions Âíå BigQuery Êê≠Âª∫Êï∞ÊçÆÁÆ°ÈÅìÔºåÊî∂ÈõÜÊé¢Á¥¢ÂºÄÊ∫êÁ§æÂå∫Ê¥ûÂØü„ÄÇÂπ∂Â∞ÜÂÖ®ÈÉ®‰ª£Á†ÅÂºÄÊ∫êÂú® GitHub„ÄÇ ÂºïÂ≠ê Êàë‰ª¨ÊÉ≥Ë¶ÅÊî∂ÈõÜ‰∏Ä‰∫õÂ∏ÆÂä© Nebula Graph Á§æÂå∫ËøêËê•ÁöÑ metricsÔºåÂ∏åÊúõËÉΩ‰ªé‰∏çÂêåÊù•Ê∫êÁöÑÊï∞ÊçÆËá™Âä®ÂåñÂë®ÊúüÊÄßÊî∂ÈõÜ„ÄÅÂ§ÑÁêÜ„ÄÅÂπ∂Êñπ‰æøÂú∞Â±ïÁé∞Âá∫Êù•ÂÅöÊï∞ÊçÆÈ©±Âä®ÂàÜÊûêÁöÑÂü∫Á°ÄËÆæÊñΩ„ÄÇ Nebula Graph ÊòØ‰∏Ä‰∏™Áé∞‰ª£ÁöÑÂºÄÊ∫êÂàÜÂ∏ÉÂºèÂõæÊï∞ÊçÆÂ∫ì(Graph Database)ÔºåÊ¨¢ËøéÂêåÂ≠¶‰ª¨‰ªé: ÂÆòÁΩë: https://nebula-graph.com.cn Bilibili: https://space.bilibili.com/472621355 GitHub:https://github.com/vesoft-inc/nebula-graph ‰∫ÜËß£Êàë‰ª¨Âìà„ÄÇ ","date":"2021-05-03","objectID":"/en/nebula-insights/:0:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#"},{"categories":["Big Data","Cloud"],"content":"1 ÈúÄÊ±Ç Êñπ‰æøÂ¢ûÂä†Êñ∞ÁöÑÊï∞ÊçÆ Êï∞ÊçÆÊî∂ÈõÜÊó†ÈúÄ‰∫∫‰∏∫Ëß¶ÂèëÔºàËá™Âä®„ÄÅÂë®ÊúüÊÄßÔºâ ÊØèÂ§©Êï∞ÊçÆÈáè‰∏çË∂ÖËøá1000Êù° Êï∞ÊçÆÂèØ‰ª•ÁîüÊàê dashboardÔºå‰πüÂèØ‰ª•ÊîØÊåÅÁªüËÆ°ÂàÜÊúü query È´òÂèØÁî®ÔºåÊï∞ÊçÆÂÆâÂÖ® ‰ΩéÈ¢ÑÁÆóÔºåÂ∞ΩÂèØËÉΩ‰∏çÈúÄË¶ÅËøêÁª¥‰∫∫Âäõ ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#ÈúÄÊ±Ç"},{"categories":["Big Data","Cloud"],"content":"1.1 ÈúÄÊ±ÇÂàÜÊûêÊàë‰ª¨ÈúÄË¶ÅÊê≠Âª∫‰∏Ä‰∏™Á≥ªÁªüËÉΩÂÆûÁé∞ ‰∏Ä‰∏™ËÉΩÂë®ÊúüÊÄßËß¶ÂèëËé∑ÂèñÊï∞ÊçÆÁöÑ‰∫ã‰ª∂ÁöÑÊúçÂä°: scheduler ‰∏Ä‰∏™Ëß¶Âèë‰πãÂêéÔºåÊääÊï∞ÊçÆ ETL Âà∞Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑÊúçÂä°: ETL worker ‰∏Ä‰∏™Êï∞ÊçÆ‰ªìÂ∫ì ‰∏Ä‰∏™ËÉΩÂ§üÊääÊï∞ÊçÆÂ∫ì‰Ωú‰∏∫Ê∫êÔºåÂÖÅËÆ∏Áî®Êà∑ queryÔºåÂ±ïÁ§∫Êï∞ÊçÆÁöÑÁïåÈù¢: Data-UI Ëøô‰∏™ÈúÄÊ±ÇÁöÑÁâπÁÇπÊòØËôΩÁÑ∂Êï∞ÊçÆÈáèÂæàÂ∞è„ÄÅ‰ΩÜÊòØË¶ÅÊ±ÇÊúçÂä°È´òÂèØÁî®„ÄÅÂÆâÂÖ®„ÄÇÂõ†‰∏∫ËøôÁßçÊÉÖÂÜµ‰∏ãËá™Âª∫ÊúçÂä°Âô®ËøòÈúÄË¶Å‰øùËØÅHAÂíåÊï∞ÊçÆÂÆâÂÖ®‰ºö‰∏ÄÂÆö‰ºöÊ∂àËÄóÊòÇË¥µËøêÁª¥‰∫∫ÂäõÔºåÊâÄ‰ª•Êàë‰ª¨Â∫îËØ•Â∞ΩÈáèÈÅøÂÖçÂú®Ëá™Â∑±Áª¥Êä§ÁöÑÊúçÂä°Âô®‰∏≠Êê≠Âª∫ scheduler, ÂíåÊï∞ÊçÆÂ∫ì„ÄÇ ÊúÄÁªàÔºåÊàë‰ª¨ÈÄâÊã©‰∫ÜÂ∞ΩÈáè‰ΩøÁî®ÂÖ¨Êúâ‰∫ëÁöÑ aaS ÁöÑÊñπÊ°à: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Google Cloud Scheduler ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ GitHub API Server ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Google Cloud Functions ‚îú‚îÄ‚îÄ‚îÄ‚î§ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Docker Hub API Server ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Google BigQuery ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ... ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Aliyun OSS API ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ Google Data Studio ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Âõ†‰∏∫Êàë‰∏™‰∫∫ÊØîËæÉÁÜüÊÇâ Google Cloud Platform(GCP)ÁöÑÂéüÂõ†ÔºåÂä†‰∏äGCPÂú®Â§ßÊï∞ÊçÆÂ§ÑÁêÜ‰∏äÊØîËæÉÈ¢ÜÂÖàÔºåÂÜçÂä†‰∏äGoogleÊèê‰æõÁöÑ free tierÈ¢ùÂ∫¶ÈùûÂ∏∏Â§ßÊñπÔºå‰ª•Ëá≥‰∫éÂú®Êàë‰ª¨Ëøô‰∏™Êï∞ÊçÆÈáè‰∏ãÔºåÊâÄÊúâworkloadÈÉΩ‰ºöÊòØÂÖçË¥πÁöÑ„ÄÇ Ëøô‰∏™ÊñπÊ°àÊúÄÂêéÈÄâÊã©‰∫ÜÂÖ®Ê†à Google CloudÔºåÁÑ∂ËÄåÔºåËøôÂÆûÈôÖ‰∏äÂè™ÊòØ‰∏Ä‰∏™ÂèÇËÄÉÔºåÂêåÂ≠¶‰ª¨ÂÆåÂÖ®ÂèØ‰ª•Âú®ÂÖ∂‰ªñÂÖ¨Êúâ‰∫ëÊèê‰æõÂïÜÈÇ£ÈáåÊâæÂà∞ÂØπÂ∫îÁöÑÊúçÂä°„ÄÇ ËøôÈáåÊàëÁÆÄÂçï‰ªãÁªç‰∏Ä‰∏ãÔºå Google Cloud SchedulerÊòØËá™Ëß£ÈáäÁöÑÔºå‰∏çÁî®Â§ö‰ªãÁªç‰∫Ü„ÄÇ ËÄå Google Cloud FunctionsÊòØGCPÁöÑÊó†ÊúçÂä°Âô®(serverless)ÁöÑ Function as a ServiceÊúçÂä°ÔºåÂÆÉÁöÑÂ•ΩÂ§ÑÊòØÊàë‰ª¨ÂèØ‰ª•ÊääÊó†Áä∂ÊÄÅÁöÑ event-driven ÁöÑ workload ‰ª£Á†ÅÊîæ‰∏äÂéªÔºåÂÆÉÊòØÊåâÈúÄ‰ªòË¥πÔºàpay as you go)ÁöÑÔºåÁ±ª‰ººÁöÑÊúçÂä°ËøòÊúâ Google Cloud RunÔºåÂêéËÄÖÁöÑÂå∫Âà´Âú®‰∫éÊàë‰ª¨Êèê‰æõÁöÑÊòØ‰∏Ä‰∏™docker/containerÔºàËøô‰ΩøÂæóËÉΩÊîØÊåÅÁöÑËøêË°åÁéØÂ¢ÉÂèØ‰ª•‰Ωø‰ªª‰ΩïËÉΩË∑ëÂú®ÂÆπÂô®ÈáåÁöÑ‰∏úË•øÔºâÔºåËÄå Cloud FunctionsÊòØÊääÊàë‰ª¨ÁöÑ‰ª£Á†ÅÊñá‰ª∂Êîæ‰∏äÂéª„ÄÇ‰ªñ‰ª¨ÁöÑÊïàÊûúÊòØÁ±ª‰ººÁöÑÔºåÂõ†‰∏∫ÊàëÂáÜÂ§áÁî®PythonÊù•ÂÅö ETLÁöÑ‰∏úË•øÔºåClouf FunctionsÂ∑≤ÁªèÊîØÊåÅ‰∫ÜÔºåÊàëÂ∞±Áõ¥Êé•ÈÄâÊã©ÂÆÉ‰∫Ü„ÄÇ Âú®schedulerÈáåËæπÔºåÊàëÂÆö‰πâ‰∫ÜÊØè‰∏ÄÂ§©ÂÆÉÂèë‰∏Ä‰∏™ pub/subÔºàÁ±ª‰ºº‰∫ékafkaÔºåËøôÈáågoogleÂèØ‰ª•‰øùËØÅËá≥Â∞ëÂèëÊàêÂäü‰∏ÄÊ¨°ÔºâÊ∂àÊÅØÁªô Cloud FunctionsÔºåÁÑ∂Âêé Cloud Functions‰ºöÂéªÂÅö ETLÁöÑÂ∑•‰Ωú„ÄÇ ËøôÈáåÔºåÂÆûÈôÖ‰∏äÊàëÁöÑËÆæËÆ°ÈáåËøô‰∏™Ëß¶ÂèëÁöÑÂáΩÊï∞Ë∞É‰ºöÊääÊï∞ÊçÆ‰ªéAPIÈÇ£ÈáåËé∑Âèñ‰∏ãÊù•ÔºåÂú®ÂÜÖÂ≠òÈáåÂ§ÑÁêÜÂ•Ω‰πãÂêéÔºåÂ≠òÂÇ®Âà∞Âú®ÂØπË±°Â≠òÂÇ®Èáå‰∏∫ JSON Êñá‰ª∂ÔºåÁÑ∂ÂêéÂÜçË∞ÉÁî® Google BigQuery ÁöÑ APIËÆ© BigQueryÁõ¥Êé•‰ªéÂØπÂØπË±°Â≠òÂÇ®ÈáåÊãâÂèñ JSON Êñá‰ª∂ÔºåÂØºÂÖ•ËÆ∞ÂΩïÂà∞Áõ∏Â∫îÁöÑË°®‰πã‰∏≠„ÄÇ Google BigQuery ‰Ωú‰∏∫GCP ÁâπÂà´ÊúâÁ´û‰∫âÂäõÁöÑ‰∏Ä‰∏™‰∫ßÂìÅÔºåÊòØÂÆÉÊï∞ÊçÆ‰ªìÂ∫ìÔºåBigQuery ÂèØ‰ª•Êó†ÈôêÊâ©ÂÆπÔºåÊîØÊåÅÊµ∑ÈáèÊï∞ÊçÆÂØºÂÖ•ÔºåÊîØÊåÅ SQL-like ÁöÑ queryÔºåËøòËá™Â∏¶MLÁÆóÊ≥ïÔºåÈÄöËøáSQLÂ∞±ËÉΩË∞ÉÁî®Ëøô‰∫õÁÆóÊ≥ï„ÄÇÂÆÉÂèØ‰ª•ÂíåÂæàÂ§öGCP‰ª•ÂèäÁ¨¨‰∏âÊñπÁöÑÁªÑ‰ª∂ÂèØ‰ª•ÈõÜÊàêËµ∑Êù•„ÄÇ Google Data Studio ÊòØGCPÁöÑÊï∞ÊçÆ Insights‰∫ßÂìÅÔºåÂ¶ÇÊûúÂ§ßÂÆ∂Áî®Ëøá Google Analytics Â∫îËØ•Â∑≤ÁªèÁî®ËøáÂÆÉ‰∫Ü„ÄÇ ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#ÈúÄÊ±ÇÂàÜÊûê"},{"categories":["Big Data","Cloud"],"content":"1.2 Êï∞ÊçÆÁöÑËé∑ÂèñÔºåAPIÊàë‰ª¨Á¨¨‰∏ÄÈò∂ÊÆµÊÉ≥Ë¶ÅÊî∂ÈõÜÁöÑÊï∞ÊçÆÊù•Ê∫êÊòØ GitHub ‰∏äÔºåÁ§æÂå∫È°πÁõÆÁöÑÁªüËÆ°Êï∞ÊçÆ„ÄÅDocker Hub‰∏äÔºåÁ§æÂå∫ÈïúÂÉèÁöÑÊãâÂèñËÆ°Êï∞Ôºå‰πãÂêéÔºå‰ºöÂ¢ûÂä†Êõ¥Â§öÁª¥Â∫¶ÁöÑÊï∞ÊçÆ„ÄÇ Github API, ref: https://pygithub.readthedocs.io ËøôÈáåÊàë‰ª¨Âà©Áî®‰∫Ü‰∏Ä‰∏™Github APIÁöÑ‰∏Ä‰∏™ Python Â∞ÅË£ÖÔºå‰∏ãËæπÊòØÂú® IDLE/iPython/Jupyter ÈáåÂ∞ùËØïÁöÑ‰æãÂ≠ê # ÂÆû‰æãÂåñ‰∏Ä‰∏™client g = Github(login_or_token=token, timeout=60, retry=Retry( total=10, status_forcelist=(500, 502, 504), backoff_factor=0.3)) # ÈÖçÁΩÆÂ•ΩË¶ÅËé∑ÂèñÁöÑrepoÁöÑ‰ø°ÊÅØ org_str = \"vesoft-inc\" org = g.get_organization(org_str) repos = org.get_repos() # ËøôÈáåreposÊòØ‰∏Ä‰∏™Ëø≠‰ª£Âô®ÔºåÊñπ‰æøÁúãÂà∞ÈáåËæπÁöÑ‰∏úË•øÔºåÊàë‰ª¨ÊääÂÆÉ list ‰∏Ä‰∏ãÂèØ‰ª•ÁúãÂà∞ÊâÄÊúâÁöÑrepo: list(repos) [Repository(full_name=\"vesoft-inc/nebula\"), Repository(full_name=\"vesoft-inc/nebula-docs\"), Repository(full_name=\"vesoft-inc/nebula-dev-docker\"), Repository(full_name=\"vesoft-inc/github-statistics\"), Repository(full_name=\"vesoft-inc/nebula-docker-compose\"), Repository(full_name=\"vesoft-inc/nebula-go\"), Repository(full_name=\"vesoft-inc/nebula-java\"), Repository(full_name=\"vesoft-inc/nebula-python\"), Repository(full_name=\"vesoft-inc/nebula-importer\"), Repository(full_name=\"vesoft-inc/nebula-third-party\"), Repository(full_name=\"vesoft-inc/nebula-storage\"), Repository(full_name=\"vesoft-inc/nebula-graph\"), Repository(full_name=\"vesoft-inc/nebula-common\"), Repository(full_name=\"vesoft-inc/nebula-stats-exporter\"), Repository(full_name=\"vesoft-inc/nebula-web-docker\"), Repository(full_name=\"vesoft-inc/nebula-bench\"), Repository(full_name=\"vesoft-inc/nebula-console\"), Repository(full_name=\"vesoft-inc/nebula-docs-cn\"), Repository(full_name=\"vesoft-inc/nebula-chaos\"), Repository(full_name=\"vesoft-inc/nebula-clients\"), Repository(full_name=\"vesoft-inc/nebula-spark-utils\"), Repository(full_name=\"vesoft-inc/nebula-node\"), Repository(full_name=\"vesoft-inc/nebula-rust\"), Repository(full_name=\"vesoft-inc/nebula-cpp\"), Repository(full_name=\"vesoft-inc/nebula-http-gateway\"), Repository(full_name=\"vesoft-inc/nebula-flink-connector\"), Repository(full_name=\"vesoft-inc/nebula-community\"), Repository(full_name=\"vesoft-inc/nebula-br\"), Repository(full_name=\"vesoft-inc/.github\")] # repo0 ÊòØ vesoft-inc/nebula Ëøô‰∏™repoÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøá get_clones_trafficÔºåget_views_traffic Êù•Ëé∑ÂèñËøáÂéªÂçÅÂá†Â§©ÁöÑ cloneÔºåview ÁªüËÆ° In [16]: repo0.get_clones_traffic() Out[16]: {'count': 362, 'uniques': 150, 'clones': [Clones(uniques=5, timestamp=2021-04-06 00:00:00, count=16), Clones(uniques=8, timestamp=2021-04-07 00:00:00, count=23), Clones(uniques=13, timestamp=2021-04-08 00:00:00, count=30), Clones(uniques=33, timestamp=2021-04-09 00:00:00, count=45), Clones(uniques=2, timestamp=2021-04-10 00:00:00, count=13), Clones(uniques=6, timestamp=2021-04-11 00:00:00, count=19), Clones(uniques=15, timestamp=2021-04-12 00:00:00, count=28), Clones(uniques=40, timestamp=2021-04-13 00:00:00, count=54), Clones(uniques=9, timestamp=2021-04-14 00:00:00, count=21), Clones(uniques=10, timestamp=2021-04-15 00:00:00, count=34), Clones(uniques=10, timestamp=2021-04-16 00:00:00, count=23), Clones(uniques=5, timestamp=2021-04-17 00:00:00, count=17), Clones(uniques=2, timestamp=2021-04-18 00:00:00, count=13), Clones(uniques=9, timestamp=2021-04-19 00:00:00, count=23), Clones(uniques=3, timestamp=2021-04-20 00:00:00, count=3)]} In [17]: repo0.get_views_traffic() Out[17]: {'count': 6019, 'uniques': 1134, 'views': [View(uniques=52, timestamp=2021-04-06 00:00:00, count=169), View(uniques=143, timestamp=2021-04-07 00:00:00, count=569), View(uniques=152, timestamp=2021-04-08 00:00:00, count=635), View(uniques=134, timestamp=2021-04-09 00:00:00, count=648), View(uniques=81, timestamp=2021-04-10 00:00:00, count=318), View(uniques=42, timestamp=2021-04-11 00:00:00, count=197), View(uniques=127, timestamp=2021-04-12 00:00:00, count=515), View(uniques=149, timestamp=2021-04-13 00:00:00, count=580), View(uniques=134, timestamp=2021-04-14 00:00:00, count=762), View(uniques=141, timestamp=2021-04-15 00:00:00, count=385), View(uniques=113, timestamp=2021-04-16 00:00:00, count=284), View(uniques=48, timestamp=2021-04-17 00:00:00, count=168), View(uniques=35, timestamp=2021-04-18 00:00:00, count=135), View(uniques=124, timestamp=2021-04-19 00:00:00, cou","date":"2021-05-03","objectID":"/en/nebula-insights/:1:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#Êï∞ÊçÆÁöÑËé∑Âèñapi"},{"categories":["Big Data","Cloud"],"content":"2 ÂÆûÁé∞","date":"2021-05-03","objectID":"/en/nebula-insights/:2:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#ÂÆûÁé∞"},{"categories":["Big Data","Cloud"],"content":"2.1 ËÆ°Âàí‰ªªÂä°Ë∞ÉÂ∫¶ with Cloud SchedulerÂâçËæπÊèêÂà∞ÔºåScheduler --\u003e Functions ‰∏≠Èó¥ÊòØÈÄöËøáÊ∂àÊÅØÈòüÂàóÂÆûÁé∞ÁöÑÂèØÈù†‰∫ã‰ª∂Ëß¶ÂèëÔºåÊàë‰ª¨ÈúÄË¶ÅÂú® Google Cloud Pub/SubÈáåÂàõÂª∫‰∏Ä‰∏™ËÆ¢ÈòÖÊ∂àÊÅØÔºåÂêéËæπÊàë‰ª¨‰ºöÊääËøô‰∏™ËÆ¢ÈòÖÊ∂àÊÅØ‰ªé Scheduler ÂÆöÊúüÂèëÈÄÅÔºåÂπ∂‰∏îÂú® FunctionÂàõÂª∫ÁöÑÊó∂ÂÄôÂÆö‰πâ‰∏∫Ëß¶ÂèëÊù°‰ª∂„ÄÇ $ gcloud pubsub topics create nebula-insights-cron-topic $ gcloud pubsub subscriptions create cron-sub --topic nebula-insights-cron-topic ‰ªªÂä°ÁöÑÂàõÂª∫ÈùûÂ∏∏Áõ¥Êé•ÔºåÂú® Scheduler Web Console ‰∏äÁõ¥Êé•ÂõæÂΩ¢ÂåñÊìç‰ΩúÂ∞±ÂèØ‰ª•‰∫ÜÔºåËÆ∞ÂæóË¶ÅÈÄâÊã©Ëß¶Âèë Pub/Sub Ê∂àÊÅØ‰∏∫ cron-subÔºåÊ∂àÊÅØ‰∏ªÈ¢ò‰∏∫ nebula-insights-cron-topic ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#ËÆ°Âàí‰ªªÂä°Ë∞ÉÂ∫¶-with-cloud-scheduler"},{"categories":["Big Data","Cloud"],"content":"2.2 ETL Worker with Python + Google FunctionsÂΩì Scheduler ÊØèÂ§©ÂÆöÊó∂ÂèëÈÄÅÊ∂àÊÅØ‰πãÂêéÔºåÊé•Êî∂ÊñπÂ∞±ÊòØÊàë‰ª¨Ë¶ÅÂÆö‰πâÁöÑ Google Functions‰∫ÜÔºåÂÆÉÁöÑÂÆö‰πâÂ¶ÇÂõæ Á¨¨‰∏ÄÊ≠•ÔºåÈÄâÊã©ÂÆÉÁöÑËß¶ÂèëÁ±ªÂûã‰∏∫ Pub/SubÔºåÂêåÊó∂Ë¶ÅÂÆö‰πâÊ∂àÊÅØÁöÑ‰∏ªÈ¢òÂíåÂêçÂ≠ó„ÄÇ Á¨¨‰∫åÊ≠•Â∞±ÊòØÊää‰ª£Á†ÅÊîæËøõÂéª: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ GitHub API Server ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Google Cloud Functions ‚óÑ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Docker Hub API Server ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Google Cloud Storage ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ... ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Aliyun OSS API ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ Google BigQuery ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ËøôÈÉ®ÂàÜÁöÑÈÄªËæëÂ∞±ÊòØÈÄöËøáÂâçËæπÂàÜÊûê‰∫ÜÁöÑAPIÂèñÂæó‰ø°ÊÅØÔºåÁÑ∂ÂêéÁªÑË£ÖÊàêÈúÄË¶ÅÁöÑÊ†ºÂºèÂ≠òÂà∞ Cloud Storage(ÂØπË±°Â≠òÂÇ®ÔºâÔºåÁÑ∂ÂêéÂÜçÂØºÂÖ•Âà∞ BigQueryÔºàÊï∞‰ªìÔºâ‰πã‰∏≠ÔºåÂÖ®ÈÉ®‰ª£Á†ÅÂú®GitHub‰∏ä: https://github.com/wey-gu/nebula-insights/blob/main/functions/data-fetching-0/main.py Âè¶Â§ñÔºåÂèØ‰ª•ÂèÇËÄÉËøô‰∏™ÂÆòÊñπÊïôÁ®ã https://cloud.google.com/scheduler/docs/tut-pub-sub ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#etl-worker-with-python--google-functions"},{"categories":["Big Data","Cloud"],"content":"2.3 Êï∞‰ªìË°®ÁªìÊûÑÂÆö‰πâÊï∞‰ªìÁöÑË°®ÁªìÊûÑÊØîËæÉÁõ¥Êé•ÔºåschemaÁöÑÂõæË¥¥Âú®‰∏ãËæπ‰∫ÜÔºåÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåBigQueryÊîØÊåÅÂµåÂ•óÁöÑË°®ÁªìÊûÑÔºàËÄå‰∏çÂÉè‰∏ÄËà¨ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÈÇ£Ê†∑ÈúÄË¶ÅÊääËøôÊ†∑ÁöÑÈÄªËæëÁªìÊûÑÁî®ËæÖÂä©Ë°®Êù•Ë°®Á§∫ÔºâÔºåÂú®Êàë‰ª¨Ëøô‰∏™Âú∫ÊôØ‰∏ãÈùûÂ∏∏Êñπ‰æøÔºåÊØîÂ¶ÇreleaseË°®‰∏≠ÁöÑ assetsÁöÑ‰∏â‰∏™ÂµåÂ•óÂ≠óÊÆµ„ÄÇ Êõ¥ËØ¶ÁªÜÁöÑ‰ø°ÊÅØÂèØ‰ª•ÂèÇËÄÉGitHub‰∏äÁöÑ‰ªãÁªçÂíå‰ª£Á†Å: https://github.com/wey-gu/nebula-insights#data-etl-bigquery-and-gcs ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:3","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#Êï∞‰ªìË°®ÁªìÊûÑÂÆö‰πâ"},{"categories":["Big Data","Cloud"],"content":"2.4 Êï∞ÊçÆÂèØËßÜÂåñÂà∞ËøôÈáåÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•Ëá™Âä®Âú®BigQueryÈáåÂ≠òÊúâÊØèÂ§©Êî∂ÈõÜÁöÑ‰∏çÂêåÊù•Ê∫êÁöÑÁªüËÆ°Êï∞ÊçÆÂï¶ÔºåÊúâ‰∫ÜÂÆÉÔºåÊàë‰ª¨ÂèØ‰ª•ÂÄüÂä© Data Studio Êù•ÁîüÊàêÂêÑÂºèÂêÑÊ†∑ÁöÑÂèØËßÜÂåñË°®Á§∫„ÄÇ ÂèÇËÄÉ https://cloud.google.com/bigquery/docs/visualize-data-studio ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:4","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#Êï∞ÊçÆÂèØËßÜÂåñ"},{"categories":["Big Data","Cloud"],"content":"3 ÊÄªÁªìËøôÊ†∑ÔºåÊàë‰ª¨ÂÆûÈôÖ‰∏ä‰∏çÈúÄË¶Å‰ªª‰ΩïËÆ§‰∏∫Áª¥Êä§ÁöÑÊàêÊú¨ÂíåÊäïÂÖ•ÔºåÂ∞±Êê≠Âª∫‰∫Ü‰∏ÄÊï¥‰∏™Êï∞ÊçÆÁöÑÊµÅÊ∞¥Á∫øÔºåÂπ∂‰∏îÂè™ÈúÄË¶ÅÊåâÁÖßÊï∞ÊçÆÁî®Èáè‰ªòË¥πÔºåÂú®Êàë‰ª¨ÁöÑÊï∞ÊçÆÈáè‰∏ãÔºåÂèäÊó∂ËÄÉËôëÊú™Êù•Â¢ûÂä†Êï∞ÂçÅ‰∏™Êñ∞ÁöÑÈáèÂ∫¶ÁöÑÊî∂ÈõÜÔºåÊàë‰ª¨‰æùÁÑ∂Ê≤°ÊúâËææÂà∞ÈúÄË¶Å‰ªòË¥πÁöÑÁî®ÈáèÔºåÊòØ‰∏çÊòØÂæàCoolÔºü Âõ†‰∏∫Êï∞ÊçÆÂêåÊó∂Â≠òÂú®‰∫éÂØπË±°Â≠òÂÇ®‰∏éÊï∞‰ªìÈáåÔºåÊàë‰ª¨ÂèØ‰ª•Êñπ‰æøÈöèÊó∂ÊääÊï∞ÊçÆÂØºÂÖ•‰ªªÊÑèÂÖ∂‰ªñÂπ≥Âè∞‰∏ä„ÄÇ BigQueryËøòÊúâ‰∏Ä‰∫õÈùûÂ∏∏Â∏∏Áî®ÁöÑÔºåËá™Â∏¶ÁöÑÊú∫Âô®Â≠¶‰π†ÁöÑÂäüËÉΩÔºåÂè™ÈúÄË¶ÅÂÜô‰∏Ä‰∏™SQL-LikeÁöÑqueryÂ∞±ËÉΩËß¶ÂèëÁÑ∂ÂêéËé∑ÂæóÈ¢ÑÊµãÁªìÊûúÔºåÂ¶ÇÊûúÊàë‰ª¨Áî®Âà∞Ëøô‰∫õÂäüËÉΩÁöÑËØù‰πü‰ºöÂõûÂà∞ datawhale ‰∏∫ÂêåÂ≠¶‰ª¨ÁªßÁª≠ÂàÜ‰∫´Âìà„ÄÇ Á¨¨‰∏ÄÊ¨°ÂÅöÊï∞ÊçÆÂ∑•Á®ãÊñπÈù¢ÁöÑÂàÜ‰∫´ÔºåÂ¶ÇÊûúÊúâÈîôËØØÁöÑÂú∞ÊñπÊ¨¢ËøéÂ§ßÂÆ∂‰∏çÂêùÊåáÂá∫Âìà~~ Ë∞¢Ë∞¢ÔºÅ ","date":"2021-05-03","objectID":"/en/nebula-insights/:3:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#ÊÄªÁªì"},{"categories":["Nebula Graph"],"content":"A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies.","date":"2021-04-26","objectID":"/en/nebula-up/","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/"},{"categories":["Nebula Graph"],"content":" A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Nebula-Up is PoC utility to enable developer to bootstrap an nebula-graph cluster with nebula-graph-studio(Web UI) + nebula-graph-console(Command UI) ready out of box in an oneliner run. All required packages will handled with nebula-up as well, including Docker on Linux(Ubuntu/CentOS), Docker Desktop on macOS(including both Intel and M1 chip based), and Docker Desktop Windows. Also, it‚Äôs optimized to leverage China Repo Mirrors(docker, brew, gitee, etc‚Ä¶) in case needed enable a smooth deployment for both Mainland China users and others. macOS and Linux with Shell: curl -fsSL nebula-up.siwei.io/install.sh | bash Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access With nebula-graph version specification support, now it‚Äôs hardcoded in 2.0.0-GA With uninstall/cleanup support Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:0:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#"},{"categories":["Nebula Graph"],"content":"IPython-nGQL is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It's easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded.","date":"2021-03-07","objectID":"/en/ipython-ngql/","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/"},{"categories":["Nebula Graph"],"content":" ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It‚Äôs easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded. ipython-ngql is inspired by ipython-sql created by Catherine Devlin ","date":"2021-03-07","objectID":"/en/ipython-ngql/:0:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#"},{"categories":["Nebula Graph"],"content":"1 Get Started","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-started"},{"categories":["Nebula Graph"],"content":"1.1 Installationipython-ngql could be installed either via pip or from this git repo itself. Install via pip pip install ipython-ngql Install inside the repo git clone git@github.com:wey-gu/ipython-ngql.git cd ipython-ngql python setup.py install ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:1","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#installation"},{"categories":["Nebula Graph"],"content":"1.2 Load it in Jupyter Notebook or iPython %load_ext ngql ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:2","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#load-it-in-jupyter-notebook-or-ipython"},{"categories":["Nebula Graph"],"content":"1.3 Connect to Nebula GraphArguments as below are needed to connect a Nebula Graph DB instance: Argument Description --address or -addr IP address of the Nebula Graph Instance --port or -P Port number of the Nebula Graph Instance --user or -u User name --password or -p Password Below is an exmple on connecting to 127.0.0.1:9669 with username: ‚Äúuser‚Äù and password: ‚Äúpassword‚Äù. %ngql --address 127.0.0.1 --port 9669 --user user --password password ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:3","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#connect-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"1.4 Make QueriesNow two kind of iPtython Magics are supported: Option 1: The one line stype with %ngql: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id; Option 2: The multiple lines stype with %%ngql %%ngql USE pokemon_club; SHOW TAGS; SHOW HOSTS; There will be other options in future, i.e. from a .ngql file. ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:4","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#make-queries"},{"categories":["Nebula Graph"],"content":"1.5 Query String with Variablesipython-ngql supports taking variables from the local namespace, with the help of Jinja2 template framework, it‚Äôs supported to have queries like the below example. The actual query string should be GO FROM \"Sue\" OVER owns_pokemon ..., and \"{{ trainer }}\" was renderred as \"Sue\" by consuming the local variable trainer: In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:5","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#query-string-with-variables"},{"categories":["Nebula Graph"],"content":"1.6 Configure \u003ccode\u003engql_result_style\u003c/code\u003eBy default, ipython-ngql will use pandas dataframe as output style to enable more human readable output, while it‚Äôs supported to use the raw thrift data format comes from the nebula2-python itself. This can be done ad-hoc with below one line: %config IPythonNGQL.ngql_result_style=\"raw\" After above line being executed, the output will be like: ResultSet(ExecutionResponse( error_code=0, latency_in_us=2844, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) The result are always stored in variable _ in Jupyter Notebook, thus, to tweak the result, just refer a new var to it like: In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:6","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#configure-ngql_result_style"},{"categories":["Nebula Graph"],"content":"1.7 Get HelpDon‚Äôt remember anything or even relying on the cheatsheet here, oen takeaway for you: the help! In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:7","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-help"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ‚ùØ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#examples"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ‚ùØ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#jupyter-notebook"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ‚ùØ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#ipython"}]