[{"categories":["Nebula Graph","LLM"],"content":"How to easily do text2cypher with NebulaGraph","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/"},{"categories":["Nebula Graph","LLM"],"content":"Since GPT-3 began to show an unexpected â€œunderstanding ability,â€ we have been engaged in research, exploration, and sharing on the complementary combination of Graph + LLM technology. As of now, we had made many leading contributions to the LlamaIndex and Langchain projects. Starting from this article, we will share some of our periodic successes and methods with everyone. The topic of this blog is what we believe to be the lowest-hanging fruit in this field: text2cypherâ€”generating graph queries from natural language. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:0:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#"},{"categories":["Nebula Graph","LLM"],"content":" 1 Text2CypherAs the name implies, Text2Cypher is about converting natural language text into Cypher query statements. In terms of form, it is no different from another scenario that everyone might be familiar with: Text2SQLâ€“converting text into SQL. Essentially, most knowledge graphs and graph database applications are about querying graphs according to human intentions. Our efforts in building convenient visualization tools on graph databases and wrapping useful APIs all serve this purpose. One of the main factors that have hindered the broader application of graph databases and knowledge graphs might be the threshold for querying graph databases. So, how did we do it before the era of large language models? ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:1:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher"},{"categories":["Nebula Graph","LLM"],"content":" 2 Text2Cypher in old daysThe field of converting text into queries has always had such a demand even before the large language models and has always been one of the most common applications of knowledge graphs. For example, the essence of KBQA (Knowledge-Based Question Answering system) is basically text2cypher. Taking a project I wrote earlier, Siwi (pronounced: /ËˆsÉªwi/, a Q\u0026A application based on a dataset of basketball players) as an example, letâ€™s look at its backend architecture: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Speech â”‚ Frontend â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Siwi, /ËˆsÉªwi/ â”‚ â”‚ â”‚ Web_Speech_API â”‚ A PoC of Dialog System â”‚ â”‚ â”‚ Vue.JS â”‚ With Graph Database â”‚ â”‚ â”‚ â”‚ Backed Knowledge Graph â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ Sentence Backend â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚â”‚ â”‚ Web API, Flask â”‚ ./app/ â”‚ â”‚ â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚â”‚ â”‚ Sentence ./bot/ â”‚ â”‚ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚â”‚ â”‚ Intent Matching, â”‚ ./bot/classifierâ”‚ â”‚ â”‚â”‚ â”‚ Symentic Processing â”‚ â”‚ â”‚ â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚â”‚ â”‚ Intent, Enties â”‚ â”‚ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚â”‚ â”‚ Intent Actor â”‚ ./bot/actions â”‚ â”‚ â”‚â””â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ Graph Query â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Graph Database â”‚ NebulaGraph â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ When a question statement is sent over, it first needs to perform intent recognition (Intent) and entity recognition (Entity). Then, using an NLP model or code, it constructs the corresponding intent and entities into query statements for the knowledge graph. Finally, it queries the graph database and constructs an answer based on the returned structure. It can be imagined that enabling a program to: Understand intent from natural language: which type of supported questions it corresponds to. Identify entities: the main individuals involved in the question. Construct query statements from intent and entities. Itâ€™s not going to be an easy development task. A truly feasible implementation might consider a vast number of boundary conditions, whether in trained models or in rule codes. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:2:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-in-old-days"},{"categories":["Nebula Graph","LLM"],"content":" 3 Text2Cypher with LLMsIn the â€œpost-large language modelâ€ era, such â€œintelligentâ€ application scenarios that previously required specialized training or rule-writing can now be accomplished with generic models combined with prompt engineering. Note: Prompt engineering refers to methods of accomplishing â€œintelligentâ€ tasks using generation models or language models through natural language descriptions. In fact, right after the release of GPT-3, I started using it to help me compose many highly complex Cypher query statements. I discovered that it could craft complex pattern matches and multi-step conditional statementsâ€”ones that I would have previously had to debug bit by bit and take half a day to write. Typically, with its generated solutions, I would only need to make minor adjustments. Moreover, often I could learn from its responses about Cypher syntax blind spots that I wasnâ€™t previously aware of. Later, in February of this year, I tried implementing a project based on GPT-3 (as GPT-3.5 was not available then): ngql-GPT (code). Its working principle is straightforward, identical to Text2SQL. The language model has already learned Cypherâ€™s syntax through public domain training. When posing a task, we only need to provide the model with the graphâ€™s Schema as context. So, the basic prompt goes as: You are a NebulaGraph Cypher expert. Based on the provided graph Schema and the question, please write the query statement. The schema is as follows: --- {schema} --- The question is: --- {question} --- Now, write down the query statement: However, real-world prompts often need additional specifications: Only return the statement, no need for explanations, and no apologies. Emphasize not to write node or edge types outside of the schema. Those interested can refer to my implementation in LlamaIndexâ€™s KnowledgeGraph Query Engine. In real-world scenarios, when we want to quickly learn and build large language model applications, we often use orchestrator tools like Langchain or LlamaIndex. These tools help us create logical abstractions, avoiding the need to implement many generic scaffolding codes from scratch: Interacting with different language models. Engaging with various vector databases. Data segmentation. Moreover, these orchestration tools have embedded many best practices of engineering methods. This way, we can often employ a method to utilize the latest and most user-friendly research paper techniques of large language models, such as FLARE and Guidence. For this purpose, I have contributed tools in both LlamaIndex and Langchain that allow for easy Text2Cypher execution on NebulaGraph, achieving Text2Cypher in just 3 lines of code. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:3:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-with-llms"},{"categories":["Nebula Graph","LLM"],"content":" 4 Text2Cypher on NebulaGraphWithin LlamaIndexâ€™s KnowledgeQueryEngine and LangChainâ€™s NebulaGraphQAChain, we donâ€™t need to concern ourselves with NebulaGraphâ€™s Schema retrieval, Cypher statement generation prompts, various LLM calls, result processing, or linkageâ€”itâ€™s all plug-and-play! ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-on-nebulagraph"},{"categories":["Nebula Graph","LLM"],"content":" 4.1 Using LlamaIndexWith LlamaIndex, all we need to do is: Create a NebulaGraphStore instance. Create a KnowledgeQueryEngine. Then you can start querying directly. Isnâ€™t that super simple? Reference documentation: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html from llama_index.query_engine import KnowledgeGraphQueryEngine from llama_index.storage.storage_context import StorageContext from llama_index.graph_stores import NebulaGraphStore graph_store = NebulaGraphStore( space_name=space_name, edge_types=edge_types, rel_prop_names=rel_prop_names, tags=tags) storage_context = StorageContext.from_defaults(graph_store=graph_store) nl2kg_query_engine = KnowledgeGraphQueryEngine( storage_context=storage_context, service_context=service_context, llm=llm, verbose=True, ) # Ask the question to query KG and answer based on the query result. response = nl2kg_query_engine.query( \"Tell me about Peter Quill?\", ) # Generate Query only. graph_query = nl2kg_query_engine.generate_query( \"Tell me about Peter Quill?\", ) ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:1","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#using-llamaindex"},{"categories":["Nebula Graph","LLM"],"content":" 4.2 Using LangChainSimilarly, in Langchain, we just need to: Create a NebulaGraph instance. Create a NebulaGraphQAChain instance. And you can start posing questions right away. Reference documentation: https://python.langchain.com/docs/modules/chains/additional/graph_nebula_qa from langchain.chat_models import ChatOpenAI from langchain.chains import NebulaGraphQAChain from langchain.graphs import NebulaGraph graph = NebulaGraph( space=space_name, username=\"root\", password=\"nebula\", address=\"127.0.0.1\", port=9669, session_pool_size=30, ) chain = NebulaGraphQAChain.from_llm( llm, graph=graph, verbose=True ) chain.run( \"Tell me about Peter Quill?\", ) ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:2","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#using-langchain"},{"categories":["Nebula Graph","LLM"],"content":" 5 DemoThe online demo. This demo showcases how to leverage LLM to extract knowledge triples from various information sources (using Wikipedia as an example) and store them in the NebulaGraph graph database(thatâ€™s the KG building like never as easy before). In this demo, we first extracted information from Wikipedia about â€œGuardians of the Galaxy Vol. 3â€ and used the knowledge triples generated by the LLM to construct a knowledge graph. We then used Cypher to query the graph, and finally, with LlamaIndex and Langchainâ€™s Text2Cypher, we implemented the function to query the graph using natural language. You can click on other tabs to personally experience the visualization of the knowledge graph, Cypher queries, natural language queries (Text2Cypher), and other features. Here you can download the complete Notebook. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:5:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#demo"},{"categories":["Nebula Graph","LLM"],"content":" 6 ConclusionWith LLM, performing Text2Cypher on data in knowledge graphs and NebulaGraph has never been so easy. A knowledge graph with enhanced human-machine and machine access signifies a new era. We may no longer need the high costs previously associated with implementing backend services on top of graph databases(on text2cypher implementations). Moreover, thereâ€™s no longer a need for specialized training to allow domain experts to extract essential insights from the graph. Using the ecosystem integrations in LlamaIndex or LangChain, we can smartly apply our apps and graph data with virtually no development costs in just a few lines of code. However, Text2Cypher is just the beginning. Please stay tuned for our subsequent articles, where we will showcase the revolutionary changes that knowledge graphs and graph databases bring to the large language model ecosystem. é¢˜å›¾ promptï¼š In an artful fusion of language and AI, this minimalist oil painting captures the essence of technological advancement. Delicate brushstrokes depict a harmony of binary code and flowing words, converging into a central point. With a refined color palette and clean composition, the artwork represents the symbiotic relationship between language and artificial intelligence, inviting contemplation and appreciation. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:6:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#conclusion"},{"categories":["Nebula Graph"],"content":"Play with NebulaGraph Database from Jupyter Notebook","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/"},{"categories":["Nebula Graph"],"content":" Introduce the brand new ipython-ngql python package that enhances your ability to connect to NebulaGraph from your Jupyter Notebook or iPython. Now we can do%ngql MATCH p=(n:player)-\u003e() RETURN p to query from Jupyter Notebook and %ng_draw to render the result. Chinese version ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:0:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#"},{"categories":["Nebula Graph"],"content":" 1 InstallationJust do %pip install ipython-ngql from Jupyter Notebook and load it up via %load_ext ngql: %pip install ipython-ngql %load_ext ngql Then, connect the NebulaGraph with a line like this: %ngql --address 127.0.0.1 --port 9669 --user root --password nebula When connected, the cell will have an output of SHOW SPACES. ğŸ’¡ Note, you could install NebulaGraph dev env from Docker Desktop Extension Marketplace and literally have it ready with one click. Then, within the extension, go to â€œNebulaGraph AIâ€ and click Install NX Mode to install NebulaGraph + Jupyter Notebook local dev env. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:1:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#installation"},{"categories":["Nebula Graph"],"content":" 2 Query NebulaGraphWe could then do one-liner query with %ngql or multi-line query with %%ngql. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#query-nebulagraph"},{"categories":["Nebula Graph"],"content":" 2.1 One-liner queryFor instance: %ngql USE basketballplayer; %ngql MATCH (v:player{name:\"Tim Duncan\"})--\u003e(v2:player) RETURN v2.player.name AS Name; ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:1","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#one-liner-query"},{"categories":["Nebula Graph"],"content":" 2.2 Multi-line queryFor instance %%ngql ADD HOSTS \"storaged3\":9779,\"storaged4\":9779; SHOW HOSTS; ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:2","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#multi-line-query"},{"categories":["Nebula Graph"],"content":" 3 Draw the resultAfter any query result, we could render it visually with %ng_draw: # one query %ngql GET SUBGRAPH 2 STEPS FROM \"player101\" YIELD VERTICES AS nodes, EDGES AS relationships; %ng_draw # another query %ngql match p=(:player)-[]-\u003e() return p LIMIT 5 %ng_draw And itâ€™ll look like: And the renderred result will be in a single-file html, which could be embeded in web pages like: ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:3:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#draw-the-result"},{"categories":["Nebula Graph"],"content":" 4 Other functionalityWe could query %ngql help to know more details of options for ipython-ngql. Here also introudce you some small features. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#other-functionality"},{"categories":["Nebula Graph"],"content":" 4.1 Get the pandas df query resultThe result could be read from _ like: ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:1","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#get-the-pandas-df-query-result"},{"categories":["Nebula Graph"],"content":" 4.2 Play with ResultSet result insteadBy default, the return result is pandas df, but we could configure it as raw to enable debugging for Python NebulaGraph Application code on query result handling, like: In [1] : %config IPythonNGQL.ngql_result_style=\"raw\" In [2] : %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[3]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), ... Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [4]: r = _ In [5]: r.column_values(key='Trainer_Name')[0].cast() Out[5]: 'Tom' ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:2","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#play-with-resultset-result-instead"},{"categories":["Nebula Graph"],"content":" 4.3 Query TemplateBesides, I brought the template support in Jinja2, thus we could do varibales like {{ variable }} : ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:3","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#query-template"},{"categories":["Nebula Graph"],"content":" 5 FutureI am planning to add more options on the %ng_draw in the future, and itâ€™s always welcome for your help to contribute more from https://github.com/wey-gu/ipython-ngql. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:5:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#future"},{"categories":["Nebula Graph","LLM"],"content":"How Graph could help build better In-context Learning LLM Applications.","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/"},{"categories":["Nebula Graph","LLM"],"content":" How Graph could help build better In-context Learning LLM Applications. Chinese Version ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:0:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#"},{"categories":["Nebula Graph","LLM"],"content":" 1 LLM App ParadigmsAs a big improvement in Cognitive intelligence, LLM had changed many industries, in a way that we didnâ€™t expect to automate, accelerate or enable. Seeing new LLM-enabled applications being created every day, we are all still exploring new methods and use cases for leveraging this magic. One of the most typical patterns to bring LLM into the loop is to ask LLM to understand things based on proprietory/ certain domain knowledge. For now, there are two paradigms we could add that knowledge to LLM: fine-tuning and in-context learning. Fine-tuning refers to performing add-on training on LLM models with extra knowledge, whereas in-context learning is to adding some extra piece of knowledge to the query prompt. What we observe now is that in-context learning has gained popularity over Fine-tuning due to its simplicity. And in this blog, Iâ€™ll share what we had been doing around the in-context learning approach. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:1:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llm-app-paradigms"},{"categories":["Nebula Graph","LLM"],"content":" 2 Llama Index: Interface between data and LLM","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llama-index-interface-between-data-and-llm"},{"categories":["Nebula Graph","LLM"],"content":" 2.1 In-context learningThe basic idea of in-context learning is to use existing LLM(not updated) to handle special tasks toward specific knowledge datasets. For instance, to build an application to answer any questions about one person, or even act as oneâ€™s digital avatar, we can apply in-context learning to an autobiography book with LLM. In practice, the application will construct a prompt with the question from the user and some information â€œsearchedâ€ from the book, then query the LLM for an answer. â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Docs/Knowledge â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ User â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ LLM â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ One of the most performant ways to enable this searching approach to get the related info from the Docs/Knowledge(the book in the above example) for the special task, is to leverage Embeddings. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:1","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#in-context-learning"},{"categories":["Nebula Graph","LLM"],"content":" 2.2 EmbeddingThe embedding normally refers to a way to map real word things into a vector in a multidimensional space, for instance, we could map images into a space of (64 x 64) dimension, and if we are doing it well enough, the distance between the two images can reflect the similarity of them. Another example of embedding is the word2vec algorithm, which literally maps every word into a vector, for instance, and if the embedding is good enough, we could have addition and subtraction on them, we may have: vec(apple) + vec(pie) =~ vec(\"apple apie\") Or the vector measure of vec(apple) + vec(pie) - vec(\"apple apie\") tends to be 0: |vec(apple) + vec(pie) - vec(\"apple apie\")| =~ 0 Similarly, we could have â€œpearâ€ should be closer than â€œdinosaurâ€ to â€œappleâ€: |vec(apple) - vec(pear)| \u003c |vec(apple) - vec(dinosaur)| With that, we could in theory search for pieces of the book which are more related to a given question. And the basic process is: Split the book into small pieces, create the embedding per each piece, and store them When a question comes, compute the embedding of the question Find top-K similar embeddings of pieces of the book by calculating the distance Construct the prompt with both the question and the pieces of the book Query the LLM with the prompt â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â” â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ â”œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¤ â”‚ Docs/Knowledge â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚ ... â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ â”‚ â”‚ 95 â”‚ 96 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ User â”‚â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â–¶ LLM â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–² â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â–¶â”‚ Tell me ....., please â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”‚ â”‚ 3 â”‚ â”‚ 96 â”‚ â”‚ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:2","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#embedding"},{"categories":["Nebula Graph","LLM"],"content":" 2.3 Llama IndexLlama Index is such an open-source toolkit to help in-context learning in best practice: It comes with a bunch of data loaders to serialize docs/knowledge in a unified format, think of PDF, Wikipedia Page, notion, Twitter, etc, and we donâ€™t have to deal with the preprocessing, split the data into pieces, etc, on our own. It helps create the Embedding(and some other form of the index) for us and stores the embeddings(in memory or vector databases), too, with one line of code. It comes out of the box with prompts and other engineering points, so that we donâ€™t have to create and study from scratch, for instance, create a chatbot on existing data with 4 lines of code. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:3","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llama-index"},{"categories":["Nebula Graph","LLM"],"content":" 3 The problem of doc split and embeddingsThe embedding and vector search worked well in many cases, while there are still challenges in some cases, and one of them is it could lose global context/cross-node context. Think of we are asking â€œPlease tell me things about the author and foo.â€, and in this book, the piece with numbers: 1, 3, 6, 19~25, 30~44, and 96~99 are all about the topic of foo. In this case, the simple way of searching top-k embedding of the pieces of the book may not work well because we normally only take a few top-related pieces, which loses many contexts. â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â” â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 4 â”‚ â”œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¤ â”‚ Docs/Knowledge â”‚ â”‚ ... â”‚ â”œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¤ â”‚ 95 â”‚ 96 â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜ The mitigation for that, for instance with Llama Index is to create composite indices, where the VectorStore is only part of it, combining with that, we could define a summary index and/or a tree index, etc to route different types of questions to different indices, thus to avoid risking the loss of global context when the question required it. Or, with the help of Knowledge Graph, we could do something differently. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:3:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#the-problem-of-doc-split-and-embeddings"},{"categories":["Nebula Graph","LLM"],"content":" 4 Knowledge GraphThe term Knowledge Graph was initially coined by Google in May 2012 as part of its efforts to enhance search results and provide more contextual information to users. The Knowledge Graph was designed to understand the relationships between entities and provide direct answers to queries rather than just returning a list of relevant web pages. A knowledge graph is a way of organizing and connecting information in a graph format, where nodes represent entities, and edges represent the relationships between those entities. The graph structure allows for efficient storage, retrieval, and analysis of data. It looks like this: But how could Knowledge Graph help? ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:4:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#knowledge-graph"},{"categories":["Nebula Graph","LLM"],"content":" 5 Combination of embeddings and Knowledge GraphThe general idea here is a knowledge graph, as the refined format of the information, can be queried/searched in way smaller granularity than the split we could do on raw data/docs. Thus, by not replacing the large pieces of the data, but combining the two, we can search queries that require global/cross-node context better. See the following diagram, assume the question is about x, and 20 of all the pieces of the data are highly related to it. We could now still get the top 3 pieces of the doc(say, no. 1, 2, and 96) as the main context to be sent, apart from that, we ask for two hops of graph traversal around x from the knowledge graph, then the full context will be: The question â€œTell me things about the author and xâ€ Raw doc from piece number 1, 2, and 96, in Llama Index, itâ€™s called node 1, node 2, and node 96. Knowledge 10 triplets contain â€œxâ€ in two-depths graph traversal: x -\u003e y(from node 1) x -\u003e a(from node 2) x -\u003e m(from node 4) x \u003c- b-\u003e c(from node 95) x -\u003e d(from node 96) n -\u003e x(from node 98) x \u003c- z \u003c- i(from node 1 and node 3) x \u003c- z \u003c- b(from node 1 and node 95) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ .â”€. .â”€. â”‚ .â”€. .â”€. â”‚ .â”€. â”‚ .â”€. .â”€. â”‚ â”‚( x )â”€â”€â”€â”€â”€â–¶ y ) â”‚ ( x )â”€â”€â”€â”€â”€â–¶ a ) â”‚ ( j ) â”‚ ( m )â—€â”€â”€â”€â”€( x ) â”‚ â”‚ `â–²' `â”€' â”‚ `â”€' `â”€' â”‚ `â”€' â”‚ `â”€' `â”€' â”‚ â”‚ â”‚ 1 â”‚ 2 â”‚ 3 â”‚ â”‚ 4 â”‚ â”‚ .â”€. â”‚ â”‚ .â–¼. â”‚ â”‚ â”‚( z )â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶( i )â”€â”â”‚ â”‚ â”‚ `â—€â”€â”€â”€â”€â” â”‚ â”‚ `â”€' â”‚â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ Docs/Knowledge â”‚ â”‚ â”‚ â”‚ ... â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ .â”€. â””â”€â”€â”€â”€â”€â”€. â”‚ .â”€. â”‚ â”‚â”‚ .â”€. â”‚ â”‚ ( x â—€â”€â”€â”€â”€â”€( b ) â”‚ ( x ) â”‚ â””â”¼â–¶( n ) â”‚ â”‚ `â”€' `â”€' â”‚ `â”€' â”‚ â”‚ `â”€' â”‚ â”‚ 95 â”‚ â”‚ â”‚ 96 â”‚ â”‚ â”‚ 98 â”‚ â”‚ .â–¼. â”‚ .â–¼. â”‚ â”‚ â–¼ â”‚ â”‚ ( c ) â”‚ ( d ) â”‚ â”‚ .â”€. â”‚ â”‚ `â”€' â”‚ `â”€' â”‚ â”‚ ( x ) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€`â”€'â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ And clearly, the refined information related to topic x that comes from both other nodes and across the nodes is included in the context to build the prompt of in-context learning. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:5:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#combination-of-embeddings-and-knowledge-graph"},{"categories":["Nebula Graph","LLM"],"content":" 6 Progress of Knowledge Graph in Llama IndexThe Knowledge Graph abstraction was initially introduced to Llama Index by William F.H. where the triplets in the knowledge graph were associated with the docs with keywords and stored in memory, then Logan Markewich enhanced it by adding embedding per triplets, too. Recently, in the last couple of weeks, I had been working with the community on bringing the â€œGraphStoreâ€ storage context to Llama Index and thus introducing external storage of Knowledge Graph, the first implementation is NebulaGraph the Open-Source Distributed Graph Database that I had been working on since 2021. During the implementation of this, the option to traverse multiple hops of the graph, and the option to collect more key entities on top-k nodes(to search in the knowledge graph to enable more global context) was introduced, and we are still refining the changes. With GraphStore introduced, it also makes it possible to perform in-context learning from an existing knowledge graph, combined with other indices, this is also quite promising due to the knowledge graph being considered with high Information density than other structured data. I will be updating the knowledge graph-related work on Llama Index in this blog in the upcoming weeks, and will then create end-to-end demo projects and tutorials, after the PR is merged, stay tuned! ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:6:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#progress-of-knowledge-graph-in-llama-index"},{"categories":["Nebula Graph"],"content":"Introducing a new project! ng_ai: NebulaGraph's graph algorithm suite, a user-friendly high-level Python Algorithm API for NebulaGraph. Its goal is to enable data scientist users of NebulaGraph to perform graph-related algorithmic tasks with minimal code.","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/"},{"categories":["Nebula Graph"],"content":" Introducing a new project! ng_ai: NebulaGraphâ€™s graph algorithm suite, a user-friendly high-level Python Algorithm API for NebulaGraph. Its goal is to enable data scientist users of NebulaGraph to perform graph-related algorithmic tasks with minimal code. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:0:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#"},{"categories":["Nebula Graph"],"content":" 1 Nebulagraph AI SuiteThis week, NebulaGraph 3.5.0 has been released, and @whitewum suggested that we make the new project ng_ai that has been launched in the NebulaGraph community public. This blog is the first one to introduce ng_ai! ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#nebulagraph-ai-suite"},{"categories":["Nebula Graph"],"content":" 1.1 What is ng_aiNebulagraph AI Suite. As the name suggests, it is a Python suite for running algorithms on NebulaGraph. Its goal is to provide data scientist users of NebulaGraph with a natural, concise high-level API to perform graph-related algorithmic tasks with minimal code. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:1","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#what-is-ng_ai"},{"categories":["Nebula Graph"],"content":" 1.2 Features Simplifying things in surprising ways. To provide a smooth algorithmic experience for NebulaGraph community users, ng_ai has the following features: Tight integration with NebulaGraph Support for multiple engines and backends, currently supporting Spark (NebulaGraph Algorithm) and NetworkX, with plans to support DGL and PyG in the future. User-friendly and intuitive API design. Seamless integration with NebulaGraphâ€™s UDF, allowing ng_ai tasks to be called from queries. Friendly custom algorithm interface, making it easy for users to implement their own algorithms (WIP). One-click playground setup (based on Docker Extension). ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#features"},{"categories":["Nebula Graph"],"content":" 2 Demos","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#demos"},{"categories":["Nebula Graph"],"content":" 2.1 Run PageRankWe could run distributed PageRank with Nebula-Algorithms(spark) backend: from ng_ai import NebulaReader # read data with spark engine, scan mode reader = NebulaReader(engine=\"spark\") reader.scan(edge=\"follow\", props=\"degree\") df = reader.read() # run pagerank algorithm pr_result = df.algo.pagerank(reset_prob=0.15, max_iter=10) ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:1","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#run-pagerank"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#write-algo-result-to-nebulagraph"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#call-ng_ai-from-ngql-udf"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#example-with-networkx-engine"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#plotting-with-networkx-engine"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#nebulagraph-jupyter-notebook-extension"},{"categories":["Nebula Graph"],"content":" 3 Future WorkNow ng_ai is still under development, we still have a lot of work to do: Improve the reader mode, now NebulaGraph/NetworkX only supports Query-Mode, we also need to support Scan-Mode Implement link prediction, node classification and other algorithms based on dgl (GNN), for example: model = g.algo.gnn_link_prediction() result = model.train() # query src, dst to be predicted model.predict(src_vertex, dst_vertices) UDA, custom algorithm Deployment tool ng_ai is completely built in public, and we welcome everyone in the community to participate in it and improve ng_ai together, making AI algorithms on NebulaGraph easier to use! ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:3:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#future-work"},{"categories":["Nebula Graph"],"content":" 4 Try ng_aiWe have prepared a one-click deployment of NebulaGraph + Studio + ng_ai in Jupyter environment, you only need to search NebulaGraph from the Extension of Docker Desktop to try it out. Install NebulaGraph Docker Extension Search NebulaGraph from docker extension marketplace, and install it. Install ng_ai playground Go to NebulaGraph extension, click Install NX Mode to install ng_aiâ€™s NetworkX playground, it usually takes a few minutes to wait for the installation to complete. Enter NetworkX playground Click Jupyter NB NetworkX to enter NetworkX playground. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:4:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#try-ng_ai"},{"categories":["Nebula Graph"],"content":" 5 ng_ai Architectureng_ai is a Python library, it is mainly composed of the following modules: Writer: responsible for writing data to NebulaGraph Engine: responsible for adapting different runtimes, such as Spark, DGL, NetowrkX, etc. Algo: algorithm module, such as PageRank, Louvain, GNN_Link_Predict, etc. In addition, in order to support the call in nGQL, there are two more modules: ng_ai-udf: responsible for registering UDF to NebulaGraph, accepting query calls from ng_ai, and accessing ng_ai API ng_ai-api: the API module of ng_ai, which is responsible for receiving requests from ng_ai-udf and calling the corresponding algorithm module â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Spark Cluster â”‚ â”‚ .â”€â”€â”€â”€â”€. .â”€â”€â”€â”€â”€. .â”€â”€â”€â”€â”€. .â”€â”€â”€â”€â”€. â”‚ â”‚ ; : ; : ; : ; : â”‚ â”Œâ”€â–¶â”‚ : ; : ; : ; : ; â”‚ â”‚ â”‚ â•² â•± â•² â•± â•² â•± â•² â•± â”‚ â”‚ â”‚ `â”€â”€â”€' `â”€â”€â”€' `â”€â”€â”€' `â”€â”€â”€' â”‚ Algo Spark â”‚ Engineâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â””â”€â”€â”¤ â”‚ â”‚ â”‚ NebulaGraph AI Suite(ngai) â”‚ ngai-api â”‚â—€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ Reader â”‚ â”‚ Algo â”‚ â”‚ Writer â”‚ â”‚ GNN â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â” â””â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”Œâ”€â”€â”¤ â”‚ SparkEngine â”‚ â”‚ NebulaEngine â”‚ â”‚ NetworkX â”‚ â”‚ DGLEngineâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ Spark â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€Reader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Spark Query Mode â”‚ â”‚ â”‚ Reader â”‚ â”‚ â”‚Scan Mode â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ ngai-udfâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ â”‚ NebulaGraph Graph Engine Nebula-GraphD â”‚ ngai-GraphD â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ NebulaGraph Storage Engine â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â–¶â”‚ Nebula-StorageD â”‚ Nebula-Metad â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space:\"basketballplayer\"}) â”‚â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ from ng_ai import NebulaReader â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ # read data with spark engine, scan mode â”‚ â”‚ â”‚ reader = NebulaReader(engine=\"spark\") â”‚ â”‚ â”‚ reader.scan(edge=\"follow\", props=\"degree\") â”‚ â””â”€â”€â”‚ df = reader.read() â”‚ â”‚ â”‚ â”‚ # run pagerank algorithm â”‚ â”‚ pr_result = df.algo.pagerank(reset_prob=0.15, max_iter=10) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:5:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#ng_ai-architecture"},{"categories":["Nebula Graph"],"content":"How NebulaGraph helps build social network systems.","date":"2022-12-29","objectID":"/en/nebulagraph-sns/","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/"},{"categories":["Nebula Graph"],"content":" How NebulaGraph helps build social network systems. This blog was origianlly posted on NebulaGraph Blog. Social networks are no stranger to everyone, whether itâ€™s Facebook, Twitter, Youtube, or services such as Yelp, Quora, Reddit, etc., the essence of their users has formed social networks. In a social network system, we can use a graph database to represent users and their connection relationships. Graph databases allow efficient querying of relationships between users, making various business implementations on social networks based on connection findings, statistics, and analysis feasible and efficient. For example, graph databases can be used to identify â€œinfluential usersâ€ in a network, to recommend new connections (friendships, content of interest) based on commonalities between users, or to find different groups of people and communities in a community to profile users. Graph databases are ideal for social networking systems where user relationships are constantly changing because they can support complex multi-hop queries and also real-time writes and updates. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:0:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#"},{"categories":["Nebula Graph"],"content":" 1 Graph ModelingTo showcase the SNS graph use cases, Iâ€™ll build most of the examples on a typically small social network, I started by adding extra data on top of the NebulaGraph default dataset, basketballplayer: Three new tags of vertices: address place post Five new types of edges: created_post commented_at lived_in belong_to It looks like this: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:1:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 2 Importing the data","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#importing-the-data"},{"categories":["Nebula Graph"],"content":" 2.1 Load the default dataset In the Command Line Console, we could just execute:play basketballplayer` to load the default dataset. Or, if we do so from NebulaGraph Studio/Explorer, just click the Download from the Demos in the welcome page: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#load-the-default-dataset"},{"categories":["Nebula Graph"],"content":" 2.2 Add the SNS Graph schemaFirst the DDL for those new tags and edges: CREATE TAG IF NOT EXISTS post(title string NOT NULL); CREATE EDGE created_post(post_time timestamp); CREATE EDGE commented_at(post_time timestamp); CREATE TAG address(address string NOT NULL, `geo_point` geography(point)); CREATE TAG place(name string NOT NULL, `geo_point` geography(point)); CREATE EDGE belong_to(); CREATE EDGE lived_in(); ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#add-the-sns-graph-schema"},{"categories":["Nebula Graph"],"content":" 2.3 Load the dataThen we load the DML part, to insert vertices and edges: INSERT VERTEX post(title) values \\ \"post1\":(\"a beautify flower\"), \"post2\":(\"my first bike\"), \"post3\":(\"I can swim\"), \\ \"post4\":(\"I love you, Dad\"), \"post5\":(\"I hate coriander\"), \"post6\":(\"my best friend, tom\"), \\ \"post7\":(\"my best friend, jerry\"), \"post8\":(\"Frank, the cat\"), \"post9\":(\"sushi rocks\"), \\ \"post10\":(\"I love you, Mom\"), \"post11\":(\"Let's have a party!\"); INSERT EDGE created_post(post_time) values \\ \"player100\"-\u003e\"post1\":(timestamp(\"2019-01-01 00:30:06\")), \\ \"player111\"-\u003e\"post2\":(timestamp(\"2016-11-23 10:04:50\")), \\ \"player101\"-\u003e\"post3\":(timestamp(\"2019-11-11 10:44:06\")), \\ \"player103\"-\u003e\"post4\":(timestamp(\"2014-12-01 20:45:11\")), \\ \"player102\"-\u003e\"post5\":(timestamp(\"2015-03-01 00:30:06\")), \\ \"player104\"-\u003e\"post6\":(timestamp(\"2017-09-21 23:30:06\")), \\ \"player125\"-\u003e\"post7\":(timestamp(\"2018-01-01 00:44:23\")), \\ \"player106\"-\u003e\"post8\":(timestamp(\"2019-01-01 00:30:06\")), \\ \"player117\"-\u003e\"post9\":(timestamp(\"2022-01-01 22:23:30\")), \\ \"player108\"-\u003e\"post10\":(timestamp(\"2011-01-01 10:00:30\")), \\ \"player100\"-\u003e\"post11\":(timestamp(\"2021-11-01 11:10:30\")); INSERT EDGE commented_at(post_time) values \\ \"player105\"-\u003e\"post1\":(timestamp(\"2019-01-02 00:30:06\")), \\ \"player109\"-\u003e\"post1\":(timestamp(\"2016-11-24 10:04:50\")), \\ \"player113\"-\u003e\"post3\":(timestamp(\"2019-11-13 10:44:06\")), \\ \"player101\"-\u003e\"post4\":(timestamp(\"2014-12-04 20:45:11\")), \\ \"player102\"-\u003e\"post1\":(timestamp(\"2015-03-03 00:30:06\")), \\ \"player103\"-\u003e\"post1\":(timestamp(\"2017-09-23 23:30:06\")), \\ \"player102\"-\u003e\"post7\":(timestamp(\"2018-01-04 00:44:23\")), \\ \"player101\"-\u003e\"post8\":(timestamp(\"2019-01-04 00:30:06\")), \\ \"player106\"-\u003e\"post9\":(timestamp(\"2022-01-02 22:23:30\")), \\ \"player105\"-\u003e\"post10\":(timestamp(\"2011-01-11 10:00:30\")), \\ \"player130\"-\u003e\"post1\":(timestamp(\"2019-01-02 00:30:06\")), \\ \"player131\"-\u003e\"post2\":(timestamp(\"2016-11-24 10:04:50\")), \\ \"player131\"-\u003e\"post3\":(timestamp(\"2019-11-13 10:44:06\")), \\ \"player133\"-\u003e\"post4\":(timestamp(\"2014-12-04 20:45:11\")), \\ \"player132\"-\u003e\"post5\":(timestamp(\"2015-03-03 00:30:06\")), \\ \"player134\"-\u003e\"post6\":(timestamp(\"2017-09-23 23:30:06\")), \\ \"player135\"-\u003e\"post7\":(timestamp(\"2018-01-04 00:44:23\")), \\ \"player136\"-\u003e\"post8\":(timestamp(\"2019-01-04 00:30:06\")), \\ \"player137\"-\u003e\"post9\":(timestamp(\"2022-01-02 22:23:30\")), \\ \"player138\"-\u003e\"post10\":(timestamp(\"2011-01-11 10:00:30\")), \\ \"player141\"-\u003e\"post1\":(timestamp(\"2019-01-03 00:30:06\")), \\ \"player142\"-\u003e\"post2\":(timestamp(\"2016-11-25 10:04:50\")), \\ \"player143\"-\u003e\"post3\":(timestamp(\"2019-11-14 10:44:06\")), \\ \"player144\"-\u003e\"post4\":(timestamp(\"2014-12-05 20:45:11\")), \\ \"player145\"-\u003e\"post5\":(timestamp(\"2015-03-04 00:30:06\")), \\ \"player146\"-\u003e\"post6\":(timestamp(\"2017-09-24 23:30:06\")), \\ \"player147\"-\u003e\"post7\":(timestamp(\"2018-01-05 00:44:23\")), \\ \"player148\"-\u003e\"post8\":(timestamp(\"2019-01-05 00:30:06\")), \\ \"player139\"-\u003e\"post9\":(timestamp(\"2022-01-03 22:23:30\")), \\ \"player140\"-\u003e\"post10\":(timestamp(\"2011-01-12 10:01:30\")), \\ \"player141\"-\u003e\"post1\":(timestamp(\"2019-01-04 00:34:06\")), \\ \"player102\"-\u003e\"post2\":(timestamp(\"2016-11-26 10:06:50\")), \\ \"player103\"-\u003e\"post3\":(timestamp(\"2019-11-15 10:45:06\")), \\ \"player104\"-\u003e\"post4\":(timestamp(\"2014-12-06 20:47:11\")), \\ \"player105\"-\u003e\"post5\":(timestamp(\"2015-03-05 00:32:06\")), \\ \"player106\"-\u003e\"post6\":(timestamp(\"2017-09-25 23:31:06\")), \\ \"player107\"-\u003e\"post7\":(timestamp(\"2018-01-06 00:46:23\")), \\ \"player118\"-\u003e\"post8\":(timestamp(\"2019-01-06 00:35:06\")), \\ \"player119\"-\u003e\"post9\":(timestamp(\"2022-01-04 22:26:30\")), \\ \"player110\"-\u003e\"post10\":(timestamp(\"2011-01-15 10:00:30\")), \\ \"player111\"-\u003e\"post1\":(timestamp(\"2019-01-06 00:30:06\")), \\ \"player104\"-\u003e\"post11\":(timestamp(\"2022-01-15 10:00:30\")), \\ \"player125\"-\u003e\"post11\":(timestamp(\"2022-02-15 10:00:30\")), \\ \"player113\"-\u003e\"post11\":(timestamp(\"2022-03-15 10:00:30\")), \\ \"player102\"-\u003e\"post11\":(timestamp(\"2022-04-15 10:00:30\")), \\ \"player108\"-\u003e\"post11\":(timestamp(\"2022-05-15 10:00:30\")); INSERT VERTEX `address` (`address`, `geo_point`) VALUES \\ \"addr_0\":(\"Brittany Forge Apt. 718 East ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:3","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#load-the-data"},{"categories":["Nebula Graph"],"content":" 2.4 First glance at the dataLetâ€™s start with the stats of the data. [basketballplayer]\u003e SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 10 | +------------+ [basketballplayer]\u003e SHOW STATS; +---------+----------------+-------+ | Type | Name | Count | +---------+----------------+-------+ | \"Tag\" | \"address\" | 19 | | \"Tag\" | \"place\" | 14 | | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"post\" | 10 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"belong_to\" | 19 | | \"Edge\" | \"commented_at\" | 40 | | \"Edge\" | \"created_post\" | 10 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"lived_in\" | 19 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 124 | | \"Space\" | \"edges\" | 321 | +---------+----------------+-------+ Got 13 rows (time spent 1038/51372 us) We could get all of the data: MATCH ()-[e]-\u003e() RETURN e LIMIT 10000 As the data volume is quite small, we could render them all in the canvas of NebulaGraph Explorer: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:4","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#first-glance-at-the-data"},{"categories":["Nebula Graph"],"content":" 3 Identifying key peopleIdentifying influencers in social networks involves using a variety of metrics and methods to identify individuals who have a lot of influence in a given network. This is useful for many business scenarios, such as for marketing or researching the spread of information in a network. There are many ways to identify them, and the specific methods and information, relationships, and perspectives considered also depend on the type of these key individuals, and the purpose of acquiring them. Some common methods include looking at the number of followers a person has or the amount of content consumed, their reader engagement on their posts, videos, and the reach of their content (retweets, citations). These methods are also doable on the graph but are rather mundane, so I wonâ€™t give examples. Here, we can try to derive these key people on the graph using graph algorithms that evaluate and calculate the importance of nodes. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#identifying-key-people"},{"categories":["Nebula Graph"],"content":" 3.1 PageRankPageRank is a very classic graph algorithm that iterates through the number of relationships between points on a graph to get a score (Rank) for each point. It was originally proposed by Google founders Larry Page and Sergey Brin and used in the early Google search engine to sort search results, where Page can be a pun on Larry Pageâ€™s last name and Web Page. PageRank has long been abandoned as too simple in modern, complex search engines, but it still shines in other graph-structured web scenarios, where we can roughly assume that all links are of similar importance and run the algorithm to find those key users in social networks. In NebulaGraph, we can use NebulaGraph Algorithm, NebulaGraph Analytics to run PageRank on large full graphs, while in the daily analysis, validation, and design phases, we donâ€™t need to run results on full data, but on very small subgraphs (up to tens of thousands), we can easily run various graph algorithms in the browser to derive methods that can be used for production. Today, we will use the built-in in-browser graph algorithm function of NebulaGraph Explorer to execute PageRank (the specific method is omitted here, you can refer to the documentation, but itâ€™s really just a matter of mouse clicks). We can see from the above that among all the green players (people) after PageRank calculation, â€œplayer.name: Tim Duncanâ€ is the largest one, and the relationship associated with it does seem to be quite a lot, so we select him on the graph, then right-click to invert, select all the points except Tim Duncan, use the backspace key to delete all the other points, and then explore 1 to 5 steps in both directions with him as the starting point. In one step, we get Tim Duncanâ€™s subgraph. As you can see from the subgraphs, Tim Duncan is associated with a very large number of other players, while some other very popular players have served with him in the very popular Spurs team, which confirms the way PageRank is evaluated. Now letâ€™s see if the algorithm will come to the same conclusion for the other dimensions of determination. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#pagerank"},{"categories":["Nebula Graph"],"content":" 3.2 Betweenness CentralityAs you can see from the subgraphs, Tim Duncan is associated with a very large number of other players, while some other very popular players have served with him in the very popular Spurs team, which confirms the way PageRank is evaluated. Now letâ€™s see if the algorithm will come to the same conclusion for the other dimensions of determination. From its five-hop subgraph, it can be seen that unlike the star-shape of the key figure Tim Duncan obtained from PageRank before, Dejounte Murrayâ€™s subgraph shows clusters, where it is sensory, intuitive to imagine that Dejounte Murray is really on the necessary path of the minimal path between many nodes, while Tim Duncan seems to be associated with more important connecters. In practical application scenarios, we usually have to understand the definitions in different ways, experiment with different execution results, and analyze to find the structural features that affect the key people we care about, and use them to choose different algorithms for different needs. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#betweenness-centrality"},{"categories":["Nebula Graph"],"content":" 4 Detect communities and clustersCommunity detection in social networks is a technique to discover community structure by analyzing social relationships. A community structure is a set of nodes that are closely connected to each other in a social network, graph, and these nodes usually have similar characteristics or interests. For example, a community structure may manifest itself as a group of users who are clustered together based on common topics or interests. The purpose of community detection is to identify the boundaries of different communities and determine the nodes in each community by analyzing the social network. This process can be done by using various algorithms such as label propagation algorithm, weakly connected component algorithm and Louvain algorithm. By discovering the community structure, we can better understand the structure and characteristics of social networks, and help social network service providers to better infer and predict behaviors in social networks, and help in good social network governance, advertisement placement, marketing, etc. Since our dataset is fake-generated, the results I get under different algorithms do not show the real meaning, so this chapter just shows the results after community identification using several graph algorithms, in real-world cases, we should also use domain knowledge or other technical means on top of that to collaboratively give the portraits and labels of different groups and communities. Effect of label propagation algorithm. Louvain algorithm: WCC algorithm: In later sections, we could in better chance verify these algorithms again on smaller and simpler subgraphs, with somewhat more interpretable results. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:4:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#detect-communities-and-clusters"},{"categories":["Nebula Graph"],"content":" 5 Friend ClosenessWith the community detection algorithm, it is actually possible to obtain friends with similar interests and close associations to some extent, in a global calculation. So how do we get the other close friends of a given user? We can get this information by counting the number of friends this user has in common with him in order to get this information! Letâ€™s take â€œTim Duncanâ€ for example, we know that his two-degree friends (friends of friends: (:player{name: \"Tim Duncan\"})-[:follow]-(f:player)-[:follow]-(fof:player)) are also his friends: Mutual Friend, then it is reasonable to believe that those who have more friends in common with Tim Duncan may have a higher closeness to him. MATCH (start:`player`{name: \"Tim Duncan\"})-[:`follow`]-(f:`player`)-[:`follow`]-(fof:`player`), (start:`player`)-[:`follow`]-(fof:`player`) RETURN fof.`player`.name, count(DISTINCT f) AS NrOfMutualF ORDER BY NrOfMutualF DESC; This query shows that â€œTony Parkerâ€ and Tim have 5 friends in common and are the closest. fof.player.name NrOfMutualF Tony Parker 5 Dejounte Murray 4 Manu Ginobili 3 Marco Belinelli 3 Danny Green 2 Boris Diaw 1 LaMarcus Aldridge 1 Tiago Splitter 1 Here, letâ€™s verify this result through visualization! First, letâ€™s see who the common friends (f:) are for each of the friends. MATCH (start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:`follow`]-(fof:player), (start:player)-[:`follow`]-(fof:player) RETURN fof.player.name, collect(DISTINCT f.player.name); The result: fof.player.name collect(distinct f.player.name) Boris Diaw [â€œTony Parkerâ€] Manu Ginobili [â€œDejounte Murrayâ€, â€œTiago Splitterâ€, â€œTony Parkerâ€] LaMarcus Aldridge [â€œTony Parkerâ€] Tiago Splitter [â€œManu Ginobiliâ€] Tony Parker [â€œDejounte Murrayâ€, â€œBoris Diawâ€, â€œManu Ginobiliâ€, â€œMarco Belinelliâ€, â€œLaMarcus Aldridgeâ€] Dejounte Murray [â€œDanny Greenâ€, â€œTony Parkerâ€, â€œManu Ginobiliâ€, â€œMarco Belinelliâ€] Danny Green [â€œDejounte Murrayâ€, â€œMarco Belinelliâ€] Marco Belinelli [â€œDejounte Murrayâ€, â€œDanny Greenâ€, â€œTony Parkerâ€] Then we visualize the result on Explorer. First, letâ€™s find out all of Timâ€™s 2-degree friend paths MATCH p=(start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:follow]-(fof:player) RETURN p Then we render the node size by degree in which we select Tim and Tony and find all paths between them for follow type edge, bidirectional, up to 2 hops: We can see that they are the closest of friends to each other and that their mutual friends are also in the paths. [\"Dejounte Murray\", \"Boris Diaw\", \"Manu Ginobili\", \"Marco Belinelli\", \"LaMarcus Aldridge\"] ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:5:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#friend-closeness"},{"categories":["Nebula Graph"],"content":" 5.1 Small groups in your friendsAt this point, as mentioned earlier, the non-authenticity of this dataset itself makes the results of the community discovery algorithm unable to get the insightful connotation of it. Now we can follow this small subgraph to see how groups and communities can be distinguished among Timâ€™s friends. Weakly connected components can split Timâ€™s friends into two or three parts that are not connected to each other, which is very much in line with the intuitive understanding and definition of connected components. Label propagation, we can control the number of iterations on-demand to delineate different degrees of division by random propagation, which results in a certain degree of differentiation. 20 iterations 1000 iterations Louvain, a more efficient and stable algorithm, basically under this subgraph we can get a very intuitive division with a very small number of iterations. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:5:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#small-groups-in-your-friends"},{"categories":["Nebula Graph"],"content":" 6 New friend recommendationFollowing the previous idea of 2-degree friends (friends of friends), we can easily add those 2-degree friends who are not yet friends as recommended friends, and the sorting rule is the number of friends they have in common with each other: MATCH (start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:`follow`]-(fof:player) WHERE NOT (start:player)-[:`follow`]-(fof:player) AND fof != start RETURN fof.player.name, count(DISTINCT f) AS NrOfMutualF ORDER BY NrOfMutualF DESC; fof.player.name NrOfMutualF LeBron James 2 James Harden 1 Chris Paul 1 Yao Ming 1 Damian Lillard 1 JaVale McGee 1 Kevin Durant 1 Kyle Anderson 1 Rudy Gay 1 Russell Westbrook 1 Obviously, LeBron is the most recommended! And look at who these mutual friends are. fof.player.name collect(distinct f.player.name) James Harden [â€œDejounte Murrayâ€] LeBron James [â€œDanny Greenâ€, â€œDejounte Murrayâ€] Chris Paul [â€œDejounte Murrayâ€] Yao Ming [â€œShaquille Oâ€™Nealâ€] Damian Lillard [â€œLaMarcus Aldridgeâ€] JaVale McGee [â€œShaquille Oâ€™Nealâ€] Kevin Durant [â€œDejounte Murrayâ€] Kyle Anderson [â€œDejounte Murrayâ€] Rudy Gay [â€œLaMarcus Aldridgeâ€] Russell Westbrook [â€œDejounte Murrayâ€] åŒæ ·ï¼Œæˆ‘ä»¬åœ¨åˆšæ‰çš„å­å›¾é‡Œæ‰¾æ‰¾ LeBron James å§ï¼æˆ‘ä»¬æŠŠå®ƒä¿©ä¹‹é—´çš„ä¸¤æ­¥ã€åŒå‘è·¯å¾„æ‰¾å‡ºæ¥ï¼Œæœç„¶åªä¼šç»è¿‡ [\"Danny Green\", \"Dejounte Murray\"] å¹¶ä¸”ï¼Œæ²¡æœ‰ç›´æ¥çš„è¿æ¥ï¼š Again, letâ€™s look for LeBron James in the subgraph we just created! And find the two-step, two-way path between them, and sure enough, it only goes through [\"Danny Green\", \"Dejounte Murray\"] and, without a direct connection. Now, the system could send reminders to both sides: â€œHEY, maybe you two should make new friends!â€ ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:6:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#new-friend-recommendation"},{"categories":["Nebula Graph"],"content":" 7 Common NeighborFinding common neighbors is a very common graph database query, and its scenarios may bring different scenarios depending on different neighbor relationships and node types. The common buddy in the first two scenarios is essentially a common neighbor between two points, and directly querying such a relationship is very simple with OpenCypher. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#common-neighbor"},{"categories":["Nebula Graph"],"content":" 7.1 A common neighbor between two verticesFor example, this expression can query the commonality, intersection between two users, the result may be common teams, places visited, interests, common participation in post replies, etc.:. MATCH p = (`v0`)--()--(`v1`) WHERE id(`v0`) == \"player100\" AND id(`v1`) == \"player104\" RETURN p And after limiting the type of edge, this query is limited to the common friend query. MATCH p = (v0)--(:`follow`)--(v1) WHERE id(v0) == \"player100\" AND id(v1) == \"player104\" RETURN p ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#a-common-neighbor-between-two-vertices"},{"categories":["Nebula Graph"],"content":" 7.2 Common neighbors among multiple vertices: content notificationBelow, we give a multi-nodes common neighbor scenario where we trigger from a post, find out all the users who have interacted on this post, and find the common neighbors in this group. What is the use of this common neighbor? Naturally, if this common neighbor has not yet had any interaction with this article, we can recommend this article to him. The implementation of this query is interesting. The first MATCH is to find the total number of people who left comments and authors on all post11 articles After the second MATCH, we find the number of friends of the interacting users who have participated in the article that is exactly equal to the number of users who have participated in the article, and they are actually the common friends of all the participating users. MATCH (blog:post)\u003c-[e]-(:player) WHERE id(blog) == \"post11\" WITH blog, count(e) AS invoved_user_count MATCH (blog:post)\u003c-[]-(users:player)-[:`follow`]-(common_neighbor:player) WITH toSet(collect(users)) AS users, common_neighbor, invoved_user_count WHERE size(users) == invoved_user_count RETURN common_neighbor And that person is . . Tony! +-----------------------------------------------------+ | common_neighbor | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ And we can easily verify it in the visualization of the query: MATCH p=(blog:post)\u003c-[]-(users:player)-[:`follow`]-(common_neighbor:player) WHERE id(blog) == \"post11\" RETURN p Rendering this query, and then looking for two-way, two-hop queries between the article called â€œLetâ€™s have a party!â€ and Tonyâ€™s comments, posts, and followers, we can see that all the people involved in the article are, without exception, Tonyâ€™s friends, and only Tony himself has not yet left a comment on the article! And how can a party be without Tony? Is it his surprise birthday party, Opps, shouldnâ€™t we tell him, or? ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#common-neighbors-among-multiple-vertices-content-notification"},{"categories":["Nebula Graph"],"content":" 8 Feed GenerationI have previously written about the implementation of recommendation systems based on graph technology, in which I described that content filtering and sorting methods in modern recommendation systems can be performed on graphs. It is also highly time-sensitive. The feed generation in a SNS is quite similar but slightly different. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#feed-generation"},{"categories":["Nebula Graph"],"content":" 8.1 Content with friend engagementThe simplest and most straightforward definition of content generation may be the facebook feed of content created and engaged by people you follow. Content created by friends within a certain period of time the content of friendsâ€™ comments within a certain time frame We can use OpenCypher to express this query for the stream of information with user id player100. MATCH (feed_owner:player)-[:`follow`]-(friend:player) WHERE id(feed_owner) == \"player100\" OPTIONAL MATCH (friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH (friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") WITH DISTINCT friend, collect(DISTINCT po.post.title) + collect(\"comment of \" + dst(newly_commented)) AS feeds WHERE size(feeds) \u003e 0 RETURN friend.player.name, feeds friend.player.name feeds Boris Diaw [â€œI love you, Momâ€, â€œcomment of post11â€] Marco Belinelli [â€œmy best friend, tomâ€, â€œcomment of post11â€] Danny Green [â€œcomment of post1â€] Tiago Splitter [â€œcomment of post1â€] Dejounte Murray [â€œcomment of post11â€] Tony Parker [â€œI can swimâ€] LaMarcus Aldridge [â€œI hate corianderâ€, â€œcomment of post11â€, â€œcomment of post1â€] Manu Ginobili [â€œmy best friend, jerryâ€, â€œcomment of post11â€, â€œcomment of post11â€] So, we can send these comments, articles to the userâ€™s feed. Letâ€™s also see what they look like on the graph, we output all the paths we queried: MATCH p=(feed_owner:player)-[:`follow`]-(friend:player) WHERE id(feed_owner) == \"player100\" OPTIONAL MATCH p_comment=(friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH p_post=(friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") RETURN p, p_comment, p_post Rendering on Explorer and selecting the â€œNeural Networkâ€ layout, you can clearly see the pink article nodes and the edges representing the comments. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#content-with-friend-engagement"},{"categories":["Nebula Graph"],"content":" 8.2 Content of nearby friendsLetâ€™s go a step further and take geographic information(GeoSpatial) into account to get content related to friends whose addresses have a latitude and longitude less than a certain distance. Here, we use NebulaGraphâ€™s GeoSpatial geography function, the constraint ST_Distance(home.address.geo_point, friend_addr.address.geo_point) AS distance WHERE distance \u003c 1000000 helps us express the distance limit. MATCH (home:address)-[:lived_in]-(feed_owner:player)-[:`follow`]-(friend:player)-[:lived_in]-(friend_addr:address) WHERE id(feed_owner) == \"player100\" WITH feed_owner, friend, ST_Distance(home.address.geo_point, friend_addr.address.geo_point) AS distance WHERE distance \u003c 1000000 OPTIONAL MATCH (friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH (friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") WITH DISTINCT friend, collect(DISTINCT po.post.title) + collect(\"comment of \" + dst(newly_commented)) AS feeds WHERE size(feeds) \u003e 0 RETURN friend.player.name, feeds friend.player.name feeds Marco Belinelli [â€œmy best friend, tomâ€, â€œcomment of post11â€] Tony Parker [â€œI can swimâ€] Danny Green [â€œcomment of post1â€] At this point, you can also see the relationship between addresses and their latitude and longitude information from the visualization of this result. I manually arranged the nodes of the addresses on the graph according to their latitude and longitude and saw that the address (7, 8) of Tim(player100), the owner of this feed, is exactly in the middle of other friendsâ€™ addresses. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#content-of-nearby-friends"},{"categories":["Nebula Graph"],"content":" 9 Spatio-temporal relationship trackingSpatio-temporal relationship tracking is a typical application that uses graph traversal to make the most of complicated and messy information in scenarios such as public safety, logistics, and epidemic prevention and control. When we build such a graph, we often need only simple graph queries to gain very useful insights. In this section, Iâ€™ll give an example of this application scenario. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#spatio-temporal-relationship-tracking"},{"categories":["Nebula Graph"],"content":" 9.1 DatasetFor this purpose, I created a fake dataset by which to build a spatio-temporal relationship graph. The dataset generation program and a file that can be used directly are placed on GitHub at https://github.com/wey-gu/covid-track-graph-datagen. It models the data as follows. We could get the data ready in three lines in any Linux System: # Install NebulaGraph + NebulaGraph Studio curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3 # Clone the dataset git clone https://github.com/wey-gu/covid-track-graph-datagen \u0026\u0026 cd covid-track-graph-datagen # Load the dataset into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/:/root \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer-config.yaml Then we could inspect the data from console: ~/.nebula-up/console.sh # access console, and sse the covid_trace graph space USE covid_trace; # check stats SHOW STATS Results: (root@nebula) [covid_trace]\u003e SHOW STATS +---------+------------+--------+ | Type | Name | Count | +---------+------------+--------+ | \"Tag\" | \"äºº\" | 10000 | | \"Tag\" | \"åœ°å€\" | 1000 | | \"Tag\" | \"åŸå¸‚\" | 341 | | \"Tag\" | \"æ‘é•‡\" | 42950 | | \"Tag\" | \"çœä»½\" | 32 | | \"Tag\" | \"è”ç³»æ–¹å¼\" | 0 | | \"Tag\" | \"è¡Œæ”¿åŒº\" | 3134 | | \"Tag\" | \"è¡—é“\" | 667911 | | \"Edge\" | \"ä½å€\" | 0 | | \"Edge\" | \"åˆ°è®¿\" | 19986 | | \"Edge\" | \"åŒä½\" | 19998 | | \"Edge\" | \"å±äº\" | 715336 | | \"Space\" | \"vertices\" | 725368 | | \"Space\" | \"edges\" | 755320 | +---------+------------+--------+ Got 14 rows (time spent 1087/46271 us) ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#dataset"},{"categories":["Nebula Graph"],"content":" 9.2 Connections between twoThis could be done with FIND PATH # SHORTEST FIND SHORTEST PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths # ALL PATH FIND ALL PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths | LIMIT 10 SHORTEST Path result: paths \u003c(â€œp_100â€)\u003c-[:åŒä½@0 {}]-(â€œp_2136â€)\u003c-[:åŒä½@0 {}]-(â€œp_3708â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_125â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e ALL Path result: paths \u003c(â€œp_100â€)\u003c-[:åŒä½@0 {}]-(â€œp_2136â€)\u003c-[:åŒä½@0 {}]-(â€œp_3708â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_125â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_328â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_6976â€)\u003c-[:åŒä½@0 {}]-(â€œp_261â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_352â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åŒä½@0 {}]-\u003e(â€œp_8709â€)-[:åŒä½@0 {}]-\u003e(â€œp_9315â€)-[:åŒä½@0 {}]-\u003e(â€œp_261â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_352â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_328â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_6311â€)-[:åŒä½@0 {}]-\u003e(â€œp_3941â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_345â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_328â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_5046â€)-[:åŒä½@0 {}]-\u003e(â€œp_3993â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_144â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åŒä½@0 {}]-\u003e(â€œp_3457â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_199â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_6771â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_458â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)\u003c-[:åŒä½@0 {}]-(â€œp_1462â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_922â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_5869â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_345â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)\u003c-[:åŒä½@0 {}]-(â€œp_9489â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_985â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_2733â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_458â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)\u003c-[:åŒä½@0 {}]-(â€œp_9489â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_905â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_2733â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_458â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e \u003c(â€œp_100â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_89â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_1333â€)\u003c-[:åŒä½@0 {}]-(â€œp_1683â€)-[:åˆ°è®¿@0 {}]-\u003e(â€œa_345â€)\u003c-[:åˆ°è®¿@0 {}]-(â€œp_101â€)\u003e We render all the paths visually, mark the two people at the starting node and end end, and check their shortest paths in between, and the inextricable relationship between them is clear at a glance, whether it is for business insight, public safety or epidemic prevention and control purposes, with this information, the corresponding work can progress downward like a tiger. Of course, on a real world system, it may be that we only need to care about the proximity of the association between two users: FIND SHORTEST PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths | YIELD collect(length($-.paths)) AS len | YIELD coalesce($-.len[0], -1) AS len In the result we only care about the length of the shortest path between them as: 4. len 4 ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#connections-between-two"},{"categories":["Nebula Graph"],"content":" 9.3 Temporal intersection of peopleFurther we can use graph semantics to outline any patterns with temporal and spatial information that we want to identify and query them in real time in the graph, e.g. for a given person whose id is p_101, we differ all the people who have temporal and spatial intersection with him at a given time, which means that those people also stay and visit a place within the time period in which p_101 visits those places. MATCH (p:person)-[`visit0`:visited]-\u003e(`addr`:address)\u003c-[`visit1`:visited]-(p1:person) WHERE id(p) == \"p_101\" AND `visit0`.`start_time` \u003c `visit1`.`end_time` RETURN `addr`.address.`name`, collect(p1.person.`name`) æˆ‘ä»¬å¾—åˆ°äº†åœ¨æ¯ä¸€ä¸ªåˆ°è®¿åœ°ç‚¹çš„æ—¶ç©ºç›¸äº¤äººåˆ—è¡¨å¦‚ä¸‹ï¼š We obtained the following list of temporal intersection people at each visited location. addr.address.name collect(p1.person.name) é—µè¡Œä»‡è·¯qåº§ 255960 [â€œå¾ç•…â€, â€œç‹ä½³â€, â€œæ›¾äº®â€, â€œå§œæ¡‚é¦™â€, â€œé‚µç§€è‹±â€, â€œéŸ¦å©·å©·â€, â€œé™¶ç‰â€, â€œé©¬å¤â€, â€œé»„æƒ³â€, â€œå¼ ç§€èŠ³â€, â€œé¢œæ¡‚èŠ³â€, â€œå¼ æ´‹â€] ä¸°éƒ½åŒ—äº¬è·¯Jåº§ 725701 [â€œé™ˆæ˜¥æ¢…â€, â€œæ–½å©·å©·â€, â€œäº•æˆâ€, â€œèŒƒæ–‡â€, â€œç‹æ¥ â€, â€œå°šæ˜â€, â€œè–›ç§€çâ€, â€œå®‹é‡‘å‡¤â€, â€œæ¨é›ªâ€, â€œé‚“ä¸½åâ€, â€œææ¨â€, â€œæ¸©ä½³â€, â€œå¶ç‰â€, â€œå‘¨æ˜â€, â€œç‹æ¡‚çâ€, â€œæ®µç‰åâ€, â€œé‡‘æˆâ€, â€œé»„é‘«â€, â€œé‚¬å…µâ€, â€œé­æŸ³â€, â€œç‹å…°è‹±â€, â€œæ¨æŸ³â€] æ™®é™€æ½œæ±Ÿè·¯Påº§ 210730 [â€œå‚¨å¹³â€, â€œæ´ªçº¢éœâ€, â€œæ²ˆç‰è‹±â€, â€œç‹æ´â€, â€œè‘£ç‰è‹±â€, â€œé‚“å‡¤è‹±â€, â€œè°¢æµ·ç‡•â€, â€œæ¢é›·â€, â€œå¼ ç•…â€, â€œä»»ç‰å…°â€, â€œè´¾å®‡â€, â€œæ±ªæˆâ€, â€œå­™ç´â€, â€œçºªçº¢æ¢…â€, â€œç‹æ¬£â€, â€œé™ˆå…µâ€, â€œå¼ æˆâ€, â€œç‹ä¸œâ€, â€œè°·éœâ€, â€œæ—æˆâ€] æ™®é™€æ­¦è¡—fåº§ 706352 [â€œé‚¢æˆâ€, â€œå¼ å»ºå†›â€, â€œå¼ é‘«â€, â€œæˆ´æ¶›â€, â€œè”¡æ´‹â€, â€œæ±ªç‡•â€, â€œå°¹äº®â€, â€œä½•åˆ©â€, â€œä½•ç‰â€, â€œå‘¨æ³¢â€, â€œé‡‘ç§€çâ€, â€œæ¨æ³¢â€, â€œå¼ å¸…â€, â€œå‘¨æŸ³â€, â€œé©¬äº‘â€, â€œå¼ å»ºåâ€, â€œç‹ä¸½ä¸½â€, â€œé™ˆä¸½â€, â€œä¸‡èâ€] åŸä¸œè´µé˜³è¡—Oåº§ 110567 [â€œææ´â€, â€œé™ˆé™â€, â€œç‹å»ºå›½â€, â€œæ–¹æ·‘åâ€, â€œå¤æƒ³â€, â€œæ¼†èâ€, â€œè©¹æ¡‚èŠ±â€, â€œç‹æˆâ€, â€œææ…§â€, â€œå­™å¨œâ€, â€œé©¬ä¼Ÿâ€, â€œè°¢æ°â€, â€œç‹é¹â€, â€œé æ¡‚è‹±â€, â€œè«æ¡‚è‹±â€, â€œæ±ªé›·â€, â€œé»„å½¬â€, â€œæç‰æ¢…â€, â€œç¥çº¢æ¢…â€] Now, letâ€™s visualize this result on a graph: MATCH (p:person)-[`visit0`:visited]-\u003e(`addr`:address)\u003c-[`visit1`:visited]-(p1:person) WHERE id(p) == \"p_101\" AND `visit0`.`start_time` \u003c `visit1`.`end_time` RETURN paths; In the result, we marked p_101 as a different icon, and identified the gathering community with the label propagation algorithm, isnâ€™t a graph worth a thousand words? ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:3","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#temporal-intersection-of-people"},{"categories":["Nebula Graph"],"content":" 9.4 Most recently visited provincesFinally, we then use a simple query pattern to express all the provinces a person has visited in a given time, say from a point in time: MATCH (p:person)-[visit:visted]-\u003e(`addr`:address)-[:belong_to*5]-(prov:province) WHERE id(p) == \"p_101\" AND visit.start_time \u003e 1625469000 RETURN prov.province.name, collect(addr.address.name); Result: prov.province.name collect(addr.address.name) å››å·çœ [â€œé—µè¡Œä»‡è·¯qåº§ 255960â€] å±±ä¸œçœ [â€œåŸä¸œè´µé˜³è¡—Oåº§ 110567â€] äº‘å—çœ [â€œä¸°éƒ½åŒ—äº¬è·¯Jåº§ 725701â€] ç¦å»ºçœ [â€œæ™®é™€æ½œæ±Ÿè·¯Påº§ 210730â€] å†…è’™å¤è‡ªæ²»åŒº [â€œæ™®é™€æ­¦è¡—fåº§ 706352â€] The usual rules, letâ€™s look at the results on the graph, this time, we choose Dagre-LR layout rendering, and the result looks like: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:4","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#most-recently-visited-provinces"},{"categories":["Nebula Graph"],"content":" 10 RecapWe have given quite a few examples of applications in social networks, including Finding key people Identifying clusters of people, communities Determining the closeness between two users Recommending new friends Pinpointing important content using common neighbors Push information flow based on friend relationship and geographic location Use spatio-temporal relationship mapping to query the relationship between people, get the people who intersected in time and space, and the provinces visited As a natural graph structure, social networks are well suited to use graph technology to store, query, compute, analyze and visualize to solve various problems on them. We hope you can have a preliminary understanding of the graph technology in SNS through this post. Feature image credit: Ryoji ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:10:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#recap"},{"categories":["Nebula Graph"],"content":"An attempt to use ChatGPT to generate code for a data scraper to predict sports events with the help of the NebulaGraph graph database and graph algorithms.","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/"},{"categories":["Nebula Graph"],"content":" An attempt to use ChatGPT to generate code for a data scraper to predict sports events with the help of the NebulaGraph graph database and graph algorithms. This post was initially published in https://www.nebula-graph.io/posts/predict-fifa-world-cup-with-chatgpt-and-nebulagraph ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:0:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#"},{"categories":["Nebula Graph"],"content":" 1 The HypeIn the hype for FIFA 2022 World Cup, when I saw a blog post from Cambridge Intelligence, where they leveraged limited information and correlations among players, teams, and clubs to predict the final winner team, I always would like to try similar things with NebulaGraph to share the ideas of graph algorithm to extract hidden information from the overall connections in a graph in the community. The initial attempt was to make it done in like 2 hours, but I noticed the dataset need to be parsed carefully from Wikipedia and I happened to be not good at doing this job, so I put the idea on hold for a couple of days. In the meantime, another hype, the OpenAI ChatGPT was announced, as I had been a user of DALL-E 2 already(to generate feature images of my blog posts), I gave it a try very quickly, too. And I witnessed how other guys(via Twitter, blogs, hacker news) tried to convince ChatGPT to do so many things that are hard to believe they could do: Help to implement a piece of code at any time Simulate any prompt interface: shell, python, virtual machine, or even a language you create Act out almost any given persona, and chat with you Write poetry, rap, prose Find a bug in a piece of code Explain the meaning of a complex regular expression/Open Cypher Query ChatGPTâ€™s ability to contextualize and understand has never been greater before, so much so that everyone is talking about a new way of working: how to master asking/convincing/triggering machines to help us do our jobs, better and faster. I commented on this tweet, where they taught ChatGPT how to draw and render basic SVGs, then they started to ask him/her to draw any other complex things just after him/her learned in one second, that itâ€™s just like Kame-sennin(human) teaches Sun Wukong kung fu as a young Saiyan. Be sure to check this Twitter thread, itâ€™s really interesting! Hey, cool, directly rendering SVGs in #ChatGPT ! pic.twitter.com/VQX9kYIrxT â€” Brğ•d Skî¨€ggs (@brdskggs) December 4, 2022 So, after trying to get ChatGPT to help me write complex graph database query statements, explain the meaning of complex graph query statements, and explain the meaning of a large chunk of Bison code, and he/she had done them WELL, I realized: why not let ChatGPT write the code that extracts the data for me? ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:1:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#the-hype"},{"categories":["Nebula Graph"],"content":" 2 Grabbing dataI really tried it and the result isâ€¦ good enough. The whole process was basically like a coding interviewer, or a product manager, presenting my requirements, and ChatGPT giving me the code implementation. I then try to run the code, find the things that donâ€™t make sense in the code, point them out, and give suggestions, and ChatGPT really understands the points I point out and makes the appropriate corrections, like: I wonâ€™t list this whole process, but I share the generated code and the whole discussion here. The final generated data is a CSV file. Raw version world_cup_squads.csv Manually modified, separated columns for birthday and age world_cup_squads_v0.csv It contains information/columns of team, group, number, position, player name, birthday, age, number of international matches played, number of goals scored, and club served. Team,Group,No.,Pos.,Player,DOB,Age,Caps,Goals,Club Ecuador,A,1,1GK,HernÃ¡n GalÃ­ndez,(1987-03-30)30 March 1987,35,12,0,Aucas Ecuador,A,2,2DF,FÃ©lix Torres,(1997-01-11)11 January 1997,25,17,2,Santos Laguna Ecuador,A,3,2DF,Piero HincapiÃ©,(2002-01-09)9 January 2002,20,21,1,Bayer Leverkusen Ecuador,A,4,2DF,Robert Arboleda,(1991-10-22)22 October 1991,31,33,2,SÃ£o Paulo Ecuador,A,5,3MF,JosÃ© Cifuentes,(1999-03-12)12 March 1999,23,11,0,Los Angeles FC Final version with header removed world_cup_squads_no_headers.csv ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:2:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#grabbing-data"},{"categories":["Nebula Graph"],"content":" 3 Graph algorithm to predict the 2022 World CupWith the help of ChatGPT, I could finally try to predict the winner of the game with Graph Magic, before that, I need to map the data into the graph view. If you donâ€™t care about the process, just go to the predicted result directly. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-algorithm-to-predict-the-2022-world-cup"},{"categories":["Nebula Graph"],"content":" 3.1 Graph modeling Prerequisites: This article uses NebulaGraph(Open-Source) and NebulaGraph Explorer(Proprietary), which you can request a trial of on AWS. Graph Modeling is the abstraction and representation of real-world information in the form of a â€œvertex-\u003e edgeâ€ graph, in our case, we will project the information parsed from Wikipedia as: Vertices: player team group club Edges: groupedin (the team belongs to which group) belongto (players belong to the national team) serve (players serve in the club) The age of the players, the number of international caps, and the number of goals scored are naturally fit as properties for the player tag(type of vertex). The following is a screenshot of this schema in NebulaGraph Explorer (will just call it Explorer later). Then, we can click the save icon in the upper right corner and the button: Apply to Space to actually create a graph space with the defined schema Note: Refer to the document https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/draft/ ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:1","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 3.2 Ingesting into NebulaGraphWith the graph modeling, we can upload the CSV file (the no-header version) into Explorer, by pointing and selecting the vid and properties that map the different columns to the vertices and edges. Click Import, we then import the whole graph to NebulaGraph, and after it succeeded, we could also get the whole CSV â€“\u003e Nebula Importer configuration file: [nebula_importer_config_fifa.yml](https://github.com/siwei-io/talks/files/10164014/config _fifa.yml.txt), so that you reuse it in the future whenever to re-import the same data or share it with others. Note: Refer to the document https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/11.import-data/ After importing, we can view the statistics on the schema view page, which will show us that 831 players participated in the 2022 Qatar World Cup, serving in 295 different clubs. Note: refer to the documentation: https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/10.create-schema/#view_statistics ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:2","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#ingesting-into-nebulagraph"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLetâ€™s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so letâ€™s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvainâ€™s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#explore-the-graph"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLetâ€™s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so letâ€™s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvainâ€™s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#querying-the-data"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLetâ€™s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so letâ€™s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvainâ€™s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#initial-observation"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLetâ€™s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so letâ€™s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvainâ€™s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-algorithm-based-analysis"},{"categories":["Nebula Graph"],"content":" 3.4 Winner Prediction AlgorithmIn order to be able to make full use of the graph magic(with the implied conditions, and information on the graph), my idea(stolen/inspired from this post) is to choose a graph algorithm that considers edges for node importance analysis, to find out the vertices that have higher importance, iterate and rank them globally, and thus get the top team rankings. These methods actually reflect the fact that awesome players have greater community, and connectivity at the same time, and at the same time, to increase the differentiation between traditionally strong teams, I am going to take into account the information of appearances and goals scored. Ultimately, my algorithm is. Take all the (player)-serve-\u003e(club) relationships and filter them for players with too few goals and too few goals per game (to balance out the disproportionate impact of older players from some weaker teams) Explore outwards from all filtered players to get national teams Run the Betweenness Centrality algorithm on the above subgraph to calculate the node importance scores Note, Betweenness Centrality is an algorithm to measure how a node is important in sense of bridging other nodes in the graph. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:4","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#winner-prediction-algorithm"},{"categories":["Nebula Graph"],"content":" 3.5 Process of the Predictioné¦–å…ˆï¼Œæˆ‘ä»¬å–å‡ºæ‰€æœ‰è¿›çƒæ•°è¶…è¿‡ 10ï¼Œåœºå‡è¿›çƒè¶…è¿‡ 0.2 çš„ (çƒå‘˜)-æœå½¹-\u003e(ä¿±ä¹éƒ¨) çš„å­å›¾ï¼š First, we take out the subgraph in the pattern of (player)-serve-\u003e(club) for those who have scored more than 10 goals and have an average of more than 0.2 goals per game. MATCH ()-[e]-\u003e() WITH e LIMIT 10000 WITH e AS e WHERE e.goals \u003e 10 AND toFloat(e.goals)/e.caps \u003e 0.2 RETURN e Note: For convenience, I have included the number of goals and caps as properties in the serve edge, too. Then, we select all the vertices on the graph, in the left toolbar, select the belongto edge of the outgoing direction, expand the graph outwards (traverse), and select the icon that marks the newly expanded vertices as flags. Now that we have the final subgraph, we use the graph algorithm function within the browser to execute BNC (Betweenness Centrality): The graph canvas then looks like this: ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:5","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#process-of-the-prediction"},{"categories":["Nebula Graph"],"content":" 4 ResultIn the end, we sorted according to the value of Betweenness Centrality to get the final winning team: Brazil! ğŸ‡§ğŸ‡·, followed by Belgium, Germany, England, France, and Argentina, so letâ€™s wait two weeks to come back and see if the prediction is accurate :D. The sorted data is as follows: Vertex Betweenness Centrality BrazilğŸ‡§ğŸ‡· 3499 Paris Saint-Germain 3073.3333333333300 Neymar 3000 Tottenham Hotspur 2740 BelgiumğŸ‡§ğŸ‡ª 2587.833333333330 Richarlison 2541 Kevin De Bruyne 2184 Manchester City 2125 Ä°lkay GÃ¼ndoÄŸan 2064 GermanyğŸ‡©ğŸ‡ª 2046 Harry Kane (captain 1869 EnglandğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ 1864 FranceğŸ‡«ğŸ‡· 1858.6666666666700 ArgentinağŸ‡¦ğŸ‡· 1834.6666666666700 Bayern Munich 1567 Kylian MbappÃ© 1535.3333333333300 Lionel Messi (captain 1535.3333333333300 Gabriel Jesus 1344 Feature Image Credit: The image was also generated with OpenAI, through the DALL-E 2 model \u0026 DALL-E 2 Outpainting, see the original image. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:4:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#result"},{"categories":["Nebula Graph"],"content":"How could we model data in Tabular sources and ETL it to NebulaGraph? This article demonstrates an end-to-end example of doing so with dbt.","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/"},{"categories":["Nebula Graph"],"content":" How could we model data in Tabular sources and ETL it to NebulaGraph? This article demonstrates an end-to-end example of doing so with dbt. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:0:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#"},{"categories":["Nebula Graph"],"content":" 1 TaskImagine we are building a Knowledge Graph for a content provider web service with NebulaGraph, thus leveraging it to support a Knowledge Base QA system, Recommendation System, and Reasoning system. The knowledge information persisted in different data sources from some Service APIs, Databases, Data Warehouses, or even some files in S3. We need to: Analyze data to extract needed knowledge Model the Graph based on relationships we care Extract the relationships and ingest them to NebulaGraph ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:1:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#task"},{"categories":["Nebula Graph"],"content":" 2 Data AnalysisAssume that we are fetching data from OMDB and MovieLens. OMDB is an open movie database, we now think of it as one of our services, and we can get the following information. Movies Classification of movies The crew in the movie (director, action director, actors, post-production, etc.) Movie covers, promos, etc. MovieLens is an open dataset, we consider it as the user data of our services, the information we can obtain is: Users Movies User interaction on movie ratings ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:2:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-analysis"},{"categories":["Nebula Graph"],"content":" 3 Graph ModelingWe were building this Graph for a recommendation system and talked about some basic methods in this article, which: In the Content-Base Filter method(CBF), the relationship of user-\u003e movie, movie-\u003e category, movie-\u003e actor, and movie-\u003e director are concerned. And the collaborative filtering approach is concerned with the relationship between the user and -\u003e movie. The recommendation reasoning service is concerned with all the above relationships. To summarize, we need the following edges: watched(rate(double)) with_genre directed_by acted_by Accordingly, the vertex types will be: user(user_id) movie(name) person(name, birthdate) genre(name) ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:3:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 4 Data TransformWith the source date finalized, letâ€™s see how they could be mapped and transformed into the graph. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-transform"},{"categories":["Nebula Graph"],"content":" 4.1 From OMDBFirst, there is the data in OMDB, which consists of many tables, such as the table all_movies, which stores all the movies and their names in different languages. movie_id name language_iso_639_1 official_translation 1 Cowboy Bebop de 1 1 Cowboy Bebop en 1 2 Ariel - Abgebrannt in Helsinki de 0 3 Shadows in Paradise en 0 3 Im Schatten des Paradieses de 0 3 Schatten im Paradies de 1 And the all_casts table holds all roles in the film industry. movie_id person_id job_id role position 11 1 21 1 11 1 13 1 11 2 15 Luke Skywalker 1 11 3 15 Han Solo 3 11 4 15 Leia Organa 2 But the name and other information of each person here, as well as the position he/she holds in the film, are in separate tables. job_names For example, 1 stands for writer, and 2 stands for producer. Interestingly, like movie id and name, job_id to name is a one-to-many relationship, because the data in OMDB is multilingual. job_id name language_iso_639_1 1 Autoren de 1 Writing Department en 1 Departamento de redacciÃ³n es 1 DÃ©partement Ã©criture fr 1 Scenariusz pl 2 Produzenten de 2 Production Department en all_people id name birthday deathday gender 1 George Lucas 1944-05-14 \\N 0 2 Mark Hamill 1951-09-25 \\N 0 3 Harrison Ford 1942-07-13 \\N 0 4 Carrie Fisher 1956-10-21 2016-12-27 1 5 Peter Cushing 1913-05-26 1994-08-11 0 6 Anthony Daniels 1946-02-21 \\N 0 This is a typical case in RDBMS where the data source is a table structure, so for the relationship movie \u003c-[directed_by]-(person), it involves four tables all_movies, all_casts, all_people, job_names: directed_by Starting from person_id in all_casts To movie_id in all_casts Where job_id is â€œdirectorâ€ in job_names movie person_id in all_casts Name from all_movies by id, language is â€œenâ€ person movie_id in all_casts Name, birthday in all_people Till now, all tables we cared about in OMDB are: ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#from-omdb"},{"categories":["Nebula Graph"],"content":" 4.2 From MovieLens datasetWhile the above is just about one data source, in real scenarios, we also need to collect and aggregate data from other sources. For example, now also need to extract knowledge from the MovieLens dataset. Here, the only relationship we utilize is user -\u003e movie. movies.csv movieId title genres 1 Toy Story (1995) Adventure 2 Jumanji (1995) Adventure 3 Grumpier Old Men (1995) Comedy 4 Waiting to Exhale (1995) Comedy ratings.csv userId movieId rating timestamp 1 1 4 964982703 1 3 4 964981247 1 6 4 964982224 From the preview of the data in the two tables, naturally, we need one type of relationship: watched and vertex: user: watched Starting from the userId in ratings.csv To movieId in ratings.csv With rating from rating in ratings.csv user With userId from ratings.csv However, you must have noticed that movieId in the MovieLens dataset and movie id in OMDB are two different systems, if we need to associate them, we need to convert movieId in MovieLens to movie id in OMDB, and the condition of association between them is a movie title. However, by observation, we know that: the titles in OMDB movies are multilingual the titles in MovieLens have the year information like (1995) at the end of the title So our conclusion is watched Starting from the userId in ratings.csv To movieId in ratings.csv Get the movie title with movieId from movies.csv and find its movie_id from OMDB Where we should match the title in language: English with the suffix of the year being removed With rating from rating in ratings.csv user With userId from ratings.csv Now the modeling puts the two tables like this figure: ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#from-movielens-dataset"},{"categories":["Nebula Graph"],"content":" 4.3 Graph Modeling (Property Graph)To summarize, we need to aggregate different tables (or CSV files in table form) from multiple data sources, such that the correspondence is shown in the figure: where the blue dashed line indicates the source of data information for the vertices in the graph, and the pink dashed line indicates the source of edge information. Then, we have to format the ids of individuals in different tables, for example, user_id, which is a self-incrementing number that we want to convert to a globally unique vertex_id. A convenient way to do this is to add a string prefix to the existing id, such as u_. Eventually, for the relationship user -[watched]-\u003e movie, we can process the table structure data as follows. user_id rating title omdb_movie_id u_1 5 Seven (a.k.a. Se7en) 807 u_1 5 Star Wars: Episode IV - A New Hope 11 u_1 5 Star Wars: Episode IV - A New Hope 10 u_1 4 Mask, The 832 u_1 3 Mrs. Doubtfire 832 Where, in each row, three variables exist to construct the graph structure: user vertex id movie vertex id the rating value as the property of the watched edge ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:3","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#graph-modeling-property-graph"},{"categories":["Nebula Graph"],"content":" 5 ToolingAt this point, we have completed the data analysis and graph modeling design, before we start the â€œextract correlations, import graph databaseâ€, letâ€™s introduce the tools we will use. â€œExtracting relationshipsâ€ can be simply considered as Extract and Transform in ETL, which is essentially the engineering of data mapping and transformation, and there are many different tools and open-source projects available on the market. Here we use one of my personal favorite tools: dbt. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#tooling"},{"categories":["Nebula Graph"],"content":" 5.1 dbtdbt is an open-source data conversion tool with a very mature community and ecology, which can perform efficient, controlled, and high-quality data conversion work in most of the mainstream data warehouses, whether it is for ad-hoc tasks or complex orchestration, dbt can be very competent. One of the features of dbt is that it uses a SQL-like language to describe the rules of data transformation. With GitOps, it is very elegant to collaborate and maintain complex data processing operations in large data teams. And the built-in data testing capabilities allow you to control the quality of your data and make it reproducible and controllable. dbt not only has many integrated subprojects but also can be combined with many other excellent open source projects (meltano, AirFlow, Amundsen, Superset, etc.) to form a set of modern data infrastructure systems, feel free to check my previous article: data lineage and metadata governance reference architecture https://siwei.io/en/data-lineage-oss-ref-solution, where the whole solution looks like: In short, dbt is a command line tool written in python, and we can create a project folder, which contains a YAML formatted configuration file, to specify where the source information for the data transformation is and where the target is (where the processed data is stored, maybe Postgres, Big Query, Spark, etc.). In the data source, we use the YAML file along with the .SQL file to describe the information about â€œwhat data to fetch from, how to do the transformation, and what to outputâ€. You can see that the information in the models/example is the core data transformation rules, and all the other data is metadata related to this transformation. DataOps. Notes. You can refer to the dbt documentation to get a hands-on understanding of it: https://docs.getdbt.com/docs/get-started/getting-started-dbt-core ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#dbt"},{"categories":["Nebula Graph"],"content":" 5.2 NebulaGraph data ingestionAfter processing the data by dbt, we can get intermediate data that maps directly to different types of vertices, edges, and table structures of their attributes, either in the form of CSV files, tables in DWs, or even data frames in Spark, and there are different options for importing them into NebulaGraph, of which NebulaGraph Exchange, Nebula-Importer, and Nebula-Spark-Connector can be used to import the data. Notes. You can learn more about the different tools for NebulaGraph data import at https://siwei.io/en/sketches/nebula-data-import-options to know how to choose one of them c. Here, I will use the simplest one, Nebula-Importer, as an example. Nebula-Importer is an open-source tool written in Golang that compiles into a single file binary, it gets the correspondence of vertices and edges from a given CSV file to a NebulaGraph for reading and importing via a preconfigured YAML format file. Notes. Nebula-Importer code: https://github.com/vesoft-inc/nebula-importer/ Nebula-Importer documentation: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#nebulagraph-data-ingestion"},{"categories":["Nebula Graph"],"content":" 6 dbt + Nebula-Importer in ActionsNow letâ€™s use dbt + Nebula-Importer to end-to-end demonstrate how to extract, transform and import multiple data sources into NebulaGraph, the whole project code has been open-sourced, the repository is at https://github.com/wey-gu/movie-recommendation-dataset, feel free to check for details there. The whole process is as follows. Preprocess and import raw data into the data warehouse(EL) Use dbt to transform the data (Transform), and export it to CSV files Import CSV into NebulaGraph using Nebula Importer (L) ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#dbt--nebula-importer-in-actions"},{"categories":["Nebula Graph"],"content":" 6.1 Preparing the dbt environmentdbt is a python project, we install dbt and dbt-postgres in a virtual python3 environment. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#preparing-the-dbt-environment"},{"categories":["Nebula Graph"],"content":" 6.2 Setup env with dbtdbt is written in python, we could install it in a python virtual env, together with dbt-Postgres, as we will use Postgres as the DW in this sample project. python3 -m venv .venv source .venv/bin/activate pip install dbt-postgres Create a dbt project: dbt init dbt_project cd dbt_project Letâ€™s see the files in this project: $ tree . . |-- README.md # README of the project |-- analyses |-- dbt_project.yml # dbt project conf |-- macros |-- models # transforms | \\-- example | |-- my_first_dbt_model.sql # meta data to describe transform rules from the source data with SELECT | |-- my_second_dbt_model.sql | \\-- schema.yml # the meta data of the rules |-- seeds # for CSV-file data sources |-- snapshots \\-- tests 7 directories, 5 files Finally, letâ€™s bootstrap a Postgress as the DW, if you already have one, you may skip this step, please ensure the configurations and dbt-plugins are aligned if you chose to use your own DW. docker run --rm --name postgres \\ -e POSTGRES_PASSWORD=nebula \\ -e POSTGRES_USER=nebula \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#setup-env-with-dbt"},{"categories":["Nebula Graph"],"content":" 6.3 Data download and preprocessLetâ€™s create a folder named raw_data and change the directory to it. mkdir -p raw_data cd raw_data And we assumed it was under our dbt project: tree .. .. |-- README.md |-- analyses |-- dbt_project.yml |-- macros |-- models | \\-- example | |-- my_first_dbt_model.sql | |-- my_second_dbt_model.sql | \\-- schema.yml |-- raw_data # \u003c--- newly created data |-- seeds |-- snapshots \\-- tests 8 directories, 5 files Download and decompress the OMDB data: wget www.omdb.org/data/all_people.csv.bz2 wget www.omdb.org/data/all_people_aliases.csv.bz2 wget www.omdb.org/data/people_links.csv.bz2 wget www.omdb.org/data/all_casts.csv.bz2 wget www.omdb.org/data/job_names.csv.bz2 wget www.omdb.org/data/all_characters.csv.bz2 wget www.omdb.org/data/movie_categories.csv.bz2 wget www.omdb.org/data/movie_keywords.csv.bz2 wget www.omdb.org/data/category_names.csv.bz2 wget www.omdb.org/data/all_categories.csv.bz2 wget www.omdb.org/data/all_movie_aliases_iso.csv.bz2 bunzip2 *.bz2 Then for the MovieLens dataset: wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip unzip ml-latest-small.zip rm *.zip Before we do the Transform with dbt, we do some simple preprocess and then put them under seeds: grep -v '\\\\\"' raw_data/all_movie_aliases_iso.csv \u003e seeds/all_movie_aliases_iso.csv grep -v '\\\\\"' raw_data/all_casts.csv \u003e seeds/all_casts.csv grep -v '\\\\\"' raw_data/all_characters.csv \u003e seeds/all_characters.csv grep -v '\\\\\"' raw_data/all_people.csv \u003e seeds/all_people.csv grep -v '\\\\\"' raw_data/category_names.csv \u003e seeds/category_names.csv grep -v '\\\\\"' raw_data/job_names.csv \u003e seeds/job_names.csv cp raw_data/movie_categories.csv seeds/movie_categories.csv cp raw_data/movie_keywords.csv seeds/movie_keywords.csv cp raw_data/all_categories.csv seeds/all_categories.csv cp raw_data/ml-latest-small/ratings.csv seeds/movielens_ratings.csv cp raw_data/ml-latest-small/movies.csv seeds/movielens_movies.csv With the above files being placed, we could load them into DW in one command: Refer to the documentation of dbt seeds https://docs.getdbt.com/docs/build/seeds dbt seed It may take a while if you like me are using a local Postgres, and it should be faster in production-level cases (i.e. load to Big Query from the file in Cloud Storage), it should be like this: $ dbt seed 05:58:27 Running with dbt=1.3.0 05:58:27 Found 2 models, 4 tests, 0 snapshots, 0 analyses, 289 macros, 0 operations, 11 seed files, 0 sources, 0 exposures, 0 metrics 05:58:28 05:58:28 Concurrency: 8 threads (target='dev') 05:58:28 05:58:28 1 of 11 START seed file public.all_casts ....................................... [RUN] ... 07:10:11 1 of 11 OK loaded seed file public.all_casts ................................... [INSERT 1082228 in 4303.78s] 07:10:11 07:10:11 Finished running 11 seeds in 1 hours 11 minutes and 43.93 seconds (4303.93s). 07:10:11 07:10:11 Completed successfully 07:10:11 07:10:11 Done. PASS=11 WARN=0 ERROR=0 SKIP=0 TOTAL=11 ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:3","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-download-and-preprocess"},{"categories":["Nebula Graph"],"content":" 6.4 Compose the Transform modelWe create transform under models: mkdir models/movie_recommedation touch models/movie_recommedation/user_watched_movies.sql touch models/movie_recommedation/schema.yml The files are like this: $ tree models models \\-- movie_recommedation |-- user_watched_movies.sql \\-- schema.yml Now there is only one transform rule under this model: to handle the edge of user_watched_movies in the user_watched_movies.sql As we planned to output three columns: user_id, movie_id, rating, thus the schema.yml is like: version: 2 models: - name: user_watched_movies description: \"The edges between users and movies they have watched\" columns: - name: user_id description: \"user id\" tests: - not_null - name: movie_id description: \"movie id\" tests: - not_null - name: rating description: \"rating given by user to movie\" tests: - not_null Please be noted the tests are about the validation and constraint of the data, with which, we could control the data quality quite easily. And here not_null ensures there is no NULL if tests are performed. Then, letâ€™s compose the user_watched_movies.sql: {{ config(materialized='table') }} /* JOIN the movieielens_ratings table with the movieielens_movies table, and removing the movie title tailing the year of release */ WITH user_watched_movies AS( SELECT moveielens_ratings.\"userId\", moveielens_ratings.\"movieId\", moveielens_ratings.rating, REGEXP_REPLACE(moveielens_movies.title, ' \\(\\d{4}\\)$', '') AS title, moveielens_movies.genres AS movielens_genres FROM moveielens_ratings JOIN moveielens_movies ON moveielens_movies.\"movieId\" = moveielens_ratings.\"movieId\" ) /* JOIN user_watched_movies table with all_movie_aliase_iso table where language is English the join condition is the movie title */ SELECT concat('u_',user_watched_movies.\"userId\") AS user_id, user_watched_movies.rating, user_watched_movies.title, all_movie_aliases_iso.\"movie_id\" AS OMDB_movie_id, user_watched_movies.movielens_genres FROM user_watched_movies JOIN all_movie_aliases_iso ON user_watched_movies.title LIKE CONCAT(all_movie_aliases_iso.name, '%') AND all_movie_aliases_iso.language_iso_639_1 = 'en' And what this SQL does is the part marked by the green circle: Select the user id, movie id, rating, and movie title (remove the year part) from moveielens_ratings and save it as the intermediate table of user_watched_movies movie title is JOINed from moveielens_movies, obtained by the same matching condition as movie_id Select user id (prefix u_), rating, title, OMDB_movie_id from user_watched_movies OMDB_movie_id is JOINed from all_movie_aliases_iso, obtained by matching the Chinese and English titles of OMDB movies with similar movie names output the final fields Tips: we could add LIMIT to debug the SQL query fast from a Postgres Console Then we could run it from dbt to transform and test the rule: dbt run -m user_watched_movies After that, we should be able to see a table after the Transform in Postgres (DW). Similarly, following the same method for all other parts of the Transform rules, we could have other models: $ tree models models \\-- movie_recommedation |-- acted_by.sql |-- directed_by.sql |-- genres.sql |-- movies.sql |-- people.sql |-- schema.yml |-- user_watched_movies.sql \\-- with_genre.sql Then run them all: dbt run -m acted_by dbt run -m directed_by dbt run -m with_genre dbt run -m people dbt run -m genres dbt run -m movies ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:4","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#compose-the-transform-model"},{"categories":["Nebula Graph"],"content":" 6.5 Export data to CSVNebulaGraph Exchange itself supports directly importing many data sources (Postgres, Clickhouse, MySQL, Hive, etc.) into NebulaGraph, but in this example, the amount of data we process is very small for NebulaGraph, so we just go with the most lightweight one: Nebula-Importer. Nebula-Importer can only CSV files, so we are doing so. First, we enter the Postgres console and execute the COPY command Refer to Postgres documentation: https://www.postgresql.org/docs/current/sql-copy.html COPY acted_by TO '/tmp/acted_by.csv' WITH DELIMITER ',' CSV HEADER; COPY directed_by TO '/tmp/directed_by.csv' WITH DELIMITER ',' CSV HEADER; COPY with_genre TO '/tmp/with_genre.csv' WITH DELIMITER ',' CSV HEADER; COPY people TO '/tmp/people.csv' WITH DELIMITER ',' CSV HEADER; COPY movies TO '/tmp/movies.csv' WITH DELIMITER ',' CSV HEADER; COPY genres TO '/tmp/genres.csv' WITH DELIMITER ',' CSV HEADER; -- for user_watched_movies, we don't output HEADER, as we will parse it in importer in a no-header way. COPY user_watched_movies TO '/tmp/user_watched_movies.csv' WITH DELIMITER ',' CSV; Then copy the CSV files into to_nebulagraph mkdir -p to_nebulagraph docker cp postgres:/tmp/. to_nebulagraph/ ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:5","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#export-data-to-csv"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLetâ€™s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After itâ€™s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same â€œOscar-winningâ€ and â€œclassicâ€ movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#ingest-data-into-nebulagraph"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLetâ€™s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After itâ€™s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same â€œOscar-winningâ€ and â€œclassicâ€ movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#bootstrap-a-nebulagraph-cluster"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLetâ€™s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After itâ€™s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same â€œOscar-winningâ€ and â€œclassicâ€ movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#define-the-data-schema"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLetâ€™s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After itâ€™s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same â€œOscar-winningâ€ and â€œclassicâ€ movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#create-a-nebula-importer-conf-file"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLetâ€™s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After itâ€™s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same â€œOscar-winningâ€ and â€œclassicâ€ movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#ingesting-the-data"},{"categories":["Nebula Graph"],"content":" 7 SummaryWhen we plan to leverage graph databases for massive data to transform knowledge and analyze insights, the first step is often to transform, process, and model multiple data sources into graph data. For beginners who have no idea where to start, a feasible idea is to start from all relevant information, picture the most concerning relationship, and then list the vertices that can be obtained and the required properties attached. After determining the initial modeling, you can use the ETL tool to clean the original data, ETL into table structure which will be mapped to the graph, and finally, use the import tool to import NebulaGraph for further model iterations. With the help of dbt, we can version control, test, iterate our modeling and data transformation, and gradually evolve and enrich the constructed knowledge graph with grace. Feature image credit: Claudio ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:7:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#summary"},{"categories":["Nebula Graph"],"content":"This is a review of Fraud Detection methods based on graph algorithms, graph databases, machine learning, and graph neural networks on NebulaGraph, and in addition to an introduction to the basic methodological ideas, I've also got a Playground you can run. it's worth mentioning that this is the first time I've introduced you to the Nebula-DGL project ğŸ˜.","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/"},{"categories":["Nebula Graph"],"content":" This is a review of Fraud Detection methods based on graph algorithms, graph databases, machine learning, and graph neural networks on NebulaGraph, and in addition to an introduction to the basic methodological ideas, Iâ€™ve also got a Playground you can run. itâ€™s worth mentioning that this is the first time Iâ€™ve introduced you to the Nebula-DGL project ğŸ˜. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:0:0","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#"},{"categories":["Nebula Graph"],"content":" 1 Fraud detection methods based on graph database","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:0","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-methods-based-on-graph-database"},{"categories":["Nebula Graph"],"content":" 1.1 Graph ModelingWe started the modeling with the existing historical data, annotated information oriented to the relationship of the property graph. The data source could be the transaction event records, user data, and risk control annotations in the banking, e-commerce, or insurance industries in multiple table structures. The modeling process is to abstract the entities we care about, the relationships between them, and the meaningful properties attached to both entities and relationships. In general, persons, corporate entities, phone numbers, addresses, devices (e.g., terminal devices, network addresses, WiFi SSIDs to which terminal devices are connected, etc.), and orders are entities we started with to consider, and other information such as is_risky label, and information about persons and corporate entities (occupation, income, education, etc.) are modeled as properties of entities. The model looks like this and the corresponding dataset could be generated with fraud-detection-datagen, with which you could generate dataset in any expected scale and community sturcture. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:1","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 1.2 Fraud detection with Graph Query With a graph that encompasses persons, companies, historical loan application records, phone calls, and online applications for web-based devices, we can uncover some interesting information with certain graph queries directly. In fact, many frauds are clusterred in nature. For example, a fraud ring may be a small group of people (e.g., 3 to 5 people) who collect ID information on a larger scale (e.g., 30) in an organized manner, initiate a large number of loans from multiple financial institutions at the same time, and then choose to discard the batch of IDs that have left a record of default after they have been disbursed, and then further choose the next batch of ID information as they have done. Due to the group of frauds keeps utilizing new identity information, itâ€™s hard to detect with historical records based blacklist mechanism. However, with the help of the patterns being queried in graph, such case could be resovled in real-time. These patterns can be categorised into two types: One is that which can be directly described by the risk control expert in terms of some pattern, e.g., a direct or indirect association with an entity that has been marked as high risk (new order applicants use the same network devices as past high risk records), and this pattern corresponds to the graph, which gives results in real time through a graph query. Another type of association is implicitly behind the correlation of the data, which needs to be mined by graph algorithms for some risk hints, e.g., although a given entity has no matching association with a limited number of labeled high-risk entities, the aggregation it forms in the graph may suggest that this may be one of the applications of an ongoing gang loan fraud that has not yet succeeded, which can be derived by periodically batching in the historical data This situation can be derived by periodically performing community discovery algorithms in bulk in historical data, and using centrality algorithms in highly aggregated communities to give core entities that are prompted to risk experts for subsequent evaluation and risk labeling. 1.2.1 Fraud detection based on expert graph pattern matchingBefore we get started, letâ€™s prepare for a NebulaGraph playground with the above graph dataset being loaded: Nebula Graph Playground setup, based on https://github.com/wey-gu/nebula-up/ curl -fsSL nebula-up.siwei.io/install.sh | bash Load graph dataset # clone dataset genarator repo git clone https://github.com/wey-gu/fraud-detection-datagen.git cp -r data_sample_numerical_vertex_id data # remove table head sed -i '1d' data/*.csv # load dataset to nebulagraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/data/:/data \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/nebula_graph_importer.yaml With such a mapping, risk control experts can explore the relationships between entities on-demand in a visual exploration tool that maps the corresponding risk patterns: In this screenshot of rendered query, we can clearly see a risk pattern for a group-controlled device that can be given to a graph database developer and abstracted into NebulaGraph database statements that can be queried by anyone or certain application in real-time fashion: ## Query started from a person for given transaction MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN p_shared_d Then we could create an API based on queries like the following, which returns count(e) as a metrics. ## group controlled device metric MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN count(e) In this way, we can build an online risk control system that uses limited labeled da","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:2","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-with-graph-query"},{"categories":["Nebula Graph"],"content":" 1.2 Fraud detection with Graph Query With a graph that encompasses persons, companies, historical loan application records, phone calls, and online applications for web-based devices, we can uncover some interesting information with certain graph queries directly. In fact, many frauds are clusterred in nature. For example, a fraud ring may be a small group of people (e.g., 3 to 5 people) who collect ID information on a larger scale (e.g., 30) in an organized manner, initiate a large number of loans from multiple financial institutions at the same time, and then choose to discard the batch of IDs that have left a record of default after they have been disbursed, and then further choose the next batch of ID information as they have done. Due to the group of frauds keeps utilizing new identity information, itâ€™s hard to detect with historical records based blacklist mechanism. However, with the help of the patterns being queried in graph, such case could be resovled in real-time. These patterns can be categorised into two types: One is that which can be directly described by the risk control expert in terms of some pattern, e.g., a direct or indirect association with an entity that has been marked as high risk (new order applicants use the same network devices as past high risk records), and this pattern corresponds to the graph, which gives results in real time through a graph query. Another type of association is implicitly behind the correlation of the data, which needs to be mined by graph algorithms for some risk hints, e.g., although a given entity has no matching association with a limited number of labeled high-risk entities, the aggregation it forms in the graph may suggest that this may be one of the applications of an ongoing gang loan fraud that has not yet succeeded, which can be derived by periodically batching in the historical data This situation can be derived by periodically performing community discovery algorithms in bulk in historical data, and using centrality algorithms in highly aggregated communities to give core entities that are prompted to risk experts for subsequent evaluation and risk labeling. 1.2.1 Fraud detection based on expert graph pattern matchingBefore we get started, letâ€™s prepare for a NebulaGraph playground with the above graph dataset being loaded: Nebula Graph Playground setup, based on https://github.com/wey-gu/nebula-up/ curl -fsSL nebula-up.siwei.io/install.sh | bash Load graph dataset # clone dataset genarator repo git clone https://github.com/wey-gu/fraud-detection-datagen.git cp -r data_sample_numerical_vertex_id data # remove table head sed -i '1d' data/*.csv # load dataset to nebulagraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/data/:/data \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/nebula_graph_importer.yaml With such a mapping, risk control experts can explore the relationships between entities on-demand in a visual exploration tool that maps the corresponding risk patterns: In this screenshot of rendered query, we can clearly see a risk pattern for a group-controlled device that can be given to a graph database developer and abstracted into NebulaGraph database statements that can be queried by anyone or certain application in real-time fashion: ## Query started from a person for given transaction MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN p_shared_d Then we could create an API based on queries like the following, which returns count(e) as a metrics. ## group controlled device metric MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN count(e) In this way, we can build an online risk control system that uses limited labeled da","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:2","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-based-on-expert-graph-pattern-matching"},{"categories":["Nebula Graph"],"content":" 1.3 Expand labels in Graph In the paper: Learning from Labeled and Unlabeled Data with Label Propagation (CMU-CALD-02-107) by Xiaojin Z. and Zoubin G., the Label Propagation algorithm is used to propagate limited labeled information on the graph to more entities through the edges. In this way, we can easily â€œpropagateâ€ more labeled information with a limited number of high-risk annotations in the graphs we build. These extended labeles can, on the one hand, give more results in real-time graph queries and, on the other hand, serve as important input for risk control experts to help advance anti-fraud investigation actions. In general, we can scan the graph data offline periodically, expand and update the labels by the graph algorithm, and then write the valid updated labels back to the graph. Note that there is a similar method, SIGNDiffusion, for those who are interested. 1.3.1 Try expanding labels in graphHere is an example that works. In this example, I use the public Yelp dataset. This data will not only be used in this example, but also in the later cases in the GNN method, so you can be patient and import the data into NebulaGraph. Load dataset into NebulaGraph More details in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection cd nebulagraph-yelp-frauddetection python3 -m pip install -r requirements.txt # download and process dataset python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml After itâ€™s done, we could see the data stats: ~/.nebula-up/console.sh -e \"USE yelp; SHOW STATS\" It should look like: (root@nebula) [(none)]\u003e USE yelp; SHOW STATS +---------+---------------------------------------+---------+ | Type | Name | Count | +---------+---------------------------------------+---------+ | \"Tag\" | \"review\" | 45954 | | \"Edge\" | \"shares_restaurant_in_one_month_with\" | 1147232 | | \"Edge\" | \"shares_restaurant_rating_with\" | 6805486 | | \"Edge\" | \"shares_user_with\" | 98630 | | \"Space\" | \"vertices\" | 45954 | | \"Space\" | \"edges\" | 8051348 | +---------+---------------------------------------+---------+ Got 6 rows (time spent 1911/4488 us) Currently, the general LPA tag propagation algorithm is used for community detection and few implementations are used for tag expansion (only SK-Learn has this implementation), here, we refer to the implementation given by [Thibaud M](https://datascience.stackexchange.com/users/77683/ thibaud-m) for the implementation given. The orginal talks could be referred: https://datascience.stackexchange.com/a/55720/138720 To make this algorithm run faster, a subgraph is taken from the NebulaGraph and an expansion of the labeling is done on this small subgraph: First, we start a Playground for Jupyter. More details in: https://github.com/wey-gu/nebula-dgl. git clone https://github.com/wey-gu/nebula-dgl.git cd nebula-dgl # run the Jupyter Notebook docker run -it --name dgl -p 8888:8888 --network nebula-net \\ -v \"$PWD\":/home/jovyan/work jupyter/datascience-notebook \\ start-notebook.sh --NotebookApp.token='nebulagraph' visit http://localhost:8888/lab/tree/work?token=nebulagraph Install depednecies(they will be used in GNN examples, too) !python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8 !python3 -m pip install . Then, letâ€™s sample a subgraph, starting from the vertex with ID 2048, to get all vertecies in two steps: import torch import json from torch import tensor from dgl import DGLHeteroGraph, heterograph from nebula3.gclient.net import ConnectionPool from nebula3.Config import Config config = Config() config.max_connection_pool_size = 2 connection_pool = ConnectionPool() connection_pool.init([('graphd', 9669)], config) vertex_id = 2048 client = connection_pool.get_session('ro","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:3","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#expand-labels-in-graph"},{"categories":["Nebula Graph"],"content":" 1.3 Expand labels in Graph In the paper: Learning from Labeled and Unlabeled Data with Label Propagation (CMU-CALD-02-107) by Xiaojin Z. and Zoubin G., the Label Propagation algorithm is used to propagate limited labeled information on the graph to more entities through the edges. In this way, we can easily â€œpropagateâ€ more labeled information with a limited number of high-risk annotations in the graphs we build. These extended labeles can, on the one hand, give more results in real-time graph queries and, on the other hand, serve as important input for risk control experts to help advance anti-fraud investigation actions. In general, we can scan the graph data offline periodically, expand and update the labels by the graph algorithm, and then write the valid updated labels back to the graph. Note that there is a similar method, SIGNDiffusion, for those who are interested. 1.3.1 Try expanding labels in graphHere is an example that works. In this example, I use the public Yelp dataset. This data will not only be used in this example, but also in the later cases in the GNN method, so you can be patient and import the data into NebulaGraph. Load dataset into NebulaGraph More details in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection cd nebulagraph-yelp-frauddetection python3 -m pip install -r requirements.txt # download and process dataset python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml After itâ€™s done, we could see the data stats: ~/.nebula-up/console.sh -e \"USE yelp; SHOW STATS\" It should look like: (root@nebula) [(none)]\u003e USE yelp; SHOW STATS +---------+---------------------------------------+---------+ | Type | Name | Count | +---------+---------------------------------------+---------+ | \"Tag\" | \"review\" | 45954 | | \"Edge\" | \"shares_restaurant_in_one_month_with\" | 1147232 | | \"Edge\" | \"shares_restaurant_rating_with\" | 6805486 | | \"Edge\" | \"shares_user_with\" | 98630 | | \"Space\" | \"vertices\" | 45954 | | \"Space\" | \"edges\" | 8051348 | +---------+---------------------------------------+---------+ Got 6 rows (time spent 1911/4488 us) Currently, the general LPA tag propagation algorithm is used for community detection and few implementations are used for tag expansion (only SK-Learn has this implementation), here, we refer to the implementation given by [Thibaud M](https://datascience.stackexchange.com/users/77683/ thibaud-m) for the implementation given. The orginal talks could be referred: https://datascience.stackexchange.com/a/55720/138720 To make this algorithm run faster, a subgraph is taken from the NebulaGraph and an expansion of the labeling is done on this small subgraph: First, we start a Playground for Jupyter. More details in: https://github.com/wey-gu/nebula-dgl. git clone https://github.com/wey-gu/nebula-dgl.git cd nebula-dgl # run the Jupyter Notebook docker run -it --name dgl -p 8888:8888 --network nebula-net \\ -v \"$PWD\":/home/jovyan/work jupyter/datascience-notebook \\ start-notebook.sh --NotebookApp.token='nebulagraph' visit http://localhost:8888/lab/tree/work?token=nebulagraph Install depednecies(they will be used in GNN examples, too) !python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8 !python3 -m pip install . Then, letâ€™s sample a subgraph, starting from the vertex with ID 2048, to get all vertecies in two steps: import torch import json from torch import tensor from dgl import DGLHeteroGraph, heterograph from nebula3.gclient.net import ConnectionPool from nebula3.Config import Config config = Config() config.max_connection_pool_size = 2 connection_pool = ConnectionPool() connection_pool.init([('graphd', 9669)], config) vertex_id = 2048 client = connection_pool.get_session('ro","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:3","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#try-expanding-labels-in-graph"},{"categories":["Nebula Graph"],"content":" 1.4 Machine Learning with Graph Features Before the field of risk control started to leverage the methods of the graph, there have been many approaches to predict high-risk behavior based on historical data using machine learning classification algorithms that use information in records that domain experts consider relevant (e.g., age, education, income) as features and historical label information to train risk prediction models. So reading this, does it occur to us that on top of these methods, models trained as features might be more effective if attributes based on graph structure were also taken into account? The answer is also yes, and there have been many papers and engineering practices revealing that such models are more effective than algorithms that do not consider graph features: these graph structure features that are tried to be effective could be PageRank values of entities, Degree values, or community ids derived from one of the community discovery algorithms. In production, we can periodically obtain real-time full graph information from the graph, analyze it in a graph computing platform to obtain the required features, go through a predefined data pipeline, import it into a machine learning model cycle to obtain new risk cues, and write some of the results back to the graph for easy extraction and reference by other systems and experts. 1.4.1 Example of ML with Graph Features Here, I will not demonstrate the end-to-end machine learning example, which is a common classification approach, on top of which we can get some new properties in the data by graph algorithms, which are then processed as new features. I will only demonstrate a community discovery method where we can run a Louvain on the full graph, derive the community identity of different nodes, and then process the community values as a classification into numerical features. In this example we also use the data from https://github.com/wey-gu/fraud-detection-datagen, on top of which I used NebulaGraph-Algorithm project, a Spark application that runs many common graph algorithms on the NebulaGraph graph database. First, letâ€™s set up a NebulaGraph cluster with Spark and NebulaGraph Algorithm, in one-liner thanks to Nebula-UP curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark After itâ€™s done, as I had put needed configuration files inside Nebula-UP, we could call the Louvain algorithm with: cd ~/.nebula-up/nebula-up/spark \u0026\u0026 ls -l docker exec -it sparkmaster /spark/bin/spark-submit \\ --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 4g /root/download/nebula-algo.jar \\ -p /root/louvain.conf And the result will be stored in sparkmaster container, under path /output # docker exec -it sparkmaster bash ls -l /output After that, we can do some pre-processing on this Louvainâ€™s graph algorithm features and start the traditional model training. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:4","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#machine-learning-with-graph-features"},{"categories":["Nebula Graph"],"content":" 1.4 Machine Learning with Graph Features Before the field of risk control started to leverage the methods of the graph, there have been many approaches to predict high-risk behavior based on historical data using machine learning classification algorithms that use information in records that domain experts consider relevant (e.g., age, education, income) as features and historical label information to train risk prediction models. So reading this, does it occur to us that on top of these methods, models trained as features might be more effective if attributes based on graph structure were also taken into account? The answer is also yes, and there have been many papers and engineering practices revealing that such models are more effective than algorithms that do not consider graph features: these graph structure features that are tried to be effective could be PageRank values of entities, Degree values, or community ids derived from one of the community discovery algorithms. In production, we can periodically obtain real-time full graph information from the graph, analyze it in a graph computing platform to obtain the required features, go through a predefined data pipeline, import it into a machine learning model cycle to obtain new risk cues, and write some of the results back to the graph for easy extraction and reference by other systems and experts. 1.4.1 Example of ML with Graph Features Here, I will not demonstrate the end-to-end machine learning example, which is a common classification approach, on top of which we can get some new properties in the data by graph algorithms, which are then processed as new features. I will only demonstrate a community discovery method where we can run a Louvain on the full graph, derive the community identity of different nodes, and then process the community values as a classification into numerical features. In this example we also use the data from https://github.com/wey-gu/fraud-detection-datagen, on top of which I used NebulaGraph-Algorithm project, a Spark application that runs many common graph algorithms on the NebulaGraph graph database. First, letâ€™s set up a NebulaGraph cluster with Spark and NebulaGraph Algorithm, in one-liner thanks to Nebula-UP curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark After itâ€™s done, as I had put needed configuration files inside Nebula-UP, we could call the Louvain algorithm with: cd ~/.nebula-up/nebula-up/spark \u0026\u0026 ls -l docker exec -it sparkmaster /spark/bin/spark-submit \\ --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 4g /root/download/nebula-algo.jar \\ -p /root/louvain.conf And the result will be stored in sparkmaster container, under path /output # docker exec -it sparkmaster bash ls -l /output After that, we can do some pre-processing on this Louvainâ€™s graph algorithm features and start the traditional model training. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:4","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#example-of-ml-with-graph-features"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#the-graph-neural-network-approach"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#example-of-gnn-fraud-detection-system"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#dataset"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#data-processing"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#model-training"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, itâ€™s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#inference-api"},{"categories":["Nebula Graph"],"content":" 1.6 Conclusion To summarize, fraud detection with NebulaGraph Graph Database could be done: With graph queries to get risk metrics from graph database With risky label being expanded by graph algorithms and written back to graph database With ML methods including graph features being fetched from graph database Process the property in the graph into the node and edge features to predict risk offline using GNN methods, some of which can be combined with the graph database to achieve online risk prediction by Inductive Learning methods Feature Image credit goes to https://unsplash.com/photos/BW0vK-FA3eg ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:6","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#conclusion"},{"categories":["Nebula Graph","Amundsen"],"content":"I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/"},{"categories":["Nebula Graph","Amundsen"],"content":" Do I have to create my own graph model and everything to set up a Data Lineage system? Thanks to many great open-source projects, the answer is: No! Today, I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:0:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#"},{"categories":["Nebula Graph","Amundsen"],"content":" 1 Metadata Governance systemA Metadata Governance system is a system providing a single view of where and how all the data are formatted, generated, transformed, consumed, presented, and owned. Metadata Governance is like a catalog of all of the data warehouses, databases, tables, dashboards, ETL jobs, etc so that people donâ€™t have to broadcast their queries on â€œHi everyone, could I change the schema of this table?â€, â€œHey, anyone who knows how I could find the raw data of table-view-foo-bar?â€, which, explains why we need a Metadata Governance system in a mature data stack with a relatively large scale of data and team(or one to be grown to). For the other term, Data Lineage, is one of the Metadata that needs to be managed, for example, some dashboard is the downstream of a table view, which has an upstream as two other tables from different databases. That information should be managed at best when possible, too, to enable a trust chain on a data-driven team. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:1:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance-system"},{"categories":["Nebula Graph","Amundsen"],"content":" 2 The reference solution","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-reference-solution"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.1 MotivationThe metadata and data lineage are by nature fitting to the graph model/graph database well, and the relationship-oriented queries, for instance, â€œfinding all n-depth data lineage per given component(i.e. a table)â€ is a FIND ALL PATH query in a graph database. This also explains one observation of mine as an OSS contributor of Nebula Graph, a distributed graph database: (from their queries/graph modeling in discussions I could tell) a bunch of teams who are already levering Nebula Graph on their tech stack, are setting up a data lineage system on their own, from scratch. A Metadata Governance system needs some of the following components: Metadata Extractor This part is needed to either pull or be pushed from the different parties of the data stack like databases, data warehouses, dashboards, or even from ETL pipeline and applications, etc. Metadata Storage This could be either a database or even large JSON manifest files Metadata Catalog This could be a system providing API and/or a GUI interface to read/write the metadata and data lineage In Nebula Graph community, I had been seeing many graph database users were building their in-house data lineage system. Itâ€™s itching witnessing this entropy increase situation not be standarized or jointly contributed instead, as most of their work are parsing metadata from well-known big-data projects, and persistent into a graph database, which, I consider high probability that the work is common. Then I came to create an opinionated reference data infra stack with some of those best open-source projects put together. Hopefully, those who were gonna define and iterate their own fashion of Graph Model on Nebula Graph and create in-house Metadata and data linage extracting pipelines can benefit from this project to have a relatively polished, beautifully designed, Metadata Governance system out of the box with a fully evolved graph model. To make the reference project self-contained and runnable, I tried to put layers of data infra stack more than just pure metadata related ones, thus, maybe it will help new data engineers who would like to try and see how far had open-source pushed a modern data lab to. This is a diagram of all the components in this reference data stack, where I see most of them as Metadata Sources: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#motivation"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-data-stack"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#database-and-data-warehouse"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dataops"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-visualization"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#job-orchestration"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, letâ€™s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: âœ… - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: âœ… - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. âœ… - EL: Singer âœ… - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! âœ… - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. âœ… - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. âœ… - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. âœ… - Full-text Search: elasticsearch âœ… - Graph Database: Nebula Graph Now, with the components in our stack being revealed, letâ€™s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance"},{"categories":["Nebula Graph","Amundsen"],"content":" 3 Environment Bootstrap, Component overviewThe reference runnable project is open-source and you could find it here: https://github.com/wey-gu/data-lineage-ref-solution I will try my best to make things clean and isolated. Itâ€™s assumed you are running on a UNIX-like system with internet and Docker Compose being installed. Please refer here to install Docker and Docker Compose before moving forward. I am running it on Ubuntu 20.04 LTS X86_64, but there shouldnâ€™t be issues on other distros or versions of Linux. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#environment-bootstrap-component-overview"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.1 Run a Data Warehouse/ DatabaseFirst, letâ€™s install Postgres as our data warehouse. This oneliner will help create a Postgres running in the background with docker, and when being stopped it will be cleaned up(--rm). docker run --rm --name postgres \\ -e POSTGRES_PASSWORD=lineage_ref \\ -e POSTGRES_USER=lineage_ref \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres Then we could verify it with Postgres CLI or GUI clients. Hint: You could use VS Code extension: SQL tools to quickly connect to multiple RDBMS(MariaDB, Postgres, etc.) or even Non-SQL DBMS like Cassandra in a GUI fashion. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#run-a-data-warehouse-database"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, letâ€™s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Pythonâ€™s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env meltano init \u003cyourprojectname\u003e â€œInstallâ€ Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init \u003cyourprojectname\u003e Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke \u003cplugin\u003e to call pluginsâ€™ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then itâ€™s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didnâ€™t provide ui in the end due to the containerâ€™s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Patâ€™s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And itâ€™s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-dataops-toolchain-for-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, letâ€™s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Pythonâ€™s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init â€œInstallâ€ Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call pluginsâ€™ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then itâ€™s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didnâ€™t provide ui in the end due to the containerâ€™s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Patâ€™s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And itâ€™s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#installation-of-meltano"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, letâ€™s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Pythonâ€™s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init â€œInstallâ€ Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call pluginsâ€™ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then itâ€™s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didnâ€™t provide ui in the end due to the containerâ€™s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Patâ€™s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And itâ€™s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-meltano-ui"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, letâ€™s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Pythonâ€™s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init â€œInstallâ€ Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call pluginsâ€™ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then itâ€™s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didnâ€™t provide ui in the end due to the containerâ€™s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Patâ€™s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And itâ€™s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#example-meltano-projects"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Patâ€™s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Letâ€™s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order statusâ€™s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after itâ€™s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the Â·Â·Â· button. Itâ€™s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, itâ€™s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-a-bi-platform-for-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Patâ€™s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Letâ€™s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order statusâ€™s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after itâ€™s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the Â·Â·Â· button. Itâ€™s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, itâ€™s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#bootstrap-meltano-and-superset"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Patâ€™s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Letâ€™s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order statusâ€™s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after itâ€™s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the Â·Â·Â· button. Itâ€™s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, itâ€™s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#create-a-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, letâ€™s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, itâ€™s building images from the codebase, which, will take some time for the very first time. After itâ€™s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search serviceâ€˜s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Letâ€™s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Frontend:5000 â”‚ â”‚ Metadata Sources â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Metaservice:5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Foo DB â”‚ â”‚ Bar App â”‚ â”‚ X Dashboard â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”¤ Nebula Proxy â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”¼â”€â”€â”€â”€â”¤ Search searvice:5002 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, letâ€™s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, itâ€™s building images from the codebase, which, will take some time for the very first time. After itâ€™s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search serviceâ€˜s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Letâ€™s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Frontend:5000 â”‚ â”‚ Metadata Sources â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Metaservice:5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Foo DB â”‚ â”‚ Bar App â”‚ â”‚ X Dashboard â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”¤ Nebula Proxy â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”¼â”€â”€â”€â”€â”¤ Search searvice:5002 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#deploy-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, letâ€™s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, itâ€™s building images from the codebase, which, will take some time for the very first time. After itâ€™s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search serviceâ€˜s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Letâ€™s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Frontend:5000 â”‚ â”‚ Metadata Sources â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Metaservice:5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Foo DB â”‚ â”‚ Bar App â”‚ â”‚ X Dashboard â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”¤ Nebula Proxy â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”¼â”€â”€â”€â”€â”¤ Search searvice:5002 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-service"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, letâ€™s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, itâ€™s building images from the codebase, which, will take some time for the very first time. After itâ€™s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search serviceâ€˜s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Letâ€™s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Frontend:5000 â”‚ â”‚ Metadata Sources â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Metaservice:5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Foo DB â”‚ â”‚ Bar App â”‚ â”‚ X Dashboard â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”¤ Nebula Proxy â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”¼â”€â”€â”€â”€â”¤ Search searvice:5002 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-builder"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, letâ€™s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, itâ€™s building images from the codebase, which, will take some time for the very first time. After itâ€™s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search serviceâ€˜s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Letâ€™s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Frontend:5000 â”‚ â”‚ Metadata Sources â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Metaservice:5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Foo DB â”‚ â”‚ Bar App â”‚ â”‚ X Dashboard â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”¤ Nebula Proxy â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”¼â”€â”€â”€â”€â”¤ Search searvice:5002 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":" 4 Connecting the dots, Metadata DiscoveryWith the basic environment being set up, letâ€™s put everything together. Remember we had ELT some data to PostgreSQL as this? How could we let Amundsen discover metadata regarding those data and ETL? ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#connecting-the-dots-metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-postgres-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-postgres-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-postgres-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click â€œStart with Verticesâ€, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Letâ€™s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the ğŸ‘ icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-dbt-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click â€œStart with Verticesâ€, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Letâ€™s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the ğŸ‘ icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-dbt-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click â€œStart with Verticesâ€, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Letâ€™s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the ğŸ‘ icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-dbt-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, letâ€™s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboardâ€™s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-superset-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, letâ€™s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboardâ€™s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-superset-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, letâ€™s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboardâ€™s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-superset-dashboard-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.4 Preview data with SupersetSuperset could be used to preview Table Data like this. Corresponding documentation could be referred here, where the API of /superset/sql_json/ will be called by Amundsen Frontend. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#preview-data-with-superset"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, thatâ€™s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now letâ€™s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studioâ€™s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#enable-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, thatâ€™s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now letâ€™s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studioâ€™s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#get-lineage-in-nebula-graph"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, thatâ€™s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now letâ€™s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studioâ€™s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extract-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, thatâ€™s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now letâ€™s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studioâ€™s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dbt"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, thatâ€™s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now letâ€™s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studioâ€™s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#open-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 5 RecapThe whole idea of Metadata Governance/Discovery is to: Put all components in the stack as Metadata Sources(from any DB or DW to dbt, Airflow, Openlineage, Superset, etc.) Run metadata ETL with Databuilder(as a script, or DAG) to store and index with Nebula Graph(or other Graph Database) and Elasticsearch Consume, manage, and discover metadata from Frontend UI(with Superset for preview) or API Have more possibilities, flexibility, and insights on Nebula Graph from queries and UI ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#recap"},{"categories":["Nebula Graph","Amundsen"],"content":" 5.1 Upstream ProjectsAll projects used in this reference project are listed below in lexicographic order. Amundsen Apache Airflow Apache Superset dbt Elasticsearch meltano Nebula Graph Open Lineage singer Feature Image credit to Phil Hearing ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#upstream-projects"},{"categories":["Nebula Graph"],"content":"What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know.","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/"},{"categories":["Nebula Graph"],"content":" What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know. In this article, I am trying to walk you through all three Spark projects of Nebula Graph with some runnable hands-on examples. Also, I managed to make PySpark usable with Nebula Graph Spark Connector, which will be contributed to the Docs later. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:0:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#"},{"categories":["Nebula Graph"],"content":" 1 The three Spark projects for Nebula GraphI used to draw a sketch around all data importing methods of Nebula Graph here, where all three of the Spark-based Nebula Graph projects were already briefly introduced. Instead, in this article, a slightly deeper dive into all of them will be made based on my recent work on them. TL;DR Nebula Spark Connector is a Spark Lib to enable spark application reading from and writing to Nebula Graph in form of a dataframe. Nebula Exchange, built on top of Nebula Spark Connector, is a Spark Lib and Application to exchange(for the Open Source version, itâ€™s one way: write, whereas for the enterprise version itâ€™s bidirectional) different data sources like(MySQL, Neo4j, PostgreSQL, Clickhouse, Hive, etc.). Besides writing directly to Nebula Graph, it could optionally generate SST files to be ingested into Nebula Graph to offload the storage computation outside of the Nebula Graph cluster. Nebula Algorithm, built on top of Nebula Spark Connector and GraphX, is a Spark Lib and Application to run de facto graph algorithms(PageRank, LPA, etcâ€¦) on a graph from Nebula Graph. Then letâ€™s have the long version of those spark projects more on how-to perspectives. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:1:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#the-three-spark-projects-for-nebula-graph"},{"categories":["Nebula Graph"],"content":" 2 Spark-Connector Codebase: https://github.com/vesoft-inc/nebula-spark-connector Documentation: https://docs.nebula-graph.io/3.0.2/nebula-spark-connector/ (itâ€™s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/ Code Examples: example ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#spark-connector"},{"categories":["Nebula Graph"],"content":" 2.1 Nebula Graph Spark ReaderTo read data from Nebula Graph, i.e. vertex, Nebula Spark Connector will scan all storage instances that hold the given label(TAG): withLabel(\"player\"), and we could optionally specify the properties of the vertex: withReturnCols(List(\"name\", \"age\")). With needed configuration being provided, a call of spark.read.nebula.loadVerticesToDF will return dataframe of the Vertex Scan call towards Nebula Graph: def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColumn(false) .withReturnCols(List(\"name\", \"age\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() vertex.printSchema() vertex.show(20) println(\"vertex count: \" + vertex.count()) } Itâ€™s similar for the writer part and one big difference here is the writing path is done via GraphD as the underlying Spark Connector is shooting nGQL INSERT queries. Then letâ€™s do the hands-on end-to-end practice. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#nebula-graph-spark-reader"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: itâ€™s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, letâ€™s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, letâ€™s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, itâ€™s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLetâ€™s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-spark-connector"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: itâ€™s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, letâ€™s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, letâ€™s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, itâ€™s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLetâ€™s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#bootstrap-a-nebula-graph-cluster"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: itâ€™s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, letâ€™s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, letâ€™s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, itâ€™s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLetâ€™s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#create-a-spark-playground"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: itâ€™s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, letâ€™s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, letâ€™s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, itâ€™s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLetâ€™s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#run-spark-connector-example"},{"categories":["Nebula Graph"],"content":" 3 Exchange Codebase: https://github.com/vesoft-inc/nebula-exchange/ Documentation: https://docs.nebula-graph.io/3.0.2/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ (itâ€™s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://github.com/vesoft-inc/nebula-exchange/releases Configuration Examples: exchange-common/src/test/resources/application.conf Nebula Exchange is a Spark Lib/App to read data from multiple sources, then, write to either Nebula Graph directly or into Nebula Graph SST Files. The way to leverage Nebula Exchange is only to firstly create the configuration files to let the exchange know how data should be fetched and written, then call the exchange package with the conf file specified. Now letâ€™s do a hands-on test with the same environment created in the last chapter. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#exchange"},{"categories":["Nebula Graph"],"content":" 3.1 Hands-on ExchangeHere, we are using Exchange to consume data source from a CSV file, where the first column is Vertex ID, and the second, and third to be properties of â€œnameâ€ and â€œageâ€. player800,\"Foo Bar\",23 player801,\"Another Name\",21 Letâ€™s get into the spark container created in the last chapter, and download the Jar package of Nebula Exchange: docker exec -it spark-master bash cd /root/ wget https://github.com/vesoft-inc/nebula-exchange/releases/download/v3.0.0/nebula-exchange_spark_2.4-3.0.0.jar Create a conf file named exchange.conf in format HOCON, where: under .nebula, information regarding Nebula Graph Cluster was configured under .tags, information regarding Vertices like how required fields are reflected our data source(here itâ€™s CSV file) was configured { # Spark relation config spark: { app: { name: Nebula Exchange } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory: 1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"graphd:9669\"] meta:[\"metad0:9559\", \"metad1:9559\", \"metad2:9559\"] } user: root pswd: nebula space: basketballplayer # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://localhost:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different data Sources. tags: [ # HDFS CSV # Import mode is client, just change type.sink to sst if you want to use client import mode. { name: player type: { source: csv sink: client } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } Finally, letâ€™s create player.csv and exchange.conf, it should be listed as the following: # ls -l -rw-r--r-- 1 root root 1912 Apr 19 08:21 exchange.conf -rw-r--r-- 1 root root 157814140 Apr 19 08:17 nebula-exchange_spark_2.4-3.0.0.jar -rw-r--r-- 1 root root 52 Apr 19 08:06 player.csv And we could call the exchange as: /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange_spark_2.4-3.0.0.jar \\ -c exchange.conf And the result should be like ... 22/04/19 08:22:08 INFO Exchange$: import for tag player cost time: 1.32 s 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchFailure.player: 0 ... Please refer to the documentation and conf examples for more data sources. For hands-on Exchange writing to SST Files, you could refer to both Documentation and Nebula Exchange SST 2.x Hands-on Guide. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-exchange"},{"categories":["Nebula Graph"],"content":" 4 Algorithm Codebase: https://github.com/vesoft-inc/nebula-algorithm Documentation: https://docs.nebula-graph.io/3.0.2/nebula-algorithm/ (itâ€™s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ Code Examples: example/src/main/scala/com/vesoft/nebula/algorithm ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#algorithm"},{"categories":["Nebula Graph"],"content":" 4.1 Calling with spark-submitWhen we call Nebula Algorithm with spark-submit, on how to use perspective, it is quite similar to Exchange. This post already showed us how to do that in action. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-with-spark-submit"},{"categories":["Nebula Graph"],"content":" 4.2 Calling as a lib in codeOn the other hand, we could call Nebula Algorithm in spark as a Spark Lib, the gain will be: More control/customization on the output format of the algorithm Possible to perform algorithm for non-numerical vertex ID cases, see here ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-as-a-lib-in-code"},{"categories":["Nebula Graph"],"content":" 5 PySpark for Nebula GraphPySpark comes with the capability to call java/scala packages inside python, thus itâ€™s also quite easy to use Spark Connector with Python. Here I am doing this from the pyspark entrypoint in /spark/bin/pyspark, with the connectorâ€™s Jar package specified with --driver-class-path and --jars docker exec -it spark-master-0 bash cd root wget https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/3.0.0/nebula-spark-connector-3.0.0.jar /spark/bin/pyspark --driver-class-path nebula-spark-connector-3.0.0.jar --jars nebula-spark-connector-3.0.0.jar Then, rather than pass NebulaConnectionConfig and ReadNebulaConfig to spark.read.nebula, we should instead call spark.read.format(\"com.vesoft.nebula.connector.NebulaDataSource\"). VoilÃ ! df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows Below are how I figured out how to make this work with almost zero scala knowledge:-P. How reader should be called def loadVerticesToDF(): DataFrame = { assert(connectionConfig != null \u0026\u0026 readConfig != null, \"nebula config is not set, please call nebula() before loadVerticesToDF\") val dfReader = reader .format(classOf[NebulaDataSource].getName) .option(NebulaOptions.TYPE, DataTypeEnum.VERTEX.toString) .option(NebulaOptions.SPACE_NAME, readConfig.getSpace) .option(NebulaOptions.LABEL, readConfig.getLabel) .option(NebulaOptions.PARTITION_NUMBER, readConfig.getPartitionNum) .option(NebulaOptions.RETURN_COLS, readConfig.getReturnCols.mkString(\",\")) .option(NebulaOptions.NO_COLUMN, readConfig.getNoColumn) .option(NebulaOptions.LIMIT, readConfig.getLimit) .option(NebulaOptions.META_ADDRESS, connectionConfig.getMetaAddress) .option(NebulaOptions.TIMEOUT, connectionConfig.getTimeout) .option(NebulaOptions.CONNECTION_RETRY, connectionConfig.getConnectionRetry) .option(NebulaOptions.EXECUTION_RETRY, connectionConfig.getExecRetry) .option(NebulaOptions.ENABLE_META_SSL, connectionConfig.getEnableMetaSSL) .option(NebulaOptions.ENABLE_STORAGE_SSL, connectionConfig.getEnableStorageSSL) if (connectionConfig.getEnableStorageSSL || connectionConfig.getEnableMetaSSL) { dfReader.option(NebulaOptions.SSL_SIGN_TYPE, connectionConfig.getSignType) SSLSignType.withName(connectionConfig.getSignType) match { case SSLSignType.CA =\u003e dfReader.option(NebulaOptions.CA_SIGN_PARAM, connectionConfig.getCaSignParam) case SSLSignType.SELF =\u003e dfReader.option(NebulaOptions.SELF_SIGN_PARAM, connectionConfig.getSelfSignParam) } } dfReader.load() } How Option String should be like # object NebulaOptions { /** nebula common config */ val SPACE_NAME: String = \"spaceName\" val META_ADDRESS: String = \"metaAddress\" val GRAPH_ADDRESS: String = \"graphAddress\" val TYPE: String = \"type\" val LABEL: String = \"label\" /** connection config */ val TIMEOUT: String = \"timeout\" val CONNECTION_RETRY: String = \"connectionRetry\" val EXECUTION_RETRY: String = \"executionRetry\" val RATE_TIME_OUT: String = \"reteTimeOut\" val USER_NAME: String = \"user\" val PASSWD: String = \"passwd\" val ENABLE_GRAPH_SSL: String = \"enableGraphSSL\" val ENABLE_META_SSL: String = \"enableMetaSSL\" val ENABLE_STORAGE_SSL: String = \"enableStorageSSL\" val SSL_SIGN_TYPE: String = \"sslSignType\" val CA_SIGN_PARAM: String = \"caSignParam\" val SELF_SIGN_PARAM: String = \"selfSignParam\" /** read config */ val RETURN_COLS: String = \"returnCols\" val NO_COLUMN: String = \"noColumn\" val PARTITION_NUMBER: String = \"partitionNumber\" val LIMIT: String = \"limit\" /** write config */ val RATE_LIMIT: String = \"rateLimit\" val VID_POLICY: String = \"vidPolicy\" val SRC_POLICY: String = \"srcPolicy\" val DST_POLI","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:5:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#pyspark-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"Running Nebula Graph Database on ARM64 Single Board Computer/Respberry Pi","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/"},{"categories":["Nebula Graph"],"content":" With the ARM64 Docker Image of Nebula Graph, itâ€™s actually quite easy to run it on SBC/Respberry Pi! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:0:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#"},{"categories":["Nebula Graph"],"content":" 1 BackgroundRecently, after Yee from Nebula Graph Community fixed ARM build on nebula-third-party#37, we could play with Nebula Graph on M1 Chip Macbook. While, I didnâ€™t get the chance to run it on a SBC/Pi. A couple of weeks before, in a twitter thread with @laixintao and @andelf I decided to purchase a Rock Pi 3A: And it looks nice!(Even come with a NPU inside) ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:1:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#background"},{"categories":["Nebula Graph"],"content":" 2 The Guide of running Nebula Graph on a Pi SBC Actually, since v3.0.0, Nebula comes with a standalone version, which suits the deep edge deployment more, but today, I will only setup the cluster version as the Docker Image is out of box to be used. I will share more on standalone version in upcoming weeks. I put the Ubuntu Server installation steps in the appendix, and now I assumed we already have an ARM64 Linux up and running on a Pi SBC. ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#the-guide-of-running-nebula-graph-on-a-pi-sbc"},{"categories":["Nebula Graph"],"content":" 2.1 Step 0, Install Docker-Compose on PiI am using debian/ubuntu here, while it should be the same for other distros, referring to https://docs.docker.com/engine/install/. sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # follow https://docs.docker.com/engine/install/linux-postinstall/ sudo groupadd docker sudo usermod -aG docker $USER exit # login again newgrp docker After Docker being installed, we install compose here, there could be issues encounterred from the Docker website on Compose installation. While, due to compose is just a python package, letâ€™s do it via python3-pip install: sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:1","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-0-install-docker-compose-on-pi"},{"categories":["Nebula Graph"],"content":" 2.2 Step 1, bootstrap Nebula Graph Cluster on PiWe clone the compose file for nebula cluster first: git clone https://github.com/vesoft-inc/nebula-docker-compose.git \u0026\u0026 cd nebula-docker-compose docker-compose up -d Then, letâ€™s download the client: nebula-console, and connect to the GraphD service: wget https://github.com/vesoft-inc/nebula-console/releases/download/v3.0.0/nebula-console-linux-arm64-v3.0.0 chmod +x nebula-console-linux-arm64-v3.0.0 ./nebula-console-linux-arm64-v3.0.0 -addr localhost -port 9669 -u root -p nebula Activate the storageD services: ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:2","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-1-bootstrap-nebula-graph-cluster-on-pi"},{"categories":["Nebula Graph"],"content":" 2.3 Step 2, Play Nebula Graph on PiWIth the SHOW HOSTS we should see StorageD services are all ONLINE, then we could run this from the console session to load the test dataset. Referennce: https://docs.nebula-graph.io/3.0.1/nebula-console/#import_a_testing_dataset $:play basketballplayer; The test data will be loaded in around 1 minute. Then, we could query something like: USE basketballplayer; GO FROM \"player100\" OVER follow YIELD dst(edge); Check this out andâ€¦ Happy Graphing! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:3","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-2-play-nebula-graph-on-pi"},{"categories":["Nebula Graph"],"content":" 3 Appendix: Installing Ubuntu Server on Rock Pi 3A SBC Get the image from https://wiki.radxa.com/Rock3/downloads decompressing the file into .img Write the image to a micro SD card with etcher Boot it! feature image credit: @_louisreed ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:3:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#appendix-installing-ubuntu-server-on-rock-pi-3a-sbc"},{"categories":["Nebula Graph"],"content":"Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph?","date":"2022-02-28","objectID":"/en/resolve-wordle/","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/"},{"categories":["Nebula Graph"],"content":" Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph? ","date":"2022-02-28","objectID":"/en/resolve-wordle/:0:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#"},{"categories":["Nebula Graph"],"content":" 1 BackgroundYou may have seen tweets like this in past months, where the color dots in emoji was shared in SNS randomly. Feel free to Google Wordle first if you donâ€™t know its meaning yet. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#background"},{"categories":["Nebula Graph"],"content":" 1.1 Wordle SolverFor all magics being used to solve wordle, I am impressed by Grant Sanderson, who explained us the information theory when solving wordle, in an elegent and delightful way. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#wordle-solver"},{"categories":["Nebula Graph"],"content":" 1.2 Chinese wordle: â€œhandleâ€I am not going create yet another wordle-solver today, instead, itâ€™s more about an intresting variant of wordle. To truly enjoy the fun of wordle, mostly we should be a native speaker, and it is not surprising that there is a Spanish wordle out there, and still tweets on wordle(es) are being shared literially every second now. While for non alphabetic languages like Chineses, do we have the luck to have fun with wordling? The answer is yes, while itâ€™s a bit different. For the reason Chinese charactors, also called Hanzi or Han chactors are in from of Logogram, each charactor in Chinese is made up of radicals in quite different forms, each Chinese word can be 2/3/4 charactors. Most of the crossword games in Chinese are idiom, proverb or ancient poetry based: the slot to be filled are one Chinese. ref: Chinese idiom puzzle Thus, a wordle but in Chinese idiom will be quite strange, or even hilarious as Chee and JinGen discussed in this tweet thread, where you could see the candidate characters are counted in thousands: there is no chance to guess the idiom in 10 attempts! As one of the fun on playing wordle could be the feedback loop of guess --\u003e result in limited attempts, while the scale of Chinese Charactor had pushed the Chinese wordle variant creators leveraging other aspects of the charactor: pronunciation. Each charactor in Chinease is monosyllabic without exceptions, and when it comes to its pronunciation, they are all made up from two parts(initial and final, and they could be written in roman letters), which comes in tens level of possbilities. There are bounch of Chinese wordle varients asking player to guess idiom leveraging pinyin: https://pinyincaichengyu.com/ https://cheeaun.github.io/chengyu-wordle/ https://apps.apple.com/cn/app/id1606194420 While, to me, a native Chinese speaker, itâ€™s either too hard to play with condtions of pronunciation parts(pinyin) or too easy to guess on given around 20 Chinese charactors. Then, the varient stands out here is the â€œhandle/æ±‰å…œ\"(Hanzi-Wordle) created by Antfu. â€œHandleâ€ introduced the tones with genius to add an extra dimension of all charactors per each guess attempt, which helped player to have more information on filtering the knowledge in the brain. Note, for each Chinese charactor, there will be a tone in 1 of 4 tones in its pronunciation. Letâ€™s see what itâ€™s like to play the â€œHandleâ€: There will be 4 Chinese Charactors to be filled in 10 times of guess Not only the charactor self will be colored in result: For example in first line, the green â€œé—¨â€ in position 2 is correct whereas in second line, the orange â€œä»“â€ is corret while the possition should be all but not the first slot. There will be extra hints on: Pinyin parts for both part1(initial) and part2(final) In third line of the boxes, the green â€œqiaoâ€ refers to the first charactor is ponouced in â€œqiaoâ€ with initial:â€œqâ€ and final:â€œiaoâ€, although we filled the wrong charactor in the writing dimension. In third line, the orange â€œuoâ€ refers to there is one chacarctor in other poisition with the final part of the pinyin as â€œuoâ€. Tones of the charactor: In third line, the green â€œ-â€ stands for the third charactor is in tone-1. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-wordle-handle"},{"categories":["Nebula Graph"],"content":" 1.3 The Chinese Wordle HelperAs a non-native English speaker, the way I was playing wordle is to cheating relying on helpers: After knowing on the third letter to be â€œOâ€, I googled and got this: 5-letter-words-with-o-in-the-middle and do the searching ğŸ˜. The way to play with helpers works for me to have fun yet not ruin it by an automated cheat resolver(itâ€™s only simulating my brain as a native-speaker!), so that I could somehow experience the same as Millions of people out there without cheating. While for Chinese â€œHandleâ€ players, from my perspective, itâ€™s still a bit harder(to find answers in 10 guesses), and the way my wife and I were playing â€œHandleâ€ when lining up at the restaurant door ended up googling: idiom list with word â€˜fooâ€™, yet still having a lot of fun. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-chinese-wordle-helper"},{"categories":["Nebula Graph"],"content":" 2 Chinese idiom Knowledge GraphCould I create something between the human brain and the game-cheater/ruiner to make it more of fun? The answer is yes: a game extension as a secondary brain. For this helper/secondary brain, the solution for â€œhandleâ€ differenciates from the English wordle, unlike the auto-solver, similar algorithms could help on both cases: In wordle(English), player searches in their brain or from a helper like the web page: 5-letter-words-with-o-in-the-middle. In handle(Chinese), itâ€™s harder to be searching based on hints like tones/initial parts of pinyin in fulltext webpage searching anymore, the reason hehind is that the multidimensional filter condtions are not indexed by normal webpages. As I mentioned, the key of the helper to be leveraged to (not ruining the game) is to be the extension of the brain, then the question is: how does our brain work on handling the knowledge of â€œhandleâ€(yes, I was preparing for this pun for so long!)? Thus, why not do it in a graph/neural network way? And here we go, letâ€™s create a knowledge graph of Chinese idiom and see how it goes with the â€œhandleâ€ game. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-idiom-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.1 TL;DRItâ€™s indeed entertaining to me, and I could write Graph Queries[*] by hand or via Visualization tools[**] to help explore things in this graph, because I can weâ€™re doing the â€œthinkingâ€ process the similar way in our own brain, but not so well-informed. # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC ** ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#tldr"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the â€œhandleâ€ game ğŸ‘‰ğŸ» https://handle.antfu.me/. We could start with one guess i.e. â€œçˆ±æ†åˆ†æ˜â€. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as â€œaiâ€, but is not â€œçˆ±â€ There is one Character in tone-1 not in 2nd position There is one Character with final part as â€œingâ€, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"æƒŠæ„šéª‡ä¿—\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"æƒŠä¸–éª‡ä¿—\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"æƒŠè§éª‡é—»\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"æ²½åå–ç›´\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"æƒŠå¿ƒéª‡ç¥\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"è†æ£˜è½½é€”\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"å‡ºå–çµé­‚\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be â€œæƒŠä¸–éª‡ä¿—â€, and letâ€™s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click â€œView Subgraphsâ€ to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-long-version-of-playing-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the â€œhandleâ€ game ğŸ‘‰ğŸ» https://handle.antfu.me/. We could start with one guess i.e. â€œçˆ±æ†åˆ†æ˜â€. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as â€œaiâ€, but is not â€œçˆ±â€ There is one Character in tone-1 not in 2nd position There is one Character with final part as â€œingâ€, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"æƒŠæ„šéª‡ä¿—\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"æƒŠä¸–éª‡ä¿—\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"æƒŠè§éª‡é—»\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"æ²½åå–ç›´\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"æƒŠå¿ƒéª‡ç¥\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"è†æ£˜è½½é€”\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"å‡ºå–çµé­‚\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be â€œæƒŠä¸–éª‡ä¿—â€, and letâ€™s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click â€œView Subgraphsâ€ to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#play-handle-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the â€œhandleâ€ game ğŸ‘‰ğŸ» https://handle.antfu.me/. We could start with one guess i.e. â€œçˆ±æ†åˆ†æ˜â€. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as â€œaiâ€, but is not â€œçˆ±â€ There is one Character in tone-1 not in 2nd position There is one Character with final part as â€œingâ€, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"æƒŠæ„šéª‡ä¿—\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"æƒŠä¸–éª‡ä¿—\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"æƒŠè§éª‡é—»\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"æ²½åå–ç›´\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"æƒŠå¿ƒéª‡ç¥\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"è†æ£˜è½½é€”\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"å‡ºå–çµé­‚\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be â€œæƒŠä¸–éª‡ä¿—â€, and letâ€™s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"çˆ±\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"çˆ±\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click â€œView Subgraphsâ€ to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-visualization-of-the-query"},{"categories":["Nebula Graph"],"content":" 3 Whatâ€™s NextIf you happened to get hands dirty(or being intrested to) on Graph Database, you could checkout the Nebula Graph project now and its Docs to have more fun of it! Also, here are some only playgrounds if you prefer to try it without deployment on your own envrioment. If you are intrested in the MATCH query syntax and would like to actually do exersices with each daily handle challenge, check below Documents: MATCH https://docs.nebula-graph.io/3.0.1/3.ngql-guide/7.general-query-statements/2.match/ Graph Patterns https://docs.nebula-graph.io/3.0.1/3.ngql-guide/1.nGQL-overview/3.graph-patterns/ nGQL command cheatsheet https://docs.nebula-graph.io/3.0.1/2.quick-start/6.cheatsheet-for-ngql/ Happy Graphing! ","date":"2022-02-28","objectID":"/en/resolve-wordle/:3:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#whats-next"},{"categories":["Nebula Graph"],"content":" 4 Appendix: Setting up the Knowledge GraphI put the code and process here: https://github.com/wey-gu/chinese-graph, feel free to check that out. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#appendix-setting-up-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.1 Build the Knowledge GraphThe process would be: Modeling the Knowledge Preprocessing the data ETL data to a Graph Database: Nebula Graph Have fun on Nebula Graph ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#build-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.2 Modeling the Knowledge GraphFor Graph Modeling, itâ€™s actually quite straight forward, the mind model for me was to put the knowledge I cares as vertcies and connect them with their relationships first. You will come back to iterate or optimize the modeling when you are actually playing with the data afterwards, thus, if you could imagine how the graph will be queried in the first place, the graph modeling could be adopted accordingly. Otherwise, donâ€™t over design it, just do it the intuitive way. Here, I put the vertices with properties as: idiom character pinyin tone pinyin_part type The edges with properteis as: with_character with_pinyin with_pinyin_part ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#modeling-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.3 Deploy Nebula Graph With Nebula-UP, itâ€™s an onliner call curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#deploy-nebula-graph"},{"categories":["Nebula Graph"],"content":" 4.4 Load data # clone the code for data genration and data loading git clone https://github.com/wey-gu/chinese-graph.git \u0026\u0026 cd chinese-graph python3 graph_data_generator.py # generate data # load data with Nebula-Importer docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:4","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#load-data"},{"categories":["Nebula Graph"],"content":"Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index?","date":"2022-02-20","objectID":"/en/nebula-index-explained/","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/"},{"categories":["Nebula Graph"],"content":" Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index? The term of Nebula Graph Index is quite similar to the index in RDBMS, while, they are not the same. Itâ€™s noticed that when getting started with Nebula Graph, the index confused some of the users in first glance on the following What exactly Nebula Graph Index is. When I should use it. How it impacts the performance. Today Iâ€™m gonna walk you through the index in Nebula Graph. Letâ€™s get started! ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:0:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#"},{"categories":["Nebula Graph"],"content":" 1 What exactly Nebula Graph Index isTL;DR, Nebula Graph Index is only to be used to enable the graph query to be started from conditions on properties of vertices or edges, instead of vertexID. Itâ€™s only used in a starting entry of a graph query. If a query is in pattern: (a-\u003eb-\u003ec, where c in condition-foobar) graph walk, due to the only filtering condition-foobar is on c, this query under the hood will be started to seek c, and then it walks through the reversed -\u003e to b, finally to a. Thus, the Nebula Graph Index will be used and only be possbily used in seeking c, when condition-foobar is not like id(c) == \"foobar\" but c.property_x == \"foobar\". ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#what-exactly-nebula-graph-index-is"},{"categories":["Nebula Graph"],"content":" 1.1 Index is used only for starting point seekWe know that in RDBMS, an INDEX is to create a duplicated sorted DATA to enable QUERY with condition filtering on the sorted data, to accelerate the query in read and involves extra writes during the write. Note: in RDBMS/Tabular DB, an INDEX on some columns means to create extra data that are sorted on those columns to make query with those columnsâ€™ condition to be scanned faster, rather than scanning from the original table data sorted based on the key only. In Nebula Graph, the INDEX is to create a duplicated sorted Vertex/Edge PROP DATA to enable starting point seek of a QUERY(itâ€™s a prerequisite rather than help accelerate it). Not all of the queries relied on index, here are some examples, letâ€™s call them pure-property-condition-start queries: #### Queries relying on Nebula Graph Index # query 0 pure-property-condition-start query LOOKUP ON `tag1` WHERE col1 \u003e 1 AND col2 == \"foo\" \\ YIELD `tag1`.col1 as col1, `tag1`.col3 as col3; # query 1 pure-property-condition-start query MATCH (v:`player` { name: 'Tim Duncan' })--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name; In both query 0 and query 1, the pattern is to â€œFind VID/EDGE only based on given the propertiy condtionsâ€. On the contrary, the starting point are VertexID based instead in query 2 and query 3: #### Queries not based on Nebula Graph Index # query 2, walk query starting from given vertex VID: \"player100\" GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age \u003e 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; # query 3, walk query starting from given vertex VID: \"player101\" or \"player102\" MATCH (v:`player` { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] \\ RETURN v2.`player`.name AS Name; If we look into query 1 and query 3, which shared condition on vertex on tag:player are both { name: 'Tim Duncan' } though, they are differenciated in starting points: For query 3 , the index is not required as the query will be started from known vertex ID in [\"player101\", \"player102\"] and thus: Itâ€™ll directly fetch vertex Data from v2â€™s vertex IDs then to GetNeighbors(): walk through edges of v2, GetVertices() for next hop: v and filter based on property: name For query 1 , the query has to start from v due to no known vertex IDs were provided: Itâ€™ll do IndexScan() first to find all vertices only with property condtion of { name: 'Tim Duncan' } Then, GetNeighbors(): walk through edges of v, GetVertices() for next hop: v2 Now, we could know the whole point that matters here is on whether to know the vertexID. And the above differences could be shown in their execution plans with PROFILE or EXPLAIN like the follow: query 1, requires index(on tag: player), pure prop condition query as starting point query 3, no index required, query starting from known vertex IDs ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:1","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#index-is-used-only-for-starting-point-seek"},{"categories":["Nebula Graph"],"content":" 1.2 Why Nebula Graph index is enabler rather than an acceleraterCanâ€™t those queries be done without indexes? Itâ€™s possible in theory with full scan, but disabled without index. The reason is Nebula Graph stores data in a distributed and graph-oriented way, the full scan of data was condiserred too expensive to be allowed. Note: from v3.0, itâ€™s possible to do TopN Scan without INDEX, where the LIMIT \u003cn\u003e is used, this is different from the fullscan case(INDEX is a must), which will be explained later. # sample vertex MATCH (v:`team`) RETURN v LIMIT 3 # or sample edge MATCH ()-[e:`follow`]-\u003e() RETURN e LIMIT 3 ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:2","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-nebula-graph-index-is-enabler-rather-than-an-accelerater"},{"categories":["Nebula Graph"],"content":" 1.3 Why starting point onlyIndex data is not used in terversal. It could confuse use to think of index is to sorting data based on properties, does it accelerate the terversal with property condition filtering? The answer is, no. In Nebula Graph, the data is structured in a way to enable fast graph-terversal, which is already indexed/sorted on vertex ID(for both vertex and edge) in raw data, where terversal(underlying in storage, itâ€™s calling GetNeighbors interface) of given vertex is cheap and fast due to the locality/stored continuously(pysically linked). So in summary: Nebula Graph Index is sorted prop data to find the starting vertex or edge on given pure prop conditions. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:3","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-starting-point-only"},{"categories":["Nebula Graph"],"content":" 2 Facts on Nebula Graph IndexTo understand more details/limitations/cost of Nebula, letâ€™s reveal more on its design and here are some facts in short: Index Data is stored and sharded together with Vertex Data Itâ€™s Left Match based only: Itâ€™s RocksDB Prefix Scan under the hood Effect on write and read path(to see its cost): Write Path: Extra Data written + Extra Read request introduced Read Path: RBO(Rule based optimization), Fan Out(to all shards) Data Full Scan LIMIT Sample(not full scan) is supported without Index LOOKUP ON t YIELD t.name | LIMIT 1 MATCH (v:`player` { name: 'Tim Duncan' })--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name LIMIT 3; The key info can be seen from one of my sketch notes: We should notice that only the left match is supported in pure-property-condition-start queries. For queries like wildcard or reguler-expression, Full-text Index/Search is to be used, where an external elastic search is integrated with nebula: please check Nebula Graph Full text index for more. Within this sketch note, more highlights are: Itâ€™s a Local Index Design The index is stored and shared locally together with the graph data. Itâ€™s sorting based on prop value, and the index search is underlying a rocksDB prefix scan, thatâ€™s why only left match is supported. There is cost in the write path The index enables the RDBMS-like Prop Condition Based Query with cost in the write path including not only the extra write, but also, random read, to ensure the data consistency. Index Data write is done in a sync way For Read path: In pure-property-condition-start queries, in GraphD, the index will be selected with Rule-based-optimization like this example, where, in a rule, the col2 to be sorted first is considered optimal with the condition: col2 equals â€˜fooâ€™. After the index was chosen, index-scan request will be fanout to storageD instances, and in the case of filters like LIMIT N, it will be pushed down to the storage side to reduce data payload. Note: it was not shown in the sketch but actually from v3.0, the nebula graph allows LIMIT N Sample Prop condition query like this w/o index, which is underlying pushing down the LIMIT filter to storage side. Take aways: Use index only when we have to, as itâ€™s costly in write cases and if limit N sample is the only needed case and itâ€™s fast enough, we can use that instead. Index is left match composite index order matters, should be created carefully. for full-text search use case, use full-text index instead. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:2:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#facts-on-nebula-graph-index"},{"categories":["Nebula Graph"],"content":" 3 How to use the indexWe should always refer to the documentation, and I just put some highlights on this here: To create an index on a tag or edge type to specify a list of props in the order that we need. CREATE INDEX If an index was created after existing data was inserted, we need to trigger an index async rebuild job, as the index data will be written in sync way only when index is created. REBUILD INDEX We can see the index status after REBUILD INDEX issued. SHOW INDEX STATUS Queries levering index could be LOOKUP, and with the pipeline, in most cases we will do follow-up graph-walk queries like: LOOKUP ON `player` \\ WHERE `player`.name == \"Kobe Bryant\"\\ YIELD id(vertex) AS VertexID, properties(vertex).name AS name |\\ GO FROM $-.VertexID OVER serve \\ YIELD $-.name, properties(edge).start_year, properties(edge).end_year, properties($$).name; Or in MATCH query like this, under the hood, v will be searched on index and v2 will be walked by default graph data structure without involving index. MATCH (v:`player`{name:\"Tim Duncan\"})--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:3:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#how-to-use-the-index"},{"categories":["Nebula Graph"],"content":" 4 RecapFinally, Letâ€™s Recap INDEX is sorting PROP DATA to enable finding starting point on given PURE PROP CONDITION INDEX is not for Trevsal INDEX is left match, not for full-text search INDEX has cost on WRITE Remember to REBUILD after CREATE INDEX on existing data Happy Graphing! Feture image credit to Alina ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:4:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#recap"},{"categories":["Nebula Graph"],"content":"How to parse nebula graph data in an interactive way and what are the best practices?","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/"},{"categories":["Nebula Graph"],"content":" How to parse nebula graph data in an interactive way and what are the best practices? I will show you an easier way in this article ğŸ˜. updated: 2022-Aug-10, adapted to nebulagraph 3.x ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:0:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#"},{"categories":["Nebula Graph"],"content":" 1 Prepare for the Java REPLThanks to https://github.com/albertlatacz/java-repl/ we could play with/debug this in an interactive way, and all we need is to leverage its docker image to have all the envrioment in a clean and quick way: docker pull albertlatacz/java-repl docker run --rm -it \\ --network=nebula-net \\ -v ~:/root \\ albertlatacz/java-repl \\ bash apt update -y \u0026\u0026 apt install ca-certificates -y wget https://dlcdn.apache.org/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz --no-check-certificate tar xzvf apache-maven-3.8.6-bin.tar.gz wget https://github.com/vesoft-inc/nebula-java/archive/refs/tags/v3.0.0.tar.gz tar xzvf v3.0.0.tar.gz cd nebula-java-3.0.0/ ../apache-maven-3.8.6/bin/mvn dependency:copy-dependencies ../apache-maven-3.8.6/bin/mvn -B package -Dmaven.test.skip=true java -jar ../javarepl/javarepl.jar Now, after executing java -jar ../javarepl/javarepl.jar we are in a Java Shell(REPL), this enable us to execute Java code in an interactive way without wasting time and patience in the slow path(code â€“\u003e build â€“\u003e execute â€“\u003e add print â€“\u003e build), isnâ€™t that cool? Like this: root@a2e26ba62bb6:/javarepl/nebula-java-3.0.0# java -jar ../javarepl/javarepl.jar Welcome to JavaREPL version 428 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111) Type expression to evaluate, :help for more options or press tab to auto-complete. Connected to local instance at http://localhost:43707 java\u003e System.out.println(\"Hello, World!\"); Hello, World! java\u003e Now we are in the java REPL, letâ€™s introduce all the class path needed and do the imports in one go: :cp /javarepl/nebula-java-3.0.0/client/target/client-3.0.0.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/fastjson-1.2.78.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/slf4j-api-1.7.25.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/slf4j-log4j12-1.7.25.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/commons-pool2-2.2.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/log4j-1.2.17.jar import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.vesoft.nebula.ErrorCode; import com.vesoft.nebula.client.graph.NebulaPoolConfig; import com.vesoft.nebula.client.graph.data.CASignedSSLParam; import com.vesoft.nebula.client.graph.data.HostAddress; import com.vesoft.nebula.client.graph.data.ResultSet; import com.vesoft.nebula.client.graph.data.SelfSignedSSLParam; import com.vesoft.nebula.client.graph.data.ValueWrapper; import com.vesoft.nebula.client.graph.net.NebulaPool; import com.vesoft.nebula.client.graph.net.Session; import java.io.UnsupportedEncodingException; import java.util.Arrays; import java.util.List; import java.util.concurrent.TimeUnit; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.*; And letâ€™s connect it to the nebula graph, please replace your graphD IP and Port here, and execute them under the propmt string of java\u003e: NebulaPoolConfig nebulaPoolConfig = new NebulaPoolConfig(); nebulaPoolConfig.setMaxConnSize(10); List\u003cHostAddress\u003e addresses = Arrays.asList(new HostAddress(\"192.168.8.127\", 9669)); NebulaPool pool = new NebulaPool(); pool.init(addresses, nebulaPoolConfig); Session session = pool.getSession(\"root\", \"nebula\", false); ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:1:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#prepare-for-the-java-repl"},{"categories":["Nebula Graph"],"content":" 2 The execute for ResultSetFirst letâ€™s check what we can do with a simple query: ResultSet resp = session.execute(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); Now you could play with it: Reference: client/graph/data/ResultSet.java java\u003e resp.isSucceeded() java.lang.Boolean res9 = true java\u003e resp.rowsSize() java.lang.Integer res16 = 1 java\u003e rows = resp.getRows() java.util.ArrayList rows = [Row ( values : [ \u003cValue vVal:Vertex ( vid : \u003cValue sVal:70 6c 61 79 65 72 31 30 30\u003e, tags : [ Tag ( name : 70 6C 61 79 65 72, props : { [B@5264a468 : \u003cValue iVal:42\u003e [B@496b8e10 : \u003cValue sVal:54 69 6d 20 44 75 6e 63 61 6e\u003e } ) ] )\u003e ] )] java\u003e row0 = resp.rowValues(0) java.lang.Iterable\u003ccom.vesoft.nebula.client.graph.data.ValueWrapper\u003e res10 = ColumnName: [n], Values: [(\"player100\" :player {name: \"Tim Duncan\", age: 42})] Remember our item is actually a vertex? (root@nebula) [basketballplayer]\u003e match (n:player) WHERE n.name == \"Tim Duncan\" return n +----------------------------------------------------+ | n | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2116/44373 us) Letâ€™s see what(methods) can be done towards a value? v = Class.forName(\"com.vesoft.nebula.Value\") v.getDeclaredMethods() We could tell itâ€™s quite Primitive on what com.vesoft.nebula.Value provided, thus we should use the ValueWrapper(or use executeJson actually) instead. To get a row of the result via iteration(as its a java iterable), we just follow how the example looped the result: import java.util.ArrayList; import java.util.List; List\u003cValueWrapper\u003e wrappedValueList = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c resp.rowsSize(); i++) { ResultSet.Record record = resp.rowValues(i); for (ValueWrapper value : record.values()) { wrappedValueList.add(value); if (value.isLong()) { System.out.printf(\"%15s |\", value.asLong()); } if (value.isBoolean()) { System.out.printf(\"%15s |\", value.asBoolean()); } if (value.isDouble()) { System.out.printf(\"%15s |\", value.asDouble()); } if (value.isString()) { System.out.printf(\"%15s |\", value.asString()); } if (value.isTime()) { System.out.printf(\"%15s |\", value.asTime()); } if (value.isDate()) { System.out.printf(\"%15s |\", value.asDate()); } if (value.isDateTime()) { System.out.printf(\"%15s |\", value.asDateTime()); } if (value.isVertex()) { System.out.printf(\"%15s |\", value.asNode()); } if (value.isEdge()) { System.out.printf(\"%15s |\", value.asRelationship()); } if (value.isPath()) { System.out.printf(\"%15s |\", value.asPath()); } if (value.isList()) { System.out.printf(\"%15s |\", value.asList()); } if (value.isSet()) { System.out.printf(\"%15s |\", value.asSet()); } if (value.isMap()) { System.out.printf(\"%15s |\", value.asMap()); } } System.out.println(); } As shown in above, the result value/item could be in properties string/int etcâ€¦ or in graph semantic vertex, edge, path, we should use correspond asXxxx methods: java\u003e v = wrappedValueList.get(0) com.vesoft.nebula.client.graph.data.ValueWrapper v = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e v.asNode() com.vesoft.nebula.client.graph.data.Node res16 = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e node = v.asNode() com.vesoft.nebula.client.graph.data.Node node = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) Btw, itâ€™s also possible to play with it with reflections(we imported already): Of courese we could also check via client/graph/data/ResultSet.java java\u003e rClass=Class.forName(\"com.vesoft.nebula.client.graph.data.ResultSet\") java.lang.Class r = class com.vesoft.nebula.client.graph.data.ResultSet java\u003e rClass.getDeclaredMethods() java.lang.reflect.Method[] res20 = [public java.util.List com.vesoft.nebula.client.graph.data.ResultSet.getColumnNames(), public int com.vesoft.nebula.client.graph.data.ResultSet.rowsSize(), public com.vesoft.nebula.client.graph.data.ResultSet$Record com.vesoft.nebula.clie","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:2:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-execute-for-resultset"},{"categories":["Nebula Graph"],"content":" 3 The executeJson methodSince 2.6, nebule finally supports json string response and we could do this: java\u003e String resp_json = session.executeJson(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); java.lang.String resp_json = \" { \"errors\":[ { \"code\":0 } ], \"results\":[ { \"spaceName\":\"basketballplayer\", \"data\":[ { \"meta\":[ { \"type\":\"vertex\", \"id\":\"player100\" } ], \"row\":[ { \"player.age\":42, \"player.name\":\"Tim Duncan\" } ] } ], \"columns\":[ \"n\" ], \"errors\":{ \"code\":0 }, \"latencyInUs\":4761 } ] } \" java\u003e And I believe you know much better than I do with it. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:3:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-executejson-method"},{"categories":["Nebula Graph"],"content":" 4 Conclusion If we go with JSON response, itâ€™ll be easier for you to have everything parsed. If we have to deal with resultSet object, just use the ValueWrapper asNode() if the value is a vertex, use asRelationship if value is an edge and asPath() if the value is a path. With the REPL tool shown together with java reflection and source code, itâ€™s possbile to inspect on how data could be parsed. Happy Graphing! Picture Creditï¼šleunesmedia ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:4:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#conclusion"},{"categories":["Nebula Graph"],"content":"Dialog System With Graph Database Backed Knowledge Graph. åŸºäºå›¾æ•°æ®åº“çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹","date":"2021-09-18","objectID":"/en/nebula-siwi/","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/"},{"categories":["Nebula Graph"],"content":" a PoC of Dialog System With Graph Database Backed Knowledge Graph. Related GitHub Repo: https://github.com/wey-gu/nebula-siwi/ I created the Katacoda Interactive Env for this project ğŸ‘‰ğŸ» https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#"},{"categories":["Nebula Graph"],"content":" Siwi the voice assistantSiwi (/ËˆsÉªwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. For now, itâ€™s a demo for task-driven(not general purpose) dialog bots with KG(Knowledge Graph) leveraging Nebula Graph with the minimal/sample dataset from Nebula Graph Manual/ NGä¸­æ–‡æ‰‹å†Œ. Tips: Now you can play with the graph online without installing yourself! Nebula Playground | Nebula Playground - China Mainland Supported queries: relation: What is the relationship between Yao Ming and Lakers? How does Yao Ming and Lakers connected? serving: Which team had Yao Ming served? friendship: Whom does Tim Duncan follow? Who are Yao Mingâ€™s friends? ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#siwi-the-voice-assistant"},{"categories":["Nebula Graph"],"content":" 1 Deploy and TryTBD (leveraging docker and nebula-up) ","date":"2021-09-18","objectID":"/en/nebula-siwi/:1:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#deploy-and-try"},{"categories":["Nebula Graph"],"content":" 2 How does it work?This is one of the most naive pipeline for a specific domain/ single purpose chat bot built on a Knowledge Graph. ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#how-does-it-work"},{"categories":["Nebula Graph"],"content":" 2.1 Backend The Backend(Siwi API) is a Flask based API server: Flask API server takes questions in HTTP POST, and calls the bot API. In bot API part there are classfier(Symentic Parsing, Intent Matching, Slot Filling), and question actors(Call corresponding actions to query Knowledge Graph with intents and slots). Knowledge Graph is built on an Open-Source Graph Database: Nebula Graph ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend"},{"categories":["Nebula Graph"],"content":" 2.2 Frontend The Frontend is a VueJS Single Page Applicaiton(SPA): I reused a Vue Bot UI to showcase a chat window in this human-agent interaction, typing is supported. In addtion, leverating Chromeâ€™s Web Speech API, a button to listen to human voice is introduced ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend"},{"categories":["Nebula Graph"],"content":" 2.3 A Query Flow â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ Speech â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Frontend â”‚ Siwi, /ËˆsÉªwi/ â”‚ â”‚ â”‚ Web_Speech_API â”‚ A PoC of â”‚ â”‚ â”‚ â”‚ Dialog System â”‚ â”‚ â”‚ Vue.JS â”‚ With Graph Database â”‚ â”‚ â”‚ â”‚ Backed Knowledge Graph â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ Sentence â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Backend â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ Web API, Flask â”‚ ./app/ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ Sentence ./bot/ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Intent matching, â”‚ ./bot/classifierâ”‚ â”‚ â”‚ â”‚ â”‚ Symentic Processing â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ Intent, Entities â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Intent Actor â”‚ ./bot/actions â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ Graph Query â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Graph Database â”‚ Nebula Graph â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:3","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#a-query-flow"},{"categories":["Nebula Graph"],"content":" 2.4 Source Code Tree . â”œâ”€â”€ README.md â”œâ”€â”€ src â”‚ â”œâ”€â”€ siwi # Siwi-API Backend â”‚ â”‚ â”œâ”€â”€ app # Web Server, take HTTP requests and calls Bot API â”‚ â”‚ â””â”€â”€ bot # Bot API â”‚ â”‚ â”œâ”€â”€ actions # Take Intent, Slots, Query Knowledge Graph here â”‚ â”‚ â”œâ”€â”€ bot # Entrypoint of the Bot API â”‚ â”‚ â”œâ”€â”€ classifier # Symentic Parsing, Intent Matching, Slot Filling â”‚ â”‚ â””â”€â”€ test # Example Data Source as equivalent/mocked module â”‚ â””â”€â”€ siwi_frontend # Browser End â”‚ â”œâ”€â”€ README.md â”‚ â”œâ”€â”€ package.json â”‚ â””â”€â”€ src â”‚ â”œâ”€â”€ App.vue # Listening to user and pass Questions to Siwi-API â”‚ â””â”€â”€ main.js â””â”€â”€ wsgi.py ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:4","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#source-code-tree"},{"categories":["Nebula Graph"],"content":" 3 Manually Run Components","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#manually-run-components"},{"categories":["Nebula Graph"],"content":" 3.1 BackendInstall and run. # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 For OpenFunction/ KNative docker build -t weygu/siwi-api . docker run --rm --name siwi-api \\ --env=PORT=5000 \\ --env=NG_ENDPOINTS=127.0.0.1:9669 \\ --net=host \\ weygu/siwi-api Try it out Web API: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"question\": \"What is the relationship between Yao Ming and Lakers?\"}' \\ http://192.168.8.128:5000/query | jq { \"answer\": \"There are at least 23 relations between Yao Ming and Lakers, one relation path is: Yao Ming follows Shaquille O'Neal serves Lakers.\" } Call Bot Python API: from nebula2.gclient.net import ConnectionPool from nebula2.Config import Config # define a config config = Config() config.max_connection_pool_size = 10 # init connection pool connection_pool = ConnectionPool() # if the given servers are ok, return true, else return false ok = connection_pool.init([('127.0.0.1', 9669)], config) # import siwi bot from siwi.bot import bot # instantiate a bot b = bot.SiwiBot(connection_pool) # make the question query b.query(\"Which team had Jonathon Simmons served?\") Then a response will be like this: In [4]: b.query(\"Which team had Jonathon Simmons serv ...: ed?\") [DEBUG] ServeAction intent: {'entities': {'Jonathon Simmons': 'player'}, 'intents': ('serve',)} [DEBUG] query for RelationshipAction: USE basketballplayer; MATCH p=(v)-[e:serve*1]-\u003e(v1) WHERE id(v) == \"player112\" RETURN p LIMIT 100; [2021-07-02 02:59:36,392]:Get connection to ('127.0.0.1', 9669) Out[4]: 'Jonathon Simmons had served 3 teams. Spurs from 2015 to 2015; 76ers from 2019 to 2019; Magic from 2017 to 2017; ' ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-1"},{"categories":["Nebula Graph"],"content":" 3.2 FrontendReferring to siwi_frontend ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-1"},{"categories":["Nebula Graph"],"content":" 4 Further work Use NBA-API to fallback undefined pattern questions Wrap and manage sessions instead of get and release session per request, this is somehow costly actually. Use NLP methods to implement proper Symentic Parsing, Intent Matching, Slot Filling Build Graph to help with Intent Matching, especially for a general purpose bot Use larger Dataset i.e. from wyattowalsh/basketball ","date":"2021-09-18","objectID":"/en/nebula-siwi/:4:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#further-work"},{"categories":["Nebula Graph"],"content":" 5 Thanks to Upstream Projects â¤ï¸","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":" 5.1 Backend I learnt a lot from the KGQA on MedicalKG created by Huanyong Liu Flask pyahocorasick created by Wojciech MuÅ‚a PyYaml ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-2"},{"categories":["Nebula Graph"],"content":" 5.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design Image credit goes to https://unsplash.com/photos/0E_vhMVqL9g ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","å›¾æ•°æ®åº“åº”ç”¨","æ™ºèƒ½åŠ©æ‰‹"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-2"},{"categories":["Nebula Graph"],"content":"Setup Nebula Graph Dev Env with CLion and Docker æ­å»ºåŸºäº Docker çš„ Nebula Graph CLion å¼€å‘ç¯å¢ƒ","date":"2021-09-18","objectID":"/en/nebula-clion/","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/"},{"categories":["Nebula Graph"],"content":" ä¹‹å‰å¡æ¯”åŒå­¦å‘æˆ‘å’¨è¯¢æ­å»º CLion ç¯å¢ƒï¼Œå¼€å‘ Nebula çš„ä¸€äº›é—®é¢˜ï¼Œæˆ‘åšäº†ä¸€äº›å·¥ä½œæ–¹ä¾¿åˆ©ç”¨ Docker åœ¨æœ¬åœ°æ­å»ºè¿™æ ·ä¸€ä¸ªç¯å¢ƒï¼Œç›¸å…³çš„ä¸œè¥¿æ”¾åœ¨ï¼šhttps://github.com/wey-gu/nebula-dev-CLion ã€‚ Related GitHub Repo: https://github.com/wey-gu/nebula-dev-CLion ","date":"2021-09-18","objectID":"/en/nebula-clion/:0:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#"},{"categories":["Nebula Graph"],"content":" 1 Run Docker Env for Nebula-Graph with CLionBuild Docker Image git clone https://github.com/wey-gu/nebula-dev-CLion.git cd nebula-dev-CLion docker build -t wey/nebula-dev-clion:v2.0 . Run Docker Container for Nebula-Dev with CLion Integration Readiness(actually mostly Rsync \u0026 SSH). cd \u003cnebula-graph-repo-you-worked-on\u003e export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker run --rm -d \\ --name nebula-dev \\ --security-opt seccomp=unconfined \\ -p 2222:22 -p 2873:873 --cap-add=ALL \\ -v $PWD:/home/nebula \\ -w /home/nebula \\ wey/nebula-dev-clion:v2.0 Verify cmake with SSH. The default password is password ssh -o StrictHostKeyChecking=no root@localhost -p 2222 # in docker cd /home/nebula mkdir build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. Access container w/o SSH. docker exec -it nebula-dev bash mkdir -p build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. ","date":"2021-09-18","objectID":"/en/nebula-clion/:1:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#run-docker-env-for-nebula-graph-with-clion"},{"categories":["Nebula Graph"],"content":" 2 Configurations in CLion Ref: https://www.jetbrains.com/help/clion/clion-toolchains-in-docker.html#build-and-run Toolchains Add a remote host root@localhost:2222 password Put /opt/vesoft/toolset/cmake/bin/cmake as CMake CMake Toochain: Select the one created in last step Build directory: /home/nebula/build ","date":"2021-09-18","objectID":"/en/nebula-clion/:2:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#configurations-in-clion"},{"categories":["Nebula Graph"],"content":" 3 The appendix","date":"2021-09-18","objectID":"/en/nebula-clion/:3:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#the-appendix"},{"categories":["Nebula Graph"],"content":" 3.1 References of CMake output: [root@4c98e3f77ce8 build]# cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. \u003e\u003e\u003e\u003e Options of Nebula Graph \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_BUILD_STORAGE : OFF (Whether to build storage) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_MODULE_FORCE_CHECKOUT : ON (Whether checkout branch of module to same as graph.) -- ENABLE_MODULE_UPDATE : OFF (Automatically update module) -- ENABLE_PACK_ONE : ON (Whether to package into one) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_VERBOSE_BISON : OFF (Enable Bison to report state) -- ENABLE_WERROR : ON (Regard warnings as errors) -- CMAKE_BUILD_TYPE : Release (Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...) -- CMAKE_INSTALL_PREFIX : /usr/local/nebula (Install path prefix, prepended onto install directories.) -- CMAKE_CXX_STANDARD : 17 -- CMAKE_CXX_COMPILER : /opt/vesoft/toolset/clang/9.0.0/bin/c++ (CXX compiler) -- CMAKE_CXX_COMPILER_ID : GNU -- NEBULA_USE_LINKER : bfd -- CCACHE_DIR : /root/.ccache \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' \u003c\u003c\u003c\u003c -- NEBULA_THIRDPARTY_ROOT : /opt/vesoft/third-party/2.0 -- Build info of nebula third party: Package : Nebula Third Party Version : 2.0 Date : Mon Jun 28 15:07:38 UTC 2021 glibc : 2.17 Arch : x86_64 Compiler : GCC 9.2.0 C++ ABI : 11 Vendor : VEsoft Inc. -- CMAKE_INCLUDE_PATH : /opt/vesoft/third-party/2.0/include -- CMAKE_LIBRARY_PATH : /opt/vesoft/third-party/2.0/lib64;/opt/vesoft/third-party/2.0/lib -- CMAKE_PROGRAM_PATH : /opt/vesoft/third-party/2.0/bin -- GLIBC_VERSION : 2.17 -- found krb5-config here /opt/vesoft/third-party/2.0/bin/krb5-config -- Found kerberos 5 headers: /opt/vesoft/third-party/2.0/include -- Found kerberos 5 libs: /opt/vesoft/third-party/2.0/lib/libgssapi_krb5.a;/opt/vesoft/third-party/2.0/lib/libkrb5.a;/opt/vesoft/third-party/2.0/lib/libk5crypto.a;/opt/vesoft/third-party/2.0/lib/libcom_err.a;/opt/vesoft/third-party/2.0/lib/libkrb5support.a \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' done \u003c\u003c\u003c\u003c -- Create the pre-commit hook -- Creating pre-commit hook done \u003e\u003e\u003e\u003e Configuring Nebula Common \u003c\u003c\u003c\u003c \u003e\u003e\u003e\u003e Options of Nebula Common \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_WERROR : ON (Regard warnings as errors) -- Set D_GLIBCXX_USE_CXX11_ABI to 1 -- CMAKE_BUI","date":"2021-09-18","objectID":"/en/nebula-clion/:3:1","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#references-of-cmake-output"},{"categories":["courses"],"content":"Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database.","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/"},{"categories":["courses"],"content":"Walk you through in actions to do below sections exercises! Bootstrap a Nebula Graph Cluster and Studio Web App Import a graph of dataset about shareholding Exploring the shareholding data with Nebula Importer Visually Exploring the shareholding data with Nebula Studio Run Graph Algorithm on Nebula Cluster Graph Data The dataset comes from https://github.com/wey-gu/nebula-shareholding-example/tree/main/data_sample The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/:0:0","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/#"},{"categories":["courses"],"content":"Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s.","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/"},{"categories":["courses"],"content":"A full solution walkthrough for a Knowledge Graph Dialog System. Boostrap a Nebula Cluster in K8s Scale out the Nebula Cluster in K8s way Import the basketballplayer Dataset Siwi, the Knowledge Graph Dialog System with Nebula Graph Siwi (/ËˆsÉªwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. The code of Siwi is here: https://github.com/wey-gu/nebula-siwi. The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/:0:0","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/#"},{"categories":["Nebula Graph"],"content":"A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. å›¾æ•°æ®åº“åº”ç”¨ç¤ºä¾‹ï¼šè‚¡æƒå…³ç³»ç©¿é€","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/"},{"categories":["Nebula Graph"],"content":" A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. å›¾æ•°æ®åº“åº”ç”¨ç¤ºä¾‹ï¼šè‚¡æƒå…³ç³»ç©¿é€ Related GitHub Repo: https://github.com/wey-gu/nebula-shareholding-example æ›´æ–°ï¼šåœ¨è¿™ä¸ªæ•°æ®é›†ç”Ÿæˆçš„å·¥ä½œåŸºç¡€ä¸Šï¼Œæˆ‘åˆåšäº†ä¸€ä¸ªå…¨æ ˆç¤ºä¾‹é¡¹ç›® ğŸ‘‰ğŸ» https://siwei.io/corp-rel-graph/ I created the Katacoda Interactive Env for this project ğŸ‘‰ğŸ» https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ This is a demo of Shareholding Relationship Analysis with Distributed open-source Graph Database: Nebula Graph. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:0:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#"},{"categories":["Nebula Graph"],"content":" 1 Data","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data"},{"categories":["Nebula Graph"],"content":" 1.1 Data Modeling There are various kinds of relationships when we checking companiesâ€™ shareholding breakthrough, here letâ€™s simplify it with only two kind of entities: person and corp, and with following relationship types. person can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp Below is the lines to reflect this graph modele in Nebula Graph, itâ€™s quite straightforward, right? CREATE TAG person(name string); CREATE TAG corp(name string); CREATE EDGE role_as(role string); CREATE EDGE is_branch_of(); CREATE EDGE hold_share(share float); CREATE EDGE reletive_with(degree int); ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-modeling"},{"categories":["Nebula Graph"],"content":" 1.2 Data GenerationWe just randomly generate some data to help with this demo, you can call data_generator.py directly to generate or reuse whatâ€™s already done under data_sample folder. The generated data are records to be fit in above data model from below .csv files. $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:2","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-generation"},{"categories":["Nebula Graph"],"content":" 1.3 Data ImportWith those data in .csv files, we can easily import them into a Nebula Graph Cluster with the help of Nebula-Importer. nebula-importer.yaml in this repo describes rules and configurations on how this import will be done by the importer. For Nebula Graph Database, plesae refer to Doc , Doc-CN to deploy on any Linux Servers, for study and test, you can run it via Docker following the Quick Start Chapter of the documentation. For Nebula-Importer, if you already have Docker env, you can run it as the following without installing anything. Or, if you prefer to install it, itâ€™s quite easy as itâ€™s written in Golang and you can run its single file binary quite easily, go check both Documentation and Nebula-Importer Repo: https://github.com/vesoft-inc/nebula-importer. Letâ€™s start! Below is the commands I used to import our data into a Nebula Graph Database. # put generated data \u0026 nebula-importor.yaml to nebula-importer server $ scp -r data nebula_graph_host:~ $ scp nebula-importer.yaml data nebula_graph_host:~/data $ ssh nebula_graph_host $ ls -l ${HOME}/data total 756 -rw-r--r--. 1 wei.gu wei.gu 23941 Jul 14 05:44 corp.csv -rw-r--r--. 1 wei.gu wei.gu 1277 Jul 14 05:44 corp_rel.csv -rw-r--r--. 1 wei.gu wei.gu 3048 Jul 14 05:44 corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 3893 Jul 14 05:44 nebula-importer.yaml -rw-r--r--. 1 wei.gu wei.gu 211661 Jul 14 05:44 person.csv -rw-r--r--. 1 wei.gu wei.gu 179770 Jul 14 05:44 person_corp_role.csv -rw-r--r--. 1 wei.gu wei.gu 322965 Jul 14 05:44 person_corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 17689 Jul 14 05:44 person_rel.csv # import data into our nebula graph database $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${HOME}/data/nebula-importer.yaml:/root/nebula-importer.yaml \\ -v ${HOME}/data:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-importer.yaml 2021/07/14 05:49:32 --- START OF NEBULA IMPORTER --- 2021/07/14 05:49:32 [WARN] config.go:491: Not set files[0].schema.vertex.vid.Type, reset to default value `string' ... 2021/07/14 05:49:43 [INFO] reader.go:180: Total lines of file(/root/person_corp_role.csv) is: 5000, error lines: 1287 2021/07/14 05:49:43 [INFO] statsmgr.go:61: Done(/root/person_corp_role.csv): Time(11.39s), Finished(12523), Failed(0), Latency AVG(1514us), Batches Req AVG(1824us), Rows AVG(1099.43/s) 2021/07/14 05:49:47 [INFO] statsmgr.go:61: Tick: Time(15.00s), Finished(25807), Failed(0), Latency AVG(1500us), Batches Req AVG(1805us), Rows AVG(1720.46/s) 2021/07/14 05:49:48 [INFO] reader.go:180: Total lines of file(/root/person.csv) is: 10000, error lines: 0 2021/07/14 05:49:48 [INFO] statsmgr.go:61: Done(/root/person.csv): Time(16.10s), Finished(29731), Failed(0), Latency AVG(1505us), Batches Req AVG(1810us), Rows AVG(1847.17/s) 2021/07/14 05:49:50 [INFO] reader.go:180: Total lines of file(/root/person_corp_share.csv) is: 20000, error lines: 0 2021/07/14 05:49:50 [INFO] statsmgr.go:61: Done(/root/person_corp_share.csv): Time(17.74s), Finished(36013), Failed(0), Latency AVG(1531us), Batches Req AVG(1844us), Rows AVG(2030.29/s) 2021/07/14 05:49:50 Finish import data, consume time: 18.25s 2021/07/14 05:49:51 --- END OF NEBULA IMPORTER --- ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:3","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-import"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the ğŸ‘ï¸ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#corporation-sharehold-relationship-breakthrough"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the ğŸ‘ï¸ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#query-in-ngql"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the ğŸ‘ï¸ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#in-a-visual-way"},{"categories":["Nebula Graph"],"content":" 3 Thanks to Upstream Projects â¤ï¸ Python Faker https://github.com/joke2k/faker/ pydbgen https://github.com/tirthajyoti/pydbgen Nebula Graph https://github.com/vesoft-inc/nebula-graph ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":" 3.1 Tips: You can deploy nebula graph in one line with: Nebula-UP, it helps install a nebula graph with Docker Nebula-operator-KIND , it helps setup all dependencies of Nebula-K8s-Operator including a K8s in Docker, PV Provider and then install a Nebula Graph with Nebula-Operator in K8s. Image Credit goes to https://unsplash.com/photos/3fPXt37X6UQ ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","å›¾æ•°æ®åº“åº”ç”¨","è‚¡æƒç©¿é€"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#tips"},{"categories":null,"content":" DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... K8s Community Day 2021 Openfunction + GraphDB Read more... COScon 2021 æˆ‘çš„å¼€æºä¹‹è·¯ Read more... PyCon China 2021 å›¾æ•°æ®åº“è§£è°œä¸ Python çš„å›¾åº“åº”ç”¨å®è·µ Read more... nMeetup: Nebula åº”ç”¨ä¸Šæ‰‹å®æ“ ä»å¤´å®æ“ Nebula çš„éƒ¨ç½²ï¼Œè‚¡æƒç©¿é€ï¼Œå›¾ç®—æ³•è¿ç®—ï¼Œè¯­éŸ³æ™ºèƒ½åŠ©æ‰‹ã€‚ Read more... How to Train your Dragon å¦‚ä½•æˆä¸ºå¼€æºå¼€å‘è€…ï¼ˆå¸ƒé“å¸ˆï¼‰ã€‚ Read more... ","date":"2021-08-26","objectID":"/en/talk/:0:0","series":null,"tags":null,"title":"My Talks","uri":"/en/talk/#"},{"categories":["Nebula Graph"],"content":"Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm å¯¼å…¥ Livejournal æ•°æ®é›†åˆ° Nebula å¹¶è¿è¡Œ Nebula Algorithm å›¾ç®—æ³•","date":"2021-08-24","objectID":"/en/nebula-livejournal/","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/"},{"categories":["Nebula Graph"],"content":" ä¸€ä¸ªå¯¼å…¥ Livejournal æ•°æ®é›†åˆ° Nebula Graph å›¾æ•°æ®åº“ï¼Œå¹¶æ‰§è¡Œ Nebula Algorithm å›¾ç®—æ³•çš„è¿‡ç¨‹åˆ†äº«ã€‚ Related GitHub Repo: https://github.com/wey-gu/nebula-LiveJournal ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#"},{"categories":["Nebula Graph"],"content":" nebula-LiveJournalLiveJournal Dataset is a Social Network Dataset in one file with two columns(FromNodeId, ToNodeId). $ head soc-LiveJournal1.txt # Directed graph (each unordered pair of nodes is saved once): soc-LiveJournal1.txt # Directed LiveJournal friednship social network # Nodes: 4847571 Edges: 68993773 # FromNodeId ToNodeId 0 1 0 2 0 3 0 4 0 5 0 6 It could be accessed in https://snap.stanford.edu/data/soc-LiveJournal1.html. Dataset statistics Nodes 4847571 Edges 68993773 Nodes in largest WCC 4843953 (0.999) Edges in largest WCC 68983820 (1.000) Nodes in largest SCC 3828682 (0.790) Edges in largest SCC 65825429 (0.954) Average clustering coefficient 0.2742 Number of triangles 285730264 Fraction of closed triangles 0.04266 Diameter (longest shortest path) 16 90-percentile effective diameter 6.5 ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#nebula-livejournal"},{"categories":["Nebula Graph"],"content":" 1 Dataset Download and Preprocessing","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#dataset-download-and-preprocessing"},{"categories":["Nebula Graph"],"content":" 1.1 DownloadIt is accesissiable from the official web page: $ cd nebula-livejournal/data $ wget https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz Comments in data file should be removed to make the data import tool happy. ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#download"},{"categories":["Nebula Graph"],"content":" 1.2 Preprocessing $ gzip -d soc-LiveJournal1.txt.gz $ sed -i '1,4d' soc-LiveJournal1.txt ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#preprocessing"},{"categories":["Nebula Graph"],"content":" 2 Import dataset to Nebula Graph","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#import-dataset-to-nebula-graph"},{"categories":["Nebula Graph"],"content":" 2.1 With Nebula ImporterNebula-Importer is a Golang Headless import tool for Nebula Graph. You may need to edit the config file under nebula-importer/importer.yaml on Nebula Graphâ€™s address and credentialã€‚ Then, Nebula-Importer could be called in Docker as follow: $ cd nebula-livejournal $ docker run --rm -ti \\ --network=nebula-net \\ -v nebula-importer/importer.yaml:/root/importer.yaml \\ -v data/:/root \\ vesoft/nebula-importer:v2 \\ --config /root/importer.yaml Or if you have the binary nebula-importer locally: $ cd data $ \u003cpath_to_nebula-importer_binary\u003e --config ../nebula-importer/importer.yaml ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-importer"},{"categories":["Nebula Graph"],"content":" 2.2 With Nebula ExchangeNebula-Exchange is a Spark Application to enable batch and streaming data import from multiple data sources to Nebula Graph. To be done. (You can refer to https://siwei.io/nebula-exchange-sst-2.x/) ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-exchange"},{"categories":["Nebula Graph"],"content":" 3 Run Algorithms with Nebula GraphNebula-Algorithm is a Spark/GraphX Application to run Graph Algorithms with data consumed from files or a Nebula Graph Cluster. Supported Algorithms for now: Name Use Case PageRank page ranking, important node digging Louvain community digging, hierarchical clustering KCore community detection, financial risk control LabelPropagation community detection, consultation propagation, advertising recommendation ConnectedComponent community detection, isolated island detection StronglyConnectedComponent community detection ShortestPath path plan, network plan TriangleCount network structure analysis BetweennessCentrality important node digging, node influence calculation DegreeStatic graph structure analysis ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms-with-nebula-graph"},{"categories":["Nebula Graph"],"content":" 3.1 Ad-hoc Spark Env setupHere I assume the Nebula Graph was bootstraped with Nebula-Up, thus nebula is running in a Docker Network named nebula-docker-compose_nebula-net. Then letâ€™s start a single server spark: docker run --name spark-master --network nebula-docker-compose_nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ -v nebula-algorithm/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 Thus we could make spark application submt inside this container: docker exec -it spark-master bash cd /root/ # download Nebula-Algorithm Jar Packagem, 2.0.0 for example, for other versions, refer to nebula-algorithm github repo and documentations. wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.0.0/nebula-algorithm-2.0.0.jar ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#ad-hoc-spark-env-setup"},{"categories":["Nebula Graph"],"content":" 3.2 Run AlgorithmsThere are many altorithms supported by Nebula-Algorithm, here some of their configuration files were put under nebula-algorithm as an example. Before using them, please first edit and change Nebula Graph Cluster Addresses and credentials. vim nebula-altorithm/algo-pagerank.conf Then we could enter the spark container and call corresponding algorithms as follow. Please adjust your --driver-memeory accordingly, i.e. pagerank altorithm: /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 16g nebula-algorithm-2.0.0.jar \\ -p pagerank.conf After the algorithm finished, the output will be under the path insdie the container defined in conf file: write:{ resultPath:/output/ } é¢˜å›¾ç‰ˆæƒï¼š@sigmund ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms"},{"categories":["Nebula Graph"],"content":"è¿™ç¯‡æ–‡ç« å¸¦å¤§å®¶ä»¥æœ€å°æ–¹å¼ï¼Œå¿«é€Ÿè¶Ÿä¸€ä¸‹ Nebula Exchange ä¸­ SST å†™å…¥æ–¹å¼çš„æ­¥éª¤ã€‚","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/"},{"categories":["Nebula Graph"],"content":"è¿™ç¯‡æ–‡ç« å¸¦å¤§å®¶ä»¥æœ€å°æ–¹å¼ï¼Œå¿«é€Ÿè¶Ÿä¸€ä¸‹ Nebula Exchange ä¸­ SST å†™å…¥æ–¹å¼çš„æ­¥éª¤ã€‚ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:0:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#"},{"categories":["Nebula Graph"],"content":" 1 ä»€ä¹ˆæ˜¯ Nebula Exchange ?ä¹‹å‰æˆ‘åœ¨ Nebula Data Import Options ä¹‹ä¸­ä»‹ç»è¿‡ï¼ŒNebula Exchange æ˜¯ä¸€ä¸ª Nebula Graph ç¤¾åŒºå¼€æºçš„ Spark Applicaitonï¼Œå®ƒä¸“é—¨ç”¨æ¥æ”¯æŒæ‰¹é‡æˆ–è€…æµå¼åœ°æŠŠæ•°æ®å¯¼å…¥ Nebula Graph Database ä¹‹ä¸­ã€‚ Nebula Exchange æ”¯æŒå¤šç§å¤šæ ·çš„æ•°æ®æºï¼ˆä» Apache Parquet, ORC, JSON, CSV, HBase, Hive MaxCompute åˆ° Neo4j, MySQL, ClickHouse, å†æœ‰ Kafka, Pulsarï¼Œæ›´å¤šçš„æ•°æ®æºä¹Ÿåœ¨ä¸æ–­å¢åŠ ä¹‹ä¸­ï¼‰ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨ Exchange å†…éƒ¨ï¼Œä»é™¤äº†ä¸åŒ Reader å¯ä»¥è¯»å–ä¸åŒæ•°æ®æºä¹‹å¤–ï¼Œåœ¨æ•°æ®ç»è¿‡ Processor å¤„ç†ä¹‹åé€šè¿‡ Writerå†™å…¥ï¼ˆsinkï¼‰ Nebula Graph å›¾æ•°æ®åº“çš„æ—¶å€™ï¼Œé™¤äº†èµ°æ­£å¸¸çš„ ServerBaseWriter çš„å†™å…¥æµç¨‹ä¹‹å¤–ï¼Œå®ƒè¿˜å¯ä»¥ç»•è¿‡æ•´ä¸ªå†™å…¥æµç¨‹ï¼Œåˆ©ç”¨ Spark çš„è®¡ç®—èƒ½åŠ›å¹¶è¡Œç”Ÿæˆåº•å±‚ RocksDB çš„ SST æ–‡ä»¶ï¼Œä»è€Œå®ç°è¶…é«˜æ€§èƒ½çš„æ•°æ®å¯¼å…¥ï¼Œè¿™ä¸ª SST æ–‡ä»¶å¯¼å…¥çš„åœºæ™¯å°±æ˜¯æœ¬æ–‡å¸¦å¤§å®¶ä¸Šæ‰‹ç†Ÿæ‚‰çš„éƒ¨åˆ†ã€‚ è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…ï¼šNebula Graph æ‰‹å†Œ:ä»€ä¹ˆæ˜¯ Nebula Exchange Nebula Graph å®˜æ–¹åšå®¢ä¹Ÿæœ‰æ›´å¤š Nebula Exchange çš„å®è·µæ–‡ç«  ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:1:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ä»€ä¹ˆæ˜¯-nebula-exchange-"},{"categories":["Nebula Graph"],"content":" 2 æ­¥éª¤æ¦‚è§‚ å®éªŒç¯å¢ƒ é…ç½® Exchange ç”Ÿæˆ SST æ–‡ä»¶ å†™å…¥ SST æ–‡ä»¶åˆ° Nebula Graph ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:2:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ­¥éª¤æ¦‚è§‚"},{"categories":["Nebula Graph"],"content":" 3 å®éªŒç¯å¢ƒå‡†å¤‡ä¸ºäº†æœ€å°åŒ–ä½¿ç”¨ Nebula Exchange çš„ SST åŠŸèƒ½ï¼Œæˆ‘ä»¬éœ€è¦ï¼š æ­å»ºä¸€ä¸ª Nebula Graph é›†ç¾¤ï¼Œåˆ›å»ºå¯¼å…¥æ•°æ®çš„ Schemaï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ Docker-Compose æ–¹å¼ã€åˆ©ç”¨ Nebula-Up å¿«é€Ÿéƒ¨ç½²ï¼Œå¹¶ç®€å•ä¿®æ”¹å…¶ç½‘ç»œï¼Œä»¥æ–¹ä¾¿åŒæ ·å®¹å™¨åŒ–çš„ Exchange ç¨‹åºå¯¹å…¶è®¿é—®ã€‚ æ­å»ºå®¹å™¨åŒ–çš„ Spark è¿è¡Œç¯å¢ƒ æ­å»ºå®¹å™¨åŒ–çš„ HDFS ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#å®éªŒç¯å¢ƒå‡†å¤‡"},{"categories":["Nebula Graph"],"content":" 3.1 æ­å»º Nebula Graph é›†ç¾¤å€ŸåŠ©äº Nebula-Up æˆ‘ä»¬å¯ä»¥åœ¨ Linux ç¯å¢ƒä¸‹ä¸€é”®éƒ¨ç½²ä¸€å¥— Nebula Graph é›†ç¾¤ï¼š curl -fsSL nebula-up.siwei.io/install.sh | bash å¾…éƒ¨ç½²æˆåŠŸä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç¯å¢ƒåšä¸€äº›ä¿®æ”¹ï¼Œè¿™é‡Œæˆ‘åšçš„ä¿®æ”¹å…¶å®å°±æ˜¯ä¸¤ç‚¹ï¼š åªä¿ç•™ä¸€ä¸ª metaD æœåŠ¡ èµ·ç”¨ Docker çš„å¤–éƒ¨ç½‘ç»œ è¯¦ç»†ä¿®æ”¹çš„éƒ¨åˆ†å‚è€ƒé™„å½•ä¸€ åº”ç”¨ docker-compose çš„ä¿®æ”¹ï¼š cd ~/.nebula-up/nebula-docker-compose vim docker-compose.yaml # å‚è€ƒé™„å½•ä¸€ docker network create nebula-net # éœ€è¦åˆ›å»ºå¤–éƒ¨ç½‘ç»œ docker-compose up -d --remove-orphans ä¹‹åï¼Œæˆ‘ä»¬æ¥åˆ›å»ºè¦æµ‹è¯•çš„å›¾ç©ºé—´ï¼Œå¹¶åˆ›å»ºå›¾çš„ Schemaï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ nebula-console ï¼ŒåŒæ ·ï¼ŒNebula-Up é‡Œè‡ªå¸¦äº†å®¹å™¨åŒ–çš„ nebula-consoleã€‚ è¿›å…¥ Nebula-Console æ‰€åœ¨çš„å®¹å™¨ ~/.nebula-up/console.sh / # åœ¨ console å®¹å™¨é‡Œå‘èµ·é“¾æ¥åˆ°å›¾æ•°æ®åº“ï¼Œå…¶ä¸­ 192.168.x.y æ˜¯æˆ‘æ‰€åœ¨çš„ Linux VM çš„ç¬¬ä¸€ä¸ªç½‘å¡åœ°å€ï¼Œè¯·æ¢æˆæ‚¨çš„ / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! åˆ›å»ºå›¾ç©ºé—´ï¼ˆæˆ‘ä»¬èµ·åå­—å« sst ï¼‰ï¼Œä»¥åŠ schema create space sst(partition_num=5,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 use sst create tag player(name string, age int); ç¤ºä¾‹è¾“å‡º (root@nebula) [(none)]\u003e create space sst(partition_num=5,replica_factor=1,vid_type=fixed_string(32)); Execution succeeded (time spent 1468/1918 us) (root@nebula) [(none)]\u003e :sleep 20 (root@nebula) [(none)]\u003e use sst Execution succeeded (time spent 1253/1566 us) Wed, 18 Aug 2021 08:18:13 UTC (root@nebula) [sst]\u003e create tag player(name string, age int); Execution succeeded (time spent 1312/1735 us) Wed, 18 Aug 2021 08:18:23 UTC ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ­å»º-nebula-graph-é›†ç¾¤"},{"categories":["Nebula Graph"],"content":" 3.2 æ­å»ºå®¹å™¨åŒ–çš„ Spark ç¯å¢ƒåˆ©ç”¨ big data europe åšçš„å·¥ä½œï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸å®¹æ˜“ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š ç°åœ¨çš„ Nebula Exchange å¯¹ Spark çš„ç‰ˆæœ¬æœ‰è¦æ±‚ï¼Œåœ¨ç°åœ¨çš„ 2021 å¹´ 8 æœˆï¼Œæˆ‘æ˜¯ç”¨äº† spark-2.4.5-hadoop-2.7 çš„ç‰ˆæœ¬ã€‚ ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘è®© Spark è¿è¡Œåœ¨ Nebula Graph ç›¸åŒçš„æœºå™¨ä¸Šï¼Œå¹¶ä¸”æŒ‡å®šäº†è¿è¡Œåœ¨åŒä¸€ä¸ª Docker ç½‘ç»œä¸‹ docker run --name spark-master --network nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ bde2020/spark-master:2.4.5-hadoop2.7 ç„¶åï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›å…¥åˆ°ç¯å¢ƒä¸­äº†ï¼š docker exec -it spark-master bash è¿›åˆ° Spark å®¹å™¨ä¸­ä¹‹åï¼Œå¯ä»¥åƒè¿™æ ·å®‰è£… maven: export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn è¿˜å¯ä»¥è¿™æ ·åœ¨å®¹å™¨é‡Œä¸‹è½½ nebula-exchange çš„ jar åŒ…ï¼š cd ~ wget https://repo1.maven.org/maven2/com/vesoft/nebula-exchange/2.1.0/nebula-exchange-2.1.0.jar ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ­å»ºå®¹å™¨åŒ–çš„-spark-ç¯å¢ƒ"},{"categories":["Nebula Graph"],"content":" 3.3 æ­å»ºå®¹å™¨åŒ–çš„ HDFSåŒæ ·å€ŸåŠ© big-data-euroupe çš„å·¥ä½œï¼Œè¿™éå¸¸ç®€å•ï¼Œä¸è¿‡æˆ‘ä»¬è¦åšä¸€ç‚¹ä¿®æ”¹ï¼Œè®©å®ƒçš„ docker-compose.yml æ–‡ä»¶é‡Œä½¿ç”¨ nebula-net è¿™ä¸ªä¹‹å‰åˆ›å»ºçš„ Docker ç½‘ç»œã€‚ è¯¦ç»†ä¿®æ”¹çš„éƒ¨åˆ†å‚è€ƒé™„å½•äºŒ git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop vim docker-compose.yml docker-compose up -d ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ­å»ºå®¹å™¨åŒ–çš„-hdfs"},{"categories":["Nebula Graph"],"content":" 4 é…ç½®Exchangeè¿™ä¸ªé…ç½®ä¸»è¦å¡«å…¥çš„ä¿¡æ¯å°±æ˜¯ Nebula Graph é›†ç¾¤æœ¬èº«å’Œå°†è¦å†™å…¥æ•°æ®çš„ Space Nameï¼Œä»¥åŠæ•°æ®æºç›¸å…³çš„é…ç½®ï¼ˆè¿™é‡Œæˆ‘ä»¬ç”¨ csv ä½œä¸ºä¾‹å­ï¼‰ï¼Œæœ€åå†é…ç½®è¾“å‡ºï¼ˆsinkï¼‰ä¸º sst Nebula Graph GraphD åœ°å€ MetaD åœ°å€ credential Space Name æ•°æ®æº source: csv path fields etc. ink: sst è¯¦ç»†çš„é…ç½®å‚è€ƒé™„å½•äºŒ æ³¨æ„ï¼Œè¿™é‡Œ metaD çš„åœ°å€å¯ä»¥è¿™æ ·è·å–ï¼Œå¯ä»¥çœ‹åˆ° 0.0.0.0:49377-\u003e9559 è¡¨ç¤º 49377 æ˜¯å¤–éƒ¨çš„åœ°å€ã€‚ $ docker ps | grep meta 887740c15750 vesoft/nebula-metad:v2.0.0 \"./bin/nebula-metad â€¦\" 6 hours ago Up 6 hours (healthy) 9560/tcp, 0.0.0.0:49377-\u003e9559/tcp, :::49377-\u003e9559/tcp, 0.0.0.0:49376-\u003e19559/tcp, :::49376-\u003e19559/tcp, 0.0.0.0:49375-\u003e19560/tcp, :::49375-\u003e19560/tcp nebula-docker-compose_metad0_1 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:4:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#é…ç½®exchange"},{"categories":["Nebula Graph"],"content":" 5 ç”ŸæˆSSTæ–‡ä»¶","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#ç”Ÿæˆsstæ–‡ä»¶"},{"categories":["Nebula Graph"],"content":" 5.1 å‡†å¤‡æºæ–‡ä»¶ã€é…ç½®æ–‡ä»¶ docker cp exchange-sst.conf spark-master:/root/ docker cp player.csv spark-master:/root/ å…¶ä¸­ player.csv çš„ä¾‹å­ï¼š 1100,Tim Duncan,42 1101,Tony Parker,36 1102,LaMarcus Aldridge,33 1103,Rudy Gay,32 1104,Marco Belinelli,32 1105,Danny Green,31 1106,Kyle Anderson,25 1107,Aron Baynes,32 1108,Boris Diaw,36 1109,Tiago Splitter,34 1110,Cory Joseph,27 1111,David West,38 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#å‡†å¤‡æºæ–‡ä»¶é…ç½®æ–‡ä»¶"},{"categories":["Nebula Graph"],"content":" 5.2 æ‰§è¡Œ exchange ç¨‹åºè¿›å…¥ spark-master å®¹å™¨ï¼Œæäº¤æ‰§è¡Œ exchange åº”ç”¨ã€‚ docker exec -it spark-master bash cd /root/ /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange-2.1.0.jar\\ -c exchange-sst.conf æ£€æŸ¥æ‰§è¡Œç»“æœï¼š spark-submit è¾“å‡ºï¼š 21/08/17 03:37:43 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 1093 ms on localhost (executor driver) (32/32) 21/08/17 03:37:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 21/08/17 03:37:43 INFO DAGScheduler: ResultStage 2 (foreachPartition at VerticesProcessor.scala:179) finished in 22.336 s 21/08/17 03:37:43 INFO DAGScheduler: Job 1 finished: foreachPartition at VerticesProcessor.scala:179, took 22.500639 s 21/08/17 03:37:43 INFO Exchange$: SST-Import: failure.player: 0 21/08/17 03:37:43 WARN Exchange$: Edge is not defined 21/08/17 03:37:43 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040 21/08/17 03:37:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! éªŒè¯ HDFS ä¸Šç”Ÿæˆçš„ SST æ–‡ä»¶ï¼š docker exec -it namenode /bin/bash root@2db58903fb53:/# hdfs dfs -ls /sst Found 10 items drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/1 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/10 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/2 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/3 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/4 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/5 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/6 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/7 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/8 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/9 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ‰§è¡Œ-exchange-ç¨‹åº"},{"categories":["Nebula Graph"],"content":" 6 å†™å…¥SSTåˆ°NebulaGraphè¿™é‡Œçš„æ“ä½œå®é™…ä¸Šéƒ½æ˜¯å‚è€ƒæ–‡æ¡£ï¼šSST å¯¼å…¥ï¼Œå¾—æ¥ã€‚å…¶ä¸­å°±æ˜¯ä» console ä¹‹ä¸­æ‰§è¡Œäº†ä¸¤æ­¥æ“ä½œï¼š Download Ingest å…¶ä¸­ Download å®é™…ä¸Šæ˜¯è§¦å‘ Nebula Graph ä»æœåŠ¡ç«¯å‘èµ· HDFS Client çš„ downloadï¼Œè·å– HDFS ä¸Šçš„ SST æ–‡ä»¶ï¼Œç„¶åæ”¾åˆ° storageD èƒ½è®¿é—®çš„æœ¬åœ°è·¯å¾„ä¸‹ï¼Œè¿™é‡Œï¼Œéœ€è¦æˆ‘ä»¬åœ¨æœåŠ¡ç«¯éƒ¨ç½² HDFS çš„ä¾èµ–ã€‚å› ä¸ºæˆ‘ä»¬æ˜¯æœ€å°å®è·µï¼Œæˆ‘å°±å·æ‡’æ‰‹åŠ¨åšäº†è¿™ä¸ª Download çš„æ“ä½œã€‚ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#å†™å…¥sståˆ°nebulagraph"},{"categories":["Nebula Graph"],"content":" 6.1 æ‰‹åŠ¨ä¸‹è½½è¿™é‡Œè¾¹æ‰‹åŠ¨ä¸‹è½½æˆ‘ä»¬å°±è¦çŸ¥é“ Nebula Graph æœåŠ¡ç«¯ä¸‹è½½çš„è·¯å¾„ï¼Œå®é™…ä¸Šæ˜¯ /data/storage/nebula/\u003cspace_id\u003e/download/ï¼Œè¿™é‡Œçš„ Space ID éœ€è¦æ‰‹åŠ¨è·å–ä¸€ä¸‹ï¼š è¿™ä¸ªä¾‹å­é‡Œï¼Œæˆ‘ä»¬çš„ Space Name æ˜¯ sstï¼Œè€Œ Space ID æ˜¯ 49ã€‚ (root@nebula) [sst]\u003e DESC space sst +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Atomic Edge | Group | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | 49 | \"sst\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"false\" | \"default\" | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ äºæ˜¯ï¼Œä¸‹è¾¹çš„æ“ä½œå°±æ˜¯æ‰‹åŠ¨æŠŠ SST æ–‡ä»¶ä» HDFS ä¹‹ä¸­ get ä¸‹æ¥ï¼Œå†æ‹·è´åˆ° storageD ä¹‹ä¸­ã€‚ docker exec -it namenode /bin/bash $ hdfs dfs -get /sst /sst exit docker cp namenode:/sst . docker exec -it nebula-docker-compose_storaged0_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged1_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged2_1 mkdir -p /data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged0_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged1_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged2_1:/data/storage/nebula/49/download/ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#æ‰‹åŠ¨ä¸‹è½½"},{"categories":["Nebula Graph"],"content":" 6.2 SST æ–‡ä»¶å¯¼å…¥ è¿›å…¥ Nebula-Console æ‰€åœ¨çš„å®¹å™¨ ~/.nebula-up/console.sh / # åœ¨ console å®¹å™¨é‡Œå‘èµ·é“¾æ¥åˆ°å›¾æ•°æ®åº“ï¼Œå…¶ä¸­ 192.168.x.y æ˜¯æˆ‘æ‰€åœ¨çš„ Linux VM çš„ç¬¬ä¸€ä¸ªç½‘å¡åœ°å€ï¼Œè¯·æ¢æˆæ‚¨çš„ / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! æ‰§è¡Œ INGEST å¼€å§‹è®© StorageD è¯»å– SST æ–‡ä»¶ (root@nebula) [(none)]\u003e use sst (root@nebula) [sst]\u003e INGEST; æˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹æ–¹æ³•å®æ—¶æŸ¥çœ‹ Nebula Graph æœåŠ¡ç«¯çš„æ—¥å¿— tail -f ~/.nebula-up/nebula-docker-compose/logs/*/* æˆåŠŸçš„ INGEST æ—¥å¿—ï¼š I0817 08:03:28.611877 169 EventListner.h:96] Ingest external SST file: column family default, the external file path /data/storage/nebula/49/download/8/8-6.sst, the internal file path /data/storage/nebula/49/data/000023.sst, the properties of the table: # data blocks=1; # entries=1; # deletions=0; # merge operands=0; # range deletions=0; raw key size=48; raw average key size=48.000000; raw value size=40; raw average value size=40.000000; data block size=75; index block size (user-key? 0, delta-value? 0)=66; filter block size=0; (estimated) table size=141; filter policy name=N/A; prefix extractor name=nullptr; column family ID=N/A; column family name=N/A; comparator name=leveldb.BytewiseComparator; merge operator name=nullptr; property collectors names=[]; SST file compression algo=Snappy; SST file compression options=window_bits=-14; level=32767; strategy=0; max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0; ; creation time=0; time stamp of earliest key=0; file creation time=0; E0817 08:03:28.611912 169 StorageHttpIngestHandler.cpp:63] SSTFile ingest successfully é¢˜å›¾ç‰ˆæƒï¼šPietro Jeng ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#sst-æ–‡ä»¶å¯¼å…¥"},{"categories":["Nebula Graph"],"content":" 7 é™„å½•","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#é™„å½•"},{"categories":["Nebula Graph"],"content":" 7.1 é™„å½•ä¸€docker-compose.yaml diff --git a/docker-compose.yaml b/docker-compose.yaml index 48854de..cfeaedb 100644 --- a/docker-compose.yaml +++ b/docker-compose.yaml @@ -6,11 +6,13 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=metad0 - --ws_ip=metad0 - --port=9559 - --ws_http_port=19559 + - --ws_storage_http_port=19779 - --data_path=/data/meta - --log_dir=/logs - --v=0 @@ -34,81 +36,14 @@ services: cap_add: - SYS_PTRACE - metad1: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad1 - - --ws_ip=metad1 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad1:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta1:/data/meta - - ./logs/meta1:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - - metad2: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad2 - - --ws_ip=metad2 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad2:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta2:/data/meta - - ./logs/meta2:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - storaged0: image: vesoft/nebula-storaged:v2.0.0 environment: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged0 - --ws_ip=storaged0 - --port=9779 @@ -119,8 +54,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged0:19779/status\"] interval: 30s @@ -146,7 +81,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged1 - --ws_ip=storaged1 - --port=9779 @@ -157,8 +92,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged1:19779/status\"] interval: 30s @@ -184,7 +119,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged2 - --ws_ip=storaged2 - --port=9779 @@ -195,8 +130,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged2:19779/status\"] interval: 30s @@ -222,17 +157,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd:19669/status\"] interval: 30s @@ -257,17 +194,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd1 - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd1:19669/status\"] interval: 30s @@ -292,17 +231,21 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=me","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#é™„å½•ä¸€"},{"categories":["Nebula Graph"],"content":" 7.2 é™„å½•äºŒhttps://github.com/big-data-europe/docker-hadoop çš„ docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index ed40dc6..66ff1f4 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -14,6 +14,8 @@ services: - CLUSTER_NAME=test env_file: - ./hadoop.env + networks: + - nebula-net datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 @@ -25,6 +27,8 @@ services: SERVICE_PRECONDITION: \"namenode:9870\" env_file: - ./hadoop.env + networks: + - nebula-net resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 @@ -34,6 +38,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\" env_file: - ./hadoop.env + networks: + - nebula-net nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 @@ -43,6 +49,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\" env_file: - ./hadoop.env + networks: + - nebula-net historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 @@ -54,8 +62,14 @@ services: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env + networks: + - nebula-net volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: + +networks: + nebula-net: + external: true ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#é™„å½•äºŒ"},{"categories":["Nebula Graph"],"content":" 7.3 é™„å½•ä¸‰nebula-exchange-sst.conf { # Spark relation config spark: { app: { name: Nebula Exchange 2.1 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"192.168.8.128:9669\"] meta:[\"192.168.8.128:49377\"] } user: root pswd: nebula space: sst # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://192.168.8.128:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is sst, just change type.sink to client if you want to use client import mode. { name: player type: { source: csv sink: sst } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#é™„å½•ä¸‰"},{"categories":["sketches"],"content":"Nebula Operator Explained","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/"},{"categories":["sketches"],"content":" Nebula Graph operator explained This note explained nebula graphâ€™s K8s Operator: Intro 00:00 Nebula K8s Operator Explained 0:25 How do we use Nebula Operator? 02:23 What is the difference between the Operator based Nebula Graph Cluster and the binary-based one? 03:50 How about the Performance impact when it comes to K8s-Operator deployment? 04:55 What is the easiest way to try out the nebula operator? 06:04 Outra 07:30 ref: https://github.com/vesoft-inc/nebula-operator ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:0:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:1:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:2:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Config Explained","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/"},{"categories":["sketches"],"content":" Nebula Graph config explained This note explained nebula graph configurations: Intro 00:00 Nebula Graph Config Explained 0:16 How about Configurations in Nebula Graph Deployed with Docker? 03:01 What about Nebula Graph in K8s Operator Deployment case? 03:55 Should we use Local-Config or Not?(spoiler: Yes!) 05:03 Outra 05:27 ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:0:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:1:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:2:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Index Demystified","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/"},{"categories":["sketches"],"content":" Nebula Graph Native Index Demystified(Chinese only now, English version will be soon uploaded) Index Demystified 0:33 When should we use index? 06:37 Index v.s. Fulltext Index 07:12 Index Performance Impact 08:03 ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:0:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:1:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:2:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Deployment Options","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/"},{"categories":["sketches"],"content":" Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:0:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:1:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:2:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Data Import Options","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/"},{"categories":["sketches"],"content":" Nebula Graph comes with multiple Data Import utils and options, how should we choose from them? ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:0:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#"},{"categories":["sketches"],"content":" 1 Youtube ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:1:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#youtube"},{"categories":["Nebula Graph"],"content":"one liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker)","date":"2021-06-09","objectID":"/en/nebula-operator-kind/","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/"},{"categories":["Nebula Graph"],"content":" Nebula-Kind, an one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND (K8s in Docker) ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:0:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#"},{"categories":["Nebula Graph"],"content":" 1 Nebula-Operator-KindAs a Cloud Native Distributed Database, Nebula Graph comes with an open-source K8s Operator to enable boostrap and maintain Nebula Graph Cluster from a K8s CRD. Normally it takes you some time to setup all the dependencies and control plane resources of the Nebula Operator. If you are as lazy as I am, this Nebula-Operator-Kind is made for you to quick start and play with Nebula Graph in KIND. Nebula-Operator-Kind is the one-liner for setup everything for you including: Docker K8s(KIND) PV Provider Nebula-Operator Nebula-Console nodePort for accessing the Cluster Kubectl for playing with KIND and Nebula Operator ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:1:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#nebula-operator-kind"},{"categories":["Nebula Graph"],"content":" 2 How To UseInstall Nebula-Operator-Kind: curl -sL nebula-kind.siwei.io/install.sh | bash You will see this after itâ€™s done You can connect to the cluster via ~/.nebula-kind/bin/console as below: ~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000 ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:2:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#how-to-use"},{"categories":["Nebula Graph"],"content":" 3 MoreItâ€™s in GitHub with more information you may be intrested in ;-), please try and feedback there~ https://github.com/wey-gu/nebula-operator-kind Updated Sept. 2021 Install on KubeSphere all-in-on clusterï¼š curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash Install on existing K8s cluster: curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash Banner Picture Credit: Maik Hankemann ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:3:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#more"},{"categories":null,"content":" Hi, this is Wey :)I am a developer @vesoft working as Chief Evangelist of NebulaGraph, the open source distributed Graph Database I create toolings and content for NebulaGraph Database to help Developers in the open source community. I am working in open source and consider it is a privilege 1. It took me a couple of my early career years to figure out that my passion lies in helping others with my thoughts \u0026 the tech/magic I have learned. ","date":"2021-06-04","objectID":"/en/about/:0:0","series":null,"tags":null,"title":"","uri":"/en/about/#hi-this-is-wey-"},{"categories":null,"content":" 1 Recent Projects Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-06-04","objectID":"/en/about/:1:0","series":null,"tags":null,"title":"","uri":"/en/about/#recent-projects"},{"categories":null,"content":" 2 Sketches Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option should I use? Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-06-04","objectID":"/en/about/:2:0","series":null,"tags":null,"title":"","uri":"/en/about/#sketches"},{"categories":null,"content":" 3 Hands-on Cources How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-06-04","objectID":"/en/about/:3:0","series":null,"tags":null,"title":"","uri":"/en/about/#hands-on-cources"},{"categories":null,"content":" 4 Talks DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... ","date":"2021-06-04","objectID":"/en/about/:4:0","series":null,"tags":null,"title":"","uri":"/en/about/#talks"},{"categories":null,"content":" 5 Previous workI worked at Ericsson for amost 10 years(2011 to 2021). As the System Manager 2 of Cloud Execution Envrioment (CEE) 3 PDU Cloud, member of CEE 10 core team and CEE System Management team. Helping evolve CEE was my main job: I studied, designed and implemented more than 20 features for CEE 6.6.2 and CEE 10, including area of compute, network, storage, lifecycle management and security. I am also responsible for Ericsson CEE evangelism (internal and external) in China. I used to share my notes and thoughts on note.siwei.info, while from 2021, I will leave more ideas on siwei.io instead. ","date":"2021-06-04","objectID":"/en/about/:5:0","series":null,"tags":null,"title":"","uri":"/en/about/#previous-work"},{"categories":null,"content":" 6 ContactYou can DM me via twitter, or wey.gu@vesoft.com. Book me a zoom call: I share the same idea with Ahmet Alp Balkanâ€™s tweet: Working in open source (and getting paid for it) is a privilege. Itâ€™s a career boost, makes you lots of friends across the industry, and gives you a public brand. I am one of the â€œlucky fewâ€ \u0026 thankful to Microsoft and Google who let me work on OSS nearly all my career. â€” ahmetb (@ahmetb) February 19, 2021 Â â†©ï¸ System Manager, PDU Cloud: Job DescriptionÂ â†©ï¸ Ericssonâ€™s Telco. Infrastructure as a Service product offerring: Cloud Execution EnvironmentÂ â†©ï¸ ","date":"2021-06-04","objectID":"/en/about/:6:0","series":null,"tags":null,"title":"","uri":"/en/about/#contact"},{"categories":["Nebula Graph"],"content":"æœ¬æ–‡åˆ†æäº† Chia Network çš„å…¨é“¾æ•°æ®ï¼Œå¹¶åšäº†å°†å…¨é“¾æ•°æ®å¯¼å…¥å›¾æ•°æ®åº“ï¼šNebula Graph ä¹‹ä¸­çš„å°è¯•ï¼Œä»è€Œå¯è§†åŒ–åœ°æ¢ç´¢äº† Chia å›¾ä¸­æ•°æ®ä¹‹é—´çš„å…³è”å…³ç³»ã€‚","date":"2021-05-26","objectID":"/en/nebula-chia/","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/"},{"categories":["Nebula Graph"],"content":" æœ¬æ–‡åˆ†æäº† Chia Network çš„å…¨é“¾æ•°æ®ï¼Œå¹¶åšäº†å°†å…¨é“¾æ•°æ®å¯¼å…¥å›¾æ•°æ®åº“ï¼šNebula Graph ä¹‹ä¸­çš„å°è¯•ï¼Œä»è€Œå¯è§†åŒ–åœ°æ¢ç´¢äº† Chia å›¾ä¸­æ•°æ®ä¹‹é—´çš„å…³è”å…³ç³»ã€‚ æˆ‘æŠŠæ¶‰åŠçš„ä»£ç å¼€æºåœ¨äº†è¿™é‡Œï¼šhttps://github.com/wey-gu/nebula-chia ","date":"2021-05-26","objectID":"/en/nebula-chia/:0:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#"},{"categories":["Nebula Graph"],"content":" 1 What is Chia Network?Chia Network æ˜¯ç”± BitTorrent çš„ä½œè€… Bram Cohen çš„å›¢é˜Ÿåœ¨ 2017 å¹´åˆ›å»ºçš„åŒºå—é“¾é¡¹ç›®ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#what-is-chia-network"},{"categories":["Nebula Graph"],"content":" 1.1 Why yet another Blockchain? ä¸ºä»€ä¹ˆå†æä¸€ä¸ªåŒºå—é“¾?Chia ç”¨äº†å…¨æ–°çš„ä¸­æœ¬èªå…±è¯†ç®—æ³•ï¼Œè¿™ä¸ªç®—æ³•é€šè¿‡ä¸å…è®¸å¹¶è¡Œè®¡ç®—ï¼Œè®©æŒ–çŸ¿ï¼ˆProof of Workï¼‰æ‰€éœ€ç®—åŠ›å’Œèƒ½è€—é™åˆ°éå¸¸ä½ï¼Œè¿™ä½¿å¾—è¶…å¤§ç»„ç»‡ã€ç©å®¶æ²¡æ³•åƒåœ¨å…¶ä»–çš„åŒºå—é“¾é¡¹ç›®é‚£æ ·æœ‰ç®—åŠ›çš„ç»å¯¹ä¼˜åŠ¿ï¼Œä¹Ÿä¸€å®šç¨‹åº¦ä¸Šè§„é¿äº†èƒ½æºçš„æµªè´¹ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#why-yet-another-blockchain-ä¸ºä»€ä¹ˆå†æä¸€ä¸ªåŒºå—é“¾"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? å¦‚ä½•è¿æ¥Chia?æˆ‘ä»¬å¯ä»¥é€šè¿‡ Chia Network çš„å®¢æˆ·ç«¯æ¥è®¿é—®å®ƒï¼Œè¿™ä¸ªå®¢æˆ·ç«¯æ˜¯ Electron + Python çš„ç¨‹åºï¼Œå¤©ç„¶è·¨å¹³å°ï¼Œæ—¢æœ‰ GUI åˆæœ‰ CLI çš„æ–¹å¼ã€‚ 1.2.1 å®‰è£…åªéœ€è¦æŒ‰ç…§å®˜æ–¹çš„ Guide æ¥ä¸‹è½½å®‰è£…å°±å¥½ï¼Œ https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLï¼Œæˆ‘åœ¨ M1 Mac ä¸‹å®‰è£…çš„æ—¶å€™è„šæœ¬å‡ºäº†ç‚¹å°é—®é¢˜ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºæ‹‰å–äºŒè¿›åˆ¶ wheel æ–‡ä»¶ç½‘ç»œå‡ºé—®é¢˜èµ°åˆ°äº†ç¼–è¯‘ wheelçš„é€»è¾‘ï¼Œè€Œé‚£é‡Œæ˜¯ä¾èµ– cargoçš„ï¼Œå¦‚æœå¤§å®¶é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æå‰æ‰‹åŠ¨å®‰è£…ä¸€ä¸‹ rustï¼Œæˆ–è€… cherry-pick æˆ‘çš„è¿™ä¸ª PR ã€‚ 1.2.2 è¿è¡Œ æŒ‰ç…§å®˜æ–¹ guideï¼Œæ¯”å¦‚ macOS æ¥è¯´ï¼Œæœ€åä¸€æ­¥æ‰§è¡Œnpm run electron \u0026 å°±æ˜¯è¿è¡Œå®ƒçš„GUIå®¢æˆ·ç«¯ã€‚ å¦‚æœå¤§å®¶åƒæˆ‘ä¸€æ ·å–œæ¬¢ CLIï¼Œç›´æ¥åœ¨æ‰§è¡Œå®Œ . ./activate ä¹‹åå°±å¯ä»¥ chia --helpäº†å“ˆâ˜ºï¼Œé‡Œè¾¹æœ‰åªå¯åŠ¨éƒ¨åˆ†æœåŠ¡çš„æ–¹å¼ï¼ˆç›¸æ¯” GUI å¯åŠ¨æ‰€æœ‰æ¥è¯´)ã€‚ åœ¨è¿è¡Œä¹‹åï¼Œå¦‚æœä½ çš„ç½‘ç»œä¸æ˜¯å¤šå±‚ NAT çš„é‚£ç§ï¼Œç†è®ºä¸Šæ‚¨å¯ä»¥è¿åˆ° mainnet å¹¶ä¸”è‡ªåŠ¨å’Œä¸»é“¾åŒæ­¥æ•°æ®äº†ï¼Œå¦‚æœæ‚¨æ˜¯ç¬¬äºŒæ¬¡è¿è¡Œï¼Œè¿æ¥ä¸»é“¾ï¼Œä¸€å¼€å§‹å¯èƒ½æœ‰ä¸€é˜µå­åŒæ­¥çš„block æ•°æ˜¯ä¸å˜çš„ï¼Œä¹Ÿæ²¡æœ‰ peer è¿è¿‡æ¥ï¼Œä¸å¿…æƒŠæ…Œï¼Œç­‰ä¸€ä¸‹å°±å¥½äº†ã€‚ Tips: ç¬¬ä¸€æ¬¡è¿åˆ° Chia Network çš„åŒå­¦ä»¬ï¼Œå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé’±åŒ…ï¼ŒåŠçš„ä¿å­˜é‚£ä¸€ä¸²è¯ï¼Œå®ƒä»¬å°±æ˜¯ä½ çš„ç§é’¥å“¦ã€‚ ä¸‡ä¸€ï¼Œå¦‚æœçœŸçš„è¿ä¸ä¸Šçš„è¯ï¼Œå¯èƒ½éœ€è¦åœ¨è·¯ç”±ä¸Šé…ç½®ï¼ŒUPnPï¼Œé˜²ç«å¢™è¦å…è®¸ 8444ã€‚ 1.2.3 è®¿é—® Chia çš„æ•°æ®Chia çš„å®¢æˆ·ç«¯æŠŠæ•°æ®å­˜åœ¨äº†å‡ ä¸ª SQLite æ•°æ®åº“é‡Œï¼Œå®ƒä»¬çš„è·¯å¾„æ˜¯æˆ‘ä»¬å®‰è£…å®¢æˆ·ç«¯çš„ç”¨æˆ·çš„å®¶ç›®å½•ï¼š~/.chia/mainnet ä¸‹è¾¹å°±æ˜¯è¿è¡Œèµ·æ¥ Chia ä¹‹åç”Ÿæˆçš„ä¸»è¦çš„ä¸¤ä¸ªæ•°æ®åº“çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼š ~/.chia/mainnet/db â¯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db â¯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨ SQLite Browserï¼Œä¸€ä¸ª SQlite æ•°æ®åº“ï¼ˆæ–‡ä»¶ï¼‰çš„æµè§ˆå™¨æ¥çœ‹çœ‹å®ƒã€‚ SQlite æµè§ˆå™¨çš„å®˜ç½‘æ˜¯ https://sqlitebrowser.org/ ã€‚åœ¨ä¸‹è½½ï¼Œå®‰è£…ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‚¹å‡» Open Database/æ‰“å¼€æ•°æ®åº“ é€‰æ‹©æµè§ˆä¸Šè¾¹åˆ—å‡ºæ¥çš„ä¸¤ä¸ª .sqlite æ‰©å±•åçš„æ•°æ®åº“æ–‡ä»¶ã€‚ æ‰“å¼€æ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç¬¬ä¸€ä¸ªæ ‡ç­¾ Database Schema æ¥çœ‹çœ‹è¡¨çš„ç»“æ„ã€‚ æˆ‘ä»¬è¿˜èƒ½åƒç±»ä¼¼äº Excel ä¸€æ ·å»çœ‹è¡¨çš„æ•°æ®ï¼Œè¿˜å¯ä»¥ Filter/è¿‡æ»¤ã€Sort/æ’åºä»»æ„åˆ—ã€‚ ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ¥ç®€å•çœ‹çœ‹è¡¨é‡Œçš„æ•°æ®ã€‚ Tips: è¿™é‡Œè¾¹ï¼Œ~/.chia/mainnet/wallet å’Œè£¸ç›®å½• ~/.chia/mainnet ä¸‹è¾¹çš„ db é‡Œåˆ†åˆ«éƒ½æœ‰è¡¨æ–‡ä»¶ï¼Œä»–ä»¬çš„ä¿¡æ¯æ˜¯æœ‰é‡å¤çš„ï¼Œå¤§å®¶å¯ä»¥åˆ†åˆ«æ‰“å¼€çœ‹çœ‹å“¦ï¼Œå³ä½¿æ˜¯ç›¸åŒçš„è¡¨çš„åå­—ï¼Œæ¯”å¦‚ block_record å†…é‡Œçš„ä¿¡æ¯ä¹Ÿç•¥æœ‰å·®åˆ«ï¼Œå¦‚æœå¤§å®¶çŸ¥é“ä¸ºä»€ä¹ˆæœ‰è¿™æ ·çš„å·®åˆ«ï¼Œæ¬¢è¿æµè§ˆå‘Šè¯‰å¤§å®¶å“ˆï¼Œå¯èƒ½è¦ä»”ç»†ç ”ç©¶ä¸€ä¸‹å®¢æˆ·ç«¯ã€é’±åŒ…ç­‰ä»£ç æ‰è¡Œï¼Œå¹¸è¿çš„æ˜¯ï¼Œå®ƒä»¬ç›¸å¯¹æ¯”è¾ƒå¥½é˜…è¯»ï¼Œæ˜¯ Python å†™çš„ï¼š https://github.com/Chia-Network/chia-blockchain ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-can-i-access-chia-network-å¦‚ä½•è¿æ¥chia"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? å¦‚ä½•è¿æ¥Chia?æˆ‘ä»¬å¯ä»¥é€šè¿‡ Chia Network çš„å®¢æˆ·ç«¯æ¥è®¿é—®å®ƒï¼Œè¿™ä¸ªå®¢æˆ·ç«¯æ˜¯ Electron + Python çš„ç¨‹åºï¼Œå¤©ç„¶è·¨å¹³å°ï¼Œæ—¢æœ‰ GUI åˆæœ‰ CLI çš„æ–¹å¼ã€‚ 1.2.1 å®‰è£…åªéœ€è¦æŒ‰ç…§å®˜æ–¹çš„ Guide æ¥ä¸‹è½½å®‰è£…å°±å¥½ï¼Œ https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLï¼Œæˆ‘åœ¨ M1 Mac ä¸‹å®‰è£…çš„æ—¶å€™è„šæœ¬å‡ºäº†ç‚¹å°é—®é¢˜ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºæ‹‰å–äºŒè¿›åˆ¶ wheel æ–‡ä»¶ç½‘ç»œå‡ºé—®é¢˜èµ°åˆ°äº†ç¼–è¯‘ wheelçš„é€»è¾‘ï¼Œè€Œé‚£é‡Œæ˜¯ä¾èµ– cargoçš„ï¼Œå¦‚æœå¤§å®¶é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æå‰æ‰‹åŠ¨å®‰è£…ä¸€ä¸‹ rustï¼Œæˆ–è€… cherry-pick æˆ‘çš„è¿™ä¸ª PR ã€‚ 1.2.2 è¿è¡Œ æŒ‰ç…§å®˜æ–¹ guideï¼Œæ¯”å¦‚ macOS æ¥è¯´ï¼Œæœ€åä¸€æ­¥æ‰§è¡Œnpm run electron \u0026 å°±æ˜¯è¿è¡Œå®ƒçš„GUIå®¢æˆ·ç«¯ã€‚ å¦‚æœå¤§å®¶åƒæˆ‘ä¸€æ ·å–œæ¬¢ CLIï¼Œç›´æ¥åœ¨æ‰§è¡Œå®Œ . ./activate ä¹‹åå°±å¯ä»¥ chia --helpäº†å“ˆâ˜ºï¼Œé‡Œè¾¹æœ‰åªå¯åŠ¨éƒ¨åˆ†æœåŠ¡çš„æ–¹å¼ï¼ˆç›¸æ¯” GUI å¯åŠ¨æ‰€æœ‰æ¥è¯´)ã€‚ åœ¨è¿è¡Œä¹‹åï¼Œå¦‚æœä½ çš„ç½‘ç»œä¸æ˜¯å¤šå±‚ NAT çš„é‚£ç§ï¼Œç†è®ºä¸Šæ‚¨å¯ä»¥è¿åˆ° mainnet å¹¶ä¸”è‡ªåŠ¨å’Œä¸»é“¾åŒæ­¥æ•°æ®äº†ï¼Œå¦‚æœæ‚¨æ˜¯ç¬¬äºŒæ¬¡è¿è¡Œï¼Œè¿æ¥ä¸»é“¾ï¼Œä¸€å¼€å§‹å¯èƒ½æœ‰ä¸€é˜µå­åŒæ­¥çš„block æ•°æ˜¯ä¸å˜çš„ï¼Œä¹Ÿæ²¡æœ‰ peer è¿è¿‡æ¥ï¼Œä¸å¿…æƒŠæ…Œï¼Œç­‰ä¸€ä¸‹å°±å¥½äº†ã€‚ Tips: ç¬¬ä¸€æ¬¡è¿åˆ° Chia Network çš„åŒå­¦ä»¬ï¼Œå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé’±åŒ…ï¼ŒåŠçš„ä¿å­˜é‚£ä¸€ä¸²è¯ï¼Œå®ƒä»¬å°±æ˜¯ä½ çš„ç§é’¥å“¦ã€‚ ä¸‡ä¸€ï¼Œå¦‚æœçœŸçš„è¿ä¸ä¸Šçš„è¯ï¼Œå¯èƒ½éœ€è¦åœ¨è·¯ç”±ä¸Šé…ç½®ï¼ŒUPnPï¼Œé˜²ç«å¢™è¦å…è®¸ 8444ã€‚ 1.2.3 è®¿é—® Chia çš„æ•°æ®Chia çš„å®¢æˆ·ç«¯æŠŠæ•°æ®å­˜åœ¨äº†å‡ ä¸ª SQLite æ•°æ®åº“é‡Œï¼Œå®ƒä»¬çš„è·¯å¾„æ˜¯æˆ‘ä»¬å®‰è£…å®¢æˆ·ç«¯çš„ç”¨æˆ·çš„å®¶ç›®å½•ï¼š~/.chia/mainnet ä¸‹è¾¹å°±æ˜¯è¿è¡Œèµ·æ¥ Chia ä¹‹åç”Ÿæˆçš„ä¸»è¦çš„ä¸¤ä¸ªæ•°æ®åº“çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼š ~/.chia/mainnet/db â¯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db â¯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨ SQLite Browserï¼Œä¸€ä¸ª SQlite æ•°æ®åº“ï¼ˆæ–‡ä»¶ï¼‰çš„æµè§ˆå™¨æ¥çœ‹çœ‹å®ƒã€‚ SQlite æµè§ˆå™¨çš„å®˜ç½‘æ˜¯ https://sqlitebrowser.org/ ã€‚åœ¨ä¸‹è½½ï¼Œå®‰è£…ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‚¹å‡» Open Database/æ‰“å¼€æ•°æ®åº“ é€‰æ‹©æµè§ˆä¸Šè¾¹åˆ—å‡ºæ¥çš„ä¸¤ä¸ª .sqlite æ‰©å±•åçš„æ•°æ®åº“æ–‡ä»¶ã€‚ æ‰“å¼€æ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç¬¬ä¸€ä¸ªæ ‡ç­¾ Database Schema æ¥çœ‹çœ‹è¡¨çš„ç»“æ„ã€‚ æˆ‘ä»¬è¿˜èƒ½åƒç±»ä¼¼äº Excel ä¸€æ ·å»çœ‹è¡¨çš„æ•°æ®ï¼Œè¿˜å¯ä»¥ Filter/è¿‡æ»¤ã€Sort/æ’åºä»»æ„åˆ—ã€‚ ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ¥ç®€å•çœ‹çœ‹è¡¨é‡Œçš„æ•°æ®ã€‚ Tips: è¿™é‡Œè¾¹ï¼Œ~/.chia/mainnet/wallet å’Œè£¸ç›®å½• ~/.chia/mainnet ä¸‹è¾¹çš„ db é‡Œåˆ†åˆ«éƒ½æœ‰è¡¨æ–‡ä»¶ï¼Œä»–ä»¬çš„ä¿¡æ¯æ˜¯æœ‰é‡å¤çš„ï¼Œå¤§å®¶å¯ä»¥åˆ†åˆ«æ‰“å¼€çœ‹çœ‹å“¦ï¼Œå³ä½¿æ˜¯ç›¸åŒçš„è¡¨çš„åå­—ï¼Œæ¯”å¦‚ block_record å†…é‡Œçš„ä¿¡æ¯ä¹Ÿç•¥æœ‰å·®åˆ«ï¼Œå¦‚æœå¤§å®¶çŸ¥é“ä¸ºä»€ä¹ˆæœ‰è¿™æ ·çš„å·®åˆ«ï¼Œæ¬¢è¿æµè§ˆå‘Šè¯‰å¤§å®¶å“ˆï¼Œå¯èƒ½è¦ä»”ç»†ç ”ç©¶ä¸€ä¸‹å®¢æˆ·ç«¯ã€é’±åŒ…ç­‰ä»£ç æ‰è¡Œï¼Œå¹¸è¿çš„æ˜¯ï¼Œå®ƒä»¬ç›¸å¯¹æ¯”è¾ƒå¥½é˜…è¯»ï¼Œæ˜¯ Python å†™çš„ï¼š https://github.com/Chia-Network/chia-blockchain ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#å®‰è£…"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? å¦‚ä½•è¿æ¥Chia?æˆ‘ä»¬å¯ä»¥é€šè¿‡ Chia Network çš„å®¢æˆ·ç«¯æ¥è®¿é—®å®ƒï¼Œè¿™ä¸ªå®¢æˆ·ç«¯æ˜¯ Electron + Python çš„ç¨‹åºï¼Œå¤©ç„¶è·¨å¹³å°ï¼Œæ—¢æœ‰ GUI åˆæœ‰ CLI çš„æ–¹å¼ã€‚ 1.2.1 å®‰è£…åªéœ€è¦æŒ‰ç…§å®˜æ–¹çš„ Guide æ¥ä¸‹è½½å®‰è£…å°±å¥½ï¼Œ https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLï¼Œæˆ‘åœ¨ M1 Mac ä¸‹å®‰è£…çš„æ—¶å€™è„šæœ¬å‡ºäº†ç‚¹å°é—®é¢˜ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºæ‹‰å–äºŒè¿›åˆ¶ wheel æ–‡ä»¶ç½‘ç»œå‡ºé—®é¢˜èµ°åˆ°äº†ç¼–è¯‘ wheelçš„é€»è¾‘ï¼Œè€Œé‚£é‡Œæ˜¯ä¾èµ– cargoçš„ï¼Œå¦‚æœå¤§å®¶é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æå‰æ‰‹åŠ¨å®‰è£…ä¸€ä¸‹ rustï¼Œæˆ–è€… cherry-pick æˆ‘çš„è¿™ä¸ª PR ã€‚ 1.2.2 è¿è¡Œ æŒ‰ç…§å®˜æ–¹ guideï¼Œæ¯”å¦‚ macOS æ¥è¯´ï¼Œæœ€åä¸€æ­¥æ‰§è¡Œnpm run electron \u0026 å°±æ˜¯è¿è¡Œå®ƒçš„GUIå®¢æˆ·ç«¯ã€‚ å¦‚æœå¤§å®¶åƒæˆ‘ä¸€æ ·å–œæ¬¢ CLIï¼Œç›´æ¥åœ¨æ‰§è¡Œå®Œ . ./activate ä¹‹åå°±å¯ä»¥ chia --helpäº†å“ˆâ˜ºï¼Œé‡Œè¾¹æœ‰åªå¯åŠ¨éƒ¨åˆ†æœåŠ¡çš„æ–¹å¼ï¼ˆç›¸æ¯” GUI å¯åŠ¨æ‰€æœ‰æ¥è¯´)ã€‚ åœ¨è¿è¡Œä¹‹åï¼Œå¦‚æœä½ çš„ç½‘ç»œä¸æ˜¯å¤šå±‚ NAT çš„é‚£ç§ï¼Œç†è®ºä¸Šæ‚¨å¯ä»¥è¿åˆ° mainnet å¹¶ä¸”è‡ªåŠ¨å’Œä¸»é“¾åŒæ­¥æ•°æ®äº†ï¼Œå¦‚æœæ‚¨æ˜¯ç¬¬äºŒæ¬¡è¿è¡Œï¼Œè¿æ¥ä¸»é“¾ï¼Œä¸€å¼€å§‹å¯èƒ½æœ‰ä¸€é˜µå­åŒæ­¥çš„block æ•°æ˜¯ä¸å˜çš„ï¼Œä¹Ÿæ²¡æœ‰ peer è¿è¿‡æ¥ï¼Œä¸å¿…æƒŠæ…Œï¼Œç­‰ä¸€ä¸‹å°±å¥½äº†ã€‚ Tips: ç¬¬ä¸€æ¬¡è¿åˆ° Chia Network çš„åŒå­¦ä»¬ï¼Œå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé’±åŒ…ï¼ŒåŠçš„ä¿å­˜é‚£ä¸€ä¸²è¯ï¼Œå®ƒä»¬å°±æ˜¯ä½ çš„ç§é’¥å“¦ã€‚ ä¸‡ä¸€ï¼Œå¦‚æœçœŸçš„è¿ä¸ä¸Šçš„è¯ï¼Œå¯èƒ½éœ€è¦åœ¨è·¯ç”±ä¸Šé…ç½®ï¼ŒUPnPï¼Œé˜²ç«å¢™è¦å…è®¸ 8444ã€‚ 1.2.3 è®¿é—® Chia çš„æ•°æ®Chia çš„å®¢æˆ·ç«¯æŠŠæ•°æ®å­˜åœ¨äº†å‡ ä¸ª SQLite æ•°æ®åº“é‡Œï¼Œå®ƒä»¬çš„è·¯å¾„æ˜¯æˆ‘ä»¬å®‰è£…å®¢æˆ·ç«¯çš„ç”¨æˆ·çš„å®¶ç›®å½•ï¼š~/.chia/mainnet ä¸‹è¾¹å°±æ˜¯è¿è¡Œèµ·æ¥ Chia ä¹‹åç”Ÿæˆçš„ä¸»è¦çš„ä¸¤ä¸ªæ•°æ®åº“çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼š ~/.chia/mainnet/db â¯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db â¯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨ SQLite Browserï¼Œä¸€ä¸ª SQlite æ•°æ®åº“ï¼ˆæ–‡ä»¶ï¼‰çš„æµè§ˆå™¨æ¥çœ‹çœ‹å®ƒã€‚ SQlite æµè§ˆå™¨çš„å®˜ç½‘æ˜¯ https://sqlitebrowser.org/ ã€‚åœ¨ä¸‹è½½ï¼Œå®‰è£…ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‚¹å‡» Open Database/æ‰“å¼€æ•°æ®åº“ é€‰æ‹©æµè§ˆä¸Šè¾¹åˆ—å‡ºæ¥çš„ä¸¤ä¸ª .sqlite æ‰©å±•åçš„æ•°æ®åº“æ–‡ä»¶ã€‚ æ‰“å¼€æ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç¬¬ä¸€ä¸ªæ ‡ç­¾ Database Schema æ¥çœ‹çœ‹è¡¨çš„ç»“æ„ã€‚ æˆ‘ä»¬è¿˜èƒ½åƒç±»ä¼¼äº Excel ä¸€æ ·å»çœ‹è¡¨çš„æ•°æ®ï¼Œè¿˜å¯ä»¥ Filter/è¿‡æ»¤ã€Sort/æ’åºä»»æ„åˆ—ã€‚ ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ¥ç®€å•çœ‹çœ‹è¡¨é‡Œçš„æ•°æ®ã€‚ Tips: è¿™é‡Œè¾¹ï¼Œ~/.chia/mainnet/wallet å’Œè£¸ç›®å½• ~/.chia/mainnet ä¸‹è¾¹çš„ db é‡Œåˆ†åˆ«éƒ½æœ‰è¡¨æ–‡ä»¶ï¼Œä»–ä»¬çš„ä¿¡æ¯æ˜¯æœ‰é‡å¤çš„ï¼Œå¤§å®¶å¯ä»¥åˆ†åˆ«æ‰“å¼€çœ‹çœ‹å“¦ï¼Œå³ä½¿æ˜¯ç›¸åŒçš„è¡¨çš„åå­—ï¼Œæ¯”å¦‚ block_record å†…é‡Œçš„ä¿¡æ¯ä¹Ÿç•¥æœ‰å·®åˆ«ï¼Œå¦‚æœå¤§å®¶çŸ¥é“ä¸ºä»€ä¹ˆæœ‰è¿™æ ·çš„å·®åˆ«ï¼Œæ¬¢è¿æµè§ˆå‘Šè¯‰å¤§å®¶å“ˆï¼Œå¯èƒ½è¦ä»”ç»†ç ”ç©¶ä¸€ä¸‹å®¢æˆ·ç«¯ã€é’±åŒ…ç­‰ä»£ç æ‰è¡Œï¼Œå¹¸è¿çš„æ˜¯ï¼Œå®ƒä»¬ç›¸å¯¹æ¯”è¾ƒå¥½é˜…è¯»ï¼Œæ˜¯ Python å†™çš„ï¼š https://github.com/Chia-Network/chia-blockchain ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#è¿è¡Œ"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? å¦‚ä½•è¿æ¥Chia?æˆ‘ä»¬å¯ä»¥é€šè¿‡ Chia Network çš„å®¢æˆ·ç«¯æ¥è®¿é—®å®ƒï¼Œè¿™ä¸ªå®¢æˆ·ç«¯æ˜¯ Electron + Python çš„ç¨‹åºï¼Œå¤©ç„¶è·¨å¹³å°ï¼Œæ—¢æœ‰ GUI åˆæœ‰ CLI çš„æ–¹å¼ã€‚ 1.2.1 å®‰è£…åªéœ€è¦æŒ‰ç…§å®˜æ–¹çš„ Guide æ¥ä¸‹è½½å®‰è£…å°±å¥½ï¼Œ https://github.com/Chia-Network/chia-blockchain/wiki/INSTALLï¼Œæˆ‘åœ¨ M1 Mac ä¸‹å®‰è£…çš„æ—¶å€™è„šæœ¬å‡ºäº†ç‚¹å°é—®é¢˜ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºæ‹‰å–äºŒè¿›åˆ¶ wheel æ–‡ä»¶ç½‘ç»œå‡ºé—®é¢˜èµ°åˆ°äº†ç¼–è¯‘ wheelçš„é€»è¾‘ï¼Œè€Œé‚£é‡Œæ˜¯ä¾èµ– cargoçš„ï¼Œå¦‚æœå¤§å®¶é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æå‰æ‰‹åŠ¨å®‰è£…ä¸€ä¸‹ rustï¼Œæˆ–è€… cherry-pick æˆ‘çš„è¿™ä¸ª PR ã€‚ 1.2.2 è¿è¡Œ æŒ‰ç…§å®˜æ–¹ guideï¼Œæ¯”å¦‚ macOS æ¥è¯´ï¼Œæœ€åä¸€æ­¥æ‰§è¡Œnpm run electron \u0026 å°±æ˜¯è¿è¡Œå®ƒçš„GUIå®¢æˆ·ç«¯ã€‚ å¦‚æœå¤§å®¶åƒæˆ‘ä¸€æ ·å–œæ¬¢ CLIï¼Œç›´æ¥åœ¨æ‰§è¡Œå®Œ . ./activate ä¹‹åå°±å¯ä»¥ chia --helpäº†å“ˆâ˜ºï¼Œé‡Œè¾¹æœ‰åªå¯åŠ¨éƒ¨åˆ†æœåŠ¡çš„æ–¹å¼ï¼ˆç›¸æ¯” GUI å¯åŠ¨æ‰€æœ‰æ¥è¯´)ã€‚ åœ¨è¿è¡Œä¹‹åï¼Œå¦‚æœä½ çš„ç½‘ç»œä¸æ˜¯å¤šå±‚ NAT çš„é‚£ç§ï¼Œç†è®ºä¸Šæ‚¨å¯ä»¥è¿åˆ° mainnet å¹¶ä¸”è‡ªåŠ¨å’Œä¸»é“¾åŒæ­¥æ•°æ®äº†ï¼Œå¦‚æœæ‚¨æ˜¯ç¬¬äºŒæ¬¡è¿è¡Œï¼Œè¿æ¥ä¸»é“¾ï¼Œä¸€å¼€å§‹å¯èƒ½æœ‰ä¸€é˜µå­åŒæ­¥çš„block æ•°æ˜¯ä¸å˜çš„ï¼Œä¹Ÿæ²¡æœ‰ peer è¿è¿‡æ¥ï¼Œä¸å¿…æƒŠæ…Œï¼Œç­‰ä¸€ä¸‹å°±å¥½äº†ã€‚ Tips: ç¬¬ä¸€æ¬¡è¿åˆ° Chia Network çš„åŒå­¦ä»¬ï¼Œå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé’±åŒ…ï¼ŒåŠçš„ä¿å­˜é‚£ä¸€ä¸²è¯ï¼Œå®ƒä»¬å°±æ˜¯ä½ çš„ç§é’¥å“¦ã€‚ ä¸‡ä¸€ï¼Œå¦‚æœçœŸçš„è¿ä¸ä¸Šçš„è¯ï¼Œå¯èƒ½éœ€è¦åœ¨è·¯ç”±ä¸Šé…ç½®ï¼ŒUPnPï¼Œé˜²ç«å¢™è¦å…è®¸ 8444ã€‚ 1.2.3 è®¿é—® Chia çš„æ•°æ®Chia çš„å®¢æˆ·ç«¯æŠŠæ•°æ®å­˜åœ¨äº†å‡ ä¸ª SQLite æ•°æ®åº“é‡Œï¼Œå®ƒä»¬çš„è·¯å¾„æ˜¯æˆ‘ä»¬å®‰è£…å®¢æˆ·ç«¯çš„ç”¨æˆ·çš„å®¶ç›®å½•ï¼š~/.chia/mainnet ä¸‹è¾¹å°±æ˜¯è¿è¡Œèµ·æ¥ Chia ä¹‹åç”Ÿæˆçš„ä¸»è¦çš„ä¸¤ä¸ªæ•°æ®åº“çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼š ~/.chia/mainnet/db â¯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db â¯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨ SQLite Browserï¼Œä¸€ä¸ª SQlite æ•°æ®åº“ï¼ˆæ–‡ä»¶ï¼‰çš„æµè§ˆå™¨æ¥çœ‹çœ‹å®ƒã€‚ SQlite æµè§ˆå™¨çš„å®˜ç½‘æ˜¯ https://sqlitebrowser.org/ ã€‚åœ¨ä¸‹è½½ï¼Œå®‰è£…ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‚¹å‡» Open Database/æ‰“å¼€æ•°æ®åº“ é€‰æ‹©æµè§ˆä¸Šè¾¹åˆ—å‡ºæ¥çš„ä¸¤ä¸ª .sqlite æ‰©å±•åçš„æ•°æ®åº“æ–‡ä»¶ã€‚ æ‰“å¼€æ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç¬¬ä¸€ä¸ªæ ‡ç­¾ Database Schema æ¥çœ‹çœ‹è¡¨çš„ç»“æ„ã€‚ æˆ‘ä»¬è¿˜èƒ½åƒç±»ä¼¼äº Excel ä¸€æ ·å»çœ‹è¡¨çš„æ•°æ®ï¼Œè¿˜å¯ä»¥ Filter/è¿‡æ»¤ã€Sort/æ’åºä»»æ„åˆ—ã€‚ ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ¥ç®€å•çœ‹çœ‹è¡¨é‡Œçš„æ•°æ®ã€‚ Tips: è¿™é‡Œè¾¹ï¼Œ~/.chia/mainnet/wallet å’Œè£¸ç›®å½• ~/.chia/mainnet ä¸‹è¾¹çš„ db é‡Œåˆ†åˆ«éƒ½æœ‰è¡¨æ–‡ä»¶ï¼Œä»–ä»¬çš„ä¿¡æ¯æ˜¯æœ‰é‡å¤çš„ï¼Œå¤§å®¶å¯ä»¥åˆ†åˆ«æ‰“å¼€çœ‹çœ‹å“¦ï¼Œå³ä½¿æ˜¯ç›¸åŒçš„è¡¨çš„åå­—ï¼Œæ¯”å¦‚ block_record å†…é‡Œçš„ä¿¡æ¯ä¹Ÿç•¥æœ‰å·®åˆ«ï¼Œå¦‚æœå¤§å®¶çŸ¥é“ä¸ºä»€ä¹ˆæœ‰è¿™æ ·çš„å·®åˆ«ï¼Œæ¬¢è¿æµè§ˆå‘Šè¯‰å¤§å®¶å“ˆï¼Œå¯èƒ½è¦ä»”ç»†ç ”ç©¶ä¸€ä¸‹å®¢æˆ·ç«¯ã€é’±åŒ…ç­‰ä»£ç æ‰è¡Œï¼Œå¹¸è¿çš„æ˜¯ï¼Œå®ƒä»¬ç›¸å¯¹æ¯”è¾ƒå¥½é˜…è¯»ï¼Œæ˜¯ Python å†™çš„ï¼š https://github.com/Chia-Network/chia-blockchain ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#è®¿é—®-chia-çš„æ•°æ®"},{"categories":["Nebula Graph"],"content":" 2 Inspect the Chia Network, åˆ†æ Chia çš„æ•°æ®å¦‚æœå¤§å®¶ä»”ç»†çœ‹äº†ä¸Šè¾¹è¡¨ç»“æ„å®šä¹‰çš„æˆªå›¾ï¼Œå°±èƒ½æ³¨æ„åˆ°ä¸€äº›è¡¨çš„ä¸»è¦ä¿¡æ¯æ˜¯åµŒå¥—äºŒè¿›åˆ¶ KV Byteï¼Œæ‰€ä»¥åªä» SQLite å¹¶ä¸èƒ½çœ‹åˆ°æ‰€æœ‰ Chia çš„æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ï¼ˆç”¨ä¸€ä¸ªç¼–ç¨‹è¯­è¨€æ¥ï¼‰è¯»å–è¡¨é‡Œçš„ Byteã€‚ å¹¸è¿çš„æ˜¯ï¼Œè¿™ä»¶äº‹å„¿å› ä¸º Chia æ˜¯å¼€æºçš„ï¼Œè€Œä¸”æ˜¯ Python çš„ä»£ç ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥ç›´æ¥äº¤äº’å¼çš„åšã€‚ æˆ‘èŠ±äº†ä¸€ç‚¹ç‚¹æ—¶é—´åœ¨ Chia å®¢æˆ·ç«¯ä»£ç é‡Œæ‰¾åˆ°äº†éœ€è¦çš„å°è£…ç±»ï¼Œå€ŸåŠ©å®ƒï¼Œå¯ä»¥æ¯”è¾ƒæ–¹ä¾¿çš„åˆ†æ Chia å®¢æˆ·ç«¯åœ¨æœ¬åœ°çš„å…¨é“¾æ•°æ®ã€‚ å¦‚æœæ‚¨ä¸æ„Ÿå…´è¶£ç»†èŠ‚ï¼Œå¯ä»¥ç›´æ¥çœ‹æˆ‘åˆ†æçš„ç»“è®ºã€‚ ç»“è®ºä¹‹åï¼Œæˆ‘ä¹Ÿç»™å¤§å®¶æ¼”ç¤ºä¸€ä¸‹æ˜¯æ€ä¹ˆè¯»å–å®ƒä»¬çš„ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#inspect-the-chia-network-åˆ†æ-chia-çš„æ•°æ®"},{"categories":["Nebula Graph"],"content":" 2.1 TL;DR, ç»“è®ºæˆ‘ä»¬å¯ä»¥ä»è¡¨ä¸­è¯»å–åˆ°åŒºå—é“¾è®°å½•ï¼ˆBlock Record ï¼‰ï¼ŒChia å¸è®°å½•ï¼ˆCoin Recordï¼‰ã€‚ ä»åŒºå—è®°å½•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å…³é”®çš„æ¶‰åŠäº¤æ˜“çš„ä¿¡æ¯ï¼š å…³è”çš„ Coin ï¼Œå…³è”çš„ Puzzleï¼ˆåœ°å€ï¼‰ï¼ŒCoin çš„å€¼(Amount) ä»å¸è®°å½•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å…³é”®çš„æ¶‰åŠåŒºå—çš„ä¿¡æ¯ï¼š ç”Ÿæˆè¿™ä¸ª Coin æ‰€åœ¨åŒºå—é“¾é‡Œçš„ç´¢å¼•é«˜åº¦ï¼ˆConfirmed Indexï¼‰ å¦‚æœè¿™ä¸ªè®°å½•æ˜¯èŠ±è´¹ Coin çš„ï¼ŒèŠ±è´¹å®ƒçš„ç´¢å¼•é«˜åº¦ï¼ˆSpent Indexï¼‰ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ Coin Record â”‚ â”‚ Block Record â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Coin Name â”‚ â”‚ Height â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”¼â”€â”€â”€â–º Puzzle â”‚ â”‚ Header â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¼â”€â”€â”€â–º Coin Parent â”‚ â”‚ Prev Header â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”¼â”€â”€â”€â–º Amount â”‚ â”‚ Block Body â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ farmer_puzzle_hash â”‚ â”‚ â”‚ â”‚ Time Stamp â”‚ â”‚ fees â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ pool_puzzle_hash â”‚ â””â”€â”€â”€â”€â”€â”¼â”€â”¼â”€â”¬â”€ Confirmed Index â”‚ â”‚ prev_transaction_block_hash â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ prev_transaction_block_height â”‚ â”‚ â”‚ â””â”€ Spent Index â”‚ â”‚ transactions_info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€ is_transaction_block â”‚ â”‚ Coinbase â”‚ â”‚ â”‚ sub_epoch_summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ is Peak â”‚ â”‚ â”‚ â””â”€â”€is Block â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”¼â”€â–º Sub Epoch Segment â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#tldr-ç»“è®º"},{"categories":["Nebula Graph"],"content":" 2.2 Preperation, å‡†å¤‡å› ä¸ºå®‰è£…å®¢æˆ·ç«¯ä¹‹åï¼Œæˆ‘ä»¬æœ¬åœ°å®é™…ä¸Šå·²ç»æœ‰äº†ç›¸å…³çš„ Python ç¯å¢ƒå’Œä¾èµ–ï¼Œåªéœ€è¦åœ¨é‡Œè¾¹è·‘èµ·æ¥å°±å¥½ã€‚ # æ³¨æ„ï¼Œæˆ‘ä»¬è¦ cd åˆ°ä¹‹å‰å®‰è£…å®¢æˆ·ç«¯æ—¶å€™å…‹éš†çš„ä»“åº“ã€‚ cd chia-blockchain # source activate è„šæœ¬æ¥åˆ‡æ¢åˆ°ä»“åº“å®‰è£…æ—¶å€™åˆ›å»ºçš„ Python è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶è¿›åˆ° IPython é‡Œã€‚ source venv/bin/activate \u0026\u0026 pip install ipython \u0026\u0026 ipython ç„¶åè¯•ç€å¯¼å…¥å®¢æˆ·ç«¯é‡Œè¾¹å¸¦æœ‰çš„ Python çš„ Chia çš„å°è£…ç±»è¯•è¯•çœ‹ã€‚ In [1]: import sqlite3 ...: from chia.consensus.block_record import BlockRecord # å¯¼å…¥æˆåŠŸï¼Œæ²¡æœ‰æŠ¥é”™ In [2]: !pwd # æˆ‘çš„å®‰è£…å…‹éš†ç›®å½• /Users/weyl/chia-blockchain æ­å–œä½ åšå¥½äº†å‡†å¤‡ï¼Œæˆ‘ä»¬çœ‹çœ‹ Block Record é‡Œéƒ½æœ‰ä»€ä¹ˆã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#preperation-å‡†å¤‡"},{"categories":["Nebula Graph"],"content":" 2.3 Block Record Chainï¼ŒåŒºå—è®°å½•åœ¨ä¸Šä¸€æ­¥çš„ IPython çª—å£ä¸‹ã€‚ # æ³¨æ„ï¼Œè¿™é‡Œçš„è·¯å¾„çš„å‰ç¼€æ˜¯æˆ‘ä»¬è‡ªå·±çš„å®¶ç›®å½•ï¼Œä¸åŒæ“ä½œç³»ç»Ÿï¼Œä¸åŒçš„ç”¨æˆ·éƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚ chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # è¿™é‡Œæˆ‘ä»¬å–ç¬¬ 201645 é«˜çš„åŒºå— rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # è¿™é‡Œ 0 è¡¨ç¤º SELECT ç»“æœçš„ç¬¬ä¸€è¡Œï¼Œ3 è¡¨ç¤ºåœ¨ BlockRecord è¿™ä¸ªè¡¨é‡Œè¾¹ï¼ŒBlock çš„äºŒè¿›åˆ¶ BLOB æ˜¯ç¬¬å››åˆ—ï¼Œå‚è€ƒæœ¬ç« åº•éƒ¨çš„è¡¨å®šä¹‰éƒ¨åˆ† block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # å¯ä»¥æŸ¥çœ‹ä¸€äº›å±æ€§ is_transaction_blockï¼Œtimestampï¼Œreward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # å¯ä»¥å¿«é€Ÿ print çœ‹å¤§éƒ¨åˆ†ä¿¡æ¯ print(block_records_201645) block_records_201645 çš„æ‰“å°ç»“æœå¦‚ä¸‹ã€‚ è¿™é‡Œæˆ‘æˆªæ–­äº†ä¸€äº›æ•°æ® {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} å¦å¤–ï¼Œæˆ‘ä»¬å–çš„è¿™ä¸ªè¡¨çš„å®šä¹‰å¦‚ä¸‹ã€‚ CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, #\u003c---- sub_epoch_summary blob, is_peak tinyint, is_block tinyint) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#block-record-chainåŒºå—è®°å½•"},{"categories":["Nebula Graph"],"content":" 2.4 Coin Record Chainï¼ŒChia å¸è®°å½•ç±»ä¼¼çš„ï¼Œæˆ‘ä»¬å¯ä»¥è·å–ä¸€ä¸ª Coin çš„è®°å½•ï¼Œè¿™é‡Œè¾¹ï¼Œä»è¡¨çš„å®šä¹‰å¯ä»¥çœ‹åˆ°ï¼Œå”¯ä¸€äºŒè¿›åˆ¶ï¼ˆä¸èƒ½ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢ä¸­è¢«äººè¯»æ‡‚ï¼‰çš„å­—æ®µå°±æ˜¯æ˜¯å¸å€¼ï¼Œä¸å­˜åœ¨åµŒå¥—çš„ç»“æ„ï¼Œæ‰€ä»¥ä¹Ÿå¹¶ä¸éœ€è¦å°è£…çš„ç±»æ‰èƒ½çœ‹æ¸…æ¥šé‡Œè¾¹çš„ä¿¡æ¯ã€‚ CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) è¿™é‡Œå€¼å¾—æ³¨æ„çš„ä¿¡æ¯ä¸»è¦æ˜¯ spent_index å’Œ confirmed_indexã€‚ from chia.util.ints import uint64 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" con = sqlite3.connect(chia_db_path) cur = con.cursor() rows = list(cur.execute('SELECT * FROM coin_record WHERE confirmed_index = 201645')) coin_amount = uint64.from_bytes(rows[0][7]) In [201]: rows[0] Out[201]: ('cf35da0f595b49dde626d676b511ee62bce886f2216751aa51bb8ff851563d35', # coin_name 201645, # confirmed_index 0, # spent_indexï¼Œè¿™é‡Œæ²¡æœ‰spentï¼Œæ‰€ä»¥å€¼æ— æ•ˆ 0, # spentï¼Œå…¶å®æ˜¯ bool 1, # coinbaseï¼Œbool 'bbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', # puzzle_hash å¯¹åº”åˆ°åœ°å€ 'ccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', b'\\x00\\x00\\x01\\x97t \\xdc\\x00', # uint64 1619662081) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:4","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#coin-record-chainchia-å¸è®°å½•"},{"categories":["Nebula Graph"],"content":" 2.5 Puzzles/ Addressï¼Œåœ°å€æˆ‘ä»¬å¯ä»¥æŠŠ Chia ä¸­çš„ Puzzle ç†è§£æˆä¸ºäº¤æ˜“ä¸­çš„åœ°å€ï¼Œä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œé€šå¸¸ä¼šæŠŠ Puzzle çš„ hash ç”¨bech32m è½¬æ¢æˆåœ°å€ã€‚ Tips: è¿™é‡Œæœ‰ä¸€ä¸ªåœ¨çº¿åŒå‘è½¬æ¢çš„åœ¨çº¿å·¥å…·æ¨èä¸€ä¸‹: https://www.chiaexplorer.com/tools/address-puzzlehash-converter ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:5","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#puzzles-addressåœ°å€"},{"categories":["Nebula Graph"],"content":" 3 How to explore Chia Network? å¦‚ä½•æ¢ç´¢ Chia é“¾éšç€æˆ‘ä»¬ä¹‹å‰åˆ†æçš„ä¿¡æ¯ï¼Œè‡ªç„¶åœ°ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ Chia åŒºå—é“¾ä¸­çš„ä¿¡æ¯å–å‡ºæ¥ï¼Œç”¨å›¾ï¼ˆGraphï¼‰æ¥è¡¨ç¤ºï¼Œè¿™é‡Œçš„å›¾å¹¶ä¸æ˜¯ï¼ˆGraphicï¼‰å›¾å½¢ã€å›¾ç”»çš„æ„æ€ï¼Œæ˜¯æ•°å­¦ã€å›¾è®ºä¸­çš„å›¾ã€‚ åœ¨å›¾çš„è¯­å¢ƒä¸‹ï¼Œæœ€ä¸»è¦çš„ä¸¤ä¸ªå…ƒç´ å°±æ˜¯é¡¶ç‚¹ï¼ˆVertexï¼‰å’Œè¾¹ï¼ˆEdgeï¼‰ã€‚ é¡¶ç‚¹è¡¨ç¤ºä¸€ä¸ªå®ä½“ï¼Œè€Œè¾¹è¡¨ç¤ºå®ä½“ä¹‹é—´çš„æŸç§å…³ç³»ï¼Œè¿™ç§å…³ç³»å¯ä»¥æ˜¯å¯¹ç­‰çš„ï¼ˆæ— æ–¹å‘çš„ï¼‰ä¹Ÿå¯ä»¥æ˜¯æœ‰æ–¹å‘çš„ã€‚ è¿™é‡Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™é‡Œçš„ä¿¡æ¯æŠ½è±¡æ˜ å°„åˆ°å¦‚å›¾çš„å›¾æ¨¡å‹é‡Œï¼š Block é¡¶ç‚¹ Coin é¡¶ç‚¹ Puzzle é¡¶ç‚¹ spends è¾¹ï¼ˆBlock åˆ° Coinï¼‰ confirms è¾¹ ï¼ˆBlock åˆ° Coinï¼‰ belongs_to è¾¹ï¼ˆCoin åˆ° Puzzleï¼‰ è¿™é‡Œï¼Œæˆ‘ä»¬åº”ç”¨çš„å›¾æ˜¯ä¸€ç§å«åšå±æ€§å›¾çš„å½¢å¼ï¼Œé™¤äº†ç‚¹å’Œè¾¹çš„å…³ç³»ä¹‹å¤–ã€‚è¿™ä¸¤ç§å®ä½“ï¼ˆç‚¹ã€è¾¹ï¼‰è¿˜æœ‰å…¶ä»–ä¿¡æ¯åªå’Œå®ƒä»¬çš„ä¸€ä¸ªå®ä¾‹ç›¸å…³ï¼Œæ‰€ä»¥å†å®šä¹‰ä¸ºé¡¶ç‚¹ã€è¾¹å°±ä¸æ˜¯å¾ˆé€‚åˆï¼Œè¿™äº›ä¿¡æ¯å°±ä½œä¸ºç‚¹ã€è¾¹çš„å±æ€§ï¼ˆprepertyï¼‰å­˜åœ¨ã€‚ è¿™ç§ä¸ºäº†å¤„ç†å®ä½“ä¹‹é—´å…³è”ã€æ¶‰åŠå®ä½“ã€å…³è”çš„å±æ€§ä¿¡æ¯çš„ï¼Œä¹Ÿå°±æ˜¯\"å±æ€§å›¾\"çš„å­˜å‚¨ä¿¡æ¯çš„æ–¹å¼åœ¨è®¡ç®—æœºé¢†åŸŸè¶Šæ¥è¶Šæµè¡Œï¼Œç”šè‡³æœ‰ä¸“é—¨ä¸ºæ­¤ç»“æ„è€ŒåŸç”Ÿå¼€å‘çš„æ•°æ®åº“â€”â€”å›¾æ•°æ®åº“ï¼ˆGraph Databaseï¼‰ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨çš„å°±æ˜¯ä¸€ä¸ªå«åš Nebula Graph çš„å›¾æ•°æ®åº“ï¼Œå®ƒæ˜¯ä¸€ä¸ªç°ä»£çš„ã€ä¸ºè¶…å¤§è§„æ¨¡åˆ†éƒ¨ç½²æ¶æ„è®¾è®¡çš„ã€åŸç”Ÿå­˜å‚¨ã€æŸ¥è¯¢ã€è®¡ç®—å›¾æ•°æ®çš„é¡¹ç›®ï¼Œæ›´æ£’çš„æ˜¯ï¼Œå®ƒæ˜¯äº§ç”Ÿäºç¤¾åŒºçš„å¼€æºäº§å“ã€‚ Tips: å®‰è£… Nebula Graph ä¸€èˆ¬æ¥è¯´ï¼Œé¢å‘è¶…å¤§è§„æ¨¡æ•°æ®çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå¤©ç„¶çš„éƒ½æ˜¯ä¸å®¹æ˜“è½»é‡éƒ¨ç½²çš„ï¼Œå¤§å®¶å¦‚æœç¬¬ä¸€æ¬¡ä½¿ç”¨çš„è¯å¯ä»¥è¯•è¯•æˆ‘å†™çš„ä¸€ä¸ªå«åš nebula-up çš„å°å·¥å…·ï¼Œå¯ä»¥ä¸€è¡ŒæŒ‡ä»¤éƒ¨ç½²ä¸€ä¸ªç”¨æ¥è¯•ç”¨ã€å­¦ä¹ çš„ Nebula Graph é›†ç¾¤ï¼Œåœ°å€åœ¨è¿™é‡Œï¼š https://github.com/wey-gu/nebula-up/ ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-to-explore-chia-network-å¦‚ä½•æ¢ç´¢-chia-é“¾"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph å¯¼å…¥ Chia æ•°æ®åˆ°å›¾æ•°æ®åº“æˆ‘ä»¬åˆ†ä¸¤æ­¥èµ°ï¼Œç¬¬ä¸€æ­¥è¿™æŠŠ Chia Network æ•°æ®è½¬æ¢æˆ CSV æ–‡ä»¶ï¼Œç¬¬äºŒæ­¥ä½¿ç”¨ Nebula çš„ Nebula-Importer æŠŠæ•°æ®å¯¼å…¥ Nebula Graphã€‚ 3.1.1 Data conversion æ•°æ®è½¬æ¢è¿™éƒ¨åˆ†çš„ä»£ç æˆ‘å¼€æºåœ¨è¿™é‡Œäº†: https://github.com/wey-gu/nebula-chia ä½¿ç”¨å®ƒåªéœ€è¦åœ¨ Chia Network çš„ python venv ä¸‹å®‰è£…å®ƒ: python3 -m pip install nebula-chia ç„¶åè°ƒç”¨ ChaiBatchConvertor å°±å¯ä»¥åœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆä¸¤ä¸ª CSV æ–‡ä»¶ã€‚ è¿™é‡Œè¾¹æœ‰ä¸€äº›å¯ä»¥é…ç½®çš„å‚æ•°ï¼Œå…·ä½“å¯ä»¥å‚è€ƒä»£ç  nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ç”Ÿæˆçš„æ–‡ä»¶ï¼š $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv è¿™é‡Œè¾¹å­—æ®µçš„å«ä¹‰å’Œç±»å‹ï¼Œå¯ä»¥å‚è€ƒä»£ç ä¸­ block_record_row å’Œ coin_record_row çš„ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import æ•°æ®å¯¼å…¥æœ‰äº† CSV æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ© Nebula-Importer å¯¼å…¥æ•°æ®åˆ°å›¾æ•°æ®åº“ä¸­ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬å†™å¥½äº† nebula-importer çš„é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…æ¶µäº†å¦‚ä¸‹ä¿¡æ¯: åœ¨ Nebula Graph ä¸­åˆ›å»ºéœ€è¦çš„æ•°æ®æ¨¡å‹ Schemaï¼Œè¿™å’Œæˆ‘ä»¬å‰è¾¹åšçš„å›¾æ˜ å°„çš„ä¿¡æ¯æ˜¯ç­‰ä»·çš„ æè¿° CSV æ–‡ä»¶ä¹‹ä¸­çš„ Column çš„æ•°æ®åˆ°å›¾æ¨¡å‹ï¼ˆç‚¹ï¼Œè¾¹ï¼Œç‚¹æˆ–è¾¹çš„å±æ€§ï¼‰æ˜ å°„å…³ç³» # è¿™é‡Œï¼Œæˆ‘çš„ csv æ–‡ä»¶å’Œ é…ç½®æ–‡ä»¶éƒ½æ”¾åœ¨ /home/wei.gu/chia ä¹‹ä¸‹ # æˆ‘ä½¿ç”¨ docker-compose é»˜è®¤é…ç½®éƒ¨ç½²çš„ Nebula Graph, # å®ƒåˆ›å»ºäº†å« nebula-docker-compose_nebula-net çš„ docker ç½‘ç»œ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml è¿™é‡Œæˆ‘å±•ç¤ºä¸€ä¸ªå¯¼å…¥çš„ç»“æœç¤ºä¾‹ï¼Œæˆ‘åœ¨å•æœºéƒ¨ç½²çš„ Nebula Graph é‡Œå¯¼å…¥äº†æˆ‘ä¸€ä¸¤å‘¨ä¹‹å‰å–çš„å…¨é‡ Chia Network æ•°æ®çš„ç»“æœã€‚ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#import-the-chia-to-a-graph-database-nebula-graph-å¯¼å…¥-chia-æ•°æ®åˆ°å›¾æ•°æ®åº“"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph å¯¼å…¥ Chia æ•°æ®åˆ°å›¾æ•°æ®åº“æˆ‘ä»¬åˆ†ä¸¤æ­¥èµ°ï¼Œç¬¬ä¸€æ­¥è¿™æŠŠ Chia Network æ•°æ®è½¬æ¢æˆ CSV æ–‡ä»¶ï¼Œç¬¬äºŒæ­¥ä½¿ç”¨ Nebula çš„ Nebula-Importer æŠŠæ•°æ®å¯¼å…¥ Nebula Graphã€‚ 3.1.1 Data conversion æ•°æ®è½¬æ¢è¿™éƒ¨åˆ†çš„ä»£ç æˆ‘å¼€æºåœ¨è¿™é‡Œäº†: https://github.com/wey-gu/nebula-chia ä½¿ç”¨å®ƒåªéœ€è¦åœ¨ Chia Network çš„ python venv ä¸‹å®‰è£…å®ƒ: python3 -m pip install nebula-chia ç„¶åè°ƒç”¨ ChaiBatchConvertor å°±å¯ä»¥åœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆä¸¤ä¸ª CSV æ–‡ä»¶ã€‚ è¿™é‡Œè¾¹æœ‰ä¸€äº›å¯ä»¥é…ç½®çš„å‚æ•°ï¼Œå…·ä½“å¯ä»¥å‚è€ƒä»£ç  nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ç”Ÿæˆçš„æ–‡ä»¶ï¼š $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv è¿™é‡Œè¾¹å­—æ®µçš„å«ä¹‰å’Œç±»å‹ï¼Œå¯ä»¥å‚è€ƒä»£ç ä¸­ block_record_row å’Œ coin_record_row çš„ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import æ•°æ®å¯¼å…¥æœ‰äº† CSV æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ© Nebula-Importer å¯¼å…¥æ•°æ®åˆ°å›¾æ•°æ®åº“ä¸­ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬å†™å¥½äº† nebula-importer çš„é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…æ¶µäº†å¦‚ä¸‹ä¿¡æ¯: åœ¨ Nebula Graph ä¸­åˆ›å»ºéœ€è¦çš„æ•°æ®æ¨¡å‹ Schemaï¼Œè¿™å’Œæˆ‘ä»¬å‰è¾¹åšçš„å›¾æ˜ å°„çš„ä¿¡æ¯æ˜¯ç­‰ä»·çš„ æè¿° CSV æ–‡ä»¶ä¹‹ä¸­çš„ Column çš„æ•°æ®åˆ°å›¾æ¨¡å‹ï¼ˆç‚¹ï¼Œè¾¹ï¼Œç‚¹æˆ–è¾¹çš„å±æ€§ï¼‰æ˜ å°„å…³ç³» # è¿™é‡Œï¼Œæˆ‘çš„ csv æ–‡ä»¶å’Œ é…ç½®æ–‡ä»¶éƒ½æ”¾åœ¨ /home/wei.gu/chia ä¹‹ä¸‹ # æˆ‘ä½¿ç”¨ docker-compose é»˜è®¤é…ç½®éƒ¨ç½²çš„ Nebula Graph, # å®ƒåˆ›å»ºäº†å« nebula-docker-compose_nebula-net çš„ docker ç½‘ç»œ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml è¿™é‡Œæˆ‘å±•ç¤ºä¸€ä¸ªå¯¼å…¥çš„ç»“æœç¤ºä¾‹ï¼Œæˆ‘åœ¨å•æœºéƒ¨ç½²çš„ Nebula Graph é‡Œå¯¼å…¥äº†æˆ‘ä¸€ä¸¤å‘¨ä¹‹å‰å–çš„å…¨é‡ Chia Network æ•°æ®çš„ç»“æœã€‚ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-conversion-æ•°æ®è½¬æ¢"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph å¯¼å…¥ Chia æ•°æ®åˆ°å›¾æ•°æ®åº“æˆ‘ä»¬åˆ†ä¸¤æ­¥èµ°ï¼Œç¬¬ä¸€æ­¥è¿™æŠŠ Chia Network æ•°æ®è½¬æ¢æˆ CSV æ–‡ä»¶ï¼Œç¬¬äºŒæ­¥ä½¿ç”¨ Nebula çš„ Nebula-Importer æŠŠæ•°æ®å¯¼å…¥ Nebula Graphã€‚ 3.1.1 Data conversion æ•°æ®è½¬æ¢è¿™éƒ¨åˆ†çš„ä»£ç æˆ‘å¼€æºåœ¨è¿™é‡Œäº†: https://github.com/wey-gu/nebula-chia ä½¿ç”¨å®ƒåªéœ€è¦åœ¨ Chia Network çš„ python venv ä¸‹å®‰è£…å®ƒ: python3 -m pip install nebula-chia ç„¶åè°ƒç”¨ ChaiBatchConvertor å°±å¯ä»¥åœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆä¸¤ä¸ª CSV æ–‡ä»¶ã€‚ è¿™é‡Œè¾¹æœ‰ä¸€äº›å¯ä»¥é…ç½®çš„å‚æ•°ï¼Œå…·ä½“å¯ä»¥å‚è€ƒä»£ç  nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() ç”Ÿæˆçš„æ–‡ä»¶ï¼š $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv è¿™é‡Œè¾¹å­—æ®µçš„å«ä¹‰å’Œç±»å‹ï¼Œå¯ä»¥å‚è€ƒä»£ç ä¸­ block_record_row å’Œ coin_record_row çš„ __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import æ•°æ®å¯¼å…¥æœ‰äº† CSV æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ© Nebula-Importer å¯¼å…¥æ•°æ®åˆ°å›¾æ•°æ®åº“ä¸­ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬å†™å¥½äº† nebula-importer çš„é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…æ¶µäº†å¦‚ä¸‹ä¿¡æ¯: åœ¨ Nebula Graph ä¸­åˆ›å»ºéœ€è¦çš„æ•°æ®æ¨¡å‹ Schemaï¼Œè¿™å’Œæˆ‘ä»¬å‰è¾¹åšçš„å›¾æ˜ å°„çš„ä¿¡æ¯æ˜¯ç­‰ä»·çš„ æè¿° CSV æ–‡ä»¶ä¹‹ä¸­çš„ Column çš„æ•°æ®åˆ°å›¾æ¨¡å‹ï¼ˆç‚¹ï¼Œè¾¹ï¼Œç‚¹æˆ–è¾¹çš„å±æ€§ï¼‰æ˜ å°„å…³ç³» # è¿™é‡Œï¼Œæˆ‘çš„ csv æ–‡ä»¶å’Œ é…ç½®æ–‡ä»¶éƒ½æ”¾åœ¨ /home/wei.gu/chia ä¹‹ä¸‹ # æˆ‘ä½¿ç”¨ docker-compose é»˜è®¤é…ç½®éƒ¨ç½²çš„ Nebula Graph, # å®ƒåˆ›å»ºäº†å« nebula-docker-compose_nebula-net çš„ docker ç½‘ç»œ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml è¿™é‡Œæˆ‘å±•ç¤ºä¸€ä¸ªå¯¼å…¥çš„ç»“æœç¤ºä¾‹ï¼Œæˆ‘åœ¨å•æœºéƒ¨ç½²çš„ Nebula Graph é‡Œå¯¼å…¥äº†æˆ‘ä¸€ä¸¤å‘¨ä¹‹å‰å–çš„å…¨é‡ Chia Network æ•°æ®çš„ç»“æœã€‚ ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-import-æ•°æ®å¯¼å…¥"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph æ¢ç´¢ Chia çš„æ•°æ® 3.2.1 Graph DB Querieså¯¼å…¥ Chia é“¾çš„ç½‘ç»œåˆ° Nebula Graph ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‡Œè¾¹å¿«é€ŸæŸ¥è¯¢æ•°æ®ä¹‹é—´çš„å…³è”ã€‚ æ¯”å¦‚è¿™ä¸ªæŸ¥è¯¢è¡¨ç¤ºä»åŒºå— 524aa2013781ff4cd9d2b5dc... æŸ¥èµ·ï¼Œç»è¿‡ä¸‰ç§è¾¹ farmer_puzzle, spends, confirms åŒå‘éå†çš„ç»“æœã€‚ GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC å†æ¯”å¦‚ï¼Œè®¡ç®—ä¸€ä¸ª Puzzle åœ°å€ä¸Šçš„ä½™é¢ï¼ˆæ‰€æœ‰ coin çš„æ€»ä»·å€¼ï¼‰æ¯”å¦‚è¿™ä¸ªpuzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ä¸ºæˆ‘ä»¬æä¾›äº†å›¾å½¢åŒ–ç•Œé¢ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ›´ç¬¦åˆäººè„‘çš„æ–¹å¼åœ°æŸ¥çœ‹ Chia Network ä¸­çš„æ•°æ®ã€‚ æ¯”å¦‚ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ°ä¸Šè¾¹çš„é‚£ä¸ªåŒºå—ï¼Œä»è¿™é‡ŒæŸ¥è¯¢ã€‚ æˆ‘ä»¬å°±è·å¾—äº†è¿™ä¸ª block ç±»å‹çš„ä¸€ä¸ªç‚¹/ vertexã€‚æˆ‘ä»¬å¯ä»¥ä»ä»–å¼€å§‹è¿›ä¸€æ­¥æ¢ç´¢ï¼Œå…ˆé¼ æ ‡å•å‡»è¿™ä¸ªç‚¹ï¼Œåœ¨æ‹“å±•æ¡ä»¶é‡ŒæŠŠæ–¹å‘é€‰æ‹©åŒå‘ï¼Œé»˜è®¤çš„è¾¹ç±»å‹æ˜¯æ‰€æœ‰çš„è¾¹ç±»å‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æŠŠæ‰€æœ‰ æ­¥æ•°å†…ç›¸å…³è”çš„æ•°æ®ä¸€ä¸‹å­å…¨éƒ½æ‰¾å‡ºæ¥ã€‚ é€‰æ‹©å¥½æ‹“å±•æ¡ä»¶ä¹‹åï¼Œç‚¹å‡»æ‹“å±•å°±å¯ä»¥ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†æ­¥æ•°ä¸º 1ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€ŸåŒå‡»å…¶ä»–çš„ç‚¹ç»§ç»­æ‹“å±•ï¼Œè¿™æ˜¯æˆ‘é¼ æ ‡ç‚¹äº†å‡ æ¬¡ä¹‹åçœ‹åˆ°çš„æ ·å­ï¼š æˆ‘ä»¬æ¥ä¸‹æ¥å†è¯•è¯•æ‹“å±•çš„æ­¥æ•°ä¸º 2ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œçœ‹èµ·æ¥æ‰¾åˆ°äº†æœ‰æ„æ€çš„ä¿¡æ¯ã€‚ æˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªæœ‰å¾ˆå¤šè¾¹çš„é»‘è‰²çš„ç‚¹ã€‚ é€šè¿‡æŸ¥çœ‹è¿™ä¸ªç‚¹å’Œæˆ‘ä»¬å¼€å§‹æŸ¥çœ‹çš„ block ä¹‹é—´çš„è¾¹ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªç‚¹æ­£æ˜¯ farm è¿™ä¸ª block çš„åœ°å€ï¼Œè¿™ä¸ªåœ°å€ä¸‹è¾¹æœ‰éå¸¸å¤šçš„ coinã€‚ è¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œæœ‰äº†è¿™ä¸ªå¯¼å…¥åˆ° Nebula Graph å›¾æ•°æ®çš„åŸºç¡€ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šæœ‰æ„æ€çš„åˆ†æå’Œæ´å¯Ÿï¼Œå¤§å®¶å¯ä»¥è‡ªå·±è¯•è¯•çœ‹ï¼Œå¾—åˆ°æ›´æœ‰æ„æ€çš„ç»“æœåˆ†äº«ç»™å…¶ä»–åŒå­¦ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#explore-the-chia-graph-æ¢ç´¢-chia-çš„æ•°æ®"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph æ¢ç´¢ Chia çš„æ•°æ® 3.2.1 Graph DB Querieså¯¼å…¥ Chia é“¾çš„ç½‘ç»œåˆ° Nebula Graph ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‡Œè¾¹å¿«é€ŸæŸ¥è¯¢æ•°æ®ä¹‹é—´çš„å…³è”ã€‚ æ¯”å¦‚è¿™ä¸ªæŸ¥è¯¢è¡¨ç¤ºä»åŒºå— 524aa2013781ff4cd9d2b5dc... æŸ¥èµ·ï¼Œç»è¿‡ä¸‰ç§è¾¹ farmer_puzzle, spends, confirms åŒå‘éå†çš„ç»“æœã€‚ GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC å†æ¯”å¦‚ï¼Œè®¡ç®—ä¸€ä¸ª Puzzle åœ°å€ä¸Šçš„ä½™é¢ï¼ˆæ‰€æœ‰ coin çš„æ€»ä»·å€¼ï¼‰æ¯”å¦‚è¿™ä¸ªpuzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ä¸ºæˆ‘ä»¬æä¾›äº†å›¾å½¢åŒ–ç•Œé¢ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ›´ç¬¦åˆäººè„‘çš„æ–¹å¼åœ°æŸ¥çœ‹ Chia Network ä¸­çš„æ•°æ®ã€‚ æ¯”å¦‚ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ°ä¸Šè¾¹çš„é‚£ä¸ªåŒºå—ï¼Œä»è¿™é‡ŒæŸ¥è¯¢ã€‚ æˆ‘ä»¬å°±è·å¾—äº†è¿™ä¸ª block ç±»å‹çš„ä¸€ä¸ªç‚¹/ vertexã€‚æˆ‘ä»¬å¯ä»¥ä»ä»–å¼€å§‹è¿›ä¸€æ­¥æ¢ç´¢ï¼Œå…ˆé¼ æ ‡å•å‡»è¿™ä¸ªç‚¹ï¼Œåœ¨æ‹“å±•æ¡ä»¶é‡ŒæŠŠæ–¹å‘é€‰æ‹©åŒå‘ï¼Œé»˜è®¤çš„è¾¹ç±»å‹æ˜¯æ‰€æœ‰çš„è¾¹ç±»å‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æŠŠæ‰€æœ‰ æ­¥æ•°å†…ç›¸å…³è”çš„æ•°æ®ä¸€ä¸‹å­å…¨éƒ½æ‰¾å‡ºæ¥ã€‚ é€‰æ‹©å¥½æ‹“å±•æ¡ä»¶ä¹‹åï¼Œç‚¹å‡»æ‹“å±•å°±å¯ä»¥ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†æ­¥æ•°ä¸º 1ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€ŸåŒå‡»å…¶ä»–çš„ç‚¹ç»§ç»­æ‹“å±•ï¼Œè¿™æ˜¯æˆ‘é¼ æ ‡ç‚¹äº†å‡ æ¬¡ä¹‹åçœ‹åˆ°çš„æ ·å­ï¼š æˆ‘ä»¬æ¥ä¸‹æ¥å†è¯•è¯•æ‹“å±•çš„æ­¥æ•°ä¸º 2ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œçœ‹èµ·æ¥æ‰¾åˆ°äº†æœ‰æ„æ€çš„ä¿¡æ¯ã€‚ æˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªæœ‰å¾ˆå¤šè¾¹çš„é»‘è‰²çš„ç‚¹ã€‚ é€šè¿‡æŸ¥çœ‹è¿™ä¸ªç‚¹å’Œæˆ‘ä»¬å¼€å§‹æŸ¥çœ‹çš„ block ä¹‹é—´çš„è¾¹ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªç‚¹æ­£æ˜¯ farm è¿™ä¸ª block çš„åœ°å€ï¼Œè¿™ä¸ªåœ°å€ä¸‹è¾¹æœ‰éå¸¸å¤šçš„ coinã€‚ è¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œæœ‰äº†è¿™ä¸ªå¯¼å…¥åˆ° Nebula Graph å›¾æ•°æ®çš„åŸºç¡€ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šæœ‰æ„æ€çš„åˆ†æå’Œæ´å¯Ÿï¼Œå¤§å®¶å¯ä»¥è‡ªå·±è¯•è¯•çœ‹ï¼Œå¾—åˆ°æ›´æœ‰æ„æ€çš„ç»“æœåˆ†äº«ç»™å…¶ä»–åŒå­¦ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#graph-db-queries"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph æ¢ç´¢ Chia çš„æ•°æ® 3.2.1 Graph DB Querieså¯¼å…¥ Chia é“¾çš„ç½‘ç»œåˆ° Nebula Graph ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‡Œè¾¹å¿«é€ŸæŸ¥è¯¢æ•°æ®ä¹‹é—´çš„å…³è”ã€‚ æ¯”å¦‚è¿™ä¸ªæŸ¥è¯¢è¡¨ç¤ºä»åŒºå— 524aa2013781ff4cd9d2b5dc... æŸ¥èµ·ï¼Œç»è¿‡ä¸‰ç§è¾¹ farmer_puzzle, spends, confirms åŒå‘éå†çš„ç»“æœã€‚ GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC å†æ¯”å¦‚ï¼Œè®¡ç®—ä¸€ä¸ª Puzzle åœ°å€ä¸Šçš„ä½™é¢ï¼ˆæ‰€æœ‰ coin çš„æ€»ä»·å€¼ï¼‰æ¯”å¦‚è¿™ä¸ªpuzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph ä¸ºæˆ‘ä»¬æä¾›äº†å›¾å½¢åŒ–ç•Œé¢ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ›´ç¬¦åˆäººè„‘çš„æ–¹å¼åœ°æŸ¥çœ‹ Chia Network ä¸­çš„æ•°æ®ã€‚ æ¯”å¦‚ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ°ä¸Šè¾¹çš„é‚£ä¸ªåŒºå—ï¼Œä»è¿™é‡ŒæŸ¥è¯¢ã€‚ æˆ‘ä»¬å°±è·å¾—äº†è¿™ä¸ª block ç±»å‹çš„ä¸€ä¸ªç‚¹/ vertexã€‚æˆ‘ä»¬å¯ä»¥ä»ä»–å¼€å§‹è¿›ä¸€æ­¥æ¢ç´¢ï¼Œå…ˆé¼ æ ‡å•å‡»è¿™ä¸ªç‚¹ï¼Œåœ¨æ‹“å±•æ¡ä»¶é‡ŒæŠŠæ–¹å‘é€‰æ‹©åŒå‘ï¼Œé»˜è®¤çš„è¾¹ç±»å‹æ˜¯æ‰€æœ‰çš„è¾¹ç±»å‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æŠŠæ‰€æœ‰ æ­¥æ•°å†…ç›¸å…³è”çš„æ•°æ®ä¸€ä¸‹å­å…¨éƒ½æ‰¾å‡ºæ¥ã€‚ é€‰æ‹©å¥½æ‹“å±•æ¡ä»¶ä¹‹åï¼Œç‚¹å‡»æ‹“å±•å°±å¯ä»¥ã€‚ è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†æ­¥æ•°ä¸º 1ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€ŸåŒå‡»å…¶ä»–çš„ç‚¹ç»§ç»­æ‹“å±•ï¼Œè¿™æ˜¯æˆ‘é¼ æ ‡ç‚¹äº†å‡ æ¬¡ä¹‹åçœ‹åˆ°çš„æ ·å­ï¼š æˆ‘ä»¬æ¥ä¸‹æ¥å†è¯•è¯•æ‹“å±•çš„æ­¥æ•°ä¸º 2ï¼Œç‚¹å‡»æ‹“å±•ï¼ˆæˆ–è€…åŒå‡»è¦æ‹“å±•çš„ç‚¹ï¼‰ï¼Œçœ‹èµ·æ¥æ‰¾åˆ°äº†æœ‰æ„æ€çš„ä¿¡æ¯ã€‚ æˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªæœ‰å¾ˆå¤šè¾¹çš„é»‘è‰²çš„ç‚¹ã€‚ é€šè¿‡æŸ¥çœ‹è¿™ä¸ªç‚¹å’Œæˆ‘ä»¬å¼€å§‹æŸ¥çœ‹çš„ block ä¹‹é—´çš„è¾¹ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªç‚¹æ­£æ˜¯ farm è¿™ä¸ª block çš„åœ°å€ï¼Œè¿™ä¸ªåœ°å€ä¸‹è¾¹æœ‰éå¸¸å¤šçš„ coinã€‚ è¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œæœ‰äº†è¿™ä¸ªå¯¼å…¥åˆ° Nebula Graph å›¾æ•°æ®çš„åŸºç¡€ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šæœ‰æ„æ€çš„åˆ†æå’Œæ´å¯Ÿï¼Œå¤§å®¶å¯ä»¥è‡ªå·±è¯•è¯•çœ‹ï¼Œå¾—åˆ°æ›´æœ‰æ„æ€çš„ç»“æœåˆ†äº«ç»™å…¶ä»–åŒå­¦ã€‚ ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#the-visulized-way-with-nebula-studio"},{"categories":["Nebula Graph"],"content":" 4 Summary æ€»ç»“è¿™ç¯‡æ–‡ç« é‡Œï¼Œåœ¨æˆ‘ä»¬ç®€å•ä»‹ç»äº† Chia Network ä¹‹åï¼Œæˆ‘ä»¬é¦–æ¬¡çš„å¸¦å¤§å®¶ä¸€èµ·ä»å®‰è£…ä¸€ä¸ª Chia ç»ˆç«¯ï¼Œåˆ°åˆ†æç»ˆç«¯åŒæ­¥åˆ°æœ¬åœ°çš„ Chia å…¨ç½‘æ•°æ®ï¼Œå€ŸåŠ©äº Chia ç»ˆç«¯å¼€æºçš„ Python ä»£ç åº“ï¼Œæˆ‘ä»¬åˆ†æäº†å…¨ç½‘æ•°æ®é‡Œçš„é‡è¦ä¿¡æ¯ã€‚ ä¹‹åï¼Œæˆ‘ä»¬å¼€æºäº†ä¸€ä¸ªå°å·¥å…· Nebula-Chiaï¼Œæœ‰äº†å®ƒï¼Œå°±å¯ä»¥æŠŠ Chia çš„å…¨ç½‘æ•°æ®è½¬æ¢æˆ CSV æ ¼å¼ï¼Œè¿™æ ·ï¼Œå°±å¯ä»¥å€ŸåŠ© nebula-importer æŠŠæ‰€æœ‰çš„æ•°æ®å¯¼å…¥åˆ°ä¸€ä¸ªå…ˆè¿›çš„å›¾æ•°æ®åº“ï¼ˆNebula Graphï¼‰ä¸­ã€‚ Nebula Graph çš„é¡¹ç›®åœ°å€æ˜¯ https://github.com/vesoft-inc/nebula-graph Nebula-Chia æˆ‘ä¹Ÿå¼€æºåœ¨ https://github.com/wey-gu/nebula-chia åœ¨å›¾æ•°æ®åº“ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åšåŸºæœ¬ Query çš„ä¾‹å­å’Œå€ŸåŠ©å›¾æ•°æ®åº“è‡ªå¸¦çš„å¯è§†åŒ–å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ˜“åœ°è·å– Chia å…¨ç½‘æ•°æ®ä¹‹é—´å…³è”å…³ç³»ï¼Œæœ‰äº†è¿™ä¸ªä½œä¸ºåŸºç¡€ï¼Œè¿™äº›æ•°æ®ä¸­æ´å¯Ÿçš„æ½œåŠ›å’Œå¯ä»¥å°è¯•çš„æœ‰æ„æ€äº‹æƒ…å¯ä»¥æ¯”è¾ƒç›´è§‚å’Œé«˜æ•ˆåœ°è¿›ä¸€æ­¥æ¢ç´¢äº†ï¼ æ˜¯ä¸æ˜¯å¾ˆé…·ï¼Ÿ ","date":"2021-05-26","objectID":"/en/nebula-chia/:4:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#summary-æ€»ç»“"},{"categories":["Nebula Graph"],"content":" 5 References https://www.chia.net/faq/ https://chialisp.com/docs/ https://www.chiaexplorer.com/chia-coins https://docs.google.com/document/d/1tmRIb7lgi4QfKkNaxuKOBHRmwbVlGL4f7EsBDr_5xZE https://github.com/sipa/bech32/tree/master/ref/python https://github.com/Chia-Network/chia-blockchain/blob/main/README.md https://www.chia.net/assets/ChiaGreenPaper.pdf https://docs.nebula-graph.com.cn Banner Picture Credit: Icons8 Team ","date":"2021-05-26","objectID":"/en/nebula-chia/:5:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#references"},{"categories":null,"content":" How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-05-26","objectID":"/en/cources/:0:0","series":null,"tags":null,"title":"Hands on Courses","uri":"/en/cources/#"},{"categories":null,"content":" Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... Nebula-Chia A exploration(and open-source utility) on extracting and loading Chia Network Blockchain into Nebula Graph. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-05-26","objectID":"/en/projects/:0:0","series":null,"tags":null,"title":"Side Projects","uri":"/en/projects/#"},{"categories":null,"content":" Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-05-26","objectID":"/en/sketch-notes/:0:0","series":null,"tags":null,"title":"Sketch Notes","uri":"/en/sketch-notes/#"},{"categories":["Nebula Graph"],"content":"nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience.","date":"2021-05-05","objectID":"/en/vscode-ngql/","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/"},{"categories":["Nebula Graph"],"content":" nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#"},{"categories":["Nebula Graph"],"content":" VS Code nGQL Syntax Highlight ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#vs-code-ngql-syntax-highlight"},{"categories":["Nebula Graph"],"content":" 1 DownloadSearch ngql from the market or click here. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:1:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#download"},{"categories":["Nebula Graph"],"content":" 2 Features Highlighting all Keywords, Functions of a given .ngql file ","date":"2021-05-05","objectID":"/en/vscode-ngql/:2:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#features"},{"categories":["Nebula Graph"],"content":" 3 Release Notes","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#release-notes"},{"categories":["Nebula Graph"],"content":" 3.1 0.0.1Initial release, only .ngql Syntax is supported. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:1","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#001"},{"categories":["Nebula Graph"],"content":" 3.2 0.0.2Lower supported vscode version till ^1.50.1 ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:2","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#002"},{"categories":["Nebula Graph"],"content":" 4 Reference https://docs.nebula-graph.io/ https://github.com/vesoft-inc/nebula-graph/blob/master/src/parser/scanner.lex ","date":"2021-05-05","objectID":"/en/vscode-ngql/:4:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#reference"},{"categories":["Big Data","Cloud"],"content":"How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub","date":"2021-05-03","objectID":"/en/nebula-insights/","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/"},{"categories":["Big Data","Cloud"],"content":" How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub è¿™æ˜¯æˆ‘é¦–å‘åœ¨ Datawhale çš„æ–‡ç« ï¼Œä»‹ç»æˆ‘ä»¬å¦‚ä½•ç”¨å…¬æœ‰äº‘ Serverless æŠ€æœ¯ï¼šGoogle Cloud Schedulerï¼ŒGoogle Cloud Functions å’Œ BigQuery æ­å»ºæ•°æ®ç®¡é“ï¼Œæ”¶é›†æ¢ç´¢å¼€æºç¤¾åŒºæ´å¯Ÿã€‚å¹¶å°†å…¨éƒ¨ä»£ç å¼€æºåœ¨ GitHubã€‚ å¼•å­ æˆ‘ä»¬æƒ³è¦æ”¶é›†ä¸€äº›å¸®åŠ© Nebula Graph ç¤¾åŒºè¿è¥çš„ metricsï¼Œå¸Œæœ›èƒ½ä»ä¸åŒæ¥æºçš„æ•°æ®è‡ªåŠ¨åŒ–å‘¨æœŸæ€§æ”¶é›†ã€å¤„ç†ã€å¹¶æ–¹ä¾¿åœ°å±•ç°å‡ºæ¥åšæ•°æ®é©±åŠ¨åˆ†æçš„åŸºç¡€è®¾æ–½ã€‚ Nebula Graph æ˜¯ä¸€ä¸ªç°ä»£çš„å¼€æºåˆ†å¸ƒå¼å›¾æ•°æ®åº“(Graph Database)ï¼Œæ¬¢è¿åŒå­¦ä»¬ä»: å®˜ç½‘: https://nebula-graph.com.cn Bilibili: https://space.bilibili.com/472621355 GitHub:https://github.com/vesoft-inc/nebula-graph äº†è§£æˆ‘ä»¬å“ˆã€‚ ","date":"2021-05-03","objectID":"/en/nebula-insights/:0:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#"},{"categories":["Big Data","Cloud"],"content":" 1 éœ€æ±‚ æ–¹ä¾¿å¢åŠ æ–°çš„æ•°æ® æ•°æ®æ”¶é›†æ— éœ€äººä¸ºè§¦å‘ï¼ˆè‡ªåŠ¨ã€å‘¨æœŸæ€§ï¼‰ æ¯å¤©æ•°æ®é‡ä¸è¶…è¿‡1000æ¡ æ•°æ®å¯ä»¥ç”Ÿæˆ dashboardï¼Œä¹Ÿå¯ä»¥æ”¯æŒç»Ÿè®¡åˆ†æœŸ query é«˜å¯ç”¨ï¼Œæ•°æ®å®‰å…¨ ä½é¢„ç®—ï¼Œå°½å¯èƒ½ä¸éœ€è¦è¿ç»´äººåŠ› ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#éœ€æ±‚"},{"categories":["Big Data","Cloud"],"content":" 1.1 éœ€æ±‚åˆ†ææˆ‘ä»¬éœ€è¦æ­å»ºä¸€ä¸ªç³»ç»Ÿèƒ½å®ç° ä¸€ä¸ªèƒ½å‘¨æœŸæ€§è§¦å‘è·å–æ•°æ®çš„äº‹ä»¶çš„æœåŠ¡: scheduler ä¸€ä¸ªè§¦å‘ä¹‹åï¼ŒæŠŠæ•°æ® ETL åˆ°æ•°æ®åº“ä¸­çš„æœåŠ¡: ETL worker ä¸€ä¸ªæ•°æ®ä»“åº“ ä¸€ä¸ªèƒ½å¤ŸæŠŠæ•°æ®åº“ä½œä¸ºæºï¼Œå…è®¸ç”¨æˆ· queryï¼Œå±•ç¤ºæ•°æ®çš„ç•Œé¢: Data-UI è¿™ä¸ªéœ€æ±‚çš„ç‰¹ç‚¹æ˜¯è™½ç„¶æ•°æ®é‡å¾ˆå°ã€ä½†æ˜¯è¦æ±‚æœåŠ¡é«˜å¯ç”¨ã€å®‰å…¨ã€‚å› ä¸ºè¿™ç§æƒ…å†µä¸‹è‡ªå»ºæœåŠ¡å™¨è¿˜éœ€è¦ä¿è¯HAå’Œæ•°æ®å®‰å…¨ä¼šä¸€å®šä¼šæ¶ˆè€—æ˜‚è´µè¿ç»´äººåŠ›ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥å°½é‡é¿å…åœ¨è‡ªå·±ç»´æŠ¤çš„æœåŠ¡å™¨ä¸­æ­å»º scheduler, å’Œæ•°æ®åº“ã€‚ æœ€ç»ˆï¼Œæˆ‘ä»¬é€‰æ‹©äº†å°½é‡ä½¿ç”¨å…¬æœ‰äº‘çš„ aaS çš„æ–¹æ¡ˆ: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Google Cloud Scheduler â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º GitHub API Server â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Google Cloud Functions â”œâ”€â”€â”€â”¤ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Docker Hub API Server â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Google BigQuery â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ... â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Aliyun OSS API â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Google Data Studio â”‚ â”‚ â”Œâ”€â”€â” â”‚ â”‚ â”Œâ”€â”€â” â”‚ â”‚ â”Œâ”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”´â”€â”€â”´â”€â”´â”€â”€â”´â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜ å› ä¸ºæˆ‘ä¸ªäººæ¯”è¾ƒç†Ÿæ‚‰ Google Cloud Platform(GCP)çš„åŸå› ï¼ŒåŠ ä¸ŠGCPåœ¨å¤§æ•°æ®å¤„ç†ä¸Šæ¯”è¾ƒé¢†å…ˆï¼Œå†åŠ ä¸ŠGoogleæä¾›çš„ free tieré¢åº¦éå¸¸å¤§æ–¹ï¼Œä»¥è‡³äºåœ¨æˆ‘ä»¬è¿™ä¸ªæ•°æ®é‡ä¸‹ï¼Œæ‰€æœ‰workloadéƒ½ä¼šæ˜¯å…è´¹çš„ã€‚ è¿™ä¸ªæ–¹æ¡ˆæœ€åé€‰æ‹©äº†å…¨æ ˆ Google Cloudï¼Œç„¶è€Œï¼Œè¿™å®é™…ä¸Šåªæ˜¯ä¸€ä¸ªå‚è€ƒï¼ŒåŒå­¦ä»¬å®Œå…¨å¯ä»¥åœ¨å…¶ä»–å…¬æœ‰äº‘æä¾›å•†é‚£é‡Œæ‰¾åˆ°å¯¹åº”çš„æœåŠ¡ã€‚ è¿™é‡Œæˆ‘ç®€å•ä»‹ç»ä¸€ä¸‹ï¼Œ Google Cloud Scheduleræ˜¯è‡ªè§£é‡Šçš„ï¼Œä¸ç”¨å¤šä»‹ç»äº†ã€‚ è€Œ Google Cloud Functionsæ˜¯GCPçš„æ— æœåŠ¡å™¨(serverless)çš„ Function as a ServiceæœåŠ¡ï¼Œå®ƒçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æŠŠæ— çŠ¶æ€çš„ event-driven çš„ workload ä»£ç æ”¾ä¸Šå»ï¼Œå®ƒæ˜¯æŒ‰éœ€ä»˜è´¹ï¼ˆpay as you go)çš„ï¼Œç±»ä¼¼çš„æœåŠ¡è¿˜æœ‰ Google Cloud Runï¼Œåè€…çš„åŒºåˆ«åœ¨äºæˆ‘ä»¬æä¾›çš„æ˜¯ä¸€ä¸ªdocker/containerï¼ˆè¿™ä½¿å¾—èƒ½æ”¯æŒçš„è¿è¡Œç¯å¢ƒå¯ä»¥ä½¿ä»»ä½•èƒ½è·‘åœ¨å®¹å™¨é‡Œçš„ä¸œè¥¿ï¼‰ï¼Œè€Œ Cloud Functionsæ˜¯æŠŠæˆ‘ä»¬çš„ä»£ç æ–‡ä»¶æ”¾ä¸Šå»ã€‚ä»–ä»¬çš„æ•ˆæœæ˜¯ç±»ä¼¼çš„ï¼Œå› ä¸ºæˆ‘å‡†å¤‡ç”¨Pythonæ¥åš ETLçš„ä¸œè¥¿ï¼ŒClouf Functionså·²ç»æ”¯æŒäº†ï¼Œæˆ‘å°±ç›´æ¥é€‰æ‹©å®ƒäº†ã€‚ åœ¨scheduleré‡Œè¾¹ï¼Œæˆ‘å®šä¹‰äº†æ¯ä¸€å¤©å®ƒå‘ä¸€ä¸ª pub/subï¼ˆç±»ä¼¼äºkafkaï¼Œè¿™é‡Œgoogleå¯ä»¥ä¿è¯è‡³å°‘å‘æˆåŠŸä¸€æ¬¡ï¼‰æ¶ˆæ¯ç»™ Cloud Functionsï¼Œç„¶å Cloud Functionsä¼šå»åš ETLçš„å·¥ä½œã€‚ è¿™é‡Œï¼Œå®é™…ä¸Šæˆ‘çš„è®¾è®¡é‡Œè¿™ä¸ªè§¦å‘çš„å‡½æ•°è°ƒä¼šæŠŠæ•°æ®ä»APIé‚£é‡Œè·å–ä¸‹æ¥ï¼Œåœ¨å†…å­˜é‡Œå¤„ç†å¥½ä¹‹åï¼Œå­˜å‚¨åˆ°åœ¨å¯¹è±¡å­˜å‚¨é‡Œä¸º JSON æ–‡ä»¶ï¼Œç„¶åå†è°ƒç”¨ Google BigQuery çš„ APIè®© BigQueryç›´æ¥ä»å¯¹å¯¹è±¡å­˜å‚¨é‡Œæ‹‰å– JSON æ–‡ä»¶ï¼Œå¯¼å…¥è®°å½•åˆ°ç›¸åº”çš„è¡¨ä¹‹ä¸­ã€‚ Google BigQuery ä½œä¸ºGCP ç‰¹åˆ«æœ‰ç«äº‰åŠ›çš„ä¸€ä¸ªäº§å“ï¼Œæ˜¯å®ƒæ•°æ®ä»“åº“ï¼ŒBigQuery å¯ä»¥æ— é™æ‰©å®¹ï¼Œæ”¯æŒæµ·é‡æ•°æ®å¯¼å…¥ï¼Œæ”¯æŒ SQL-like çš„ queryï¼Œè¿˜è‡ªå¸¦MLç®—æ³•ï¼Œé€šè¿‡SQLå°±èƒ½è°ƒç”¨è¿™äº›ç®—æ³•ã€‚å®ƒå¯ä»¥å’Œå¾ˆå¤šGCPä»¥åŠç¬¬ä¸‰æ–¹çš„ç»„ä»¶å¯ä»¥é›†æˆèµ·æ¥ã€‚ Google Data Studio æ˜¯GCPçš„æ•°æ® Insightsäº§å“ï¼Œå¦‚æœå¤§å®¶ç”¨è¿‡ Google Analytics åº”è¯¥å·²ç»ç”¨è¿‡å®ƒäº†ã€‚ ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#éœ€æ±‚åˆ†æ"},{"categories":["Big Data","Cloud"],"content":" 1.2 æ•°æ®çš„è·å–ï¼ŒAPIæˆ‘ä»¬ç¬¬ä¸€é˜¶æ®µæƒ³è¦æ”¶é›†çš„æ•°æ®æ¥æºæ˜¯ GitHub ä¸Šï¼Œç¤¾åŒºé¡¹ç›®çš„ç»Ÿè®¡æ•°æ®ã€Docker Hubä¸Šï¼Œç¤¾åŒºé•œåƒçš„æ‹‰å–è®¡æ•°ï¼Œä¹‹åï¼Œä¼šå¢åŠ æ›´å¤šç»´åº¦çš„æ•°æ®ã€‚ Github API, ref: https://pygithub.readthedocs.io è¿™é‡Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªGithub APIçš„ä¸€ä¸ª Python å°è£…ï¼Œä¸‹è¾¹æ˜¯åœ¨ IDLE/iPython/Jupyter é‡Œå°è¯•çš„ä¾‹å­ # å®ä¾‹åŒ–ä¸€ä¸ªclient g = Github(login_or_token=token, timeout=60, retry=Retry( total=10, status_forcelist=(500, 502, 504), backoff_factor=0.3)) # é…ç½®å¥½è¦è·å–çš„repoçš„ä¿¡æ¯ org_str = \"vesoft-inc\" org = g.get_organization(org_str) repos = org.get_repos() # è¿™é‡Œreposæ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ–¹ä¾¿çœ‹åˆ°é‡Œè¾¹çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬æŠŠå®ƒ list ä¸€ä¸‹å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„repo: list(repos) [Repository(full_name=\"vesoft-inc/nebula\"), Repository(full_name=\"vesoft-inc/nebula-docs\"), Repository(full_name=\"vesoft-inc/nebula-dev-docker\"), Repository(full_name=\"vesoft-inc/github-statistics\"), Repository(full_name=\"vesoft-inc/nebula-docker-compose\"), Repository(full_name=\"vesoft-inc/nebula-go\"), Repository(full_name=\"vesoft-inc/nebula-java\"), Repository(full_name=\"vesoft-inc/nebula-python\"), Repository(full_name=\"vesoft-inc/nebula-importer\"), Repository(full_name=\"vesoft-inc/nebula-third-party\"), Repository(full_name=\"vesoft-inc/nebula-storage\"), Repository(full_name=\"vesoft-inc/nebula-graph\"), Repository(full_name=\"vesoft-inc/nebula-common\"), Repository(full_name=\"vesoft-inc/nebula-stats-exporter\"), Repository(full_name=\"vesoft-inc/nebula-web-docker\"), Repository(full_name=\"vesoft-inc/nebula-bench\"), Repository(full_name=\"vesoft-inc/nebula-console\"), Repository(full_name=\"vesoft-inc/nebula-docs-cn\"), Repository(full_name=\"vesoft-inc/nebula-chaos\"), Repository(full_name=\"vesoft-inc/nebula-clients\"), Repository(full_name=\"vesoft-inc/nebula-spark-utils\"), Repository(full_name=\"vesoft-inc/nebula-node\"), Repository(full_name=\"vesoft-inc/nebula-rust\"), Repository(full_name=\"vesoft-inc/nebula-cpp\"), Repository(full_name=\"vesoft-inc/nebula-http-gateway\"), Repository(full_name=\"vesoft-inc/nebula-flink-connector\"), Repository(full_name=\"vesoft-inc/nebula-community\"), Repository(full_name=\"vesoft-inc/nebula-br\"), Repository(full_name=\"vesoft-inc/.github\")] # repo0 æ˜¯ vesoft-inc/nebula è¿™ä¸ªrepoï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ get_clones_trafficï¼Œget_views_traffic æ¥è·å–è¿‡å»åå‡ å¤©çš„ cloneï¼Œview ç»Ÿè®¡ In [16]: repo0.get_clones_traffic() Out[16]: {'count': 362, 'uniques': 150, 'clones': [Clones(uniques=5, timestamp=2021-04-06 00:00:00, count=16), Clones(uniques=8, timestamp=2021-04-07 00:00:00, count=23), Clones(uniques=13, timestamp=2021-04-08 00:00:00, count=30), Clones(uniques=33, timestamp=2021-04-09 00:00:00, count=45), Clones(uniques=2, timestamp=2021-04-10 00:00:00, count=13), Clones(uniques=6, timestamp=2021-04-11 00:00:00, count=19), Clones(uniques=15, timestamp=2021-04-12 00:00:00, count=28), Clones(uniques=40, timestamp=2021-04-13 00:00:00, count=54), Clones(uniques=9, timestamp=2021-04-14 00:00:00, count=21), Clones(uniques=10, timestamp=2021-04-15 00:00:00, count=34), Clones(uniques=10, timestamp=2021-04-16 00:00:00, count=23), Clones(uniques=5, timestamp=2021-04-17 00:00:00, count=17), Clones(uniques=2, timestamp=2021-04-18 00:00:00, count=13), Clones(uniques=9, timestamp=2021-04-19 00:00:00, count=23), Clones(uniques=3, timestamp=2021-04-20 00:00:00, count=3)]} In [17]: repo0.get_views_traffic() Out[17]: {'count': 6019, 'uniques': 1134, 'views': [View(uniques=52, timestamp=2021-04-06 00:00:00, count=169), View(uniques=143, timestamp=2021-04-07 00:00:00, count=569), View(uniques=152, timestamp=2021-04-08 00:00:00, count=635), View(uniques=134, timestamp=2021-04-09 00:00:00, count=648), View(uniques=81, timestamp=2021-04-10 00:00:00, count=318), View(uniques=42, timestamp=2021-04-11 00:00:00, count=197), View(uniques=127, timestamp=2021-04-12 00:00:00, count=515), View(uniques=149, timestamp=2021-04-13 00:00:00, count=580), View(uniques=134, timestamp=2021-04-14 00:00:00, count=762), View(uniques=141, timestamp=2021-04-15 00:00:00, count=385), View(uniques=113, timestamp=2021-04-16 00:00:00, count=284), View(uniques=48, timestamp=2021-04-17 00:00:00, count=168), View(uniques=35, timestamp=2021-04-18 00:00:00, count=135), View(uniques=124, timestamp=2021-04-19 00:00:00, co","date":"2021-05-03","objectID":"/en/nebula-insights/:1:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#æ•°æ®çš„è·å–api"},{"categories":["Big Data","Cloud"],"content":" 2 å®ç°","date":"2021-05-03","objectID":"/en/nebula-insights/:2:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#å®ç°"},{"categories":["Big Data","Cloud"],"content":" 2.1 è®¡åˆ’ä»»åŠ¡è°ƒåº¦ with Cloud Schedulerå‰è¾¹æåˆ°ï¼ŒScheduler --\u003e Functions ä¸­é—´æ˜¯é€šè¿‡æ¶ˆæ¯é˜Ÿåˆ—å®ç°çš„å¯é äº‹ä»¶è§¦å‘ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Google Cloud Pub/Subé‡Œåˆ›å»ºä¸€ä¸ªè®¢é˜…æ¶ˆæ¯ï¼Œåè¾¹æˆ‘ä»¬ä¼šæŠŠè¿™ä¸ªè®¢é˜…æ¶ˆæ¯ä» Scheduler å®šæœŸå‘é€ï¼Œå¹¶ä¸”åœ¨ Functionåˆ›å»ºçš„æ—¶å€™å®šä¹‰ä¸ºè§¦å‘æ¡ä»¶ã€‚ $ gcloud pubsub topics create nebula-insights-cron-topic $ gcloud pubsub subscriptions create cron-sub --topic nebula-insights-cron-topic ä»»åŠ¡çš„åˆ›å»ºéå¸¸ç›´æ¥ï¼Œåœ¨ Scheduler Web Console ä¸Šç›´æ¥å›¾å½¢åŒ–æ“ä½œå°±å¯ä»¥äº†ï¼Œè®°å¾—è¦é€‰æ‹©è§¦å‘ Pub/Sub æ¶ˆæ¯ä¸º cron-subï¼Œæ¶ˆæ¯ä¸»é¢˜ä¸º nebula-insights-cron-topic ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#è®¡åˆ’ä»»åŠ¡è°ƒåº¦-with-cloud-scheduler"},{"categories":["Big Data","Cloud"],"content":" 2.2 ETL Worker with Python + Google Functionså½“ Scheduler æ¯å¤©å®šæ—¶å‘é€æ¶ˆæ¯ä¹‹åï¼Œæ¥æ”¶æ–¹å°±æ˜¯æˆ‘ä»¬è¦å®šä¹‰çš„ Google Functionsäº†ï¼Œå®ƒçš„å®šä¹‰å¦‚å›¾ ç¬¬ä¸€æ­¥ï¼Œé€‰æ‹©å®ƒçš„è§¦å‘ç±»å‹ä¸º Pub/Subï¼ŒåŒæ—¶è¦å®šä¹‰æ¶ˆæ¯çš„ä¸»é¢˜å’Œåå­—ã€‚ ç¬¬äºŒæ­¥å°±æ˜¯æŠŠä»£ç æ”¾è¿›å»: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º GitHub API Server â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Google Cloud Functions â—„â”€â”€â”€â–º â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Docker Hub API Server â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Google Cloud Storage â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ ... â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Aliyun OSS API â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Google BigQuery â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ è¿™éƒ¨åˆ†çš„é€»è¾‘å°±æ˜¯é€šè¿‡å‰è¾¹åˆ†æäº†çš„APIå–å¾—ä¿¡æ¯ï¼Œç„¶åç»„è£…æˆéœ€è¦çš„æ ¼å¼å­˜åˆ° Cloud Storage(å¯¹è±¡å­˜å‚¨ï¼‰ï¼Œç„¶åå†å¯¼å…¥åˆ° BigQueryï¼ˆæ•°ä»“ï¼‰ä¹‹ä¸­ï¼Œå…¨éƒ¨ä»£ç åœ¨GitHubä¸Š: https://github.com/wey-gu/nebula-insights/blob/main/functions/data-fetching-0/main.py å¦å¤–ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªå®˜æ–¹æ•™ç¨‹ https://cloud.google.com/scheduler/docs/tut-pub-sub ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#etl-worker-with-python--google-functions"},{"categories":["Big Data","Cloud"],"content":" 2.3 æ•°ä»“è¡¨ç»“æ„å®šä¹‰æ•°ä»“çš„è¡¨ç»“æ„æ¯”è¾ƒç›´æ¥ï¼Œschemaçš„å›¾è´´åœ¨ä¸‹è¾¹äº†ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒBigQueryæ”¯æŒåµŒå¥—çš„è¡¨ç»“æ„ï¼ˆè€Œä¸åƒä¸€èˆ¬å…³ç³»å‹æ•°æ®åº“é‚£æ ·éœ€è¦æŠŠè¿™æ ·çš„é€»è¾‘ç»“æ„ç”¨è¾…åŠ©è¡¨æ¥è¡¨ç¤ºï¼‰ï¼Œåœ¨æˆ‘ä»¬è¿™ä¸ªåœºæ™¯ä¸‹éå¸¸æ–¹ä¾¿ï¼Œæ¯”å¦‚releaseè¡¨ä¸­çš„ assetsçš„ä¸‰ä¸ªåµŒå¥—å­—æ®µã€‚ æ›´è¯¦ç»†çš„ä¿¡æ¯å¯ä»¥å‚è€ƒGitHubä¸Šçš„ä»‹ç»å’Œä»£ç : https://github.com/wey-gu/nebula-insights#data-etl-bigquery-and-gcs ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:3","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#æ•°ä»“è¡¨ç»“æ„å®šä¹‰"},{"categories":["Big Data","Cloud"],"content":" 2.4 æ•°æ®å¯è§†åŒ–åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å°±å¯ä»¥è‡ªåŠ¨åœ¨BigQueryé‡Œå­˜æœ‰æ¯å¤©æ”¶é›†çš„ä¸åŒæ¥æºçš„ç»Ÿè®¡æ•°æ®å•¦ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ© Data Studio æ¥ç”Ÿæˆå„å¼å„æ ·çš„å¯è§†åŒ–è¡¨ç¤ºã€‚ å‚è€ƒ https://cloud.google.com/bigquery/docs/visualize-data-studio ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:4","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#æ•°æ®å¯è§†åŒ–"},{"categories":["Big Data","Cloud"],"content":" 3 æ€»ç»“è¿™æ ·ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦ä»»ä½•è®¤ä¸ºç»´æŠ¤çš„æˆæœ¬å’ŒæŠ•å…¥ï¼Œå°±æ­å»ºäº†ä¸€æ•´ä¸ªæ•°æ®çš„æµæ°´çº¿ï¼Œå¹¶ä¸”åªéœ€è¦æŒ‰ç…§æ•°æ®ç”¨é‡ä»˜è´¹ï¼Œåœ¨æˆ‘ä»¬çš„æ•°æ®é‡ä¸‹ï¼ŒåŠæ—¶è€ƒè™‘æœªæ¥å¢åŠ æ•°åä¸ªæ–°çš„é‡åº¦çš„æ”¶é›†ï¼Œæˆ‘ä»¬ä¾ç„¶æ²¡æœ‰è¾¾åˆ°éœ€è¦ä»˜è´¹çš„ç”¨é‡ï¼Œæ˜¯ä¸æ˜¯å¾ˆCoolï¼Ÿ å› ä¸ºæ•°æ®åŒæ—¶å­˜åœ¨äºå¯¹è±¡å­˜å‚¨ä¸æ•°ä»“é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿éšæ—¶æŠŠæ•°æ®å¯¼å…¥ä»»æ„å…¶ä»–å¹³å°ä¸Šã€‚ BigQueryè¿˜æœ‰ä¸€äº›éå¸¸å¸¸ç”¨çš„ï¼Œè‡ªå¸¦çš„æœºå™¨å­¦ä¹ çš„åŠŸèƒ½ï¼Œåªéœ€è¦å†™ä¸€ä¸ªSQL-Likeçš„queryå°±èƒ½è§¦å‘ç„¶åè·å¾—é¢„æµ‹ç»“æœï¼Œå¦‚æœæˆ‘ä»¬ç”¨åˆ°è¿™äº›åŠŸèƒ½çš„è¯ä¹Ÿä¼šå›åˆ° datawhale ä¸ºåŒå­¦ä»¬ç»§ç»­åˆ†äº«å“ˆã€‚ ç¬¬ä¸€æ¬¡åšæ•°æ®å·¥ç¨‹æ–¹é¢çš„åˆ†äº«ï¼Œå¦‚æœæœ‰é”™è¯¯çš„åœ°æ–¹æ¬¢è¿å¤§å®¶ä¸åæŒ‡å‡ºå“ˆ~~ è°¢è°¢ï¼ ","date":"2021-05-03","objectID":"/en/nebula-insights/:3:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#æ€»ç»“"},{"categories":["Nebula Graph"],"content":"A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command.","date":"2021-04-26","objectID":"/en/nebula-up/","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/"},{"categories":["Nebula Graph"],"content":" Update: the All-in-one mode is introduced! Check here and try it! Nebula-Up is PoC utility to enable developer to bootstrap an nebula-graph cluster with nebula-graph-studio(Web UI) + nebula-graph-console(Command UI) ready out of box in an oneliner run. All required packages will handled with nebula-up as well, including Docker on Linux(Ubuntu/CentOS), Docker Desktop on macOS(including both Intel and M1 chip based), and Docker Desktop Windows. Also, itâ€™s optimized to leverage China Repo Mirrors(docker, brew, gitee, etcâ€¦) in case needed enable a smooth deployment for both Mainland China users and others. macOS and Linux with Shell: curl -fsSL nebula-up.siwei.io/install.sh | bash Note: you could specify the version of Nebula Graph like: curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v2.6 ","date":"2021-04-26","objectID":"/en/nebula-up/:0:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#"},{"categories":["Nebula Graph"],"content":" 1 All-in-one modeWith all-in-one mode, you could play with many Nebula Tools in one command, too: Supported tools: Nebula Dashboard Nebula Graph Studio Nebula Graph Console Nebula BR(backup \u0026 restore) Nebula Graph Spark utils Nebula Graph Spark Connector/PySpark Nebula Graph Algorithm Nebula Graph Exchange Nebula Graph Importer Nebula Graph Fulltext Search Nebula Bench ","date":"2021-04-26","objectID":"/en/nebula-up/:1:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#all-in-one-mode"},{"categories":["Nebula Graph"],"content":" 1.1 Install all in one # Install Nebula Core with all-in-one mode curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash ","date":"2021-04-26","objectID":"/en/nebula-up/:1:1","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-all-in-one"},{"categories":["Nebula Graph"],"content":" 1.2 Install Nebula Core and One of the coponent: # Install Core with Backup and Restore with MinIO curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 br # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark # Install Core with Dashboard curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 dashboard ","date":"2021-04-26","objectID":"/en/nebula-up/:1:2","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-nebula-core-and-one-of-the-coponent"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be \u003chost-ip\u003e:9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#how-to-play-with-all-in-one-mode"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#console-and-basketballplayer-dataset-loading"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#monitor-the-whole-cluster-with-nebula-dashboard"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#access-nebula-graph-studio"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#query-data-with-nebula-spark-connector-in-pyspark-shell"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-exchange"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-graph-algorithm"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#try-backup-and-restore-with-minio-as-storage"},{"categories":["Nebula Graph"],"content":"IPython-nGQL is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It's easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded.","date":"2021-03-07","objectID":"/en/ipython-ngql/","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/"},{"categories":["Nebula Graph"],"content":" ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. Itâ€™s easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded. ipython-ngql is inspired by ipython-sql created by Catherine Devlin ","date":"2021-03-07","objectID":"/en/ipython-ngql/:0:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#"},{"categories":["Nebula Graph"],"content":" 1 Get Started","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-started"},{"categories":["Nebula Graph"],"content":" 1.1 Installationipython-ngql could be installed either via pip or from this git repo itself. Install via pip pip install ipython-ngql Install inside the repo git clone git@github.com:wey-gu/ipython-ngql.git cd ipython-ngql python setup.py install ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:1","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#installation"},{"categories":["Nebula Graph"],"content":" 1.2 Load it in Jupyter Notebook or iPython %load_ext ngql ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:2","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#load-it-in-jupyter-notebook-or-ipython"},{"categories":["Nebula Graph"],"content":" 1.3 Connect to Nebula GraphArguments as below are needed to connect a Nebula Graph DB instance: Argument Description --address or -addr IP address of the Nebula Graph Instance --port or -P Port number of the Nebula Graph Instance --user or -u User name --password or -p Password Below is an exmple on connecting to 127.0.0.1:9669 with username: â€œuserâ€ and password: â€œpasswordâ€. %ngql --address 127.0.0.1 --port 9669 --user user --password password ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:3","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#connect-to-nebula-graph"},{"categories":["Nebula Graph"],"content":" 1.4 Make QueriesNow two kind of iPtython Magics are supported: Option 1: The one line stype with %ngql: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id; Option 2: The multiple lines stype with %%ngql %%ngql USE pokemon_club; SHOW TAGS; SHOW HOSTS; There will be other options in future, i.e. from a .ngql file. ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:4","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#make-queries"},{"categories":["Nebula Graph"],"content":" 1.5 Query String with Variablesipython-ngql supports taking variables from the local namespace, with the help of Jinja2 template framework, itâ€™s supported to have queries like the below example. The actual query string should be GO FROM \"Sue\" OVER owns_pokemon ..., and \"{{ trainer }}\" was renderred as \"Sue\" by consuming the local variable trainer: In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:5","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#query-string-with-variables"},{"categories":["Nebula Graph"],"content":" 1.6 Configure ngql_result_styleBy default, ipython-ngql will use pandas dataframe as output style to enable more human readable output, while itâ€™s supported to use the raw thrift data format comes from the nebula2-python itself. This can be done ad-hoc with below one line: %config IPythonNGQL.ngql_result_style=\"raw\" After above line being executed, the output will be like: ResultSet(ExecutionResponse( error_code=0, latency_in_us=2844, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) The result are always stored in variable _ in Jupyter Notebook, thus, to tweak the result, just refer a new var to it like: In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:6","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#configure-ngql_result_style"},{"categories":["Nebula Graph"],"content":" 1.7 Get HelpDonâ€™t remember anything or even relying on the cheatsheet here, oen takeaway for you: the help! In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:7","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-help"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv â¯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#examples"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv â¯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#jupyter-notebook"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv â¯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#ipython"}]