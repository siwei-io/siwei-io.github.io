[{"categories":["Nebula Graph","LLM"],"content":"How to easily do text2cypher with NebulaGraph","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/"},{"categories":["Nebula Graph","LLM"],"content":"Since GPT-3 began to show an unexpected “understanding ability,” we have been engaged in research, exploration, and sharing on the complementary combination of Graph + LLM technology. As of now, we had made many leading contributions to the LlamaIndex and Langchain projects. Starting from this article, we will share some of our periodic successes and methods with everyone. The topic of this blog is what we believe to be the lowest-hanging fruit in this field: text2cypher—generating graph queries from natural language. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:0:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#"},{"categories":["Nebula Graph","LLM"],"content":" 1 Text2CypherAs the name implies, Text2Cypher is about converting natural language text into Cypher query statements. In terms of form, it is no different from another scenario that everyone might be familiar with: Text2SQL–converting text into SQL. Essentially, most knowledge graphs and graph database applications are about querying graphs according to human intentions. Our efforts in building convenient visualization tools on graph databases and wrapping useful APIs all serve this purpose. One of the main factors that have hindered the broader application of graph databases and knowledge graphs might be the threshold for querying graph databases. So, how did we do it before the era of large language models? ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:1:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher"},{"categories":["Nebula Graph","LLM"],"content":" 2 Text2Cypher in old daysThe field of converting text into queries has always had such a demand even before the large language models and has always been one of the most common applications of knowledge graphs. For example, the essence of KBQA (Knowledge-Based Question Answering system) is basically text2cypher. Taking a project I wrote earlier, Siwi (pronounced: /ˈsɪwi/, a Q\u0026A application based on a dataset of basketball players) as an example, let’s look at its backend architecture: ┌─────────────┬───────────────────────────────────┐ │ Speech │ Frontend │ │ ┌──────────▼──────────┐ Siwi, /ˈsɪwi/ │ │ │ Web_Speech_API │ A PoC of Dialog System │ │ │ Vue.JS │ With Graph Database │ │ │ │ Backed Knowledge Graph │ │ └──────────┬──────────┘ │ │ │ Sentence Backend │ │┌────────────┼────────────────────────────┐ │ ││ ┌──────────▼──────────┐ │ │ ││ │ Web API, Flask │ ./app/ │ │ ││ └──────────┬──────────┘ │ │ ││ │ Sentence ./bot/ │ │ ││ ┌──────────▼──────────┐ │ │ ││ │ Intent Matching, │ ./bot/classifier│ │ ││ │ Symentic Processing │ │ │ ││ └──────────┬──────────┘ │ │ ││ │ Intent, Enties │ │ ││ ┌──────────▼──────────┐ │ │ ││ │ Intent Actor │ ./bot/actions │ │ │└─┴──────────┬──────────┴─────────────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ Graph Database │ NebulaGraph │ │ └─────────────────────┘ │ └─────────────────────────────────────────────────┘ When a question statement is sent over, it first needs to perform intent recognition (Intent) and entity recognition (Entity). Then, using an NLP model or code, it constructs the corresponding intent and entities into query statements for the knowledge graph. Finally, it queries the graph database and constructs an answer based on the returned structure. It can be imagined that enabling a program to: Understand intent from natural language: which type of supported questions it corresponds to. Identify entities: the main individuals involved in the question. Construct query statements from intent and entities. It’s not going to be an easy development task. A truly feasible implementation might consider a vast number of boundary conditions, whether in trained models or in rule codes. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:2:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-in-old-days"},{"categories":["Nebula Graph","LLM"],"content":" 3 Text2Cypher with LLMsIn the “post-large language model” era, such “intelligent” application scenarios that previously required specialized training or rule-writing can now be accomplished with generic models combined with prompt engineering. Note: Prompt engineering refers to methods of accomplishing “intelligent” tasks using generation models or language models through natural language descriptions. In fact, right after the release of GPT-3, I started using it to help me compose many highly complex Cypher query statements. I discovered that it could craft complex pattern matches and multi-step conditional statements—ones that I would have previously had to debug bit by bit and take half a day to write. Typically, with its generated solutions, I would only need to make minor adjustments. Moreover, often I could learn from its responses about Cypher syntax blind spots that I wasn’t previously aware of. Later, in February of this year, I tried implementing a project based on GPT-3 (as GPT-3.5 was not available then): ngql-GPT (code). Its working principle is straightforward, identical to Text2SQL. The language model has already learned Cypher’s syntax through public domain training. When posing a task, we only need to provide the model with the graph’s Schema as context. So, the basic prompt goes as: You are a NebulaGraph Cypher expert. Based on the provided graph Schema and the question, please write the query statement. The schema is as follows: --- {schema} --- The question is: --- {question} --- Now, write down the query statement: However, real-world prompts often need additional specifications: Only return the statement, no need for explanations, and no apologies. Emphasize not to write node or edge types outside of the schema. Those interested can refer to my implementation in LlamaIndex’s KnowledgeGraph Query Engine. In real-world scenarios, when we want to quickly learn and build large language model applications, we often use orchestrator tools like Langchain or LlamaIndex. These tools help us create logical abstractions, avoiding the need to implement many generic scaffolding codes from scratch: Interacting with different language models. Engaging with various vector databases. Data segmentation. Moreover, these orchestration tools have embedded many best practices of engineering methods. This way, we can often employ a method to utilize the latest and most user-friendly research paper techniques of large language models, such as FLARE and Guidence. For this purpose, I have contributed tools in both LlamaIndex and Langchain that allow for easy Text2Cypher execution on NebulaGraph, achieving Text2Cypher in just 3 lines of code. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:3:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-with-llms"},{"categories":["Nebula Graph","LLM"],"content":" 4 Text2Cypher on NebulaGraphWithin LlamaIndex’s KnowledgeQueryEngine and LangChain’s NebulaGraphQAChain, we don’t need to concern ourselves with NebulaGraph’s Schema retrieval, Cypher statement generation prompts, various LLM calls, result processing, or linkage—it’s all plug-and-play! ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#text2cypher-on-nebulagraph"},{"categories":["Nebula Graph","LLM"],"content":" 4.1 Using LlamaIndexWith LlamaIndex, all we need to do is: Create a NebulaGraphStore instance. Create a KnowledgeQueryEngine. Then you can start querying directly. Isn’t that super simple? Reference documentation: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html from llama_index.query_engine import KnowledgeGraphQueryEngine from llama_index.storage.storage_context import StorageContext from llama_index.graph_stores import NebulaGraphStore graph_store = NebulaGraphStore( space_name=space_name, edge_types=edge_types, rel_prop_names=rel_prop_names, tags=tags) storage_context = StorageContext.from_defaults(graph_store=graph_store) nl2kg_query_engine = KnowledgeGraphQueryEngine( storage_context=storage_context, service_context=service_context, llm=llm, verbose=True, ) # Ask the question to query KG and answer based on the query result. response = nl2kg_query_engine.query( \"Tell me about Peter Quill?\", ) # Generate Query only. graph_query = nl2kg_query_engine.generate_query( \"Tell me about Peter Quill?\", ) ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:1","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#using-llamaindex"},{"categories":["Nebula Graph","LLM"],"content":" 4.2 Using LangChainSimilarly, in Langchain, we just need to: Create a NebulaGraph instance. Create a NebulaGraphQAChain instance. And you can start posing questions right away. Reference documentation: https://python.langchain.com/docs/modules/chains/additional/graph_nebula_qa from langchain.chat_models import ChatOpenAI from langchain.chains import NebulaGraphQAChain from langchain.graphs import NebulaGraph graph = NebulaGraph( space=space_name, username=\"root\", password=\"nebula\", address=\"127.0.0.1\", port=9669, session_pool_size=30, ) chain = NebulaGraphQAChain.from_llm( llm, graph=graph, verbose=True ) chain.run( \"Tell me about Peter Quill?\", ) ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:4:2","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#using-langchain"},{"categories":["Nebula Graph","LLM"],"content":" 5 DemoThe online demo. This demo showcases how to leverage LLM to extract knowledge triples from various information sources (using Wikipedia as an example) and store them in the NebulaGraph graph database(that’s the KG building like never as easy before). In this demo, we first extracted information from Wikipedia about “Guardians of the Galaxy Vol. 3” and used the knowledge triples generated by the LLM to construct a knowledge graph. We then used Cypher to query the graph, and finally, with LlamaIndex and Langchain’s Text2Cypher, we implemented the function to query the graph using natural language. You can click on other tabs to personally experience the visualization of the knowledge graph, Cypher queries, natural language queries (Text2Cypher), and other features. Here you can download the complete Notebook. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:5:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#demo"},{"categories":["Nebula Graph","LLM"],"content":" 6 ConclusionWith LLM, performing Text2Cypher on data in knowledge graphs and NebulaGraph has never been so easy. A knowledge graph with enhanced human-machine and machine access signifies a new era. We may no longer need the high costs previously associated with implementing backend services on top of graph databases(on text2cypher implementations). Moreover, there’s no longer a need for specialized training to allow domain experts to extract essential insights from the graph. Using the ecosystem integrations in LlamaIndex or LangChain, we can smartly apply our apps and graph data with virtually no development costs in just a few lines of code. However, Text2Cypher is just the beginning. Please stay tuned for our subsequent articles, where we will showcase the revolutionary changes that knowledge graphs and graph databases bring to the large language model ecosystem. 题图 prompt： In an artful fusion of language and AI, this minimalist oil painting captures the essence of technological advancement. Delicate brushstrokes depict a harmony of binary code and flowing words, converging into a central point. With a refined color palette and clean composition, the artwork represents the symbiotic relationship between language and artificial intelligence, inviting contemplation and appreciation. ","date":"2023-07-17","objectID":"/en/llm-text-to-nebulagraph-query/:6:0","series":null,"tags":["Nebula Graph","LLM","LlamaIndex","Langchain","text2cypher","Knowledge Graph"],"title":"Text2Cypher, the beginning of the Graph + LLM stack","uri":"/en/llm-text-to-nebulagraph-query/#conclusion"},{"categories":["Nebula Graph"],"content":"Play with NebulaGraph Database from Jupyter Notebook","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/"},{"categories":["Nebula Graph"],"content":" Introduce the brand new ipython-ngql python package that enhances your ability to connect to NebulaGraph from your Jupyter Notebook or iPython. Now we can do%ngql MATCH p=(n:player)-\u003e() RETURN p to query from Jupyter Notebook and %ng_draw to render the result. Chinese version ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:0:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#"},{"categories":["Nebula Graph"],"content":" 1 InstallationJust do %pip install ipython-ngql from Jupyter Notebook and load it up via %load_ext ngql: %pip install ipython-ngql %load_ext ngql Then, connect the NebulaGraph with a line like this: %ngql --address 127.0.0.1 --port 9669 --user root --password nebula When connected, the cell will have an output of SHOW SPACES. 💡 Note, you could install NebulaGraph dev env from Docker Desktop Extension Marketplace and literally have it ready with one click. Then, within the extension, go to “NebulaGraph AI” and click Install NX Mode to install NebulaGraph + Jupyter Notebook local dev env. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:1:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#installation"},{"categories":["Nebula Graph"],"content":" 2 Query NebulaGraphWe could then do one-liner query with %ngql or multi-line query with %%ngql. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#query-nebulagraph"},{"categories":["Nebula Graph"],"content":" 2.1 One-liner queryFor instance: %ngql USE basketballplayer; %ngql MATCH (v:player{name:\"Tim Duncan\"})--\u003e(v2:player) RETURN v2.player.name AS Name; ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:1","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#one-liner-query"},{"categories":["Nebula Graph"],"content":" 2.2 Multi-line queryFor instance %%ngql ADD HOSTS \"storaged3\":9779,\"storaged4\":9779; SHOW HOSTS; ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:2:2","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#multi-line-query"},{"categories":["Nebula Graph"],"content":" 3 Draw the resultAfter any query result, we could render it visually with %ng_draw: # one query %ngql GET SUBGRAPH 2 STEPS FROM \"player101\" YIELD VERTICES AS nodes, EDGES AS relationships; %ng_draw # another query %ngql match p=(:player)-[]-\u003e() return p LIMIT 5 %ng_draw And it’ll look like: And the renderred result will be in a single-file html, which could be embeded in web pages like: ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:3:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#draw-the-result"},{"categories":["Nebula Graph"],"content":" 4 Other functionalityWe could query %ngql help to know more details of options for ipython-ngql. Here also introudce you some small features. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#other-functionality"},{"categories":["Nebula Graph"],"content":" 4.1 Get the pandas df query resultThe result could be read from _ like: ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:1","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#get-the-pandas-df-query-result"},{"categories":["Nebula Graph"],"content":" 4.2 Play with ResultSet result insteadBy default, the return result is pandas df, but we could configure it as raw to enable debugging for Python NebulaGraph Application code on query result handling, like: In [1] : %config IPythonNGQL.ngql_result_style=\"raw\" In [2] : %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[3]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), ... Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [4]: r = _ In [5]: r.column_values(key='Trainer_Name')[0].cast() Out[5]: 'Tom' ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:2","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#play-with-resultset-result-instead"},{"categories":["Nebula Graph"],"content":" 4.3 Query TemplateBesides, I brought the template support in Jinja2, thus we could do varibales like {{ variable }} : ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:4:3","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#query-template"},{"categories":["Nebula Graph"],"content":" 5 FutureI am planning to add more options on the %ng_draw in the future, and it’s always welcome for your help to contribute more from https://github.com/wey-gu/ipython-ngql. ","date":"2023-06-06","objectID":"/en/nebulagraph-in-jupyter-notebook/:5:0","series":null,"tags":["Nebula Graph","Jupyter","Python","Graph Database Visualization","data science"],"title":"NebulaGraph in Jupyter Notebook","uri":"/en/nebulagraph-in-jupyter-notebook/#future"},{"categories":["Nebula Graph","LLM"],"content":"How Graph could help build better In-context Learning LLM Applications.","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/"},{"categories":["Nebula Graph","LLM"],"content":" How Graph could help build better In-context Learning LLM Applications. Chinese Version ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:0:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#"},{"categories":["Nebula Graph","LLM"],"content":" 1 LLM App ParadigmsAs a big improvement in Cognitive intelligence, LLM had changed many industries, in a way that we didn’t expect to automate, accelerate or enable. Seeing new LLM-enabled applications being created every day, we are all still exploring new methods and use cases for leveraging this magic. One of the most typical patterns to bring LLM into the loop is to ask LLM to understand things based on proprietory/ certain domain knowledge. For now, there are two paradigms we could add that knowledge to LLM: fine-tuning and in-context learning. Fine-tuning refers to performing add-on training on LLM models with extra knowledge, whereas in-context learning is to adding some extra piece of knowledge to the query prompt. What we observe now is that in-context learning has gained popularity over Fine-tuning due to its simplicity. And in this blog, I’ll share what we had been doing around the in-context learning approach. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:1:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llm-app-paradigms"},{"categories":["Nebula Graph","LLM"],"content":" 2 Llama Index: Interface between data and LLM","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llama-index-interface-between-data-and-llm"},{"categories":["Nebula Graph","LLM"],"content":" 2.1 In-context learningThe basic idea of in-context learning is to use existing LLM(not updated) to handle special tasks toward specific knowledge datasets. For instance, to build an application to answer any questions about one person, or even act as one’s digital avatar, we can apply in-context learning to an autobiography book with LLM. In practice, the application will construct a prompt with the question from the user and some information “searched” from the book, then query the LLM for an answer. ┌───────┐ ┌─────────────────┐ ┌─────────┐ │ │ │ Docs/Knowledge │ │ │ │ │ └─────────────────┘ │ │ │ User │─────────────────────────────────────▶ LLM │ │ │ │ │ │ │ │ │ └───────┘ └─────────┘ One of the most performant ways to enable this searching approach to get the related info from the Docs/Knowledge(the book in the above example) for the special task, is to leverage Embeddings. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:1","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#in-context-learning"},{"categories":["Nebula Graph","LLM"],"content":" 2.2 EmbeddingThe embedding normally refers to a way to map real word things into a vector in a multidimensional space, for instance, we could map images into a space of (64 x 64) dimension, and if we are doing it well enough, the distance between the two images can reflect the similarity of them. Another example of embedding is the word2vec algorithm, which literally maps every word into a vector, for instance, and if the embedding is good enough, we could have addition and subtraction on them, we may have: vec(apple) + vec(pie) =~ vec(\"apple apie\") Or the vector measure of vec(apple) + vec(pie) - vec(\"apple apie\") tends to be 0: |vec(apple) + vec(pie) - vec(\"apple apie\")| =~ 0 Similarly, we could have “pear” should be closer than “dinosaur” to “apple”: |vec(apple) - vec(pear)| \u003c |vec(apple) - vec(dinosaur)| With that, we could in theory search for pieces of the book which are more related to a given question. And the basic process is: Split the book into small pieces, create the embedding per each piece, and store them When a question comes, compute the embedding of the question Find top-K similar embeddings of pieces of the book by calculating the distance Construct the prompt with both the question and the pieces of the book Query the LLM with the prompt ┌────┬────┬────┬────┐ │ 1 │ 2 │ 3 │ 4 │ ├────┴────┴────┴────┤ │ Docs/Knowledge │ ┌───────┐ │ ... │ ┌─────────┐ │ │ ├────┬────┬────┬────┤ │ │ │ │ │ 95 │ 96 │ │ │ │ │ │ │ └────┴────┴────┴────┘ │ │ │ User │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶ LLM │ │ │ │ │ │ │ │ │ └───────┘ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ └─────────┘ │ ┌──────────────────────────┐ ▲ └────────┼▶│ Tell me ....., please │├───────┘ └──────────────────────────┘ │ ┌────┐ ┌────┐ │ │ 3 │ │ 96 │ │ └────┘ └────┘ │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:2","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#embedding"},{"categories":["Nebula Graph","LLM"],"content":" 2.3 Llama IndexLlama Index is such an open-source toolkit to help in-context learning in best practice: It comes with a bunch of data loaders to serialize docs/knowledge in a unified format, think of PDF, Wikipedia Page, notion, Twitter, etc, and we don’t have to deal with the preprocessing, split the data into pieces, etc, on our own. It helps create the Embedding(and some other form of the index) for us and stores the embeddings(in memory or vector databases), too, with one line of code. It comes out of the box with prompts and other engineering points, so that we don’t have to create and study from scratch, for instance, create a chatbot on existing data with 4 lines of code. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:2:3","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#llama-index"},{"categories":["Nebula Graph","LLM"],"content":" 3 The problem of doc split and embeddingsThe embedding and vector search worked well in many cases, while there are still challenges in some cases, and one of them is it could lose global context/cross-node context. Think of we are asking “Please tell me things about the author and foo.”, and in this book, the piece with numbers: 1, 3, 6, 19~25, 30~44, and 96~99 are all about the topic of foo. In this case, the simple way of searching top-k embedding of the pieces of the book may not work well because we normally only take a few top-related pieces, which loses many contexts. ┌────┬────┬────┬────┐ │ 1 │ 2 │ 3 │ 4 │ ├────┴────┴────┴────┤ │ Docs/Knowledge │ │ ... │ ├────┬────┬────┬────┤ │ 95 │ 96 │ │ │ └────┴────┴────┴────┘ The mitigation for that, for instance with Llama Index is to create composite indices, where the VectorStore is only part of it, combining with that, we could define a summary index and/or a tree index, etc to route different types of questions to different indices, thus to avoid risking the loss of global context when the question required it. Or, with the help of Knowledge Graph, we could do something differently. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:3:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#the-problem-of-doc-split-and-embeddings"},{"categories":["Nebula Graph","LLM"],"content":" 4 Knowledge GraphThe term Knowledge Graph was initially coined by Google in May 2012 as part of its efforts to enhance search results and provide more contextual information to users. The Knowledge Graph was designed to understand the relationships between entities and provide direct answers to queries rather than just returning a list of relevant web pages. A knowledge graph is a way of organizing and connecting information in a graph format, where nodes represent entities, and edges represent the relationships between those entities. The graph structure allows for efficient storage, retrieval, and analysis of data. It looks like this: But how could Knowledge Graph help? ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:4:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#knowledge-graph"},{"categories":["Nebula Graph","LLM"],"content":" 5 Combination of embeddings and Knowledge GraphThe general idea here is a knowledge graph, as the refined format of the information, can be queried/searched in way smaller granularity than the split we could do on raw data/docs. Thus, by not replacing the large pieces of the data, but combining the two, we can search queries that require global/cross-node context better. See the following diagram, assume the question is about x, and 20 of all the pieces of the data are highly related to it. We could now still get the top 3 pieces of the doc(say, no. 1, 2, and 96) as the main context to be sent, apart from that, we ask for two hops of graph traversal around x from the knowledge graph, then the full context will be: The question “Tell me things about the author and x” Raw doc from piece number 1, 2, and 96, in Llama Index, it’s called node 1, node 2, and node 96. Knowledge 10 triplets contain “x” in two-depths graph traversal: x -\u003e y(from node 1) x -\u003e a(from node 2) x -\u003e m(from node 4) x \u003c- b-\u003e c(from node 95) x -\u003e d(from node 96) n -\u003e x(from node 98) x \u003c- z \u003c- i(from node 1 and node 3) x \u003c- z \u003c- b(from node 1 and node 95) ┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐ │ .─. .─. │ .─. .─. │ .─. │ .─. .─. │ │( x )─────▶ y ) │ ( x )─────▶ a ) │ ( j ) │ ( m )◀────( x ) │ │ `▲' `─' │ `─' `─' │ `─' │ `─' `─' │ │ │ 1 │ 2 │ 3 │ │ 4 │ │ .─. │ │ .▼. │ │ │( z )─────────────┼──────────────────┼──────────▶( i )─┐│ │ │ `◀────┐ │ │ `─' ││ │ ├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤ │ │ Docs/Knowledge │ │ │ │ ... │ │ │ │ │ │ ├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤ │ .─. └──────. │ .─. │ ││ .─. │ │ ( x ◀─────( b ) │ ( x ) │ └┼▶( n ) │ │ `─' `─' │ `─' │ │ `─' │ │ 95 │ │ │ 96 │ │ │ 98 │ │ .▼. │ .▼. │ │ ▼ │ │ ( c ) │ ( d ) │ │ .─. │ │ `─' │ `─' │ │ ( x ) │ └──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘ And clearly, the refined information related to topic x that comes from both other nodes and across the nodes is included in the context to build the prompt of in-context learning. ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:5:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#combination-of-embeddings-and-knowledge-graph"},{"categories":["Nebula Graph","LLM"],"content":" 6 Progress of Knowledge Graph in Llama IndexThe Knowledge Graph abstraction was initially introduced to Llama Index by William F.H. where the triplets in the knowledge graph were associated with the docs with keywords and stored in memory, then Logan Markewich enhanced it by adding embedding per triplets, too. Recently, in the last couple of weeks, I had been working with the community on bringing the “GraphStore” storage context to Llama Index and thus introducing external storage of Knowledge Graph, the first implementation is NebulaGraph the Open-Source Distributed Graph Database that I had been working on since 2021. During the implementation of this, the option to traverse multiple hops of the graph, and the option to collect more key entities on top-k nodes(to search in the knowledge graph to enable more global context) was introduced, and we are still refining the changes. With GraphStore introduced, it also makes it possible to perform in-context learning from an existing knowledge graph, combined with other indices, this is also quite promising due to the knowledge graph being considered with high Information density than other structured data. I will be updating the knowledge graph-related work on Llama Index in this blog in the upcoming weeks, and will then create end-to-end demo projects and tutorials, after the PR is merged, stay tuned! ","date":"2023-06-01","objectID":"/en/graph-enabled-llama-index/:6:0","series":null,"tags":["Nebula Graph","LLM","Llama-Index","In-context Learning"],"title":"Graph Enabled Llama Index","uri":"/en/graph-enabled-llama-index/#progress-of-knowledge-graph-in-llama-index"},{"categories":["Nebula Graph"],"content":"Introducing a new project! ng_ai: NebulaGraph's graph algorithm suite, a user-friendly high-level Python Algorithm API for NebulaGraph. Its goal is to enable data scientist users of NebulaGraph to perform graph-related algorithmic tasks with minimal code.","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/"},{"categories":["Nebula Graph"],"content":" Introducing a new project! ng_ai: NebulaGraph’s graph algorithm suite, a user-friendly high-level Python Algorithm API for NebulaGraph. Its goal is to enable data scientist users of NebulaGraph to perform graph-related algorithmic tasks with minimal code. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:0:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#"},{"categories":["Nebula Graph"],"content":" 1 Nebulagraph AI SuiteThis week, NebulaGraph 3.5.0 has been released, and @whitewum suggested that we make the new project ng_ai that has been launched in the NebulaGraph community public. This blog is the first one to introduce ng_ai! ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#nebulagraph-ai-suite"},{"categories":["Nebula Graph"],"content":" 1.1 What is ng_aiNebulagraph AI Suite. As the name suggests, it is a Python suite for running algorithms on NebulaGraph. Its goal is to provide data scientist users of NebulaGraph with a natural, concise high-level API to perform graph-related algorithmic tasks with minimal code. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:1","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#what-is-ng_ai"},{"categories":["Nebula Graph"],"content":" 1.2 Features Simplifying things in surprising ways. To provide a smooth algorithmic experience for NebulaGraph community users, ng_ai has the following features: Tight integration with NebulaGraph Support for multiple engines and backends, currently supporting Spark (NebulaGraph Algorithm) and NetworkX, with plans to support DGL and PyG in the future. User-friendly and intuitive API design. Seamless integration with NebulaGraph’s UDF, allowing ng_ai tasks to be called from queries. Friendly custom algorithm interface, making it easy for users to implement their own algorithms (WIP). One-click playground setup (based on Docker Extension). ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:1:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#features"},{"categories":["Nebula Graph"],"content":" 2 Demos","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#demos"},{"categories":["Nebula Graph"],"content":" 2.1 Run PageRankWe could run distributed PageRank with Nebula-Algorithms(spark) backend: from ng_ai import NebulaReader # read data with spark engine, scan mode reader = NebulaReader(engine=\"spark\") reader.scan(edge=\"follow\", props=\"degree\") df = reader.read() # run pagerank algorithm pr_result = df.algo.pagerank(reset_prob=0.15, max_iter=10) ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:1","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#run-pagerank"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#write-algo-result-to-nebulagraph"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#call-ng_ai-from-ngql-udf"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#example-with-networkx-engine"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#plotting-with-networkx-engine"},{"categories":["Nebula Graph"],"content":" 2.2 Write Algo Result to NebulaGraphAssuming we want to run a label propagation algorithm and write the results back to NebulaGraph, we can do the following: First, make sure that the schema of the TAG to be written back has been created, and write it to the label_propagation.cluster_id field: CREATE TAG IF NOT EXISTS label_propagation ( cluster_id string NOT NULL ); The algorithm is run as follows: df_result = df.algo.label_propagation() We could see its schema: df_result.printSchema() # result root |-- _id: string (nullable = false) |-- lpa: string (nullable = false) Then, we write the result back to the cluster_id field in NebulaGraph ({\"lpa\": \"cluster_id\"}): from ng_ai import NebulaWriter from ng_ai.config import NebulaGraphConfig config = NebulaGraphConfig() writer = NebulaWriter( data=df_result, sink=\"nebulagraph_vertex\", config=config, engine=\"spark\" ) # map column louvain into property cluster_id properties = {\"lpa\": \"cluster_id\"} writer.set_options( tag=\"label_propagation\", vid_field=\"_id\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Finally, we can verify the results: USE basketballplayer; MATCH (v:label_propagation) RETURN id(v), v.label_propagation.cluster_id LIMIT 3; The results are as follows: +-------------+--------------------------------+ | id(v) | v.label_propagation.cluster_id | +-------------+--------------------------------+ | \"player103\" | \"player101\" | | \"player113\" | \"player129\" | | \"player121\" | \"player129\" | +-------------+--------------------------------+ See more examples: ng_ai/examples 2.2.1 Call ng_ai from nGQL UDFSince NebulaGraph 3.5.0, we can write our own UDF to call our own functions from nGQL. ng_ai also uses this capability to implement an ng_ai function that can call ng_ai algorithms from nGQL, for example: -- Prepare the write schema USE basketballplayer; CREATE TAG IF NOT EXISTS pagerank(pagerank string); :sleep 20; -- Call with ng_ai() RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space: \"basketballplayer\", max_iter: 10}, {write_mode: \"insert\"}) See more examples: ng_ai/examples 2.2.2 Example with NetworkX EngineIn a local environment, ng_ai supports running algorithms based on NetworkX, for example: Read the graph as an ng_ai object: from ng_ai import NebulaReader from ng_ai.config import NebulaGraphConfig # read data with nebula/networkx engine, query mode config_dict = { \"graphd_hosts\": \"graphd:9669\", \"user\": \"root\", \"password\": \"nebula\", \"space\": \"basketballplayer\", } config = NebulaGraphConfig(**config_dict) reader = NebulaReader(engine=\"nebula\", config=config) reader.query(edges=[\"follow\", \"serve\"], props=[[\"degree\"], []]) g = reader.read() Show and draw the graph: g.show(5) g.draw() Run PageRank: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) Write the result back to NebulaGraph: from ng_ai import NebulaWriter writer = NebulaWriter( data=pr_result, sink=\"nebulagraph_vertex\", config=config, engine=\"nebula\", ) # properties to write properties = [\"pagerank\"] writer.set_options( tag=\"pagerank\", properties=properties, batch_size=256, write_mode=\"insert\", ) # write back to NebulaGraph writer.write() Other algorithms are similar, for example: # get all algorithms g.algo.get_all_algo() # get help of each algo help(g.algo.node2vec) # call the algo g.algo.node2vec() See more examples: ng_ai/examples 2.2.3 Plotting with NetworkX EngineWe could also run Louvain and PageRank and visualize the results with NetworkX engine: First, we read the graph and run the algorithm: pr_result = g.algo.pagerank(reset_prob=0.15, max_iter=10) louvain_result = g.algo.louvain() Now we create a fancy function to draw the graph: from matplotlib.colors import ListedColormap def draw_graph_louvain_pr(G, pr_result, louvain_result, colors=[\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#a7d5ed\", \"#e2e2e2\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]): # Define positions for the nodes pos = nx.spring_layout(G) # Create a figure and set the axis ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:2:2","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#nebulagraph-jupyter-notebook-extension"},{"categories":["Nebula Graph"],"content":" 3 Future WorkNow ng_ai is still under development, we still have a lot of work to do: Improve the reader mode, now NebulaGraph/NetworkX only supports Query-Mode, we also need to support Scan-Mode Implement link prediction, node classification and other algorithms based on dgl (GNN), for example: model = g.algo.gnn_link_prediction() result = model.train() # query src, dst to be predicted model.predict(src_vertex, dst_vertices) UDA, custom algorithm Deployment tool ng_ai is completely built in public, and we welcome everyone in the community to participate in it and improve ng_ai together, making AI algorithms on NebulaGraph easier to use! ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:3:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#future-work"},{"categories":["Nebula Graph"],"content":" 4 Try ng_aiWe have prepared a one-click deployment of NebulaGraph + Studio + ng_ai in Jupyter environment, you only need to search NebulaGraph from the Extension of Docker Desktop to try it out. Install NebulaGraph Docker Extension Search NebulaGraph from docker extension marketplace, and install it. Install ng_ai playground Go to NebulaGraph extension, click Install NX Mode to install ng_ai’s NetworkX playground, it usually takes a few minutes to wait for the installation to complete. Enter NetworkX playground Click Jupyter NB NetworkX to enter NetworkX playground. ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:4:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#try-ng_ai"},{"categories":["Nebula Graph"],"content":" 5 ng_ai Architectureng_ai is a Python library, it is mainly composed of the following modules: Writer: responsible for writing data to NebulaGraph Engine: responsible for adapting different runtimes, such as Spark, DGL, NetowrkX, etc. Algo: algorithm module, such as PageRank, Louvain, GNN_Link_Predict, etc. In addition, in order to support the call in nGQL, there are two more modules: ng_ai-udf: responsible for registering UDF to NebulaGraph, accepting query calls from ng_ai, and accessing ng_ai API ng_ai-api: the API module of ng_ai, which is responsible for receiving requests from ng_ai-udf and calling the corresponding algorithm module ┌───────────────────────────────────────────────────┐ │ Spark Cluster │ │ .─────. .─────. .─────. .─────. │ │ ; : ; : ; : ; : │ ┌─▶│ : ; : ; : ; : ; │ │ │ ╲ ╱ ╲ ╱ ╲ ╱ ╲ ╱ │ │ │ `───' `───' `───' `───' │ Algo Spark │ Engine└───────────────────────────────────────────────────┘ │ ┌────────────────────────────────────────────────────┬──────────┐ └──┤ │ │ │ NebulaGraph AI Suite(ngai) │ ngai-api │◀─┐ │ │ │ │ │ └──────────┤ │ │ ┌────────┐ ┌──────┐ ┌────────┐ ┌─────┐ │ │ │ │ Reader │ │ Algo │ │ Writer │ │ GNN │ │ │ ┌───────▶│ └────────┘ └──────┘ └────────┘ └─────┘ │ │ │ │ │ │ │ │ │ │ │ │ ├────────────┴───┬────────┴─────┐ └──────┐ │ │ │ │ ▼ ▼ ▼ ▼ │ │ │ │ ┌─────────────┐ ┌──────────────┐ ┌──────────┐ ┌──────────┐ │ │ │ ┌──┤ │ SparkEngine │ │ NebulaEngine │ │ NetworkX │ │ DGLEngine│ │ │ │ │ │ └─────────────┘ └──────────────┘ └──────────┘ └──────────┘ │ │ │ │ └──────────┬────────────────────────────────────────────────────┘ │ │ │ │ Spark │ │ │ └────────Reader ────────────┐ │ │ Spark Query Mode │ │ │ Reader │ │ │Scan Mode ▼ ┌─────────┐ │ │ ┌───────────────────────────────────────────────────┬─────────┤ ngai-udf│◀─────────────┐ │ │ │ │ └─────────┤ │ │ │ │ NebulaGraph Graph Engine Nebula-GraphD │ ngai-GraphD │ │ │ │ ├──────────────────────────────┬────────────────────┼───────────────────┘ │ │ │ │ │ │ │ │ │ │ NebulaGraph Storage Engine │ │ │ │ │ │ │ │ │ │ └─▶│ Nebula-StorageD │ Nebula-Metad │ │ │ │ │ │ │ │ └──────────────────────────────┴────────────────────┘ │ │ │ │ ┌───────────────────────────────────────────────────────────────────────────────────────┐ │ │ │ RETURN ng_ai(\"pagerank\", [\"follow\"], [\"degree\"], \"spark\", {space:\"basketballplayer\"}) │──┘ │ └───────────────────────────────────────────────────────────────────────────────────────┘ │ ┌─────────────────────────────────────────────────────────────┐ │ │ from ng_ai import NebulaReader │ │ │ │ │ │ # read data with spark engine, scan mode │ │ │ reader = NebulaReader(engine=\"spark\") │ │ │ reader.scan(edge=\"follow\", props=\"degree\") │ └──│ df = reader.read() │ │ │ │ # run pagerank algorithm │ │ pr_result = df.algo.pagerank(reset_prob=0.15, max_iter=10) │ │ │ └─────────────────────────────────────────────────────────────┘ ","date":"2023-05-02","objectID":"/en/nebulagraph-ai-suite/:5:0","series":null,"tags":["Nebula Graph","Algorithm","Graph Algorithm","Graph Computing","Python"],"title":"Nebulagraph Artificial Intelligence Suite","uri":"/en/nebulagraph-ai-suite/#ng_ai-architecture"},{"categories":["Nebula Graph"],"content":"How NebulaGraph helps build social network systems.","date":"2022-12-29","objectID":"/en/nebulagraph-sns/","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/"},{"categories":["Nebula Graph"],"content":" How NebulaGraph helps build social network systems. This blog was origianlly posted on NebulaGraph Blog. Social networks are no stranger to everyone, whether it’s Facebook, Twitter, Youtube, or services such as Yelp, Quora, Reddit, etc., the essence of their users has formed social networks. In a social network system, we can use a graph database to represent users and their connection relationships. Graph databases allow efficient querying of relationships between users, making various business implementations on social networks based on connection findings, statistics, and analysis feasible and efficient. For example, graph databases can be used to identify “influential users” in a network, to recommend new connections (friendships, content of interest) based on commonalities between users, or to find different groups of people and communities in a community to profile users. Graph databases are ideal for social networking systems where user relationships are constantly changing because they can support complex multi-hop queries and also real-time writes and updates. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:0:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#"},{"categories":["Nebula Graph"],"content":" 1 Graph ModelingTo showcase the SNS graph use cases, I’ll build most of the examples on a typically small social network, I started by adding extra data on top of the NebulaGraph default dataset, basketballplayer: Three new tags of vertices: address place post Five new types of edges: created_post commented_at lived_in belong_to It looks like this: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:1:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 2 Importing the data","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#importing-the-data"},{"categories":["Nebula Graph"],"content":" 2.1 Load the default dataset In the Command Line Console, we could just execute:play basketballplayer` to load the default dataset. Or, if we do so from NebulaGraph Studio/Explorer, just click the Download from the Demos in the welcome page: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#load-the-default-dataset"},{"categories":["Nebula Graph"],"content":" 2.2 Add the SNS Graph schemaFirst the DDL for those new tags and edges: CREATE TAG IF NOT EXISTS post(title string NOT NULL); CREATE EDGE created_post(post_time timestamp); CREATE EDGE commented_at(post_time timestamp); CREATE TAG address(address string NOT NULL, `geo_point` geography(point)); CREATE TAG place(name string NOT NULL, `geo_point` geography(point)); CREATE EDGE belong_to(); CREATE EDGE lived_in(); ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#add-the-sns-graph-schema"},{"categories":["Nebula Graph"],"content":" 2.3 Load the dataThen we load the DML part, to insert vertices and edges: INSERT VERTEX post(title) values \\ \"post1\":(\"a beautify flower\"), \"post2\":(\"my first bike\"), \"post3\":(\"I can swim\"), \\ \"post4\":(\"I love you, Dad\"), \"post5\":(\"I hate coriander\"), \"post6\":(\"my best friend, tom\"), \\ \"post7\":(\"my best friend, jerry\"), \"post8\":(\"Frank, the cat\"), \"post9\":(\"sushi rocks\"), \\ \"post10\":(\"I love you, Mom\"), \"post11\":(\"Let's have a party!\"); INSERT EDGE created_post(post_time) values \\ \"player100\"-\u003e\"post1\":(timestamp(\"2019-01-01 00:30:06\")), \\ \"player111\"-\u003e\"post2\":(timestamp(\"2016-11-23 10:04:50\")), \\ \"player101\"-\u003e\"post3\":(timestamp(\"2019-11-11 10:44:06\")), \\ \"player103\"-\u003e\"post4\":(timestamp(\"2014-12-01 20:45:11\")), \\ \"player102\"-\u003e\"post5\":(timestamp(\"2015-03-01 00:30:06\")), \\ \"player104\"-\u003e\"post6\":(timestamp(\"2017-09-21 23:30:06\")), \\ \"player125\"-\u003e\"post7\":(timestamp(\"2018-01-01 00:44:23\")), \\ \"player106\"-\u003e\"post8\":(timestamp(\"2019-01-01 00:30:06\")), \\ \"player117\"-\u003e\"post9\":(timestamp(\"2022-01-01 22:23:30\")), \\ \"player108\"-\u003e\"post10\":(timestamp(\"2011-01-01 10:00:30\")), \\ \"player100\"-\u003e\"post11\":(timestamp(\"2021-11-01 11:10:30\")); INSERT EDGE commented_at(post_time) values \\ \"player105\"-\u003e\"post1\":(timestamp(\"2019-01-02 00:30:06\")), \\ \"player109\"-\u003e\"post1\":(timestamp(\"2016-11-24 10:04:50\")), \\ \"player113\"-\u003e\"post3\":(timestamp(\"2019-11-13 10:44:06\")), \\ \"player101\"-\u003e\"post4\":(timestamp(\"2014-12-04 20:45:11\")), \\ \"player102\"-\u003e\"post1\":(timestamp(\"2015-03-03 00:30:06\")), \\ \"player103\"-\u003e\"post1\":(timestamp(\"2017-09-23 23:30:06\")), \\ \"player102\"-\u003e\"post7\":(timestamp(\"2018-01-04 00:44:23\")), \\ \"player101\"-\u003e\"post8\":(timestamp(\"2019-01-04 00:30:06\")), \\ \"player106\"-\u003e\"post9\":(timestamp(\"2022-01-02 22:23:30\")), \\ \"player105\"-\u003e\"post10\":(timestamp(\"2011-01-11 10:00:30\")), \\ \"player130\"-\u003e\"post1\":(timestamp(\"2019-01-02 00:30:06\")), \\ \"player131\"-\u003e\"post2\":(timestamp(\"2016-11-24 10:04:50\")), \\ \"player131\"-\u003e\"post3\":(timestamp(\"2019-11-13 10:44:06\")), \\ \"player133\"-\u003e\"post4\":(timestamp(\"2014-12-04 20:45:11\")), \\ \"player132\"-\u003e\"post5\":(timestamp(\"2015-03-03 00:30:06\")), \\ \"player134\"-\u003e\"post6\":(timestamp(\"2017-09-23 23:30:06\")), \\ \"player135\"-\u003e\"post7\":(timestamp(\"2018-01-04 00:44:23\")), \\ \"player136\"-\u003e\"post8\":(timestamp(\"2019-01-04 00:30:06\")), \\ \"player137\"-\u003e\"post9\":(timestamp(\"2022-01-02 22:23:30\")), \\ \"player138\"-\u003e\"post10\":(timestamp(\"2011-01-11 10:00:30\")), \\ \"player141\"-\u003e\"post1\":(timestamp(\"2019-01-03 00:30:06\")), \\ \"player142\"-\u003e\"post2\":(timestamp(\"2016-11-25 10:04:50\")), \\ \"player143\"-\u003e\"post3\":(timestamp(\"2019-11-14 10:44:06\")), \\ \"player144\"-\u003e\"post4\":(timestamp(\"2014-12-05 20:45:11\")), \\ \"player145\"-\u003e\"post5\":(timestamp(\"2015-03-04 00:30:06\")), \\ \"player146\"-\u003e\"post6\":(timestamp(\"2017-09-24 23:30:06\")), \\ \"player147\"-\u003e\"post7\":(timestamp(\"2018-01-05 00:44:23\")), \\ \"player148\"-\u003e\"post8\":(timestamp(\"2019-01-05 00:30:06\")), \\ \"player139\"-\u003e\"post9\":(timestamp(\"2022-01-03 22:23:30\")), \\ \"player140\"-\u003e\"post10\":(timestamp(\"2011-01-12 10:01:30\")), \\ \"player141\"-\u003e\"post1\":(timestamp(\"2019-01-04 00:34:06\")), \\ \"player102\"-\u003e\"post2\":(timestamp(\"2016-11-26 10:06:50\")), \\ \"player103\"-\u003e\"post3\":(timestamp(\"2019-11-15 10:45:06\")), \\ \"player104\"-\u003e\"post4\":(timestamp(\"2014-12-06 20:47:11\")), \\ \"player105\"-\u003e\"post5\":(timestamp(\"2015-03-05 00:32:06\")), \\ \"player106\"-\u003e\"post6\":(timestamp(\"2017-09-25 23:31:06\")), \\ \"player107\"-\u003e\"post7\":(timestamp(\"2018-01-06 00:46:23\")), \\ \"player118\"-\u003e\"post8\":(timestamp(\"2019-01-06 00:35:06\")), \\ \"player119\"-\u003e\"post9\":(timestamp(\"2022-01-04 22:26:30\")), \\ \"player110\"-\u003e\"post10\":(timestamp(\"2011-01-15 10:00:30\")), \\ \"player111\"-\u003e\"post1\":(timestamp(\"2019-01-06 00:30:06\")), \\ \"player104\"-\u003e\"post11\":(timestamp(\"2022-01-15 10:00:30\")), \\ \"player125\"-\u003e\"post11\":(timestamp(\"2022-02-15 10:00:30\")), \\ \"player113\"-\u003e\"post11\":(timestamp(\"2022-03-15 10:00:30\")), \\ \"player102\"-\u003e\"post11\":(timestamp(\"2022-04-15 10:00:30\")), \\ \"player108\"-\u003e\"post11\":(timestamp(\"2022-05-15 10:00:30\")); INSERT VERTEX `address` (`address`, `geo_point`) VALUES \\ \"addr_0\":(\"Brittany Forge Apt. 718 East ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:3","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#load-the-data"},{"categories":["Nebula Graph"],"content":" 2.4 First glance at the dataLet’s start with the stats of the data. [basketballplayer]\u003e SUBMIT JOB STATS; +------------+ | New Job Id | +------------+ | 10 | +------------+ [basketballplayer]\u003e SHOW STATS; +---------+----------------+-------+ | Type | Name | Count | +---------+----------------+-------+ | \"Tag\" | \"address\" | 19 | | \"Tag\" | \"place\" | 14 | | \"Tag\" | \"player\" | 51 | | \"Tag\" | \"post\" | 10 | | \"Tag\" | \"team\" | 30 | | \"Edge\" | \"belong_to\" | 19 | | \"Edge\" | \"commented_at\" | 40 | | \"Edge\" | \"created_post\" | 10 | | \"Edge\" | \"follow\" | 81 | | \"Edge\" | \"lived_in\" | 19 | | \"Edge\" | \"serve\" | 152 | | \"Space\" | \"vertices\" | 124 | | \"Space\" | \"edges\" | 321 | +---------+----------------+-------+ Got 13 rows (time spent 1038/51372 us) We could get all of the data: MATCH ()-[e]-\u003e() RETURN e LIMIT 10000 As the data volume is quite small, we could render them all in the canvas of NebulaGraph Explorer: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:2:4","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#first-glance-at-the-data"},{"categories":["Nebula Graph"],"content":" 3 Identifying key peopleIdentifying influencers in social networks involves using a variety of metrics and methods to identify individuals who have a lot of influence in a given network. This is useful for many business scenarios, such as for marketing or researching the spread of information in a network. There are many ways to identify them, and the specific methods and information, relationships, and perspectives considered also depend on the type of these key individuals, and the purpose of acquiring them. Some common methods include looking at the number of followers a person has or the amount of content consumed, their reader engagement on their posts, videos, and the reach of their content (retweets, citations). These methods are also doable on the graph but are rather mundane, so I won’t give examples. Here, we can try to derive these key people on the graph using graph algorithms that evaluate and calculate the importance of nodes. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#identifying-key-people"},{"categories":["Nebula Graph"],"content":" 3.1 PageRankPageRank is a very classic graph algorithm that iterates through the number of relationships between points on a graph to get a score (Rank) for each point. It was originally proposed by Google founders Larry Page and Sergey Brin and used in the early Google search engine to sort search results, where Page can be a pun on Larry Page’s last name and Web Page. PageRank has long been abandoned as too simple in modern, complex search engines, but it still shines in other graph-structured web scenarios, where we can roughly assume that all links are of similar importance and run the algorithm to find those key users in social networks. In NebulaGraph, we can use NebulaGraph Algorithm, NebulaGraph Analytics to run PageRank on large full graphs, while in the daily analysis, validation, and design phases, we don’t need to run results on full data, but on very small subgraphs (up to tens of thousands), we can easily run various graph algorithms in the browser to derive methods that can be used for production. Today, we will use the built-in in-browser graph algorithm function of NebulaGraph Explorer to execute PageRank (the specific method is omitted here, you can refer to the documentation, but it’s really just a matter of mouse clicks). We can see from the above that among all the green players (people) after PageRank calculation, “player.name: Tim Duncan” is the largest one, and the relationship associated with it does seem to be quite a lot, so we select him on the graph, then right-click to invert, select all the points except Tim Duncan, use the backspace key to delete all the other points, and then explore 1 to 5 steps in both directions with him as the starting point. In one step, we get Tim Duncan’s subgraph. As you can see from the subgraphs, Tim Duncan is associated with a very large number of other players, while some other very popular players have served with him in the very popular Spurs team, which confirms the way PageRank is evaluated. Now let’s see if the algorithm will come to the same conclusion for the other dimensions of determination. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#pagerank"},{"categories":["Nebula Graph"],"content":" 3.2 Betweenness CentralityAs you can see from the subgraphs, Tim Duncan is associated with a very large number of other players, while some other very popular players have served with him in the very popular Spurs team, which confirms the way PageRank is evaluated. Now let’s see if the algorithm will come to the same conclusion for the other dimensions of determination. From its five-hop subgraph, it can be seen that unlike the star-shape of the key figure Tim Duncan obtained from PageRank before, Dejounte Murray’s subgraph shows clusters, where it is sensory, intuitive to imagine that Dejounte Murray is really on the necessary path of the minimal path between many nodes, while Tim Duncan seems to be associated with more important connecters. In practical application scenarios, we usually have to understand the definitions in different ways, experiment with different execution results, and analyze to find the structural features that affect the key people we care about, and use them to choose different algorithms for different needs. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:3:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#betweenness-centrality"},{"categories":["Nebula Graph"],"content":" 4 Detect communities and clustersCommunity detection in social networks is a technique to discover community structure by analyzing social relationships. A community structure is a set of nodes that are closely connected to each other in a social network, graph, and these nodes usually have similar characteristics or interests. For example, a community structure may manifest itself as a group of users who are clustered together based on common topics or interests. The purpose of community detection is to identify the boundaries of different communities and determine the nodes in each community by analyzing the social network. This process can be done by using various algorithms such as label propagation algorithm, weakly connected component algorithm and Louvain algorithm. By discovering the community structure, we can better understand the structure and characteristics of social networks, and help social network service providers to better infer and predict behaviors in social networks, and help in good social network governance, advertisement placement, marketing, etc. Since our dataset is fake-generated, the results I get under different algorithms do not show the real meaning, so this chapter just shows the results after community identification using several graph algorithms, in real-world cases, we should also use domain knowledge or other technical means on top of that to collaboratively give the portraits and labels of different groups and communities. Effect of label propagation algorithm. Louvain algorithm: WCC algorithm: In later sections, we could in better chance verify these algorithms again on smaller and simpler subgraphs, with somewhat more interpretable results. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:4:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#detect-communities-and-clusters"},{"categories":["Nebula Graph"],"content":" 5 Friend ClosenessWith the community detection algorithm, it is actually possible to obtain friends with similar interests and close associations to some extent, in a global calculation. So how do we get the other close friends of a given user? We can get this information by counting the number of friends this user has in common with him in order to get this information! Let’s take “Tim Duncan” for example, we know that his two-degree friends (friends of friends: (:player{name: \"Tim Duncan\"})-[:follow]-(f:player)-[:follow]-(fof:player)) are also his friends: Mutual Friend, then it is reasonable to believe that those who have more friends in common with Tim Duncan may have a higher closeness to him. MATCH (start:`player`{name: \"Tim Duncan\"})-[:`follow`]-(f:`player`)-[:`follow`]-(fof:`player`), (start:`player`)-[:`follow`]-(fof:`player`) RETURN fof.`player`.name, count(DISTINCT f) AS NrOfMutualF ORDER BY NrOfMutualF DESC; This query shows that “Tony Parker” and Tim have 5 friends in common and are the closest. fof.player.name NrOfMutualF Tony Parker 5 Dejounte Murray 4 Manu Ginobili 3 Marco Belinelli 3 Danny Green 2 Boris Diaw 1 LaMarcus Aldridge 1 Tiago Splitter 1 Here, let’s verify this result through visualization! First, let’s see who the common friends (f:) are for each of the friends. MATCH (start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:`follow`]-(fof:player), (start:player)-[:`follow`]-(fof:player) RETURN fof.player.name, collect(DISTINCT f.player.name); The result: fof.player.name collect(distinct f.player.name) Boris Diaw [“Tony Parker”] Manu Ginobili [“Dejounte Murray”, “Tiago Splitter”, “Tony Parker”] LaMarcus Aldridge [“Tony Parker”] Tiago Splitter [“Manu Ginobili”] Tony Parker [“Dejounte Murray”, “Boris Diaw”, “Manu Ginobili”, “Marco Belinelli”, “LaMarcus Aldridge”] Dejounte Murray [“Danny Green”, “Tony Parker”, “Manu Ginobili”, “Marco Belinelli”] Danny Green [“Dejounte Murray”, “Marco Belinelli”] Marco Belinelli [“Dejounte Murray”, “Danny Green”, “Tony Parker”] Then we visualize the result on Explorer. First, let’s find out all of Tim’s 2-degree friend paths MATCH p=(start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:follow]-(fof:player) RETURN p Then we render the node size by degree in which we select Tim and Tony and find all paths between them for follow type edge, bidirectional, up to 2 hops: We can see that they are the closest of friends to each other and that their mutual friends are also in the paths. [\"Dejounte Murray\", \"Boris Diaw\", \"Manu Ginobili\", \"Marco Belinelli\", \"LaMarcus Aldridge\"] ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:5:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#friend-closeness"},{"categories":["Nebula Graph"],"content":" 5.1 Small groups in your friendsAt this point, as mentioned earlier, the non-authenticity of this dataset itself makes the results of the community discovery algorithm unable to get the insightful connotation of it. Now we can follow this small subgraph to see how groups and communities can be distinguished among Tim’s friends. Weakly connected components can split Tim’s friends into two or three parts that are not connected to each other, which is very much in line with the intuitive understanding and definition of connected components. Label propagation, we can control the number of iterations on-demand to delineate different degrees of division by random propagation, which results in a certain degree of differentiation. 20 iterations 1000 iterations Louvain, a more efficient and stable algorithm, basically under this subgraph we can get a very intuitive division with a very small number of iterations. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:5:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#small-groups-in-your-friends"},{"categories":["Nebula Graph"],"content":" 6 New friend recommendationFollowing the previous idea of 2-degree friends (friends of friends), we can easily add those 2-degree friends who are not yet friends as recommended friends, and the sorting rule is the number of friends they have in common with each other: MATCH (start:player{name: \"Tim Duncan\"})-[:`follow`]-(f:player)-[:`follow`]-(fof:player) WHERE NOT (start:player)-[:`follow`]-(fof:player) AND fof != start RETURN fof.player.name, count(DISTINCT f) AS NrOfMutualF ORDER BY NrOfMutualF DESC; fof.player.name NrOfMutualF LeBron James 2 James Harden 1 Chris Paul 1 Yao Ming 1 Damian Lillard 1 JaVale McGee 1 Kevin Durant 1 Kyle Anderson 1 Rudy Gay 1 Russell Westbrook 1 Obviously, LeBron is the most recommended! And look at who these mutual friends are. fof.player.name collect(distinct f.player.name) James Harden [“Dejounte Murray”] LeBron James [“Danny Green”, “Dejounte Murray”] Chris Paul [“Dejounte Murray”] Yao Ming [“Shaquille O’Neal”] Damian Lillard [“LaMarcus Aldridge”] JaVale McGee [“Shaquille O’Neal”] Kevin Durant [“Dejounte Murray”] Kyle Anderson [“Dejounte Murray”] Rudy Gay [“LaMarcus Aldridge”] Russell Westbrook [“Dejounte Murray”] 同样，我们在刚才的子图里找找 LeBron James 吧！我们把它俩之间的两步、双向路径找出来，果然只会经过 [\"Danny Green\", \"Dejounte Murray\"] 并且，没有直接的连接： Again, let’s look for LeBron James in the subgraph we just created! And find the two-step, two-way path between them, and sure enough, it only goes through [\"Danny Green\", \"Dejounte Murray\"] and, without a direct connection. Now, the system could send reminders to both sides: “HEY, maybe you two should make new friends!” ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:6:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#new-friend-recommendation"},{"categories":["Nebula Graph"],"content":" 7 Common NeighborFinding common neighbors is a very common graph database query, and its scenarios may bring different scenarios depending on different neighbor relationships and node types. The common buddy in the first two scenarios is essentially a common neighbor between two points, and directly querying such a relationship is very simple with OpenCypher. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#common-neighbor"},{"categories":["Nebula Graph"],"content":" 7.1 A common neighbor between two verticesFor example, this expression can query the commonality, intersection between two users, the result may be common teams, places visited, interests, common participation in post replies, etc.:. MATCH p = (`v0`)--()--(`v1`) WHERE id(`v0`) == \"player100\" AND id(`v1`) == \"player104\" RETURN p And after limiting the type of edge, this query is limited to the common friend query. MATCH p = (v0)--(:`follow`)--(v1) WHERE id(v0) == \"player100\" AND id(v1) == \"player104\" RETURN p ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#a-common-neighbor-between-two-vertices"},{"categories":["Nebula Graph"],"content":" 7.2 Common neighbors among multiple vertices: content notificationBelow, we give a multi-nodes common neighbor scenario where we trigger from a post, find out all the users who have interacted on this post, and find the common neighbors in this group. What is the use of this common neighbor? Naturally, if this common neighbor has not yet had any interaction with this article, we can recommend this article to him. The implementation of this query is interesting. The first MATCH is to find the total number of people who left comments and authors on all post11 articles After the second MATCH, we find the number of friends of the interacting users who have participated in the article that is exactly equal to the number of users who have participated in the article, and they are actually the common friends of all the participating users. MATCH (blog:post)\u003c-[e]-(:player) WHERE id(blog) == \"post11\" WITH blog, count(e) AS invoved_user_count MATCH (blog:post)\u003c-[]-(users:player)-[:`follow`]-(common_neighbor:player) WITH toSet(collect(users)) AS users, common_neighbor, invoved_user_count WHERE size(users) == invoved_user_count RETURN common_neighbor And that person is . . Tony! +-----------------------------------------------------+ | common_neighbor | +-----------------------------------------------------+ | (\"player101\" :player{age: 36, name: \"Tony Parker\"}) | +-----------------------------------------------------+ And we can easily verify it in the visualization of the query: MATCH p=(blog:post)\u003c-[]-(users:player)-[:`follow`]-(common_neighbor:player) WHERE id(blog) == \"post11\" RETURN p Rendering this query, and then looking for two-way, two-hop queries between the article called “Let’s have a party!” and Tony’s comments, posts, and followers, we can see that all the people involved in the article are, without exception, Tony’s friends, and only Tony himself has not yet left a comment on the article! And how can a party be without Tony? Is it his surprise birthday party, Opps, shouldn’t we tell him, or? ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:7:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#common-neighbors-among-multiple-vertices-content-notification"},{"categories":["Nebula Graph"],"content":" 8 Feed GenerationI have previously written about the implementation of recommendation systems based on graph technology, in which I described that content filtering and sorting methods in modern recommendation systems can be performed on graphs. It is also highly time-sensitive. The feed generation in a SNS is quite similar but slightly different. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#feed-generation"},{"categories":["Nebula Graph"],"content":" 8.1 Content with friend engagementThe simplest and most straightforward definition of content generation may be the facebook feed of content created and engaged by people you follow. Content created by friends within a certain period of time the content of friends’ comments within a certain time frame We can use OpenCypher to express this query for the stream of information with user id player100. MATCH (feed_owner:player)-[:`follow`]-(friend:player) WHERE id(feed_owner) == \"player100\" OPTIONAL MATCH (friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH (friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") WITH DISTINCT friend, collect(DISTINCT po.post.title) + collect(\"comment of \" + dst(newly_commented)) AS feeds WHERE size(feeds) \u003e 0 RETURN friend.player.name, feeds friend.player.name feeds Boris Diaw [“I love you, Mom”, “comment of post11”] Marco Belinelli [“my best friend, tom”, “comment of post11”] Danny Green [“comment of post1”] Tiago Splitter [“comment of post1”] Dejounte Murray [“comment of post11”] Tony Parker [“I can swim”] LaMarcus Aldridge [“I hate coriander”, “comment of post11”, “comment of post1”] Manu Ginobili [“my best friend, jerry”, “comment of post11”, “comment of post11”] So, we can send these comments, articles to the user’s feed. Let’s also see what they look like on the graph, we output all the paths we queried: MATCH p=(feed_owner:player)-[:`follow`]-(friend:player) WHERE id(feed_owner) == \"player100\" OPTIONAL MATCH p_comment=(friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH p_post=(friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") RETURN p, p_comment, p_post Rendering on Explorer and selecting the “Neural Network” layout, you can clearly see the pink article nodes and the edges representing the comments. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#content-with-friend-engagement"},{"categories":["Nebula Graph"],"content":" 8.2 Content of nearby friendsLet’s go a step further and take geographic information(GeoSpatial) into account to get content related to friends whose addresses have a latitude and longitude less than a certain distance. Here, we use NebulaGraph’s GeoSpatial geography function, the constraint ST_Distance(home.address.geo_point, friend_addr.address.geo_point) AS distance WHERE distance \u003c 1000000 helps us express the distance limit. MATCH (home:address)-[:lived_in]-(feed_owner:player)-[:`follow`]-(friend:player)-[:lived_in]-(friend_addr:address) WHERE id(feed_owner) == \"player100\" WITH feed_owner, friend, ST_Distance(home.address.geo_point, friend_addr.address.geo_point) AS distance WHERE distance \u003c 1000000 OPTIONAL MATCH (friend:player)-[newly_commented:commented_at]-\u003e(:post)\u003c-[:created_post]-(feed_owner:player) WHERE newly_commented.post_time \u003e timestamp(\"2010-01-01 00:00:00\") OPTIONAL MATCH (friend:player)-[newly_created:created_post]-\u003e(po:post) WHERE newly_created.post_time \u003e timestamp(\"2010-01-01 00:00:00\") WITH DISTINCT friend, collect(DISTINCT po.post.title) + collect(\"comment of \" + dst(newly_commented)) AS feeds WHERE size(feeds) \u003e 0 RETURN friend.player.name, feeds friend.player.name feeds Marco Belinelli [“my best friend, tom”, “comment of post11”] Tony Parker [“I can swim”] Danny Green [“comment of post1”] At this point, you can also see the relationship between addresses and their latitude and longitude information from the visualization of this result. I manually arranged the nodes of the addresses on the graph according to their latitude and longitude and saw that the address (7, 8) of Tim(player100), the owner of this feed, is exactly in the middle of other friends’ addresses. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:8:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#content-of-nearby-friends"},{"categories":["Nebula Graph"],"content":" 9 Spatio-temporal relationship trackingSpatio-temporal relationship tracking is a typical application that uses graph traversal to make the most of complicated and messy information in scenarios such as public safety, logistics, and epidemic prevention and control. When we build such a graph, we often need only simple graph queries to gain very useful insights. In this section, I’ll give an example of this application scenario. ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#spatio-temporal-relationship-tracking"},{"categories":["Nebula Graph"],"content":" 9.1 DatasetFor this purpose, I created a fake dataset by which to build a spatio-temporal relationship graph. The dataset generation program and a file that can be used directly are placed on GitHub at https://github.com/wey-gu/covid-track-graph-datagen. It models the data as follows. We could get the data ready in three lines in any Linux System: # Install NebulaGraph + NebulaGraph Studio curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3 # Clone the dataset git clone https://github.com/wey-gu/covid-track-graph-datagen \u0026\u0026 cd covid-track-graph-datagen # Load the dataset into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/:/root \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer-config.yaml Then we could inspect the data from console: ~/.nebula-up/console.sh # access console, and sse the covid_trace graph space USE covid_trace; # check stats SHOW STATS Results: (root@nebula) [covid_trace]\u003e SHOW STATS +---------+------------+--------+ | Type | Name | Count | +---------+------------+--------+ | \"Tag\" | \"人\" | 10000 | | \"Tag\" | \"地址\" | 1000 | | \"Tag\" | \"城市\" | 341 | | \"Tag\" | \"村镇\" | 42950 | | \"Tag\" | \"省份\" | 32 | | \"Tag\" | \"联系方式\" | 0 | | \"Tag\" | \"行政区\" | 3134 | | \"Tag\" | \"街道\" | 667911 | | \"Edge\" | \"住址\" | 0 | | \"Edge\" | \"到访\" | 19986 | | \"Edge\" | \"同住\" | 19998 | | \"Edge\" | \"属于\" | 715336 | | \"Space\" | \"vertices\" | 725368 | | \"Space\" | \"edges\" | 755320 | +---------+------------+--------+ Got 14 rows (time spent 1087/46271 us) ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:1","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#dataset"},{"categories":["Nebula Graph"],"content":" 9.2 Connections between twoThis could be done with FIND PATH # SHORTEST FIND SHORTEST PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths # ALL PATH FIND ALL PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths | LIMIT 10 SHORTEST Path result: paths \u003c(“p_100”)\u003c-[:同住@0 {}]-(“p_2136”)\u003c-[:同住@0 {}]-(“p_3708”)-[:到访@0 {}]-\u003e(“a_125”)\u003c-[:到访@0 {}]-(“p_101”)\u003e ALL Path result: paths \u003c(“p_100”)\u003c-[:同住@0 {}]-(“p_2136”)\u003c-[:同住@0 {}]-(“p_3708”)-[:到访@0 {}]-\u003e(“a_125”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:到访@0 {}]-\u003e(“a_328”)\u003c-[:到访@0 {}]-(“p_6976”)\u003c-[:同住@0 {}]-(“p_261”)-[:到访@0 {}]-\u003e(“a_352”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:同住@0 {}]-\u003e(“p_8709”)-[:同住@0 {}]-\u003e(“p_9315”)-[:同住@0 {}]-\u003e(“p_261”)-[:到访@0 {}]-\u003e(“a_352”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:到访@0 {}]-\u003e(“a_328”)\u003c-[:到访@0 {}]-(“p_6311”)-[:同住@0 {}]-\u003e(“p_3941”)-[:到访@0 {}]-\u003e(“a_345”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:到访@0 {}]-\u003e(“a_328”)\u003c-[:到访@0 {}]-(“p_5046”)-[:同住@0 {}]-\u003e(“p_3993”)-[:到访@0 {}]-\u003e(“a_144”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:同住@0 {}]-\u003e(“p_3457”)-[:到访@0 {}]-\u003e(“a_199”)\u003c-[:到访@0 {}]-(“p_6771”)-[:到访@0 {}]-\u003e(“a_458”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)\u003c-[:同住@0 {}]-(“p_1462”)-[:到访@0 {}]-\u003e(“a_922”)\u003c-[:到访@0 {}]-(“p_5869”)-[:到访@0 {}]-\u003e(“a_345”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)\u003c-[:同住@0 {}]-(“p_9489”)-[:到访@0 {}]-\u003e(“a_985”)\u003c-[:到访@0 {}]-(“p_2733”)-[:到访@0 {}]-\u003e(“a_458”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)\u003c-[:同住@0 {}]-(“p_9489”)-[:到访@0 {}]-\u003e(“a_905”)\u003c-[:到访@0 {}]-(“p_2733”)-[:到访@0 {}]-\u003e(“a_458”)\u003c-[:到访@0 {}]-(“p_101”)\u003e \u003c(“p_100”)-[:到访@0 {}]-\u003e(“a_89”)\u003c-[:到访@0 {}]-(“p_1333”)\u003c-[:同住@0 {}]-(“p_1683”)-[:到访@0 {}]-\u003e(“a_345”)\u003c-[:到访@0 {}]-(“p_101”)\u003e We render all the paths visually, mark the two people at the starting node and end end, and check their shortest paths in between, and the inextricable relationship between them is clear at a glance, whether it is for business insight, public safety or epidemic prevention and control purposes, with this information, the corresponding work can progress downward like a tiger. Of course, on a real world system, it may be that we only need to care about the proximity of the association between two users: FIND SHORTEST PATH FROM \"p_100\" TO \"p_101\" OVER * BIDIRECT YIELD PATH AS paths | YIELD collect(length($-.paths)) AS len | YIELD coalesce($-.len[0], -1) AS len In the result we only care about the length of the shortest path between them as: 4. len 4 ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:2","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#connections-between-two"},{"categories":["Nebula Graph"],"content":" 9.3 Temporal intersection of peopleFurther we can use graph semantics to outline any patterns with temporal and spatial information that we want to identify and query them in real time in the graph, e.g. for a given person whose id is p_101, we differ all the people who have temporal and spatial intersection with him at a given time, which means that those people also stay and visit a place within the time period in which p_101 visits those places. MATCH (p:person)-[`visit0`:visited]-\u003e(`addr`:address)\u003c-[`visit1`:visited]-(p1:person) WHERE id(p) == \"p_101\" AND `visit0`.`start_time` \u003c `visit1`.`end_time` RETURN `addr`.address.`name`, collect(p1.person.`name`) 我们得到了在每一个到访地点的时空相交人列表如下： We obtained the following list of temporal intersection people at each visited location. addr.address.name collect(p1.person.name) 闵行仇路q座 255960 [“徐畅”, “王佳”, “曾亮”, “姜桂香”, “邵秀英”, “韦婷婷”, “陶玉”, “马坤”, “黄想”, “张秀芳”, “颜桂芳”, “张洋”] 丰都北京路J座 725701 [“陈春梅”, “施婷婷”, “井成”, “范文”, “王楠”, “尚明”, “薛秀珍”, “宋金凤”, “杨雪”, “邓丽华”, “李杨”, “温佳”, “叶玉”, “周明”, “王桂珍”, “段玉华”, “金成”, “黄鑫”, “邬兵”, “魏柳”, “王兰英”, “杨柳”] 普陀潜江路P座 210730 [“储平”, “洪红霞”, “沈玉英”, “王洁”, “董玉英”, “邓凤英”, “谢海燕”, “梁雷”, “张畅”, “任玉兰”, “贾宇”, “汪成”, “孙琴”, “纪红梅”, “王欣”, “陈兵”, “张成”, “王东”, “谷霞”, “林成”] 普陀武街f座 706352 [“邢成”, “张建军”, “张鑫”, “戴涛”, “蔡洋”, “汪燕”, “尹亮”, “何利”, “何玉”, “周波”, “金秀珍”, “杨波”, “张帅”, “周柳”, “马云”, “张建华”, “王丽丽”, “陈丽”, “万萍”] 城东贵阳街O座 110567 [“李洁”, “陈静”, “王建国”, “方淑华”, “古想”, “漆萍”, “詹桂花”, “王成”, “李慧”, “孙娜”, “马伟”, “谢杰”, “王鹏”, “鞠桂英”, “莫桂英”, “汪雷”, “黄彬”, “李玉梅”, “祝红梅”] Now, let’s visualize this result on a graph: MATCH (p:person)-[`visit0`:visited]-\u003e(`addr`:address)\u003c-[`visit1`:visited]-(p1:person) WHERE id(p) == \"p_101\" AND `visit0`.`start_time` \u003c `visit1`.`end_time` RETURN paths; In the result, we marked p_101 as a different icon, and identified the gathering community with the label propagation algorithm, isn’t a graph worth a thousand words? ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:3","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#temporal-intersection-of-people"},{"categories":["Nebula Graph"],"content":" 9.4 Most recently visited provincesFinally, we then use a simple query pattern to express all the provinces a person has visited in a given time, say from a point in time: MATCH (p:person)-[visit:visted]-\u003e(`addr`:address)-[:belong_to*5]-(prov:province) WHERE id(p) == \"p_101\" AND visit.start_time \u003e 1625469000 RETURN prov.province.name, collect(addr.address.name); Result: prov.province.name collect(addr.address.name) 四川省 [“闵行仇路q座 255960”] 山东省 [“城东贵阳街O座 110567”] 云南省 [“丰都北京路J座 725701”] 福建省 [“普陀潜江路P座 210730”] 内蒙古自治区 [“普陀武街f座 706352”] The usual rules, let’s look at the results on the graph, this time, we choose Dagre-LR layout rendering, and the result looks like: ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:9:4","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#most-recently-visited-provinces"},{"categories":["Nebula Graph"],"content":" 10 RecapWe have given quite a few examples of applications in social networks, including Finding key people Identifying clusters of people, communities Determining the closeness between two users Recommending new friends Pinpointing important content using common neighbors Push information flow based on friend relationship and geographic location Use spatio-temporal relationship mapping to query the relationship between people, get the people who intersected in time and space, and the provinces visited As a natural graph structure, social networks are well suited to use graph technology to store, query, compute, analyze and visualize to solve various problems on them. We hope you can have a preliminary understanding of the graph technology in SNS through this post. Feature image credit: Ryoji ","date":"2022-12-29","objectID":"/en/nebulagraph-sns/:10:0","series":null,"tags":["Nebula Graph","SNS","Graph Algorithm"],"title":"Social Network with NebulaGraph","uri":"/en/nebulagraph-sns/#recap"},{"categories":["Nebula Graph"],"content":"An attempt to use ChatGPT to generate code for a data scraper to predict sports events with the help of the NebulaGraph graph database and graph algorithms.","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/"},{"categories":["Nebula Graph"],"content":" An attempt to use ChatGPT to generate code for a data scraper to predict sports events with the help of the NebulaGraph graph database and graph algorithms. This post was initially published in https://www.nebula-graph.io/posts/predict-fifa-world-cup-with-chatgpt-and-nebulagraph ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:0:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#"},{"categories":["Nebula Graph"],"content":" 1 The HypeIn the hype for FIFA 2022 World Cup, when I saw a blog post from Cambridge Intelligence, where they leveraged limited information and correlations among players, teams, and clubs to predict the final winner team, I always would like to try similar things with NebulaGraph to share the ideas of graph algorithm to extract hidden information from the overall connections in a graph in the community. The initial attempt was to make it done in like 2 hours, but I noticed the dataset need to be parsed carefully from Wikipedia and I happened to be not good at doing this job, so I put the idea on hold for a couple of days. In the meantime, another hype, the OpenAI ChatGPT was announced, as I had been a user of DALL-E 2 already(to generate feature images of my blog posts), I gave it a try very quickly, too. And I witnessed how other guys(via Twitter, blogs, hacker news) tried to convince ChatGPT to do so many things that are hard to believe they could do: Help to implement a piece of code at any time Simulate any prompt interface: shell, python, virtual machine, or even a language you create Act out almost any given persona, and chat with you Write poetry, rap, prose Find a bug in a piece of code Explain the meaning of a complex regular expression/Open Cypher Query ChatGPT’s ability to contextualize and understand has never been greater before, so much so that everyone is talking about a new way of working: how to master asking/convincing/triggering machines to help us do our jobs, better and faster. I commented on this tweet, where they taught ChatGPT how to draw and render basic SVGs, then they started to ask him/her to draw any other complex things just after him/her learned in one second, that it’s just like Kame-sennin(human) teaches Sun Wukong kung fu as a young Saiyan. Be sure to check this Twitter thread, it’s really interesting! Hey, cool, directly rendering SVGs in #ChatGPT ! pic.twitter.com/VQX9kYIrxT — Br𝕏d Skggs (@brdskggs) December 4, 2022 So, after trying to get ChatGPT to help me write complex graph database query statements, explain the meaning of complex graph query statements, and explain the meaning of a large chunk of Bison code, and he/she had done them WELL, I realized: why not let ChatGPT write the code that extracts the data for me? ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:1:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#the-hype"},{"categories":["Nebula Graph"],"content":" 2 Grabbing dataI really tried it and the result is… good enough. The whole process was basically like a coding interviewer, or a product manager, presenting my requirements, and ChatGPT giving me the code implementation. I then try to run the code, find the things that don’t make sense in the code, point them out, and give suggestions, and ChatGPT really understands the points I point out and makes the appropriate corrections, like: I won’t list this whole process, but I share the generated code and the whole discussion here. The final generated data is a CSV file. Raw version world_cup_squads.csv Manually modified, separated columns for birthday and age world_cup_squads_v0.csv It contains information/columns of team, group, number, position, player name, birthday, age, number of international matches played, number of goals scored, and club served. Team,Group,No.,Pos.,Player,DOB,Age,Caps,Goals,Club Ecuador,A,1,1GK,Hernán Galíndez,(1987-03-30)30 March 1987,35,12,0,Aucas Ecuador,A,2,2DF,Félix Torres,(1997-01-11)11 January 1997,25,17,2,Santos Laguna Ecuador,A,3,2DF,Piero Hincapié,(2002-01-09)9 January 2002,20,21,1,Bayer Leverkusen Ecuador,A,4,2DF,Robert Arboleda,(1991-10-22)22 October 1991,31,33,2,São Paulo Ecuador,A,5,3MF,José Cifuentes,(1999-03-12)12 March 1999,23,11,0,Los Angeles FC Final version with header removed world_cup_squads_no_headers.csv ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:2:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#grabbing-data"},{"categories":["Nebula Graph"],"content":" 3 Graph algorithm to predict the 2022 World CupWith the help of ChatGPT, I could finally try to predict the winner of the game with Graph Magic, before that, I need to map the data into the graph view. If you don’t care about the process, just go to the predicted result directly. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-algorithm-to-predict-the-2022-world-cup"},{"categories":["Nebula Graph"],"content":" 3.1 Graph modeling Prerequisites: This article uses NebulaGraph(Open-Source) and NebulaGraph Explorer(Proprietary), which you can request a trial of on AWS. Graph Modeling is the abstraction and representation of real-world information in the form of a “vertex-\u003e edge” graph, in our case, we will project the information parsed from Wikipedia as: Vertices: player team group club Edges: groupedin (the team belongs to which group) belongto (players belong to the national team) serve (players serve in the club) The age of the players, the number of international caps, and the number of goals scored are naturally fit as properties for the player tag(type of vertex). The following is a screenshot of this schema in NebulaGraph Explorer (will just call it Explorer later). Then, we can click the save icon in the upper right corner and the button: Apply to Space to actually create a graph space with the defined schema Note: Refer to the document https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/draft/ ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:1","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 3.2 Ingesting into NebulaGraphWith the graph modeling, we can upload the CSV file (the no-header version) into Explorer, by pointing and selecting the vid and properties that map the different columns to the vertices and edges. Click Import, we then import the whole graph to NebulaGraph, and after it succeeded, we could also get the whole CSV –\u003e Nebula Importer configuration file: [nebula_importer_config_fifa.yml](https://github.com/siwei-io/talks/files/10164014/config _fifa.yml.txt), so that you reuse it in the future whenever to re-import the same data or share it with others. Note: Refer to the document https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/11.import-data/ After importing, we can view the statistics on the schema view page, which will show us that 831 players participated in the 2022 Qatar World Cup, serving in 295 different clubs. Note: refer to the documentation: https://docs.nebula-graph.io/3.3.0/nebula-explorer/db-management/10.create-schema/#view_statistics ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:2","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#ingesting-into-nebulagraph"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLet’s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so let’s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvain’s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#explore-the-graph"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLet’s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so let’s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvain’s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#querying-the-data"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLet’s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so let’s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvain’s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#initial-observation"},{"categories":["Nebula Graph"],"content":" 3.3 Explore the graphLet’s see what insights we could get from the information/ knowledge in form of a graph. 3.3.1 Querying the dataWe could start by showing all the data and see what we will get. First, with the help of NebulaGraph Explorer, I simply did drag and drop to draw any type of vertex type (TAG) and any type of edge between vertex types (TAG), here we know that all the vertices are connected with others, so no isolated vertices will be missed by this query pattern: Let it generate the query statement for me. Here, it defaults to LIMIT 100, so let’s change it to something larger (LIMIT 10000) and let it execute in the Console. 3.3.2 Initial observationThe result renders out like this, and you can see that it naturally forms a pattern of clusters. These peripheral clusters are mostly made up of players from clubs that are not traditionally strong ones (now we learned that they could win, though, who knows!), and many of those clubs have only one or two players and yet concentrated in one national team or region, so they are kind of isolated from other clusters. 3.3.3 Graph algorithm based analysisAfter I clicked on the two buttons(Sized by Degrees, Colored by Louvain Algorithm) in Explorer (refer to the document for details), in the browser, we can see that the entire graph has become something like this: Here, two graph algorithms are utilized to analyze the insights here. change the display size of vertices to highlight importance using their degrees using Louvain’s algorithm to distinguish the community of the vertices You can see that the big red circle is the famous Barcelona, and its players are marked in red, too. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:3","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#graph-algorithm-based-analysis"},{"categories":["Nebula Graph"],"content":" 3.4 Winner Prediction AlgorithmIn order to be able to make full use of the graph magic(with the implied conditions, and information on the graph), my idea(stolen/inspired from this post) is to choose a graph algorithm that considers edges for node importance analysis, to find out the vertices that have higher importance, iterate and rank them globally, and thus get the top team rankings. These methods actually reflect the fact that awesome players have greater community, and connectivity at the same time, and at the same time, to increase the differentiation between traditionally strong teams, I am going to take into account the information of appearances and goals scored. Ultimately, my algorithm is. Take all the (player)-serve-\u003e(club) relationships and filter them for players with too few goals and too few goals per game (to balance out the disproportionate impact of older players from some weaker teams) Explore outwards from all filtered players to get national teams Run the Betweenness Centrality algorithm on the above subgraph to calculate the node importance scores Note, Betweenness Centrality is an algorithm to measure how a node is important in sense of bridging other nodes in the graph. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:4","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#winner-prediction-algorithm"},{"categories":["Nebula Graph"],"content":" 3.5 Process of the Prediction首先，我们取出所有进球数超过 10，场均进球超过 0.2 的 (球员)-服役-\u003e(俱乐部) 的子图： First, we take out the subgraph in the pattern of (player)-serve-\u003e(club) for those who have scored more than 10 goals and have an average of more than 0.2 goals per game. MATCH ()-[e]-\u003e() WITH e LIMIT 10000 WITH e AS e WHERE e.goals \u003e 10 AND toFloat(e.goals)/e.caps \u003e 0.2 RETURN e Note: For convenience, I have included the number of goals and caps as properties in the serve edge, too. Then, we select all the vertices on the graph, in the left toolbar, select the belongto edge of the outgoing direction, expand the graph outwards (traverse), and select the icon that marks the newly expanded vertices as flags. Now that we have the final subgraph, we use the graph algorithm function within the browser to execute BNC (Betweenness Centrality): The graph canvas then looks like this: ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:3:5","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#process-of-the-prediction"},{"categories":["Nebula Graph"],"content":" 4 ResultIn the end, we sorted according to the value of Betweenness Centrality to get the final winning team: Brazil! 🇧🇷, followed by Belgium, Germany, England, France, and Argentina, so let’s wait two weeks to come back and see if the prediction is accurate :D. The sorted data is as follows: Vertex Betweenness Centrality Brazil🇧🇷 3499 Paris Saint-Germain 3073.3333333333300 Neymar 3000 Tottenham Hotspur 2740 Belgium🇧🇪 2587.833333333330 Richarlison 2541 Kevin De Bruyne 2184 Manchester City 2125 İlkay Gündoğan 2064 Germany🇩🇪 2046 Harry Kane (captain 1869 England🏴󠁧󠁢󠁥󠁮󠁧󠁿 1864 France🇫🇷 1858.6666666666700 Argentina🇦🇷 1834.6666666666700 Bayern Munich 1567 Kylian Mbappé 1535.3333333333300 Lionel Messi (captain 1535.3333333333300 Gabriel Jesus 1344 Feature Image Credit: The image was also generated with OpenAI, through the DALL-E 2 model \u0026 DALL-E 2 Outpainting, see the original image. ","date":"2022-12-07","objectID":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/:4:0","series":null,"tags":["Nebula Graph","FIFA 2022","Graph Modeling","OpenAI","chatGPT","Graph Algorithm"],"title":"Use ChatGPT and Nebulagraph to Predict Fifa World Cup","uri":"/en/chatgpt-and-nebulagraph-predict-fifa-world-cup/#result"},{"categories":["Nebula Graph"],"content":"How could we model data in Tabular sources and ETL it to NebulaGraph? This article demonstrates an end-to-end example of doing so with dbt.","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/"},{"categories":["Nebula Graph"],"content":" How could we model data in Tabular sources and ETL it to NebulaGraph? This article demonstrates an end-to-end example of doing so with dbt. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:0:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#"},{"categories":["Nebula Graph"],"content":" 1 TaskImagine we are building a Knowledge Graph for a content provider web service with NebulaGraph, thus leveraging it to support a Knowledge Base QA system, Recommendation System, and Reasoning system. The knowledge information persisted in different data sources from some Service APIs, Databases, Data Warehouses, or even some files in S3. We need to: Analyze data to extract needed knowledge Model the Graph based on relationships we care Extract the relationships and ingest them to NebulaGraph ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:1:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#task"},{"categories":["Nebula Graph"],"content":" 2 Data AnalysisAssume that we are fetching data from OMDB and MovieLens. OMDB is an open movie database, we now think of it as one of our services, and we can get the following information. Movies Classification of movies The crew in the movie (director, action director, actors, post-production, etc.) Movie covers, promos, etc. MovieLens is an open dataset, we consider it as the user data of our services, the information we can obtain is: Users Movies User interaction on movie ratings ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:2:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-analysis"},{"categories":["Nebula Graph"],"content":" 3 Graph ModelingWe were building this Graph for a recommendation system and talked about some basic methods in this article, which: In the Content-Base Filter method(CBF), the relationship of user-\u003e movie, movie-\u003e category, movie-\u003e actor, and movie-\u003e director are concerned. And the collaborative filtering approach is concerned with the relationship between the user and -\u003e movie. The recommendation reasoning service is concerned with all the above relationships. To summarize, we need the following edges: watched(rate(double)) with_genre directed_by acted_by Accordingly, the vertex types will be: user(user_id) movie(name) person(name, birthdate) genre(name) ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:3:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 4 Data TransformWith the source date finalized, let’s see how they could be mapped and transformed into the graph. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-transform"},{"categories":["Nebula Graph"],"content":" 4.1 From OMDBFirst, there is the data in OMDB, which consists of many tables, such as the table all_movies, which stores all the movies and their names in different languages. movie_id name language_iso_639_1 official_translation 1 Cowboy Bebop de 1 1 Cowboy Bebop en 1 2 Ariel - Abgebrannt in Helsinki de 0 3 Shadows in Paradise en 0 3 Im Schatten des Paradieses de 0 3 Schatten im Paradies de 1 And the all_casts table holds all roles in the film industry. movie_id person_id job_id role position 11 1 21 1 11 1 13 1 11 2 15 Luke Skywalker 1 11 3 15 Han Solo 3 11 4 15 Leia Organa 2 But the name and other information of each person here, as well as the position he/she holds in the film, are in separate tables. job_names For example, 1 stands for writer, and 2 stands for producer. Interestingly, like movie id and name, job_id to name is a one-to-many relationship, because the data in OMDB is multilingual. job_id name language_iso_639_1 1 Autoren de 1 Writing Department en 1 Departamento de redacción es 1 Département écriture fr 1 Scenariusz pl 2 Produzenten de 2 Production Department en all_people id name birthday deathday gender 1 George Lucas 1944-05-14 \\N 0 2 Mark Hamill 1951-09-25 \\N 0 3 Harrison Ford 1942-07-13 \\N 0 4 Carrie Fisher 1956-10-21 2016-12-27 1 5 Peter Cushing 1913-05-26 1994-08-11 0 6 Anthony Daniels 1946-02-21 \\N 0 This is a typical case in RDBMS where the data source is a table structure, so for the relationship movie \u003c-[directed_by]-(person), it involves four tables all_movies, all_casts, all_people, job_names: directed_by Starting from person_id in all_casts To movie_id in all_casts Where job_id is “director” in job_names movie person_id in all_casts Name from all_movies by id, language is “en” person movie_id in all_casts Name, birthday in all_people Till now, all tables we cared about in OMDB are: ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#from-omdb"},{"categories":["Nebula Graph"],"content":" 4.2 From MovieLens datasetWhile the above is just about one data source, in real scenarios, we also need to collect and aggregate data from other sources. For example, now also need to extract knowledge from the MovieLens dataset. Here, the only relationship we utilize is user -\u003e movie. movies.csv movieId title genres 1 Toy Story (1995) Adventure 2 Jumanji (1995) Adventure 3 Grumpier Old Men (1995) Comedy 4 Waiting to Exhale (1995) Comedy ratings.csv userId movieId rating timestamp 1 1 4 964982703 1 3 4 964981247 1 6 4 964982224 From the preview of the data in the two tables, naturally, we need one type of relationship: watched and vertex: user: watched Starting from the userId in ratings.csv To movieId in ratings.csv With rating from rating in ratings.csv user With userId from ratings.csv However, you must have noticed that movieId in the MovieLens dataset and movie id in OMDB are two different systems, if we need to associate them, we need to convert movieId in MovieLens to movie id in OMDB, and the condition of association between them is a movie title. However, by observation, we know that: the titles in OMDB movies are multilingual the titles in MovieLens have the year information like (1995) at the end of the title So our conclusion is watched Starting from the userId in ratings.csv To movieId in ratings.csv Get the movie title with movieId from movies.csv and find its movie_id from OMDB Where we should match the title in language: English with the suffix of the year being removed With rating from rating in ratings.csv user With userId from ratings.csv Now the modeling puts the two tables like this figure: ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#from-movielens-dataset"},{"categories":["Nebula Graph"],"content":" 4.3 Graph Modeling (Property Graph)To summarize, we need to aggregate different tables (or CSV files in table form) from multiple data sources, such that the correspondence is shown in the figure: where the blue dashed line indicates the source of data information for the vertices in the graph, and the pink dashed line indicates the source of edge information. Then, we have to format the ids of individuals in different tables, for example, user_id, which is a self-incrementing number that we want to convert to a globally unique vertex_id. A convenient way to do this is to add a string prefix to the existing id, such as u_. Eventually, for the relationship user -[watched]-\u003e movie, we can process the table structure data as follows. user_id rating title omdb_movie_id u_1 5 Seven (a.k.a. Se7en) 807 u_1 5 Star Wars: Episode IV - A New Hope 11 u_1 5 Star Wars: Episode IV - A New Hope 10 u_1 4 Mask, The 832 u_1 3 Mrs. Doubtfire 832 Where, in each row, three variables exist to construct the graph structure: user vertex id movie vertex id the rating value as the property of the watched edge ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:4:3","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#graph-modeling-property-graph"},{"categories":["Nebula Graph"],"content":" 5 ToolingAt this point, we have completed the data analysis and graph modeling design, before we start the “extract correlations, import graph database”, let’s introduce the tools we will use. “Extracting relationships” can be simply considered as Extract and Transform in ETL, which is essentially the engineering of data mapping and transformation, and there are many different tools and open-source projects available on the market. Here we use one of my personal favorite tools: dbt. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#tooling"},{"categories":["Nebula Graph"],"content":" 5.1 dbtdbt is an open-source data conversion tool with a very mature community and ecology, which can perform efficient, controlled, and high-quality data conversion work in most of the mainstream data warehouses, whether it is for ad-hoc tasks or complex orchestration, dbt can be very competent. One of the features of dbt is that it uses a SQL-like language to describe the rules of data transformation. With GitOps, it is very elegant to collaborate and maintain complex data processing operations in large data teams. And the built-in data testing capabilities allow you to control the quality of your data and make it reproducible and controllable. dbt not only has many integrated subprojects but also can be combined with many other excellent open source projects (meltano, AirFlow, Amundsen, Superset, etc.) to form a set of modern data infrastructure systems, feel free to check my previous article: data lineage and metadata governance reference architecture https://siwei.io/en/data-lineage-oss-ref-solution, where the whole solution looks like: In short, dbt is a command line tool written in python, and we can create a project folder, which contains a YAML formatted configuration file, to specify where the source information for the data transformation is and where the target is (where the processed data is stored, maybe Postgres, Big Query, Spark, etc.). In the data source, we use the YAML file along with the .SQL file to describe the information about “what data to fetch from, how to do the transformation, and what to output”. You can see that the information in the models/example is the core data transformation rules, and all the other data is metadata related to this transformation. DataOps. Notes. You can refer to the dbt documentation to get a hands-on understanding of it: https://docs.getdbt.com/docs/get-started/getting-started-dbt-core ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#dbt"},{"categories":["Nebula Graph"],"content":" 5.2 NebulaGraph data ingestionAfter processing the data by dbt, we can get intermediate data that maps directly to different types of vertices, edges, and table structures of their attributes, either in the form of CSV files, tables in DWs, or even data frames in Spark, and there are different options for importing them into NebulaGraph, of which NebulaGraph Exchange, Nebula-Importer, and Nebula-Spark-Connector can be used to import the data. Notes. You can learn more about the different tools for NebulaGraph data import at https://siwei.io/en/sketches/nebula-data-import-options to know how to choose one of them c. Here, I will use the simplest one, Nebula-Importer, as an example. Nebula-Importer is an open-source tool written in Golang that compiles into a single file binary, it gets the correspondence of vertices and edges from a given CSV file to a NebulaGraph for reading and importing via a preconfigured YAML format file. Notes. Nebula-Importer code: https://github.com/vesoft-inc/nebula-importer/ Nebula-Importer documentation: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:5:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#nebulagraph-data-ingestion"},{"categories":["Nebula Graph"],"content":" 6 dbt + Nebula-Importer in ActionsNow let’s use dbt + Nebula-Importer to end-to-end demonstrate how to extract, transform and import multiple data sources into NebulaGraph, the whole project code has been open-sourced, the repository is at https://github.com/wey-gu/movie-recommendation-dataset, feel free to check for details there. The whole process is as follows. Preprocess and import raw data into the data warehouse(EL) Use dbt to transform the data (Transform), and export it to CSV files Import CSV into NebulaGraph using Nebula Importer (L) ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#dbt--nebula-importer-in-actions"},{"categories":["Nebula Graph"],"content":" 6.1 Preparing the dbt environmentdbt is a python project, we install dbt and dbt-postgres in a virtual python3 environment. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:1","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#preparing-the-dbt-environment"},{"categories":["Nebula Graph"],"content":" 6.2 Setup env with dbtdbt is written in python, we could install it in a python virtual env, together with dbt-Postgres, as we will use Postgres as the DW in this sample project. python3 -m venv .venv source .venv/bin/activate pip install dbt-postgres Create a dbt project: dbt init dbt_project cd dbt_project Let’s see the files in this project: $ tree . . |-- README.md # README of the project |-- analyses |-- dbt_project.yml # dbt project conf |-- macros |-- models # transforms | \\-- example | |-- my_first_dbt_model.sql # meta data to describe transform rules from the source data with SELECT | |-- my_second_dbt_model.sql | \\-- schema.yml # the meta data of the rules |-- seeds # for CSV-file data sources |-- snapshots \\-- tests 7 directories, 5 files Finally, let’s bootstrap a Postgress as the DW, if you already have one, you may skip this step, please ensure the configurations and dbt-plugins are aligned if you chose to use your own DW. docker run --rm --name postgres \\ -e POSTGRES_PASSWORD=nebula \\ -e POSTGRES_USER=nebula \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:2","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#setup-env-with-dbt"},{"categories":["Nebula Graph"],"content":" 6.3 Data download and preprocessLet’s create a folder named raw_data and change the directory to it. mkdir -p raw_data cd raw_data And we assumed it was under our dbt project: tree .. .. |-- README.md |-- analyses |-- dbt_project.yml |-- macros |-- models | \\-- example | |-- my_first_dbt_model.sql | |-- my_second_dbt_model.sql | \\-- schema.yml |-- raw_data # \u003c--- newly created data |-- seeds |-- snapshots \\-- tests 8 directories, 5 files Download and decompress the OMDB data: wget www.omdb.org/data/all_people.csv.bz2 wget www.omdb.org/data/all_people_aliases.csv.bz2 wget www.omdb.org/data/people_links.csv.bz2 wget www.omdb.org/data/all_casts.csv.bz2 wget www.omdb.org/data/job_names.csv.bz2 wget www.omdb.org/data/all_characters.csv.bz2 wget www.omdb.org/data/movie_categories.csv.bz2 wget www.omdb.org/data/movie_keywords.csv.bz2 wget www.omdb.org/data/category_names.csv.bz2 wget www.omdb.org/data/all_categories.csv.bz2 wget www.omdb.org/data/all_movie_aliases_iso.csv.bz2 bunzip2 *.bz2 Then for the MovieLens dataset: wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip unzip ml-latest-small.zip rm *.zip Before we do the Transform with dbt, we do some simple preprocess and then put them under seeds: grep -v '\\\\\"' raw_data/all_movie_aliases_iso.csv \u003e seeds/all_movie_aliases_iso.csv grep -v '\\\\\"' raw_data/all_casts.csv \u003e seeds/all_casts.csv grep -v '\\\\\"' raw_data/all_characters.csv \u003e seeds/all_characters.csv grep -v '\\\\\"' raw_data/all_people.csv \u003e seeds/all_people.csv grep -v '\\\\\"' raw_data/category_names.csv \u003e seeds/category_names.csv grep -v '\\\\\"' raw_data/job_names.csv \u003e seeds/job_names.csv cp raw_data/movie_categories.csv seeds/movie_categories.csv cp raw_data/movie_keywords.csv seeds/movie_keywords.csv cp raw_data/all_categories.csv seeds/all_categories.csv cp raw_data/ml-latest-small/ratings.csv seeds/movielens_ratings.csv cp raw_data/ml-latest-small/movies.csv seeds/movielens_movies.csv With the above files being placed, we could load them into DW in one command: Refer to the documentation of dbt seeds https://docs.getdbt.com/docs/build/seeds dbt seed It may take a while if you like me are using a local Postgres, and it should be faster in production-level cases (i.e. load to Big Query from the file in Cloud Storage), it should be like this: $ dbt seed 05:58:27 Running with dbt=1.3.0 05:58:27 Found 2 models, 4 tests, 0 snapshots, 0 analyses, 289 macros, 0 operations, 11 seed files, 0 sources, 0 exposures, 0 metrics 05:58:28 05:58:28 Concurrency: 8 threads (target='dev') 05:58:28 05:58:28 1 of 11 START seed file public.all_casts ....................................... [RUN] ... 07:10:11 1 of 11 OK loaded seed file public.all_casts ................................... [INSERT 1082228 in 4303.78s] 07:10:11 07:10:11 Finished running 11 seeds in 1 hours 11 minutes and 43.93 seconds (4303.93s). 07:10:11 07:10:11 Completed successfully 07:10:11 07:10:11 Done. PASS=11 WARN=0 ERROR=0 SKIP=0 TOTAL=11 ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:3","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#data-download-and-preprocess"},{"categories":["Nebula Graph"],"content":" 6.4 Compose the Transform modelWe create transform under models: mkdir models/movie_recommedation touch models/movie_recommedation/user_watched_movies.sql touch models/movie_recommedation/schema.yml The files are like this: $ tree models models \\-- movie_recommedation |-- user_watched_movies.sql \\-- schema.yml Now there is only one transform rule under this model: to handle the edge of user_watched_movies in the user_watched_movies.sql As we planned to output three columns: user_id, movie_id, rating, thus the schema.yml is like: version: 2 models: - name: user_watched_movies description: \"The edges between users and movies they have watched\" columns: - name: user_id description: \"user id\" tests: - not_null - name: movie_id description: \"movie id\" tests: - not_null - name: rating description: \"rating given by user to movie\" tests: - not_null Please be noted the tests are about the validation and constraint of the data, with which, we could control the data quality quite easily. And here not_null ensures there is no NULL if tests are performed. Then, let’s compose the user_watched_movies.sql: {{ config(materialized='table') }} /* JOIN the movieielens_ratings table with the movieielens_movies table, and removing the movie title tailing the year of release */ WITH user_watched_movies AS( SELECT moveielens_ratings.\"userId\", moveielens_ratings.\"movieId\", moveielens_ratings.rating, REGEXP_REPLACE(moveielens_movies.title, ' \\(\\d{4}\\)$', '') AS title, moveielens_movies.genres AS movielens_genres FROM moveielens_ratings JOIN moveielens_movies ON moveielens_movies.\"movieId\" = moveielens_ratings.\"movieId\" ) /* JOIN user_watched_movies table with all_movie_aliase_iso table where language is English the join condition is the movie title */ SELECT concat('u_',user_watched_movies.\"userId\") AS user_id, user_watched_movies.rating, user_watched_movies.title, all_movie_aliases_iso.\"movie_id\" AS OMDB_movie_id, user_watched_movies.movielens_genres FROM user_watched_movies JOIN all_movie_aliases_iso ON user_watched_movies.title LIKE CONCAT(all_movie_aliases_iso.name, '%') AND all_movie_aliases_iso.language_iso_639_1 = 'en' And what this SQL does is the part marked by the green circle: Select the user id, movie id, rating, and movie title (remove the year part) from moveielens_ratings and save it as the intermediate table of user_watched_movies movie title is JOINed from moveielens_movies, obtained by the same matching condition as movie_id Select user id (prefix u_), rating, title, OMDB_movie_id from user_watched_movies OMDB_movie_id is JOINed from all_movie_aliases_iso, obtained by matching the Chinese and English titles of OMDB movies with similar movie names output the final fields Tips: we could add LIMIT to debug the SQL query fast from a Postgres Console Then we could run it from dbt to transform and test the rule: dbt run -m user_watched_movies After that, we should be able to see a table after the Transform in Postgres (DW). Similarly, following the same method for all other parts of the Transform rules, we could have other models: $ tree models models \\-- movie_recommedation |-- acted_by.sql |-- directed_by.sql |-- genres.sql |-- movies.sql |-- people.sql |-- schema.yml |-- user_watched_movies.sql \\-- with_genre.sql Then run them all: dbt run -m acted_by dbt run -m directed_by dbt run -m with_genre dbt run -m people dbt run -m genres dbt run -m movies ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:4","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#compose-the-transform-model"},{"categories":["Nebula Graph"],"content":" 6.5 Export data to CSVNebulaGraph Exchange itself supports directly importing many data sources (Postgres, Clickhouse, MySQL, Hive, etc.) into NebulaGraph, but in this example, the amount of data we process is very small for NebulaGraph, so we just go with the most lightweight one: Nebula-Importer. Nebula-Importer can only CSV files, so we are doing so. First, we enter the Postgres console and execute the COPY command Refer to Postgres documentation: https://www.postgresql.org/docs/current/sql-copy.html COPY acted_by TO '/tmp/acted_by.csv' WITH DELIMITER ',' CSV HEADER; COPY directed_by TO '/tmp/directed_by.csv' WITH DELIMITER ',' CSV HEADER; COPY with_genre TO '/tmp/with_genre.csv' WITH DELIMITER ',' CSV HEADER; COPY people TO '/tmp/people.csv' WITH DELIMITER ',' CSV HEADER; COPY movies TO '/tmp/movies.csv' WITH DELIMITER ',' CSV HEADER; COPY genres TO '/tmp/genres.csv' WITH DELIMITER ',' CSV HEADER; -- for user_watched_movies, we don't output HEADER, as we will parse it in importer in a no-header way. COPY user_watched_movies TO '/tmp/user_watched_movies.csv' WITH DELIMITER ',' CSV; Then copy the CSV files into to_nebulagraph mkdir -p to_nebulagraph docker cp postgres:/tmp/. to_nebulagraph/ ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:5","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#export-data-to-csv"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLet’s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After it’s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same “Oscar-winning” and “classic” movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#ingest-data-into-nebulagraph"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLet’s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After it’s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same “Oscar-winning” and “classic” movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#bootstrap-a-nebulagraph-cluster"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLet’s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After it’s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same “Oscar-winning” and “classic” movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#define-the-data-schema"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLet’s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After it’s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same “Oscar-winning” and “classic” movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#create-a-nebula-importer-conf-file"},{"categories":["Nebula Graph"],"content":" 6.6 Ingest data into NebulaGraph 6.6.1 Bootstrap a NebulaGraph clusterWe can use Nebula-Up to have a NebulaGraph playground cluster with the oneliner. Note: Nebula-UP: https://github.com/wey-gu/nebula-up Dataset repository: https://github.com/wey-gu/movie-recommendation-dataset curl -fsSL nebula-up.siwei.io/install.sh | bash 6.6.2 Define the Data SchemaFirst, we need to create a graph space, and then create tag(type of vertex) and edge type on it: Access the Nebula-Console(CLI client for NebulaGraph): ~/.nebula-up/console.sh Run the following DDL(Data Definition Language): CREATE SPACE moviegraph(partition_num=10,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 USE moviegraph; CREATE TAG person(name string, birthdate string); CREATE TAG movie(name string); CREATE TAG genre(name string); CREATE TAG user(user_id string); CREATE EDGE acted_by(); CREATE EDGE directed_by(); CREATE EDGE with_genre(); CREATE EDGE watched(rate float); exit 6.6.3 Create a Nebula-Importer conf fileThis conf is a YAML file that describes the correspondence between the CSV file and the vertex or edge data in the cluster. Please refer to the document: https://docs.nebula-graph.io/master/nebula-importer/use-importer/ for details. I already created one for it, which can be downloaded at https://github.com/wey-gu/movie-recommendation-dataset/blob/main/nebula-importer.yaml. Here, we will directly download the configuration file. Note that this file should not be part of the dbt project file.: cd .. wget https://raw.githubusercontent.com/wey-gu/movie-recommendation-dataset/main/nebula-importer.yaml 6.6.4 Ingesting the dataLet’s use the Nebula-Importer in docker to avoid any installation: docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/dbt_project/to_nebulagraph/:/data \\ vesoft/nebula-importer:v3.2.0 \\ --config /root/nebula-importer.yaml After it’s executed, all data are in NebulaGraph, and we could check the data from Nebula-Console: First, access the console again: ~/.nebula-up/console.sh Enter the graph space and execute SHOW STATS USE moviegraph; SHOW STATS; The result should be like this: (root@nebula) [moviegraph]\u003e SHOW STATS; +---------+---------------+---------+ | Type | Name | Count | +---------+---------------+---------+ | \"Tag\" | \"genre\" | 14397 | | \"Tag\" | \"movie\" | 20701 | | \"Tag\" | \"person\" | 263907 | | \"Tag\" | \"user\" | 610 | | \"Edge\" | \"acted_by\" | 673763 | | \"Edge\" | \"directed_by\" | 101949 | | \"Edge\" | \"watched\" | 31781 | | \"Edge\" | \"with_genre\" | 194009 | | \"Space\" | \"vertices\" | 299615 | | \"Space\" | \"edges\" | 1001502 | +---------+---------------+---------+ Got 10 rows (time spent 1693/15136 us) With Nebula-Studio, we can also explore this graph in the visual interface, for example, by executing this query, we could see the reason why it recommended the movie with id 1891 to the user with id u_124. FIND NOLOOP PATH FROM \"u_124\" TO \"1891\" over * BIDIRECT UPTO 4 STEPS yield path as `p` | LIMIT 20 The result could be: Most of the cast and crew of the once-favorite Star Wars movies are also involved in this and the same “Oscar-winning” and “classic” movie. In another article, I used the same graph to demonstrate the application of more graph databases and graph algorithms in recommendation systems. If you are interested, please read https://siwei.io/recommendation-system-with-graphdb/. ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:6:6","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#ingesting-the-data"},{"categories":["Nebula Graph"],"content":" 7 SummaryWhen we plan to leverage graph databases for massive data to transform knowledge and analyze insights, the first step is often to transform, process, and model multiple data sources into graph data. For beginners who have no idea where to start, a feasible idea is to start from all relevant information, picture the most concerning relationship, and then list the vertices that can be obtained and the required properties attached. After determining the initial modeling, you can use the ETL tool to clean the original data, ETL into table structure which will be mapped to the graph, and finally, use the import tool to import NebulaGraph for further model iterations. With the help of dbt, we can version control, test, iterate our modeling and data transformation, and gradually evolve and enrich the constructed knowledge graph with grace. Feature image credit: Claudio ","date":"2022-11-23","objectID":"/en/nebulagraph-etl-dbt/:7:0","series":null,"tags":["Nebula Graph","etl","dbt","Graph Modeling","MovieLens","OMDB"],"title":"Tabular Data ETL to NebulaGraph with dbt","uri":"/en/nebulagraph-etl-dbt/#summary"},{"categories":["Nebula Graph"],"content":"This is a review of Fraud Detection methods based on graph algorithms, graph databases, machine learning, and graph neural networks on NebulaGraph, and in addition to an introduction to the basic methodological ideas, I've also got a Playground you can run. it's worth mentioning that this is the first time I've introduced you to the Nebula-DGL project 😁.","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/"},{"categories":["Nebula Graph"],"content":" This is a review of Fraud Detection methods based on graph algorithms, graph databases, machine learning, and graph neural networks on NebulaGraph, and in addition to an introduction to the basic methodological ideas, I’ve also got a Playground you can run. it’s worth mentioning that this is the first time I’ve introduced you to the Nebula-DGL project 😁. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:0:0","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#"},{"categories":["Nebula Graph"],"content":" 1 Fraud detection methods based on graph database","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:0","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-methods-based-on-graph-database"},{"categories":["Nebula Graph"],"content":" 1.1 Graph ModelingWe started the modeling with the existing historical data, annotated information oriented to the relationship of the property graph. The data source could be the transaction event records, user data, and risk control annotations in the banking, e-commerce, or insurance industries in multiple table structures. The modeling process is to abstract the entities we care about, the relationships between them, and the meaningful properties attached to both entities and relationships. In general, persons, corporate entities, phone numbers, addresses, devices (e.g., terminal devices, network addresses, WiFi SSIDs to which terminal devices are connected, etc.), and orders are entities we started with to consider, and other information such as is_risky label, and information about persons and corporate entities (occupation, income, education, etc.) are modeled as properties of entities. The model looks like this and the corresponding dataset could be generated with fraud-detection-datagen, with which you could generate dataset in any expected scale and community sturcture. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:1","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#graph-modeling"},{"categories":["Nebula Graph"],"content":" 1.2 Fraud detection with Graph Query With a graph that encompasses persons, companies, historical loan application records, phone calls, and online applications for web-based devices, we can uncover some interesting information with certain graph queries directly. In fact, many frauds are clusterred in nature. For example, a fraud ring may be a small group of people (e.g., 3 to 5 people) who collect ID information on a larger scale (e.g., 30) in an organized manner, initiate a large number of loans from multiple financial institutions at the same time, and then choose to discard the batch of IDs that have left a record of default after they have been disbursed, and then further choose the next batch of ID information as they have done. Due to the group of frauds keeps utilizing new identity information, it’s hard to detect with historical records based blacklist mechanism. However, with the help of the patterns being queried in graph, such case could be resovled in real-time. These patterns can be categorised into two types: One is that which can be directly described by the risk control expert in terms of some pattern, e.g., a direct or indirect association with an entity that has been marked as high risk (new order applicants use the same network devices as past high risk records), and this pattern corresponds to the graph, which gives results in real time through a graph query. Another type of association is implicitly behind the correlation of the data, which needs to be mined by graph algorithms for some risk hints, e.g., although a given entity has no matching association with a limited number of labeled high-risk entities, the aggregation it forms in the graph may suggest that this may be one of the applications of an ongoing gang loan fraud that has not yet succeeded, which can be derived by periodically batching in the historical data This situation can be derived by periodically performing community discovery algorithms in bulk in historical data, and using centrality algorithms in highly aggregated communities to give core entities that are prompted to risk experts for subsequent evaluation and risk labeling. 1.2.1 Fraud detection based on expert graph pattern matchingBefore we get started, let’s prepare for a NebulaGraph playground with the above graph dataset being loaded: Nebula Graph Playground setup, based on https://github.com/wey-gu/nebula-up/ curl -fsSL nebula-up.siwei.io/install.sh | bash Load graph dataset # clone dataset genarator repo git clone https://github.com/wey-gu/fraud-detection-datagen.git cp -r data_sample_numerical_vertex_id data # remove table head sed -i '1d' data/*.csv # load dataset to nebulagraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/data/:/data \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/nebula_graph_importer.yaml With such a mapping, risk control experts can explore the relationships between entities on-demand in a visual exploration tool that maps the corresponding risk patterns: In this screenshot of rendered query, we can clearly see a risk pattern for a group-controlled device that can be given to a graph database developer and abstracted into NebulaGraph database statements that can be queried by anyone or certain application in real-time fashion: ## Query started from a person for given transaction MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN p_shared_d Then we could create an API based on queries like the following, which returns count(e) as a metrics. ## group controlled device metric MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN count(e) In this way, we can build an online risk control system that uses limited labeled da","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:2","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-with-graph-query"},{"categories":["Nebula Graph"],"content":" 1.2 Fraud detection with Graph Query With a graph that encompasses persons, companies, historical loan application records, phone calls, and online applications for web-based devices, we can uncover some interesting information with certain graph queries directly. In fact, many frauds are clusterred in nature. For example, a fraud ring may be a small group of people (e.g., 3 to 5 people) who collect ID information on a larger scale (e.g., 30) in an organized manner, initiate a large number of loans from multiple financial institutions at the same time, and then choose to discard the batch of IDs that have left a record of default after they have been disbursed, and then further choose the next batch of ID information as they have done. Due to the group of frauds keeps utilizing new identity information, it’s hard to detect with historical records based blacklist mechanism. However, with the help of the patterns being queried in graph, such case could be resovled in real-time. These patterns can be categorised into two types: One is that which can be directly described by the risk control expert in terms of some pattern, e.g., a direct or indirect association with an entity that has been marked as high risk (new order applicants use the same network devices as past high risk records), and this pattern corresponds to the graph, which gives results in real time through a graph query. Another type of association is implicitly behind the correlation of the data, which needs to be mined by graph algorithms for some risk hints, e.g., although a given entity has no matching association with a limited number of labeled high-risk entities, the aggregation it forms in the graph may suggest that this may be one of the applications of an ongoing gang loan fraud that has not yet succeeded, which can be derived by periodically batching in the historical data This situation can be derived by periodically performing community discovery algorithms in bulk in historical data, and using centrality algorithms in highly aggregated communities to give core entities that are prompted to risk experts for subsequent evaluation and risk labeling. 1.2.1 Fraud detection based on expert graph pattern matchingBefore we get started, let’s prepare for a NebulaGraph playground with the above graph dataset being loaded: Nebula Graph Playground setup, based on https://github.com/wey-gu/nebula-up/ curl -fsSL nebula-up.siwei.io/install.sh | bash Load graph dataset # clone dataset genarator repo git clone https://github.com/wey-gu/fraud-detection-datagen.git cp -r data_sample_numerical_vertex_id data # remove table head sed -i '1d' data/*.csv # load dataset to nebulagraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}:/root/ \\ -v ${PWD}/data/:/data \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/nebula_graph_importer.yaml With such a mapping, risk control experts can explore the relationships between entities on-demand in a visual exploration tool that maps the corresponding risk patterns: In this screenshot of rendered query, we can clearly see a risk pattern for a group-controlled device that can be given to a graph database developer and abstracted into NebulaGraph database statements that can be queried by anyone or certain application in real-time fashion: ## Query started from a person for given transaction MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN p_shared_d Then we could create an API based on queries like the following, which returns count(e) as a metrics. ## group controlled device metric MATCH (n) WHERE id(n) == \"200000010265\" OPTIONAL MATCH p_shared_d=(n)-[:`used_device`]-\u003e(d)\u003c-[:`used_device`]-(:`applicant`)-[:`with_phone_num`]-\u003e(pn:`phone_num`)\u003c-[e:`with_phone_num`]-(:`applicant`) RETURN count(e) In this way, we can build an online risk control system that uses limited labeled da","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:2","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#fraud-detection-based-on-expert-graph-pattern-matching"},{"categories":["Nebula Graph"],"content":" 1.3 Expand labels in Graph In the paper: Learning from Labeled and Unlabeled Data with Label Propagation (CMU-CALD-02-107) by Xiaojin Z. and Zoubin G., the Label Propagation algorithm is used to propagate limited labeled information on the graph to more entities through the edges. In this way, we can easily “propagate” more labeled information with a limited number of high-risk annotations in the graphs we build. These extended labeles can, on the one hand, give more results in real-time graph queries and, on the other hand, serve as important input for risk control experts to help advance anti-fraud investigation actions. In general, we can scan the graph data offline periodically, expand and update the labels by the graph algorithm, and then write the valid updated labels back to the graph. Note that there is a similar method, SIGNDiffusion, for those who are interested. 1.3.1 Try expanding labels in graphHere is an example that works. In this example, I use the public Yelp dataset. This data will not only be used in this example, but also in the later cases in the GNN method, so you can be patient and import the data into NebulaGraph. Load dataset into NebulaGraph More details in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection cd nebulagraph-yelp-frauddetection python3 -m pip install -r requirements.txt # download and process dataset python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml After it’s done, we could see the data stats: ~/.nebula-up/console.sh -e \"USE yelp; SHOW STATS\" It should look like: (root@nebula) [(none)]\u003e USE yelp; SHOW STATS +---------+---------------------------------------+---------+ | Type | Name | Count | +---------+---------------------------------------+---------+ | \"Tag\" | \"review\" | 45954 | | \"Edge\" | \"shares_restaurant_in_one_month_with\" | 1147232 | | \"Edge\" | \"shares_restaurant_rating_with\" | 6805486 | | \"Edge\" | \"shares_user_with\" | 98630 | | \"Space\" | \"vertices\" | 45954 | | \"Space\" | \"edges\" | 8051348 | +---------+---------------------------------------+---------+ Got 6 rows (time spent 1911/4488 us) Currently, the general LPA tag propagation algorithm is used for community detection and few implementations are used for tag expansion (only SK-Learn has this implementation), here, we refer to the implementation given by [Thibaud M](https://datascience.stackexchange.com/users/77683/ thibaud-m) for the implementation given. The orginal talks could be referred: https://datascience.stackexchange.com/a/55720/138720 To make this algorithm run faster, a subgraph is taken from the NebulaGraph and an expansion of the labeling is done on this small subgraph: First, we start a Playground for Jupyter. More details in: https://github.com/wey-gu/nebula-dgl. git clone https://github.com/wey-gu/nebula-dgl.git cd nebula-dgl # run the Jupyter Notebook docker run -it --name dgl -p 8888:8888 --network nebula-net \\ -v \"$PWD\":/home/jovyan/work jupyter/datascience-notebook \\ start-notebook.sh --NotebookApp.token='nebulagraph' visit http://localhost:8888/lab/tree/work?token=nebulagraph Install depednecies(they will be used in GNN examples, too) !python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8 !python3 -m pip install . Then, let’s sample a subgraph, starting from the vertex with ID 2048, to get all vertecies in two steps: import torch import json from torch import tensor from dgl import DGLHeteroGraph, heterograph from nebula3.gclient.net import ConnectionPool from nebula3.Config import Config config = Config() config.max_connection_pool_size = 2 connection_pool = ConnectionPool() connection_pool.init([('graphd', 9669)], config) vertex_id = 2048 client = connection_pool.get_session('ro","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:3","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#expand-labels-in-graph"},{"categories":["Nebula Graph"],"content":" 1.3 Expand labels in Graph In the paper: Learning from Labeled and Unlabeled Data with Label Propagation (CMU-CALD-02-107) by Xiaojin Z. and Zoubin G., the Label Propagation algorithm is used to propagate limited labeled information on the graph to more entities through the edges. In this way, we can easily “propagate” more labeled information with a limited number of high-risk annotations in the graphs we build. These extended labeles can, on the one hand, give more results in real-time graph queries and, on the other hand, serve as important input for risk control experts to help advance anti-fraud investigation actions. In general, we can scan the graph data offline periodically, expand and update the labels by the graph algorithm, and then write the valid updated labels back to the graph. Note that there is a similar method, SIGNDiffusion, for those who are interested. 1.3.1 Try expanding labels in graphHere is an example that works. In this example, I use the public Yelp dataset. This data will not only be used in this example, but also in the later cases in the GNN method, so you can be patient and import the data into NebulaGraph. Load dataset into NebulaGraph More details in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection cd nebulagraph-yelp-frauddetection python3 -m pip install -r requirements.txt # download and process dataset python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml After it’s done, we could see the data stats: ~/.nebula-up/console.sh -e \"USE yelp; SHOW STATS\" It should look like: (root@nebula) [(none)]\u003e USE yelp; SHOW STATS +---------+---------------------------------------+---------+ | Type | Name | Count | +---------+---------------------------------------+---------+ | \"Tag\" | \"review\" | 45954 | | \"Edge\" | \"shares_restaurant_in_one_month_with\" | 1147232 | | \"Edge\" | \"shares_restaurant_rating_with\" | 6805486 | | \"Edge\" | \"shares_user_with\" | 98630 | | \"Space\" | \"vertices\" | 45954 | | \"Space\" | \"edges\" | 8051348 | +---------+---------------------------------------+---------+ Got 6 rows (time spent 1911/4488 us) Currently, the general LPA tag propagation algorithm is used for community detection and few implementations are used for tag expansion (only SK-Learn has this implementation), here, we refer to the implementation given by [Thibaud M](https://datascience.stackexchange.com/users/77683/ thibaud-m) for the implementation given. The orginal talks could be referred: https://datascience.stackexchange.com/a/55720/138720 To make this algorithm run faster, a subgraph is taken from the NebulaGraph and an expansion of the labeling is done on this small subgraph: First, we start a Playground for Jupyter. More details in: https://github.com/wey-gu/nebula-dgl. git clone https://github.com/wey-gu/nebula-dgl.git cd nebula-dgl # run the Jupyter Notebook docker run -it --name dgl -p 8888:8888 --network nebula-net \\ -v \"$PWD\":/home/jovyan/work jupyter/datascience-notebook \\ start-notebook.sh --NotebookApp.token='nebulagraph' visit http://localhost:8888/lab/tree/work?token=nebulagraph Install depednecies(they will be used in GNN examples, too) !python3 -m pip install git+https://github.com/vesoft-inc/nebula-python.git@8c328c534413b04ccecfd42e64ce6491e09c6ca8 !python3 -m pip install . Then, let’s sample a subgraph, starting from the vertex with ID 2048, to get all vertecies in two steps: import torch import json from torch import tensor from dgl import DGLHeteroGraph, heterograph from nebula3.gclient.net import ConnectionPool from nebula3.Config import Config config = Config() config.max_connection_pool_size = 2 connection_pool = ConnectionPool() connection_pool.init([('graphd', 9669)], config) vertex_id = 2048 client = connection_pool.get_session('ro","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:3","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#try-expanding-labels-in-graph"},{"categories":["Nebula Graph"],"content":" 1.4 Machine Learning with Graph Features Before the field of risk control started to leverage the methods of the graph, there have been many approaches to predict high-risk behavior based on historical data using machine learning classification algorithms that use information in records that domain experts consider relevant (e.g., age, education, income) as features and historical label information to train risk prediction models. So reading this, does it occur to us that on top of these methods, models trained as features might be more effective if attributes based on graph structure were also taken into account? The answer is also yes, and there have been many papers and engineering practices revealing that such models are more effective than algorithms that do not consider graph features: these graph structure features that are tried to be effective could be PageRank values of entities, Degree values, or community ids derived from one of the community discovery algorithms. In production, we can periodically obtain real-time full graph information from the graph, analyze it in a graph computing platform to obtain the required features, go through a predefined data pipeline, import it into a machine learning model cycle to obtain new risk cues, and write some of the results back to the graph for easy extraction and reference by other systems and experts. 1.4.1 Example of ML with Graph Features Here, I will not demonstrate the end-to-end machine learning example, which is a common classification approach, on top of which we can get some new properties in the data by graph algorithms, which are then processed as new features. I will only demonstrate a community discovery method where we can run a Louvain on the full graph, derive the community identity of different nodes, and then process the community values as a classification into numerical features. In this example we also use the data from https://github.com/wey-gu/fraud-detection-datagen, on top of which I used NebulaGraph-Algorithm project, a Spark application that runs many common graph algorithms on the NebulaGraph graph database. First, let’s set up a NebulaGraph cluster with Spark and NebulaGraph Algorithm, in one-liner thanks to Nebula-UP curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark After it’s done, as I had put needed configuration files inside Nebula-UP, we could call the Louvain algorithm with: cd ~/.nebula-up/nebula-up/spark \u0026\u0026 ls -l docker exec -it sparkmaster /spark/bin/spark-submit \\ --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 4g /root/download/nebula-algo.jar \\ -p /root/louvain.conf And the result will be stored in sparkmaster container, under path /output # docker exec -it sparkmaster bash ls -l /output After that, we can do some pre-processing on this Louvain’s graph algorithm features and start the traditional model training. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:4","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#machine-learning-with-graph-features"},{"categories":["Nebula Graph"],"content":" 1.4 Machine Learning with Graph Features Before the field of risk control started to leverage the methods of the graph, there have been many approaches to predict high-risk behavior based on historical data using machine learning classification algorithms that use information in records that domain experts consider relevant (e.g., age, education, income) as features and historical label information to train risk prediction models. So reading this, does it occur to us that on top of these methods, models trained as features might be more effective if attributes based on graph structure were also taken into account? The answer is also yes, and there have been many papers and engineering practices revealing that such models are more effective than algorithms that do not consider graph features: these graph structure features that are tried to be effective could be PageRank values of entities, Degree values, or community ids derived from one of the community discovery algorithms. In production, we can periodically obtain real-time full graph information from the graph, analyze it in a graph computing platform to obtain the required features, go through a predefined data pipeline, import it into a machine learning model cycle to obtain new risk cues, and write some of the results back to the graph for easy extraction and reference by other systems and experts. 1.4.1 Example of ML with Graph Features Here, I will not demonstrate the end-to-end machine learning example, which is a common classification approach, on top of which we can get some new properties in the data by graph algorithms, which are then processed as new features. I will only demonstrate a community discovery method where we can run a Louvain on the full graph, derive the community identity of different nodes, and then process the community values as a classification into numerical features. In this example we also use the data from https://github.com/wey-gu/fraud-detection-datagen, on top of which I used NebulaGraph-Algorithm project, a Spark application that runs many common graph algorithms on the NebulaGraph graph database. First, let’s set up a NebulaGraph cluster with Spark and NebulaGraph Algorithm, in one-liner thanks to Nebula-UP curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark After it’s done, as I had put needed configuration files inside Nebula-UP, we could call the Louvain algorithm with: cd ~/.nebula-up/nebula-up/spark \u0026\u0026 ls -l docker exec -it sparkmaster /spark/bin/spark-submit \\ --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 4g /root/download/nebula-algo.jar \\ -p /root/louvain.conf And the result will be stored in sparkmaster container, under path /output # docker exec -it sparkmaster bash ls -l /output After that, we can do some pre-processing on this Louvain’s graph algorithm features and start the traditional model training. ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:4","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#example-of-ml-with-graph-features"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#the-graph-neural-network-approach"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#example-of-gnn-fraud-detection-system"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#dataset"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#data-processing"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#model-training"},{"categories":["Nebula Graph"],"content":" 1.5 The Graph Neural Network approach However, the problem with these previous graph feature-based approaches is that graph features do not fully reflect the correlations and the locality nature of the data, into our models/ methods. graph feature engineering could be expensive and cumbersome. In recent years, GNN-based approaches have enabled us to get better results than traditional graph feature-based machine learning by embedding graph structure and attribute information into the representation without graph feature extraction, feature engineering, and data annotation by experts and engineering methods. Interestingly, this is the period when these methods are rapidly being discovered and evolving, and graph-based deep learning is one of the hottest machine learning research directions in the previous years. At the same time, some methods of graph deep learning can do Inductive Learning - models can inference/reason on new points and edges, so that, together with the ability to query subgraphs on the graph database online, online real-time risk prediction becomes simple and feasible. 1.5.1 Example of GNN fraud detection system The storage of data can be in several other common media, but the graph database can maximize the benefit of model training, model updating, and online results updating. When we use the graph database as the single source of truth for the data, all online, offline, and graph-based approaches can be easily integrated to combine the advantages and results of all approaches to make a more effective composite system for fraud detection. In this example we are divided into the same parts: Data processing, Model training, Building an online detection system. Note, I will use Deep Graph library(DGL), NebulaGraph and the bridge between them: Nebula-DGL. DGL: https://www.dgl.ai/ Nebula-DGL: https://github.com/wey-gu/nebula-dgl, and yes, I am the author of this project :). 1.5.1.1 DatasetIn this case, the dataset we use is Yelp-Fraud, who comes from the paper [Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters](https:// paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud). There is one type of vertex in this diagram and three types of edges. Top points: reviews from restaurants, hotels in Yelp with two types of attributes. Each review has a label labeled whether it is a false or fraudulent review 32 numeric attributes that have been processed Edge: the association between the three types of reviews R-U-R: two reviews issued by the same user shares_user_with R-S-R: two reviews by the same restaurant with the same rating (rating can be 1 to 5) shares_restaurant_rating_with R-T-R: two ratings are from the same restaurant in the same month of submission shares_restaurant_in_one_month_with Before we start, it’s asumed this grpah is already loaded into NebulaGraph. To load yelp dataset into NeublaGraph, in short, you just do: # Deploy NebulaGraph curl -fsSL nebula-up.siwei.io/install.sh | bash # Clone the datadownloader repo git clone https://github.com/wey-gu/nebulagraph-yelp-frauddetection \u0026\u0026 cd nebulagraph-yelp-frauddetection # Install dep python3 -m pip install -r requirements.txt python3 data_download.py # load it into NebulaGraph docker run --rm -ti \\ --network=nebula-net \\ -v ${PWD}/yelp_nebulagraph_importer.yaml:/root/importer.yaml \\ -v ${PWD}/data:/root \\ vesoft/nebula-importer:v3.1.0 \\ --config /root/importer.yaml More detilas in: https://github.com/wey-gu/nebulagraph-yelp-frauddetection 1.5.1.2 Data Processing The task of this part is to engineer the topological representation of the risk-related subgraphs of the graph and the related features (attributes) in them, and serialize them into graph objects of the DGL. DGL itself supports constructing its graph objects from CSV files in the form of point and edge lists (edgelist), or from data in the serialized sparse adjacency matrix of NetworkX and SciPy, and we can export the raw graph data or the full amount","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:5","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#inference-api"},{"categories":["Nebula Graph"],"content":" 1.6 Conclusion To summarize, fraud detection with NebulaGraph Graph Database could be done: With graph queries to get risk metrics from graph database With risky label being expanded by graph algorithms and written back to graph database With ML methods including graph features being fetched from graph database Process the property in the graph into the node and edge features to predict risk offline using GNN methods, some of which can be combined with the graph database to achieve online risk prediction by Inductive Learning methods Feature Image credit goes to https://unsplash.com/photos/BW0vK-FA3eg ","date":"2022-08-01","objectID":"/en/fraud-detection-with-nebulagraph/:1:6","series":null,"tags":["Nebula Graph","Fraud Detection","GNN","DGL","PyTorch"],"title":"Fraud Detection with NebulaGraph GraphDatabase in action","uri":"/en/fraud-detection-with-nebulagraph/#conclusion"},{"categories":["Nebula Graph","Amundsen"],"content":"I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/"},{"categories":["Nebula Graph","Amundsen"],"content":" Do I have to create my own graph model and everything to set up a Data Lineage system? Thanks to many great open-source projects, the answer is: No! Today, I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:0:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#"},{"categories":["Nebula Graph","Amundsen"],"content":" 1 Metadata Governance systemA Metadata Governance system is a system providing a single view of where and how all the data are formatted, generated, transformed, consumed, presented, and owned. Metadata Governance is like a catalog of all of the data warehouses, databases, tables, dashboards, ETL jobs, etc so that people don’t have to broadcast their queries on “Hi everyone, could I change the schema of this table?”, “Hey, anyone who knows how I could find the raw data of table-view-foo-bar?”, which, explains why we need a Metadata Governance system in a mature data stack with a relatively large scale of data and team(or one to be grown to). For the other term, Data Lineage, is one of the Metadata that needs to be managed, for example, some dashboard is the downstream of a table view, which has an upstream as two other tables from different databases. That information should be managed at best when possible, too, to enable a trust chain on a data-driven team. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:1:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance-system"},{"categories":["Nebula Graph","Amundsen"],"content":" 2 The reference solution","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-reference-solution"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.1 MotivationThe metadata and data lineage are by nature fitting to the graph model/graph database well, and the relationship-oriented queries, for instance, “finding all n-depth data lineage per given component(i.e. a table)” is a FIND ALL PATH query in a graph database. This also explains one observation of mine as an OSS contributor of Nebula Graph, a distributed graph database: (from their queries/graph modeling in discussions I could tell) a bunch of teams who are already levering Nebula Graph on their tech stack, are setting up a data lineage system on their own, from scratch. A Metadata Governance system needs some of the following components: Metadata Extractor This part is needed to either pull or be pushed from the different parties of the data stack like databases, data warehouses, dashboards, or even from ETL pipeline and applications, etc. Metadata Storage This could be either a database or even large JSON manifest files Metadata Catalog This could be a system providing API and/or a GUI interface to read/write the metadata and data lineage In Nebula Graph community, I had been seeing many graph database users were building their in-house data lineage system. It’s itching witnessing this entropy increase situation not be standarized or jointly contributed instead, as most of their work are parsing metadata from well-known big-data projects, and persistent into a graph database, which, I consider high probability that the work is common. Then I came to create an opinionated reference data infra stack with some of those best open-source projects put together. Hopefully, those who were gonna define and iterate their own fashion of Graph Model on Nebula Graph and create in-house Metadata and data linage extracting pipelines can benefit from this project to have a relatively polished, beautifully designed, Metadata Governance system out of the box with a fully evolved graph model. To make the reference project self-contained and runnable, I tried to put layers of data infra stack more than just pure metadata related ones, thus, maybe it will help new data engineers who would like to try and see how far had open-source pushed a modern data lab to. This is a diagram of all the components in this reference data stack, where I see most of them as Metadata Sources: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#motivation"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-data-stack"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#database-and-data-warehouse"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dataops"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-visualization"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#job-orchestration"},{"categories":["Nebula Graph","Amundsen"],"content":" 2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance"},{"categories":["Nebula Graph","Amundsen"],"content":" 3 Environment Bootstrap, Component overviewThe reference runnable project is open-source and you could find it here: https://github.com/wey-gu/data-lineage-ref-solution I will try my best to make things clean and isolated. It’s assumed you are running on a UNIX-like system with internet and Docker Compose being installed. Please refer here to install Docker and Docker Compose before moving forward. I am running it on Ubuntu 20.04 LTS X86_64, but there shouldn’t be issues on other distros or versions of Linux. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#environment-bootstrap-component-overview"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.1 Run a Data Warehouse/ DatabaseFirst, let’s install Postgres as our data warehouse. This oneliner will help create a Postgres running in the background with docker, and when being stopped it will be cleaned up(--rm). docker run --rm --name postgres \\ -e POSTGRES_PASSWORD=lineage_ref \\ -e POSTGRES_USER=lineage_ref \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres Then we could verify it with Postgres CLI or GUI clients. Hint: You could use VS Code extension: SQL tools to quickly connect to multiple RDBMS(MariaDB, Postgres, etc.) or even Non-SQL DBMS like Cassandra in a GUI fashion. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#run-a-data-warehouse-database"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env meltano init \u003cyourprojectname\u003e “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init \u003cyourprojectname\u003e Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke \u003cplugin\u003e to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-dataops-toolchain-for-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#installation-of-meltano"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-meltano-ui"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#example-meltano-projects"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-a-bi-platform-for-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#bootstrap-meltano-and-superset"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#create-a-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#deploy-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-service"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-builder"},{"categories":["Nebula Graph","Amundsen"],"content":" 3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen 3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":" 4 Connecting the dots, Metadata DiscoveryWith the basic environment being set up, let’s put everything together. Remember we had ELT some data to PostgreSQL as this? How could we let Amundsen discover metadata regarding those data and ETL? ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#connecting-the-dots-metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-postgres-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-postgres-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-postgres-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-dbt-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-dbt-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-dbt-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-superset-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-superset-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-superset-dashboard-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.4 Preview data with SupersetSuperset could be used to preview Table Data like this. Corresponding documentation could be referred here, where the API of /superset/sql_json/ will be called by Amundsen Frontend. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#preview-data-with-superset"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#enable-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#get-lineage-in-nebula-graph"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extract-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dbt"},{"categories":["Nebula Graph","Amundsen"],"content":" 4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:`Table`) -[:`HAS_UPSTREAM`|:`HAS_DOWNSTREAM` *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage 4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#open-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":" 5 RecapThe whole idea of Metadata Governance/Discovery is to: Put all components in the stack as Metadata Sources(from any DB or DW to dbt, Airflow, Openlineage, Superset, etc.) Run metadata ETL with Databuilder(as a script, or DAG) to store and index with Nebula Graph(or other Graph Database) and Elasticsearch Consume, manage, and discover metadata from Frontend UI(with Superset for preview) or API Have more possibilities, flexibility, and insights on Nebula Graph from queries and UI ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#recap"},{"categories":["Nebula Graph","Amundsen"],"content":" 5.1 Upstream ProjectsAll projects used in this reference project are listed below in lexicographic order. Amundsen Apache Airflow Apache Superset dbt Elasticsearch meltano Nebula Graph Open Lineage singer Feature Image credit to Phil Hearing ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#upstream-projects"},{"categories":["Nebula Graph"],"content":"What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know.","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/"},{"categories":["Nebula Graph"],"content":" What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know. In this article, I am trying to walk you through all three Spark projects of Nebula Graph with some runnable hands-on examples. Also, I managed to make PySpark usable with Nebula Graph Spark Connector, which will be contributed to the Docs later. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:0:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#"},{"categories":["Nebula Graph"],"content":" 1 The three Spark projects for Nebula GraphI used to draw a sketch around all data importing methods of Nebula Graph here, where all three of the Spark-based Nebula Graph projects were already briefly introduced. Instead, in this article, a slightly deeper dive into all of them will be made based on my recent work on them. TL;DR Nebula Spark Connector is a Spark Lib to enable spark application reading from and writing to Nebula Graph in form of a dataframe. Nebula Exchange, built on top of Nebula Spark Connector, is a Spark Lib and Application to exchange(for the Open Source version, it’s one way: write, whereas for the enterprise version it’s bidirectional) different data sources like(MySQL, Neo4j, PostgreSQL, Clickhouse, Hive, etc.). Besides writing directly to Nebula Graph, it could optionally generate SST files to be ingested into Nebula Graph to offload the storage computation outside of the Nebula Graph cluster. Nebula Algorithm, built on top of Nebula Spark Connector and GraphX, is a Spark Lib and Application to run de facto graph algorithms(PageRank, LPA, etc…) on a graph from Nebula Graph. Then let’s have the long version of those spark projects more on how-to perspectives. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:1:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#the-three-spark-projects-for-nebula-graph"},{"categories":["Nebula Graph"],"content":" 2 Spark-Connector Codebase: https://github.com/vesoft-inc/nebula-spark-connector Documentation: https://docs.nebula-graph.io/3.0.2/nebula-spark-connector/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/ Code Examples: example ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#spark-connector"},{"categories":["Nebula Graph"],"content":" 2.1 Nebula Graph Spark ReaderTo read data from Nebula Graph, i.e. vertex, Nebula Spark Connector will scan all storage instances that hold the given label(TAG): withLabel(\"player\"), and we could optionally specify the properties of the vertex: withReturnCols(List(\"name\", \"age\")). With needed configuration being provided, a call of spark.read.nebula.loadVerticesToDF will return dataframe of the Vertex Scan call towards Nebula Graph: def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColumn(false) .withReturnCols(List(\"name\", \"age\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() vertex.printSchema() vertex.show(20) println(\"vertex count: \" + vertex.count()) } It’s similar for the writer part and one big difference here is the writing path is done via GraphD as the underlying Spark Connector is shooting nGQL INSERT queries. Then let’s do the hands-on end-to-end practice. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#nebula-graph-spark-reader"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-spark-connector"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#bootstrap-a-nebula-graph-cluster"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#create-a-spark-playground"},{"categories":["Nebula Graph"],"content":" 2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoCol","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#run-spark-connector-example"},{"categories":["Nebula Graph"],"content":" 3 Exchange Codebase: https://github.com/vesoft-inc/nebula-exchange/ Documentation: https://docs.nebula-graph.io/3.0.2/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://github.com/vesoft-inc/nebula-exchange/releases Configuration Examples: exchange-common/src/test/resources/application.conf Nebula Exchange is a Spark Lib/App to read data from multiple sources, then, write to either Nebula Graph directly or into Nebula Graph SST Files. The way to leverage Nebula Exchange is only to firstly create the configuration files to let the exchange know how data should be fetched and written, then call the exchange package with the conf file specified. Now let’s do a hands-on test with the same environment created in the last chapter. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#exchange"},{"categories":["Nebula Graph"],"content":" 3.1 Hands-on ExchangeHere, we are using Exchange to consume data source from a CSV file, where the first column is Vertex ID, and the second, and third to be properties of “name” and “age”. player800,\"Foo Bar\",23 player801,\"Another Name\",21 Let’s get into the spark container created in the last chapter, and download the Jar package of Nebula Exchange: docker exec -it spark-master bash cd /root/ wget https://github.com/vesoft-inc/nebula-exchange/releases/download/v3.0.0/nebula-exchange_spark_2.4-3.0.0.jar Create a conf file named exchange.conf in format HOCON, where: under .nebula, information regarding Nebula Graph Cluster was configured under .tags, information regarding Vertices like how required fields are reflected our data source(here it’s CSV file) was configured { # Spark relation config spark: { app: { name: Nebula Exchange } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory: 1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"graphd:9669\"] meta:[\"metad0:9559\", \"metad1:9559\", \"metad2:9559\"] } user: root pswd: nebula space: basketballplayer # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://localhost:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different data Sources. tags: [ # HDFS CSV # Import mode is client, just change type.sink to sst if you want to use client import mode. { name: player type: { source: csv sink: client } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } Finally, let’s create player.csv and exchange.conf, it should be listed as the following: # ls -l -rw-r--r-- 1 root root 1912 Apr 19 08:21 exchange.conf -rw-r--r-- 1 root root 157814140 Apr 19 08:17 nebula-exchange_spark_2.4-3.0.0.jar -rw-r--r-- 1 root root 52 Apr 19 08:06 player.csv And we could call the exchange as: /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange_spark_2.4-3.0.0.jar \\ -c exchange.conf And the result should be like ... 22/04/19 08:22:08 INFO Exchange$: import for tag player cost time: 1.32 s 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchFailure.player: 0 ... Please refer to the documentation and conf examples for more data sources. For hands-on Exchange writing to SST Files, you could refer to both Documentation and Nebula Exchange SST 2.x Hands-on Guide. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-exchange"},{"categories":["Nebula Graph"],"content":" 4 Algorithm Codebase: https://github.com/vesoft-inc/nebula-algorithm Documentation: https://docs.nebula-graph.io/3.0.2/nebula-algorithm/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ Code Examples: example/src/main/scala/com/vesoft/nebula/algorithm ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#algorithm"},{"categories":["Nebula Graph"],"content":" 4.1 Calling with spark-submitWhen we call Nebula Algorithm with spark-submit, on how to use perspective, it is quite similar to Exchange. This post already showed us how to do that in action. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-with-spark-submit"},{"categories":["Nebula Graph"],"content":" 4.2 Calling as a lib in codeOn the other hand, we could call Nebula Algorithm in spark as a Spark Lib, the gain will be: More control/customization on the output format of the algorithm Possible to perform algorithm for non-numerical vertex ID cases, see here ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-as-a-lib-in-code"},{"categories":["Nebula Graph"],"content":" 5 PySpark for Nebula GraphPySpark comes with the capability to call java/scala packages inside python, thus it’s also quite easy to use Spark Connector with Python. Here I am doing this from the pyspark entrypoint in /spark/bin/pyspark, with the connector’s Jar package specified with --driver-class-path and --jars docker exec -it spark-master-0 bash cd root wget https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/3.0.0/nebula-spark-connector-3.0.0.jar /spark/bin/pyspark --driver-class-path nebula-spark-connector-3.0.0.jar --jars nebula-spark-connector-3.0.0.jar Then, rather than pass NebulaConnectionConfig and ReadNebulaConfig to spark.read.nebula, we should instead call spark.read.format(\"com.vesoft.nebula.connector.NebulaDataSource\"). Voilà! df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows Below are how I figured out how to make this work with almost zero scala knowledge:-P. How reader should be called def loadVerticesToDF(): DataFrame = { assert(connectionConfig != null \u0026\u0026 readConfig != null, \"nebula config is not set, please call nebula() before loadVerticesToDF\") val dfReader = reader .format(classOf[NebulaDataSource].getName) .option(NebulaOptions.TYPE, DataTypeEnum.VERTEX.toString) .option(NebulaOptions.SPACE_NAME, readConfig.getSpace) .option(NebulaOptions.LABEL, readConfig.getLabel) .option(NebulaOptions.PARTITION_NUMBER, readConfig.getPartitionNum) .option(NebulaOptions.RETURN_COLS, readConfig.getReturnCols.mkString(\",\")) .option(NebulaOptions.NO_COLUMN, readConfig.getNoColumn) .option(NebulaOptions.LIMIT, readConfig.getLimit) .option(NebulaOptions.META_ADDRESS, connectionConfig.getMetaAddress) .option(NebulaOptions.TIMEOUT, connectionConfig.getTimeout) .option(NebulaOptions.CONNECTION_RETRY, connectionConfig.getConnectionRetry) .option(NebulaOptions.EXECUTION_RETRY, connectionConfig.getExecRetry) .option(NebulaOptions.ENABLE_META_SSL, connectionConfig.getEnableMetaSSL) .option(NebulaOptions.ENABLE_STORAGE_SSL, connectionConfig.getEnableStorageSSL) if (connectionConfig.getEnableStorageSSL || connectionConfig.getEnableMetaSSL) { dfReader.option(NebulaOptions.SSL_SIGN_TYPE, connectionConfig.getSignType) SSLSignType.withName(connectionConfig.getSignType) match { case SSLSignType.CA =\u003e dfReader.option(NebulaOptions.CA_SIGN_PARAM, connectionConfig.getCaSignParam) case SSLSignType.SELF =\u003e dfReader.option(NebulaOptions.SELF_SIGN_PARAM, connectionConfig.getSelfSignParam) } } dfReader.load() } How Option String should be like # object NebulaOptions { /** nebula common config */ val SPACE_NAME: String = \"spaceName\" val META_ADDRESS: String = \"metaAddress\" val GRAPH_ADDRESS: String = \"graphAddress\" val TYPE: String = \"type\" val LABEL: String = \"label\" /** connection config */ val TIMEOUT: String = \"timeout\" val CONNECTION_RETRY: String = \"connectionRetry\" val EXECUTION_RETRY: String = \"executionRetry\" val RATE_TIME_OUT: String = \"reteTimeOut\" val USER_NAME: String = \"user\" val PASSWD: String = \"passwd\" val ENABLE_GRAPH_SSL: String = \"enableGraphSSL\" val ENABLE_META_SSL: String = \"enableMetaSSL\" val ENABLE_STORAGE_SSL: String = \"enableStorageSSL\" val SSL_SIGN_TYPE: String = \"sslSignType\" val CA_SIGN_PARAM: String = \"caSignParam\" val SELF_SIGN_PARAM: String = \"selfSignParam\" /** read config */ val RETURN_COLS: String = \"returnCols\" val NO_COLUMN: String = \"noColumn\" val PARTITION_NUMBER: String = \"partitionNumber\" val LIMIT: String = \"limit\" /** write config */ val RATE_LIMIT: String = \"rateLimit\" val VID_POLICY: String = \"vidPolicy\" val SRC_POLICY: String = \"srcPolicy\" val DST_POLI","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:5:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#pyspark-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"Running Nebula Graph Database on ARM64 Single Board Computer/Respberry Pi","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/"},{"categories":["Nebula Graph"],"content":" With the ARM64 Docker Image of Nebula Graph, it’s actually quite easy to run it on SBC/Respberry Pi! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:0:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#"},{"categories":["Nebula Graph"],"content":" 1 BackgroundRecently, after Yee from Nebula Graph Community fixed ARM build on nebula-third-party#37, we could play with Nebula Graph on M1 Chip Macbook. While, I didn’t get the chance to run it on a SBC/Pi. A couple of weeks before, in a twitter thread with @laixintao and @andelf I decided to purchase a Rock Pi 3A: And it looks nice!(Even come with a NPU inside) ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:1:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#background"},{"categories":["Nebula Graph"],"content":" 2 The Guide of running Nebula Graph on a Pi SBC Actually, since v3.0.0, Nebula comes with a standalone version, which suits the deep edge deployment more, but today, I will only setup the cluster version as the Docker Image is out of box to be used. I will share more on standalone version in upcoming weeks. I put the Ubuntu Server installation steps in the appendix, and now I assumed we already have an ARM64 Linux up and running on a Pi SBC. ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#the-guide-of-running-nebula-graph-on-a-pi-sbc"},{"categories":["Nebula Graph"],"content":" 2.1 Step 0, Install Docker-Compose on PiI am using debian/ubuntu here, while it should be the same for other distros, referring to https://docs.docker.com/engine/install/. sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # follow https://docs.docker.com/engine/install/linux-postinstall/ sudo groupadd docker sudo usermod -aG docker $USER exit # login again newgrp docker After Docker being installed, we install compose here, there could be issues encounterred from the Docker website on Compose installation. While, due to compose is just a python package, let’s do it via python3-pip install: sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:1","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-0-install-docker-compose-on-pi"},{"categories":["Nebula Graph"],"content":" 2.2 Step 1, bootstrap Nebula Graph Cluster on PiWe clone the compose file for nebula cluster first: git clone https://github.com/vesoft-inc/nebula-docker-compose.git \u0026\u0026 cd nebula-docker-compose docker-compose up -d Then, let’s download the client: nebula-console, and connect to the GraphD service: wget https://github.com/vesoft-inc/nebula-console/releases/download/v3.0.0/nebula-console-linux-arm64-v3.0.0 chmod +x nebula-console-linux-arm64-v3.0.0 ./nebula-console-linux-arm64-v3.0.0 -addr localhost -port 9669 -u root -p nebula Activate the storageD services: ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:2","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-1-bootstrap-nebula-graph-cluster-on-pi"},{"categories":["Nebula Graph"],"content":" 2.3 Step 2, Play Nebula Graph on PiWIth the SHOW HOSTS we should see StorageD services are all ONLINE, then we could run this from the console session to load the test dataset. Referennce: https://docs.nebula-graph.io/3.0.1/nebula-console/#import_a_testing_dataset $:play basketballplayer; The test data will be loaded in around 1 minute. Then, we could query something like: USE basketballplayer; GO FROM \"player100\" OVER follow YIELD dst(edge); Check this out and… Happy Graphing! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:3","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-2-play-nebula-graph-on-pi"},{"categories":["Nebula Graph"],"content":" 3 Appendix: Installing Ubuntu Server on Rock Pi 3A SBC Get the image from https://wiki.radxa.com/Rock3/downloads decompressing the file into .img Write the image to a micro SD card with etcher Boot it! feature image credit: @_louisreed ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:3:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#appendix-installing-ubuntu-server-on-rock-pi-3a-sbc"},{"categories":["Nebula Graph"],"content":"Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph?","date":"2022-02-28","objectID":"/en/resolve-wordle/","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/"},{"categories":["Nebula Graph"],"content":" Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph? ","date":"2022-02-28","objectID":"/en/resolve-wordle/:0:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#"},{"categories":["Nebula Graph"],"content":" 1 BackgroundYou may have seen tweets like this in past months, where the color dots in emoji was shared in SNS randomly. Feel free to Google Wordle first if you don’t know its meaning yet. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#background"},{"categories":["Nebula Graph"],"content":" 1.1 Wordle SolverFor all magics being used to solve wordle, I am impressed by Grant Sanderson, who explained us the information theory when solving wordle, in an elegent and delightful way. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#wordle-solver"},{"categories":["Nebula Graph"],"content":" 1.2 Chinese wordle: “handle”I am not going create yet another wordle-solver today, instead, it’s more about an intresting variant of wordle. To truly enjoy the fun of wordle, mostly we should be a native speaker, and it is not surprising that there is a Spanish wordle out there, and still tweets on wordle(es) are being shared literially every second now. While for non alphabetic languages like Chineses, do we have the luck to have fun with wordling? The answer is yes, while it’s a bit different. For the reason Chinese charactors, also called Hanzi or Han chactors are in from of Logogram, each charactor in Chinese is made up of radicals in quite different forms, each Chinese word can be 2/3/4 charactors. Most of the crossword games in Chinese are idiom, proverb or ancient poetry based: the slot to be filled are one Chinese. ref: Chinese idiom puzzle Thus, a wordle but in Chinese idiom will be quite strange, or even hilarious as Chee and JinGen discussed in this tweet thread, where you could see the candidate characters are counted in thousands: there is no chance to guess the idiom in 10 attempts! As one of the fun on playing wordle could be the feedback loop of guess --\u003e result in limited attempts, while the scale of Chinese Charactor had pushed the Chinese wordle variant creators leveraging other aspects of the charactor: pronunciation. Each charactor in Chinease is monosyllabic without exceptions, and when it comes to its pronunciation, they are all made up from two parts(initial and final, and they could be written in roman letters), which comes in tens level of possbilities. There are bounch of Chinese wordle varients asking player to guess idiom leveraging pinyin: https://pinyincaichengyu.com/ https://cheeaun.github.io/chengyu-wordle/ https://apps.apple.com/cn/app/id1606194420 While, to me, a native Chinese speaker, it’s either too hard to play with condtions of pronunciation parts(pinyin) or too easy to guess on given around 20 Chinese charactors. Then, the varient stands out here is the “handle/汉兜\"(Hanzi-Wordle) created by Antfu. “Handle” introduced the tones with genius to add an extra dimension of all charactors per each guess attempt, which helped player to have more information on filtering the knowledge in the brain. Note, for each Chinese charactor, there will be a tone in 1 of 4 tones in its pronunciation. Let’s see what it’s like to play the “Handle”: There will be 4 Chinese Charactors to be filled in 10 times of guess Not only the charactor self will be colored in result: For example in first line, the green “门” in position 2 is correct whereas in second line, the orange “仓” is corret while the possition should be all but not the first slot. There will be extra hints on: Pinyin parts for both part1(initial) and part2(final) In third line of the boxes, the green “qiao” refers to the first charactor is ponouced in “qiao” with initial:“q” and final:“iao”, although we filled the wrong charactor in the writing dimension. In third line, the orange “uo” refers to there is one chacarctor in other poisition with the final part of the pinyin as “uo”. Tones of the charactor: In third line, the green “-” stands for the third charactor is in tone-1. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-wordle-handle"},{"categories":["Nebula Graph"],"content":" 1.3 The Chinese Wordle HelperAs a non-native English speaker, the way I was playing wordle is to cheating relying on helpers: After knowing on the third letter to be “O”, I googled and got this: 5-letter-words-with-o-in-the-middle and do the searching 😁. The way to play with helpers works for me to have fun yet not ruin it by an automated cheat resolver(it’s only simulating my brain as a native-speaker!), so that I could somehow experience the same as Millions of people out there without cheating. While for Chinese “Handle” players, from my perspective, it’s still a bit harder(to find answers in 10 guesses), and the way my wife and I were playing “Handle” when lining up at the restaurant door ended up googling: idiom list with word ‘foo’, yet still having a lot of fun. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-chinese-wordle-helper"},{"categories":["Nebula Graph"],"content":" 2 Chinese idiom Knowledge GraphCould I create something between the human brain and the game-cheater/ruiner to make it more of fun? The answer is yes: a game extension as a secondary brain. For this helper/secondary brain, the solution for “handle” differenciates from the English wordle, unlike the auto-solver, similar algorithms could help on both cases: In wordle(English), player searches in their brain or from a helper like the web page: 5-letter-words-with-o-in-the-middle. In handle(Chinese), it’s harder to be searching based on hints like tones/initial parts of pinyin in fulltext webpage searching anymore, the reason hehind is that the multidimensional filter condtions are not indexed by normal webpages. As I mentioned, the key of the helper to be leveraged to (not ruining the game) is to be the extension of the brain, then the question is: how does our brain work on handling the knowledge of “handle”(yes, I was preparing for this pun for so long!)? Thus, why not do it in a graph/neural network way? And here we go, let’s create a knowledge graph of Chinese idiom and see how it goes with the “handle” game. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-idiom-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.1 TL;DRIt’s indeed entertaining to me, and I could write Graph Queries[*] by hand or via Visualization tools[**] to help explore things in this graph, because I can we’re doing the “thinking” process the similar way in our own brain, but not so well-informed. # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC ** ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#tldr"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-long-version-of-playing-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#play-handle-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:`character`)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-visualization-of-the-query"},{"categories":["Nebula Graph"],"content":" 3 What’s NextIf you happened to get hands dirty(or being intrested to) on Graph Database, you could checkout the Nebula Graph project now and its Docs to have more fun of it! Also, here are some only playgrounds if you prefer to try it without deployment on your own envrioment. If you are intrested in the MATCH query syntax and would like to actually do exersices with each daily handle challenge, check below Documents: MATCH https://docs.nebula-graph.io/3.0.1/3.ngql-guide/7.general-query-statements/2.match/ Graph Patterns https://docs.nebula-graph.io/3.0.1/3.ngql-guide/1.nGQL-overview/3.graph-patterns/ nGQL command cheatsheet https://docs.nebula-graph.io/3.0.1/2.quick-start/6.cheatsheet-for-ngql/ Happy Graphing! ","date":"2022-02-28","objectID":"/en/resolve-wordle/:3:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#whats-next"},{"categories":["Nebula Graph"],"content":" 4 Appendix: Setting up the Knowledge GraphI put the code and process here: https://github.com/wey-gu/chinese-graph, feel free to check that out. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#appendix-setting-up-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.1 Build the Knowledge GraphThe process would be: Modeling the Knowledge Preprocessing the data ETL data to a Graph Database: Nebula Graph Have fun on Nebula Graph ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#build-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.2 Modeling the Knowledge GraphFor Graph Modeling, it’s actually quite straight forward, the mind model for me was to put the knowledge I cares as vertcies and connect them with their relationships first. You will come back to iterate or optimize the modeling when you are actually playing with the data afterwards, thus, if you could imagine how the graph will be queried in the first place, the graph modeling could be adopted accordingly. Otherwise, don’t over design it, just do it the intuitive way. Here, I put the vertices with properties as: idiom character pinyin tone pinyin_part type The edges with properteis as: with_character with_pinyin with_pinyin_part ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#modeling-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":" 4.3 Deploy Nebula Graph With Nebula-UP, it’s an onliner call curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#deploy-nebula-graph"},{"categories":["Nebula Graph"],"content":" 4.4 Load data # clone the code for data genration and data loading git clone https://github.com/wey-gu/chinese-graph.git \u0026\u0026 cd chinese-graph python3 graph_data_generator.py # generate data # load data with Nebula-Importer docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:4","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#load-data"},{"categories":["Nebula Graph"],"content":"Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index?","date":"2022-02-20","objectID":"/en/nebula-index-explained/","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/"},{"categories":["Nebula Graph"],"content":" Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index? The term of Nebula Graph Index is quite similar to the index in RDBMS, while, they are not the same. It’s noticed that when getting started with Nebula Graph, the index confused some of the users in first glance on the following What exactly Nebula Graph Index is. When I should use it. How it impacts the performance. Today I’m gonna walk you through the index in Nebula Graph. Let’s get started! ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:0:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#"},{"categories":["Nebula Graph"],"content":" 1 What exactly Nebula Graph Index isTL;DR, Nebula Graph Index is only to be used to enable the graph query to be started from conditions on properties of vertices or edges, instead of vertexID. It’s only used in a starting entry of a graph query. If a query is in pattern: (a-\u003eb-\u003ec, where c in condition-foobar) graph walk, due to the only filtering condition-foobar is on c, this query under the hood will be started to seek c, and then it walks through the reversed -\u003e to b, finally to a. Thus, the Nebula Graph Index will be used and only be possbily used in seeking c, when condition-foobar is not like id(c) == \"foobar\" but c.property_x == \"foobar\". ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#what-exactly-nebula-graph-index-is"},{"categories":["Nebula Graph"],"content":" 1.1 Index is used only for starting point seekWe know that in RDBMS, an INDEX is to create a duplicated sorted DATA to enable QUERY with condition filtering on the sorted data, to accelerate the query in read and involves extra writes during the write. Note: in RDBMS/Tabular DB, an INDEX on some columns means to create extra data that are sorted on those columns to make query with those columns’ condition to be scanned faster, rather than scanning from the original table data sorted based on the key only. In Nebula Graph, the INDEX is to create a duplicated sorted Vertex/Edge PROP DATA to enable starting point seek of a QUERY(it’s a prerequisite rather than help accelerate it). Not all of the queries relied on index, here are some examples, let’s call them pure-property-condition-start queries: #### Queries relying on Nebula Graph Index # query 0 pure-property-condition-start query LOOKUP ON `tag1` WHERE col1 \u003e 1 AND col2 == \"foo\" \\ YIELD `tag1`.col1 as col1, `tag1`.col3 as col3; # query 1 pure-property-condition-start query MATCH (v:`player` { name: 'Tim Duncan' })--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name; In both query 0 and query 1, the pattern is to “Find VID/EDGE only based on given the propertiy condtions”. On the contrary, the starting point are VertexID based instead in query 2 and query 3: #### Queries not based on Nebula Graph Index # query 2, walk query starting from given vertex VID: \"player100\" GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age \u003e 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; # query 3, walk query starting from given vertex VID: \"player101\" or \"player102\" MATCH (v:`player` { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] \\ RETURN v2.`player`.name AS Name; If we look into query 1 and query 3, which shared condition on vertex on tag:player are both { name: 'Tim Duncan' } though, they are differenciated in starting points: For query 3 , the index is not required as the query will be started from known vertex ID in [\"player101\", \"player102\"] and thus: It’ll directly fetch vertex Data from v2’s vertex IDs then to GetNeighbors(): walk through edges of v2, GetVertices() for next hop: v and filter based on property: name For query 1 , the query has to start from v due to no known vertex IDs were provided: It’ll do IndexScan() first to find all vertices only with property condtion of { name: 'Tim Duncan' } Then, GetNeighbors(): walk through edges of v, GetVertices() for next hop: v2 Now, we could know the whole point that matters here is on whether to know the vertexID. And the above differences could be shown in their execution plans with PROFILE or EXPLAIN like the follow: query 1, requires index(on tag: player), pure prop condition query as starting point query 3, no index required, query starting from known vertex IDs ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:1","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#index-is-used-only-for-starting-point-seek"},{"categories":["Nebula Graph"],"content":" 1.2 Why Nebula Graph index is enabler rather than an acceleraterCan’t those queries be done without indexes? It’s possible in theory with full scan, but disabled without index. The reason is Nebula Graph stores data in a distributed and graph-oriented way, the full scan of data was condiserred too expensive to be allowed. Note: from v3.0, it’s possible to do TopN Scan without INDEX, where the LIMIT \u003cn\u003e is used, this is different from the fullscan case(INDEX is a must), which will be explained later. # sample vertex MATCH (v:`team`) RETURN v LIMIT 3 # or sample edge MATCH ()-[e:`follow`]-\u003e() RETURN e LIMIT 3 ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:2","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-nebula-graph-index-is-enabler-rather-than-an-accelerater"},{"categories":["Nebula Graph"],"content":" 1.3 Why starting point onlyIndex data is not used in terversal. It could confuse use to think of index is to sorting data based on properties, does it accelerate the terversal with property condition filtering? The answer is, no. In Nebula Graph, the data is structured in a way to enable fast graph-terversal, which is already indexed/sorted on vertex ID(for both vertex and edge) in raw data, where terversal(underlying in storage, it’s calling GetNeighbors interface) of given vertex is cheap and fast due to the locality/stored continuously(pysically linked). So in summary: Nebula Graph Index is sorted prop data to find the starting vertex or edge on given pure prop conditions. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:3","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-starting-point-only"},{"categories":["Nebula Graph"],"content":" 2 Facts on Nebula Graph IndexTo understand more details/limitations/cost of Nebula, let’s reveal more on its design and here are some facts in short: Index Data is stored and sharded together with Vertex Data It’s Left Match based only: It’s RocksDB Prefix Scan under the hood Effect on write and read path(to see its cost): Write Path: Extra Data written + Extra Read request introduced Read Path: RBO(Rule based optimization), Fan Out(to all shards) Data Full Scan LIMIT Sample(not full scan) is supported without Index LOOKUP ON t YIELD t.name | LIMIT 1 MATCH (v:`player` { name: 'Tim Duncan' })--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name LIMIT 3; The key info can be seen from one of my sketch notes: We should notice that only the left match is supported in pure-property-condition-start queries. For queries like wildcard or reguler-expression, Full-text Index/Search is to be used, where an external elastic search is integrated with nebula: please check Nebula Graph Full text index for more. Within this sketch note, more highlights are: It’s a Local Index Design The index is stored and shared locally together with the graph data. It’s sorting based on prop value, and the index search is underlying a rocksDB prefix scan, that’s why only left match is supported. There is cost in the write path The index enables the RDBMS-like Prop Condition Based Query with cost in the write path including not only the extra write, but also, random read, to ensure the data consistency. Index Data write is done in a sync way For Read path: In pure-property-condition-start queries, in GraphD, the index will be selected with Rule-based-optimization like this example, where, in a rule, the col2 to be sorted first is considered optimal with the condition: col2 equals ‘foo’. After the index was chosen, index-scan request will be fanout to storageD instances, and in the case of filters like LIMIT N, it will be pushed down to the storage side to reduce data payload. Note: it was not shown in the sketch but actually from v3.0, the nebula graph allows LIMIT N Sample Prop condition query like this w/o index, which is underlying pushing down the LIMIT filter to storage side. Take aways: Use index only when we have to, as it’s costly in write cases and if limit N sample is the only needed case and it’s fast enough, we can use that instead. Index is left match composite index order matters, should be created carefully. for full-text search use case, use full-text index instead. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:2:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#facts-on-nebula-graph-index"},{"categories":["Nebula Graph"],"content":" 3 How to use the indexWe should always refer to the documentation, and I just put some highlights on this here: To create an index on a tag or edge type to specify a list of props in the order that we need. CREATE INDEX If an index was created after existing data was inserted, we need to trigger an index async rebuild job, as the index data will be written in sync way only when index is created. REBUILD INDEX We can see the index status after REBUILD INDEX issued. SHOW INDEX STATUS Queries levering index could be LOOKUP, and with the pipeline, in most cases we will do follow-up graph-walk queries like: LOOKUP ON `player` \\ WHERE `player`.name == \"Kobe Bryant\"\\ YIELD id(vertex) AS VertexID, properties(vertex).name AS name |\\ GO FROM $-.VertexID OVER serve \\ YIELD $-.name, properties(edge).start_year, properties(edge).end_year, properties($$).name; Or in MATCH query like this, under the hood, v will be searched on index and v2 will be walked by default graph data structure without involving index. MATCH (v:`player`{name:\"Tim Duncan\"})--\u003e(v2:`player`) \\ RETURN v2.`player`.name AS Name; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:3:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#how-to-use-the-index"},{"categories":["Nebula Graph"],"content":" 4 RecapFinally, Let’s Recap INDEX is sorting PROP DATA to enable finding starting point on given PURE PROP CONDITION INDEX is not for Trevsal INDEX is left match, not for full-text search INDEX has cost on WRITE Remember to REBUILD after CREATE INDEX on existing data Happy Graphing! Feture image credit to Alina ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:4:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#recap"},{"categories":["Nebula Graph"],"content":"How to parse nebula graph data in an interactive way and what are the best practices?","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/"},{"categories":["Nebula Graph"],"content":" How to parse nebula graph data in an interactive way and what are the best practices? I will show you an easier way in this article 😁. updated: 2022-Aug-10, adapted to nebulagraph 3.x ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:0:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#"},{"categories":["Nebula Graph"],"content":" 1 Prepare for the Java REPLThanks to https://github.com/albertlatacz/java-repl/ we could play with/debug this in an interactive way, and all we need is to leverage its docker image to have all the envrioment in a clean and quick way: docker pull albertlatacz/java-repl docker run --rm -it \\ --network=nebula-net \\ -v ~:/root \\ albertlatacz/java-repl \\ bash apt update -y \u0026\u0026 apt install ca-certificates -y wget https://dlcdn.apache.org/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz --no-check-certificate tar xzvf apache-maven-3.8.6-bin.tar.gz wget https://github.com/vesoft-inc/nebula-java/archive/refs/tags/v3.0.0.tar.gz tar xzvf v3.0.0.tar.gz cd nebula-java-3.0.0/ ../apache-maven-3.8.6/bin/mvn dependency:copy-dependencies ../apache-maven-3.8.6/bin/mvn -B package -Dmaven.test.skip=true java -jar ../javarepl/javarepl.jar Now, after executing java -jar ../javarepl/javarepl.jar we are in a Java Shell(REPL), this enable us to execute Java code in an interactive way without wasting time and patience in the slow path(code –\u003e build –\u003e execute –\u003e add print –\u003e build), isn’t that cool? Like this: root@a2e26ba62bb6:/javarepl/nebula-java-3.0.0# java -jar ../javarepl/javarepl.jar Welcome to JavaREPL version 428 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111) Type expression to evaluate, :help for more options or press tab to auto-complete. Connected to local instance at http://localhost:43707 java\u003e System.out.println(\"Hello, World!\"); Hello, World! java\u003e Now we are in the java REPL, let’s introduce all the class path needed and do the imports in one go: :cp /javarepl/nebula-java-3.0.0/client/target/client-3.0.0.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/fastjson-1.2.78.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/slf4j-api-1.7.25.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/slf4j-log4j12-1.7.25.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/commons-pool2-2.2.jar :cp /javarepl/nebula-java-3.0.0/client/target/dependency/log4j-1.2.17.jar import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.vesoft.nebula.ErrorCode; import com.vesoft.nebula.client.graph.NebulaPoolConfig; import com.vesoft.nebula.client.graph.data.CASignedSSLParam; import com.vesoft.nebula.client.graph.data.HostAddress; import com.vesoft.nebula.client.graph.data.ResultSet; import com.vesoft.nebula.client.graph.data.SelfSignedSSLParam; import com.vesoft.nebula.client.graph.data.ValueWrapper; import com.vesoft.nebula.client.graph.net.NebulaPool; import com.vesoft.nebula.client.graph.net.Session; import java.io.UnsupportedEncodingException; import java.util.Arrays; import java.util.List; import java.util.concurrent.TimeUnit; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.*; And let’s connect it to the nebula graph, please replace your graphD IP and Port here, and execute them under the propmt string of java\u003e: NebulaPoolConfig nebulaPoolConfig = new NebulaPoolConfig(); nebulaPoolConfig.setMaxConnSize(10); List\u003cHostAddress\u003e addresses = Arrays.asList(new HostAddress(\"192.168.8.127\", 9669)); NebulaPool pool = new NebulaPool(); pool.init(addresses, nebulaPoolConfig); Session session = pool.getSession(\"root\", \"nebula\", false); ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:1:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#prepare-for-the-java-repl"},{"categories":["Nebula Graph"],"content":" 2 The execute for ResultSetFirst let’s check what we can do with a simple query: ResultSet resp = session.execute(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); Now you could play with it: Reference: client/graph/data/ResultSet.java java\u003e resp.isSucceeded() java.lang.Boolean res9 = true java\u003e resp.rowsSize() java.lang.Integer res16 = 1 java\u003e rows = resp.getRows() java.util.ArrayList rows = [Row ( values : [ \u003cValue vVal:Vertex ( vid : \u003cValue sVal:70 6c 61 79 65 72 31 30 30\u003e, tags : [ Tag ( name : 70 6C 61 79 65 72, props : { [B@5264a468 : \u003cValue iVal:42\u003e [B@496b8e10 : \u003cValue sVal:54 69 6d 20 44 75 6e 63 61 6e\u003e } ) ] )\u003e ] )] java\u003e row0 = resp.rowValues(0) java.lang.Iterable\u003ccom.vesoft.nebula.client.graph.data.ValueWrapper\u003e res10 = ColumnName: [n], Values: [(\"player100\" :player {name: \"Tim Duncan\", age: 42})] Remember our item is actually a vertex? (root@nebula) [basketballplayer]\u003e match (n:player) WHERE n.name == \"Tim Duncan\" return n +----------------------------------------------------+ | n | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2116/44373 us) Let’s see what(methods) can be done towards a value? v = Class.forName(\"com.vesoft.nebula.Value\") v.getDeclaredMethods() We could tell it’s quite Primitive on what com.vesoft.nebula.Value provided, thus we should use the ValueWrapper(or use executeJson actually) instead. To get a row of the result via iteration(as its a java iterable), we just follow how the example looped the result: import java.util.ArrayList; import java.util.List; List\u003cValueWrapper\u003e wrappedValueList = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c resp.rowsSize(); i++) { ResultSet.Record record = resp.rowValues(i); for (ValueWrapper value : record.values()) { wrappedValueList.add(value); if (value.isLong()) { System.out.printf(\"%15s |\", value.asLong()); } if (value.isBoolean()) { System.out.printf(\"%15s |\", value.asBoolean()); } if (value.isDouble()) { System.out.printf(\"%15s |\", value.asDouble()); } if (value.isString()) { System.out.printf(\"%15s |\", value.asString()); } if (value.isTime()) { System.out.printf(\"%15s |\", value.asTime()); } if (value.isDate()) { System.out.printf(\"%15s |\", value.asDate()); } if (value.isDateTime()) { System.out.printf(\"%15s |\", value.asDateTime()); } if (value.isVertex()) { System.out.printf(\"%15s |\", value.asNode()); } if (value.isEdge()) { System.out.printf(\"%15s |\", value.asRelationship()); } if (value.isPath()) { System.out.printf(\"%15s |\", value.asPath()); } if (value.isList()) { System.out.printf(\"%15s |\", value.asList()); } if (value.isSet()) { System.out.printf(\"%15s |\", value.asSet()); } if (value.isMap()) { System.out.printf(\"%15s |\", value.asMap()); } } System.out.println(); } As shown in above, the result value/item could be in properties string/int etc… or in graph semantic vertex, edge, path, we should use correspond asXxxx methods: java\u003e v = wrappedValueList.get(0) com.vesoft.nebula.client.graph.data.ValueWrapper v = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e v.asNode() com.vesoft.nebula.client.graph.data.Node res16 = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e node = v.asNode() com.vesoft.nebula.client.graph.data.Node node = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) Btw, it’s also possible to play with it with reflections(we imported already): Of courese we could also check via client/graph/data/ResultSet.java java\u003e rClass=Class.forName(\"com.vesoft.nebula.client.graph.data.ResultSet\") java.lang.Class r = class com.vesoft.nebula.client.graph.data.ResultSet java\u003e rClass.getDeclaredMethods() java.lang.reflect.Method[] res20 = [public java.util.List com.vesoft.nebula.client.graph.data.ResultSet.getColumnNames(), public int com.vesoft.nebula.client.graph.data.ResultSet.rowsSize(), public com.vesoft.nebula.client.graph.data.ResultSet$Record com.vesoft.nebula.clie","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:2:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-execute-for-resultset"},{"categories":["Nebula Graph"],"content":" 3 The executeJson methodSince 2.6, nebule finally supports json string response and we could do this: java\u003e String resp_json = session.executeJson(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); java.lang.String resp_json = \" { \"errors\":[ { \"code\":0 } ], \"results\":[ { \"spaceName\":\"basketballplayer\", \"data\":[ { \"meta\":[ { \"type\":\"vertex\", \"id\":\"player100\" } ], \"row\":[ { \"player.age\":42, \"player.name\":\"Tim Duncan\" } ] } ], \"columns\":[ \"n\" ], \"errors\":{ \"code\":0 }, \"latencyInUs\":4761 } ] } \" java\u003e And I believe you know much better than I do with it. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:3:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-executejson-method"},{"categories":["Nebula Graph"],"content":" 4 Conclusion If we go with JSON response, it’ll be easier for you to have everything parsed. If we have to deal with resultSet object, just use the ValueWrapper asNode() if the value is a vertex, use asRelationship if value is an edge and asPath() if the value is a path. With the REPL tool shown together with java reflection and source code, it’s possbile to inspect on how data could be parsed. Happy Graphing! Picture Credit：leunesmedia ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:4:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#conclusion"},{"categories":["Nebula Graph"],"content":"Dialog System With Graph Database Backed Knowledge Graph. 基于图数据库的智能问答助手","date":"2021-09-18","objectID":"/en/nebula-siwi/","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/"},{"categories":["Nebula Graph"],"content":" a PoC of Dialog System With Graph Database Backed Knowledge Graph. Related GitHub Repo: https://github.com/wey-gu/nebula-siwi/ I created the Katacoda Interactive Env for this project 👉🏻 https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#"},{"categories":["Nebula Graph"],"content":" Siwi the voice assistantSiwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. For now, it’s a demo for task-driven(not general purpose) dialog bots with KG(Knowledge Graph) leveraging Nebula Graph with the minimal/sample dataset from Nebula Graph Manual/ NG中文手册. Tips: Now you can play with the graph online without installing yourself! Nebula Playground | Nebula Playground - China Mainland Supported queries: relation: What is the relationship between Yao Ming and Lakers? How does Yao Ming and Lakers connected? serving: Which team had Yao Ming served? friendship: Whom does Tim Duncan follow? Who are Yao Ming’s friends? ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#siwi-the-voice-assistant"},{"categories":["Nebula Graph"],"content":" 1 Deploy and TryTBD (leveraging docker and nebula-up) ","date":"2021-09-18","objectID":"/en/nebula-siwi/:1:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#deploy-and-try"},{"categories":["Nebula Graph"],"content":" 2 How does it work?This is one of the most naive pipeline for a specific domain/ single purpose chat bot built on a Knowledge Graph. ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#how-does-it-work"},{"categories":["Nebula Graph"],"content":" 2.1 Backend The Backend(Siwi API) is a Flask based API server: Flask API server takes questions in HTTP POST, and calls the bot API. In bot API part there are classfier(Symentic Parsing, Intent Matching, Slot Filling), and question actors(Call corresponding actions to query Knowledge Graph with intents and slots). Knowledge Graph is built on an Open-Source Graph Database: Nebula Graph ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend"},{"categories":["Nebula Graph"],"content":" 2.2 Frontend The Frontend is a VueJS Single Page Applicaiton(SPA): I reused a Vue Bot UI to showcase a chat window in this human-agent interaction, typing is supported. In addtion, leverating Chrome’s Web Speech API, a button to listen to human voice is introduced ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend"},{"categories":["Nebula Graph"],"content":" 2.3 A Query Flow ┌────────────────┬──────────────────────────────────────┐ │ │ │ │ │ Speech │ │ ┌──────────▼──────────┐ │ │ │ Frontend │ Siwi, /ˈsɪwi/ │ │ │ Web_Speech_API │ A PoC of │ │ │ │ Dialog System │ │ │ Vue.JS │ With Graph Database │ │ │ │ Backed Knowledge Graph │ │ └──────────┬──────────┘ │ │ │ Sentence │ │ │ │ │ ┌────────────┼──────────────────────────────┐ │ │ │ │ │ │ │ │ │ Backend │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Web API, Flask │ ./app/ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Sentence ./bot/ │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent matching, │ ./bot/classifier│ │ │ │ │ Symentic Processing │ │ │ │ │ │ │ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Intent, Entities │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent Actor │ ./bot/actions │ │ │ │ │ │ │ │ │ └─┴──────────┬──────────┴───────────────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ Graph Database │ Nebula Graph │ │ │ │ │ │ └─────────────────────┘ │ │ │ │ │ │ │ └───────────────────────────────────────────────────────┘ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:3","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#a-query-flow"},{"categories":["Nebula Graph"],"content":" 2.4 Source Code Tree . ├── README.md ├── src │ ├── siwi # Siwi-API Backend │ │ ├── app # Web Server, take HTTP requests and calls Bot API │ │ └── bot # Bot API │ │ ├── actions # Take Intent, Slots, Query Knowledge Graph here │ │ ├── bot # Entrypoint of the Bot API │ │ ├── classifier # Symentic Parsing, Intent Matching, Slot Filling │ │ └── test # Example Data Source as equivalent/mocked module │ └── siwi_frontend # Browser End │ ├── README.md │ ├── package.json │ └── src │ ├── App.vue # Listening to user and pass Questions to Siwi-API │ └── main.js └── wsgi.py ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:4","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#source-code-tree"},{"categories":["Nebula Graph"],"content":" 3 Manually Run Components","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#manually-run-components"},{"categories":["Nebula Graph"],"content":" 3.1 BackendInstall and run. # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 For OpenFunction/ KNative docker build -t weygu/siwi-api . docker run --rm --name siwi-api \\ --env=PORT=5000 \\ --env=NG_ENDPOINTS=127.0.0.1:9669 \\ --net=host \\ weygu/siwi-api Try it out Web API: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"question\": \"What is the relationship between Yao Ming and Lakers?\"}' \\ http://192.168.8.128:5000/query | jq { \"answer\": \"There are at least 23 relations between Yao Ming and Lakers, one relation path is: Yao Ming follows Shaquille O'Neal serves Lakers.\" } Call Bot Python API: from nebula2.gclient.net import ConnectionPool from nebula2.Config import Config # define a config config = Config() config.max_connection_pool_size = 10 # init connection pool connection_pool = ConnectionPool() # if the given servers are ok, return true, else return false ok = connection_pool.init([('127.0.0.1', 9669)], config) # import siwi bot from siwi.bot import bot # instantiate a bot b = bot.SiwiBot(connection_pool) # make the question query b.query(\"Which team had Jonathon Simmons served?\") Then a response will be like this: In [4]: b.query(\"Which team had Jonathon Simmons serv ...: ed?\") [DEBUG] ServeAction intent: {'entities': {'Jonathon Simmons': 'player'}, 'intents': ('serve',)} [DEBUG] query for RelationshipAction: USE basketballplayer; MATCH p=(v)-[e:serve*1]-\u003e(v1) WHERE id(v) == \"player112\" RETURN p LIMIT 100; [2021-07-02 02:59:36,392]:Get connection to ('127.0.0.1', 9669) Out[4]: 'Jonathon Simmons had served 3 teams. Spurs from 2015 to 2015; 76ers from 2019 to 2019; Magic from 2017 to 2017; ' ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-1"},{"categories":["Nebula Graph"],"content":" 3.2 FrontendReferring to siwi_frontend ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-1"},{"categories":["Nebula Graph"],"content":" 4 Further work Use NBA-API to fallback undefined pattern questions Wrap and manage sessions instead of get and release session per request, this is somehow costly actually. Use NLP methods to implement proper Symentic Parsing, Intent Matching, Slot Filling Build Graph to help with Intent Matching, especially for a general purpose bot Use larger Dataset i.e. from wyattowalsh/basketball ","date":"2021-09-18","objectID":"/en/nebula-siwi/:4:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#further-work"},{"categories":["Nebula Graph"],"content":" 5 Thanks to Upstream Projects ❤️","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":" 5.1 Backend I learnt a lot from the KGQA on MedicalKG created by Huanyong Liu Flask pyahocorasick created by Wojciech Muła PyYaml ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-2"},{"categories":["Nebula Graph"],"content":" 5.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design Image credit goes to https://unsplash.com/photos/0E_vhMVqL9g ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-2"},{"categories":["Nebula Graph"],"content":"Setup Nebula Graph Dev Env with CLion and Docker 搭建基于 Docker 的 Nebula Graph CLion 开发环境","date":"2021-09-18","objectID":"/en/nebula-clion/","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/"},{"categories":["Nebula Graph"],"content":" 之前卡比同学向我咨询搭建 CLion 环境，开发 Nebula 的一些问题，我做了一些工作方便利用 Docker 在本地搭建这样一个环境，相关的东西放在：https://github.com/wey-gu/nebula-dev-CLion 。 Related GitHub Repo: https://github.com/wey-gu/nebula-dev-CLion ","date":"2021-09-18","objectID":"/en/nebula-clion/:0:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#"},{"categories":["Nebula Graph"],"content":" 1 Run Docker Env for Nebula-Graph with CLionBuild Docker Image git clone https://github.com/wey-gu/nebula-dev-CLion.git cd nebula-dev-CLion docker build -t wey/nebula-dev-clion:v2.0 . Run Docker Container for Nebula-Dev with CLion Integration Readiness(actually mostly Rsync \u0026 SSH). cd \u003cnebula-graph-repo-you-worked-on\u003e export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker run --rm -d \\ --name nebula-dev \\ --security-opt seccomp=unconfined \\ -p 2222:22 -p 2873:873 --cap-add=ALL \\ -v $PWD:/home/nebula \\ -w /home/nebula \\ wey/nebula-dev-clion:v2.0 Verify cmake with SSH. The default password is password ssh -o StrictHostKeyChecking=no root@localhost -p 2222 # in docker cd /home/nebula mkdir build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. Access container w/o SSH. docker exec -it nebula-dev bash mkdir -p build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. ","date":"2021-09-18","objectID":"/en/nebula-clion/:1:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#run-docker-env-for-nebula-graph-with-clion"},{"categories":["Nebula Graph"],"content":" 2 Configurations in CLion Ref: https://www.jetbrains.com/help/clion/clion-toolchains-in-docker.html#build-and-run Toolchains Add a remote host root@localhost:2222 password Put /opt/vesoft/toolset/cmake/bin/cmake as CMake CMake Toochain: Select the one created in last step Build directory: /home/nebula/build ","date":"2021-09-18","objectID":"/en/nebula-clion/:2:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#configurations-in-clion"},{"categories":["Nebula Graph"],"content":" 3 The appendix","date":"2021-09-18","objectID":"/en/nebula-clion/:3:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#the-appendix"},{"categories":["Nebula Graph"],"content":" 3.1 References of CMake output: [root@4c98e3f77ce8 build]# cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. \u003e\u003e\u003e\u003e Options of Nebula Graph \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_BUILD_STORAGE : OFF (Whether to build storage) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_MODULE_FORCE_CHECKOUT : ON (Whether checkout branch of module to same as graph.) -- ENABLE_MODULE_UPDATE : OFF (Automatically update module) -- ENABLE_PACK_ONE : ON (Whether to package into one) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_VERBOSE_BISON : OFF (Enable Bison to report state) -- ENABLE_WERROR : ON (Regard warnings as errors) -- CMAKE_BUILD_TYPE : Release (Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...) -- CMAKE_INSTALL_PREFIX : /usr/local/nebula (Install path prefix, prepended onto install directories.) -- CMAKE_CXX_STANDARD : 17 -- CMAKE_CXX_COMPILER : /opt/vesoft/toolset/clang/9.0.0/bin/c++ (CXX compiler) -- CMAKE_CXX_COMPILER_ID : GNU -- NEBULA_USE_LINKER : bfd -- CCACHE_DIR : /root/.ccache \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' \u003c\u003c\u003c\u003c -- NEBULA_THIRDPARTY_ROOT : /opt/vesoft/third-party/2.0 -- Build info of nebula third party: Package : Nebula Third Party Version : 2.0 Date : Mon Jun 28 15:07:38 UTC 2021 glibc : 2.17 Arch : x86_64 Compiler : GCC 9.2.0 C++ ABI : 11 Vendor : VEsoft Inc. -- CMAKE_INCLUDE_PATH : /opt/vesoft/third-party/2.0/include -- CMAKE_LIBRARY_PATH : /opt/vesoft/third-party/2.0/lib64;/opt/vesoft/third-party/2.0/lib -- CMAKE_PROGRAM_PATH : /opt/vesoft/third-party/2.0/bin -- GLIBC_VERSION : 2.17 -- found krb5-config here /opt/vesoft/third-party/2.0/bin/krb5-config -- Found kerberos 5 headers: /opt/vesoft/third-party/2.0/include -- Found kerberos 5 libs: /opt/vesoft/third-party/2.0/lib/libgssapi_krb5.a;/opt/vesoft/third-party/2.0/lib/libkrb5.a;/opt/vesoft/third-party/2.0/lib/libk5crypto.a;/opt/vesoft/third-party/2.0/lib/libcom_err.a;/opt/vesoft/third-party/2.0/lib/libkrb5support.a \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' done \u003c\u003c\u003c\u003c -- Create the pre-commit hook -- Creating pre-commit hook done \u003e\u003e\u003e\u003e Configuring Nebula Common \u003c\u003c\u003c\u003c \u003e\u003e\u003e\u003e Options of Nebula Common \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_WERROR : ON (Regard warnings as errors) -- Set D_GLIBCXX_USE_CXX11_ABI to 1 -- CMAKE_BUI","date":"2021-09-18","objectID":"/en/nebula-clion/:3:1","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#references-of-cmake-output"},{"categories":["courses"],"content":"Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database.","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/"},{"categories":["courses"],"content":"Walk you through in actions to do below sections exercises! Bootstrap a Nebula Graph Cluster and Studio Web App Import a graph of dataset about shareholding Exploring the shareholding data with Nebula Importer Visually Exploring the shareholding data with Nebula Studio Run Graph Algorithm on Nebula Cluster Graph Data The dataset comes from https://github.com/wey-gu/nebula-shareholding-example/tree/main/data_sample The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/:0:0","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/#"},{"categories":["courses"],"content":"Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s.","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/"},{"categories":["courses"],"content":"A full solution walkthrough for a Knowledge Graph Dialog System. Boostrap a Nebula Cluster in K8s Scale out the Nebula Cluster in K8s way Import the basketballplayer Dataset Siwi, the Knowledge Graph Dialog System with Nebula Graph Siwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. The code of Siwi is here: https://github.com/wey-gu/nebula-siwi. The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/:0:0","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/#"},{"categories":["Nebula Graph"],"content":"A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/"},{"categories":["Nebula Graph"],"content":" A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透 Related GitHub Repo: https://github.com/wey-gu/nebula-shareholding-example 更新：在这个数据集生成的工作基础上，我又做了一个全栈示例项目 👉🏻 https://siwei.io/corp-rel-graph/ I created the Katacoda Interactive Env for this project 👉🏻 https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ This is a demo of Shareholding Relationship Analysis with Distributed open-source Graph Database: Nebula Graph. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:0:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#"},{"categories":["Nebula Graph"],"content":" 1 Data","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data"},{"categories":["Nebula Graph"],"content":" 1.1 Data Modeling There are various kinds of relationships when we checking companies’ shareholding breakthrough, here let’s simplify it with only two kind of entities: person and corp, and with following relationship types. person can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp Below is the lines to reflect this graph modele in Nebula Graph, it’s quite straightforward, right? CREATE TAG person(name string); CREATE TAG corp(name string); CREATE EDGE role_as(role string); CREATE EDGE is_branch_of(); CREATE EDGE hold_share(share float); CREATE EDGE reletive_with(degree int); ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-modeling"},{"categories":["Nebula Graph"],"content":" 1.2 Data GenerationWe just randomly generate some data to help with this demo, you can call data_generator.py directly to generate or reuse what’s already done under data_sample folder. The generated data are records to be fit in above data model from below .csv files. $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:2","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-generation"},{"categories":["Nebula Graph"],"content":" 1.3 Data ImportWith those data in .csv files, we can easily import them into a Nebula Graph Cluster with the help of Nebula-Importer. nebula-importer.yaml in this repo describes rules and configurations on how this import will be done by the importer. For Nebula Graph Database, plesae refer to Doc , Doc-CN to deploy on any Linux Servers, for study and test, you can run it via Docker following the Quick Start Chapter of the documentation. For Nebula-Importer, if you already have Docker env, you can run it as the following without installing anything. Or, if you prefer to install it, it’s quite easy as it’s written in Golang and you can run its single file binary quite easily, go check both Documentation and Nebula-Importer Repo: https://github.com/vesoft-inc/nebula-importer. Let’s start! Below is the commands I used to import our data into a Nebula Graph Database. # put generated data \u0026 nebula-importor.yaml to nebula-importer server $ scp -r data nebula_graph_host:~ $ scp nebula-importer.yaml data nebula_graph_host:~/data $ ssh nebula_graph_host $ ls -l ${HOME}/data total 756 -rw-r--r--. 1 wei.gu wei.gu 23941 Jul 14 05:44 corp.csv -rw-r--r--. 1 wei.gu wei.gu 1277 Jul 14 05:44 corp_rel.csv -rw-r--r--. 1 wei.gu wei.gu 3048 Jul 14 05:44 corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 3893 Jul 14 05:44 nebula-importer.yaml -rw-r--r--. 1 wei.gu wei.gu 211661 Jul 14 05:44 person.csv -rw-r--r--. 1 wei.gu wei.gu 179770 Jul 14 05:44 person_corp_role.csv -rw-r--r--. 1 wei.gu wei.gu 322965 Jul 14 05:44 person_corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 17689 Jul 14 05:44 person_rel.csv # import data into our nebula graph database $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${HOME}/data/nebula-importer.yaml:/root/nebula-importer.yaml \\ -v ${HOME}/data:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-importer.yaml 2021/07/14 05:49:32 --- START OF NEBULA IMPORTER --- 2021/07/14 05:49:32 [WARN] config.go:491: Not set files[0].schema.vertex.vid.Type, reset to default value `string' ... 2021/07/14 05:49:43 [INFO] reader.go:180: Total lines of file(/root/person_corp_role.csv) is: 5000, error lines: 1287 2021/07/14 05:49:43 [INFO] statsmgr.go:61: Done(/root/person_corp_role.csv): Time(11.39s), Finished(12523), Failed(0), Latency AVG(1514us), Batches Req AVG(1824us), Rows AVG(1099.43/s) 2021/07/14 05:49:47 [INFO] statsmgr.go:61: Tick: Time(15.00s), Finished(25807), Failed(0), Latency AVG(1500us), Batches Req AVG(1805us), Rows AVG(1720.46/s) 2021/07/14 05:49:48 [INFO] reader.go:180: Total lines of file(/root/person.csv) is: 10000, error lines: 0 2021/07/14 05:49:48 [INFO] statsmgr.go:61: Done(/root/person.csv): Time(16.10s), Finished(29731), Failed(0), Latency AVG(1505us), Batches Req AVG(1810us), Rows AVG(1847.17/s) 2021/07/14 05:49:50 [INFO] reader.go:180: Total lines of file(/root/person_corp_share.csv) is: 20000, error lines: 0 2021/07/14 05:49:50 [INFO] statsmgr.go:61: Done(/root/person_corp_share.csv): Time(17.74s), Finished(36013), Failed(0), Latency AVG(1531us), Batches Req AVG(1844us), Rows AVG(2030.29/s) 2021/07/14 05:49:50 Finish import data, consume time: 18.25s 2021/07/14 05:49:51 --- END OF NEBULA IMPORTER --- ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:3","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-import"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#corporation-sharehold-relationship-breakthrough"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#query-in-ngql"},{"categories":["Nebula Graph"],"content":" 2 Corporation sharehold relationship breakthrough 2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#in-a-visual-way"},{"categories":["Nebula Graph"],"content":" 3 Thanks to Upstream Projects ❤️ Python Faker https://github.com/joke2k/faker/ pydbgen https://github.com/tirthajyoti/pydbgen Nebula Graph https://github.com/vesoft-inc/nebula-graph ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":" 3.1 Tips: You can deploy nebula graph in one line with: Nebula-UP, it helps install a nebula graph with Docker Nebula-operator-KIND , it helps setup all dependencies of Nebula-K8s-Operator including a K8s in Docker, PV Provider and then install a Nebula Graph with Nebula-Operator in K8s. Image Credit goes to https://unsplash.com/photos/3fPXt37X6UQ ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#tips"},{"categories":null,"content":" DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... K8s Community Day 2021 Openfunction + GraphDB Read more... COScon 2021 我的开源之路 Read more... PyCon China 2021 图数据库解谜与 Python 的图库应用实践 Read more... nMeetup: Nebula 应用上手实操 从头实操 Nebula 的部署，股权穿透，图算法运算，语音智能助手。 Read more... How to Train your Dragon 如何成为开源开发者（布道师）。 Read more... ","date":"2021-08-26","objectID":"/en/talk/:0:0","series":null,"tags":null,"title":"My Talks","uri":"/en/talk/#"},{"categories":["Nebula Graph"],"content":"Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm 导入 Livejournal 数据集到 Nebula 并运行 Nebula Algorithm 图算法","date":"2021-08-24","objectID":"/en/nebula-livejournal/","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/"},{"categories":["Nebula Graph"],"content":" 一个导入 Livejournal 数据集到 Nebula Graph 图数据库，并执行 Nebula Algorithm 图算法的过程分享。 Related GitHub Repo: https://github.com/wey-gu/nebula-LiveJournal ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#"},{"categories":["Nebula Graph"],"content":" nebula-LiveJournalLiveJournal Dataset is a Social Network Dataset in one file with two columns(FromNodeId, ToNodeId). $ head soc-LiveJournal1.txt # Directed graph (each unordered pair of nodes is saved once): soc-LiveJournal1.txt # Directed LiveJournal friednship social network # Nodes: 4847571 Edges: 68993773 # FromNodeId ToNodeId 0 1 0 2 0 3 0 4 0 5 0 6 It could be accessed in https://snap.stanford.edu/data/soc-LiveJournal1.html. Dataset statistics Nodes 4847571 Edges 68993773 Nodes in largest WCC 4843953 (0.999) Edges in largest WCC 68983820 (1.000) Nodes in largest SCC 3828682 (0.790) Edges in largest SCC 65825429 (0.954) Average clustering coefficient 0.2742 Number of triangles 285730264 Fraction of closed triangles 0.04266 Diameter (longest shortest path) 16 90-percentile effective diameter 6.5 ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#nebula-livejournal"},{"categories":["Nebula Graph"],"content":" 1 Dataset Download and Preprocessing","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#dataset-download-and-preprocessing"},{"categories":["Nebula Graph"],"content":" 1.1 DownloadIt is accesissiable from the official web page: $ cd nebula-livejournal/data $ wget https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz Comments in data file should be removed to make the data import tool happy. ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#download"},{"categories":["Nebula Graph"],"content":" 1.2 Preprocessing $ gzip -d soc-LiveJournal1.txt.gz $ sed -i '1,4d' soc-LiveJournal1.txt ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#preprocessing"},{"categories":["Nebula Graph"],"content":" 2 Import dataset to Nebula Graph","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#import-dataset-to-nebula-graph"},{"categories":["Nebula Graph"],"content":" 2.1 With Nebula ImporterNebula-Importer is a Golang Headless import tool for Nebula Graph. You may need to edit the config file under nebula-importer/importer.yaml on Nebula Graph’s address and credential。 Then, Nebula-Importer could be called in Docker as follow: $ cd nebula-livejournal $ docker run --rm -ti \\ --network=nebula-net \\ -v nebula-importer/importer.yaml:/root/importer.yaml \\ -v data/:/root \\ vesoft/nebula-importer:v2 \\ --config /root/importer.yaml Or if you have the binary nebula-importer locally: $ cd data $ \u003cpath_to_nebula-importer_binary\u003e --config ../nebula-importer/importer.yaml ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-importer"},{"categories":["Nebula Graph"],"content":" 2.2 With Nebula ExchangeNebula-Exchange is a Spark Application to enable batch and streaming data import from multiple data sources to Nebula Graph. To be done. (You can refer to https://siwei.io/nebula-exchange-sst-2.x/) ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-exchange"},{"categories":["Nebula Graph"],"content":" 3 Run Algorithms with Nebula GraphNebula-Algorithm is a Spark/GraphX Application to run Graph Algorithms with data consumed from files or a Nebula Graph Cluster. Supported Algorithms for now: Name Use Case PageRank page ranking, important node digging Louvain community digging, hierarchical clustering KCore community detection, financial risk control LabelPropagation community detection, consultation propagation, advertising recommendation ConnectedComponent community detection, isolated island detection StronglyConnectedComponent community detection ShortestPath path plan, network plan TriangleCount network structure analysis BetweennessCentrality important node digging, node influence calculation DegreeStatic graph structure analysis ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms-with-nebula-graph"},{"categories":["Nebula Graph"],"content":" 3.1 Ad-hoc Spark Env setupHere I assume the Nebula Graph was bootstraped with Nebula-Up, thus nebula is running in a Docker Network named nebula-docker-compose_nebula-net. Then let’s start a single server spark: docker run --name spark-master --network nebula-docker-compose_nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ -v nebula-algorithm/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 Thus we could make spark application submt inside this container: docker exec -it spark-master bash cd /root/ # download Nebula-Algorithm Jar Packagem, 2.0.0 for example, for other versions, refer to nebula-algorithm github repo and documentations. wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.0.0/nebula-algorithm-2.0.0.jar ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#ad-hoc-spark-env-setup"},{"categories":["Nebula Graph"],"content":" 3.2 Run AlgorithmsThere are many altorithms supported by Nebula-Algorithm, here some of their configuration files were put under nebula-algorithm as an example. Before using them, please first edit and change Nebula Graph Cluster Addresses and credentials. vim nebula-altorithm/algo-pagerank.conf Then we could enter the spark container and call corresponding algorithms as follow. Please adjust your --driver-memeory accordingly, i.e. pagerank altorithm: /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 16g nebula-algorithm-2.0.0.jar \\ -p pagerank.conf After the algorithm finished, the output will be under the path insdie the container defined in conf file: write:{ resultPath:/output/ } 题图版权：@sigmund ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:0:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#"},{"categories":["Nebula Graph"],"content":" 1 什么是 Nebula Exchange ?之前我在 Nebula Data Import Options 之中介绍过，Nebula Exchange 是一个 Nebula Graph 社区开源的 Spark Applicaiton，它专门用来支持批量或者流式地把数据导入 Nebula Graph Database 之中。 Nebula Exchange 支持多种多样的数据源（从 Apache Parquet, ORC, JSON, CSV, HBase, Hive MaxCompute 到 Neo4j, MySQL, ClickHouse, 再有 Kafka, Pulsar，更多的数据源也在不断增加之中）。 如上图所示，在 Exchange 内部，从除了不同 Reader 可以读取不同数据源之外，在数据经过 Processor 处理之后通过 Writer写入（sink） Nebula Graph 图数据库的时候，除了走正常的 ServerBaseWriter 的写入流程之外，它还可以绕过整个写入流程，利用 Spark 的计算能力并行生成底层 RocksDB 的 SST 文件，从而实现超高性能的数据导入，这个 SST 文件导入的场景就是本文带大家上手熟悉的部分。 详细信息请参阅：Nebula Graph 手册:什么是 Nebula Exchange Nebula Graph 官方博客也有更多 Nebula Exchange 的实践文章 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:1:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#什么是-nebula-exchange-"},{"categories":["Nebula Graph"],"content":" 2 步骤概观 实验环境 配置 Exchange 生成 SST 文件 写入 SST 文件到 Nebula Graph ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:2:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#步骤概观"},{"categories":["Nebula Graph"],"content":" 3 实验环境准备为了最小化使用 Nebula Exchange 的 SST 功能，我们需要： 搭建一个 Nebula Graph 集群，创建导入数据的 Schema，我们选择使用 Docker-Compose 方式、利用 Nebula-Up 快速部署，并简单修改其网络，以方便同样容器化的 Exchange 程序对其访问。 搭建容器化的 Spark 运行环境 搭建容器化的 HDFS ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#实验环境准备"},{"categories":["Nebula Graph"],"content":" 3.1 搭建 Nebula Graph 集群借助于 Nebula-Up 我们可以在 Linux 环境下一键部署一套 Nebula Graph 集群： curl -fsSL nebula-up.siwei.io/install.sh | bash 待部署成功之后，我们需要对环境做一些修改，这里我做的修改其实就是两点： 只保留一个 metaD 服务 起用 Docker 的外部网络 详细修改的部分参考附录一 应用 docker-compose 的修改： cd ~/.nebula-up/nebula-docker-compose vim docker-compose.yaml # 参考附录一 docker network create nebula-net # 需要创建外部网络 docker-compose up -d --remove-orphans 之后，我们来创建要测试的图空间，并创建图的 Schema，为此，我们可以利用 nebula-console ，同样，Nebula-Up 里自带了容器化的 nebula-console。 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 创建图空间（我们起名字叫 sst ），以及 schema create space sst(partition_num=5,replica_factor=1,vid_type=fixed_string(32)); :sleep 20 use sst create tag player(name string, age int); 示例输出 (root@nebula) [(none)]\u003e create space sst(partition_num=5,replica_factor=1,vid_type=fixed_string(32)); Execution succeeded (time spent 1468/1918 us) (root@nebula) [(none)]\u003e :sleep 20 (root@nebula) [(none)]\u003e use sst Execution succeeded (time spent 1253/1566 us) Wed, 18 Aug 2021 08:18:13 UTC (root@nebula) [sst]\u003e create tag player(name string, age int); Execution succeeded (time spent 1312/1735 us) Wed, 18 Aug 2021 08:18:23 UTC ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建-nebula-graph-集群"},{"categories":["Nebula Graph"],"content":" 3.2 搭建容器化的 Spark 环境利用 big data europe 做的工作，这个过程非常容易。 值得注意的是： 现在的 Nebula Exchange 对 Spark 的版本有要求，在现在的 2021 年 8 月，我是用了 spark-2.4.5-hadoop-2.7 的版本。 为了方便，我让 Spark 运行在 Nebula Graph 相同的机器上，并且指定了运行在同一个 Docker 网络下 docker run --name spark-master --network nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ bde2020/spark-master:2.4.5-hadoop2.7 然后，我们就可以进入到环境中了： docker exec -it spark-master bash 进到 Spark 容器中之后，可以像这样安装 maven: export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 还可以这样在容器里下载 nebula-exchange 的 jar 包： cd ~ wget https://repo1.maven.org/maven2/com/vesoft/nebula-exchange/2.1.0/nebula-exchange-2.1.0.jar ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建容器化的-spark-环境"},{"categories":["Nebula Graph"],"content":" 3.3 搭建容器化的 HDFS同样借助 big-data-euroupe 的工作，这非常简单，不过我们要做一点修改，让它的 docker-compose.yml 文件里使用 nebula-net 这个之前创建的 Docker 网络。 详细修改的部分参考附录二 git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop vim docker-compose.yml docker-compose up -d ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建容器化的-hdfs"},{"categories":["Nebula Graph"],"content":" 4 配置Exchange这个配置主要填入的信息就是 Nebula Graph 集群本身和将要写入数据的 Space Name，以及数据源相关的配置（这里我们用 csv 作为例子），最后再配置输出（sink）为 sst Nebula Graph GraphD 地址 MetaD 地址 credential Space Name 数据源 source: csv path fields etc. ink: sst 详细的配置参考附录二 注意，这里 metaD 的地址可以这样获取，可以看到 0.0.0.0:49377-\u003e9559 表示 49377 是外部的地址。 $ docker ps | grep meta 887740c15750 vesoft/nebula-metad:v2.0.0 \"./bin/nebula-metad …\" 6 hours ago Up 6 hours (healthy) 9560/tcp, 0.0.0.0:49377-\u003e9559/tcp, :::49377-\u003e9559/tcp, 0.0.0.0:49376-\u003e19559/tcp, :::49376-\u003e19559/tcp, 0.0.0.0:49375-\u003e19560/tcp, :::49375-\u003e19560/tcp nebula-docker-compose_metad0_1 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:4:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#配置exchange"},{"categories":["Nebula Graph"],"content":" 5 生成SST文件","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#生成sst文件"},{"categories":["Nebula Graph"],"content":" 5.1 准备源文件、配置文件 docker cp exchange-sst.conf spark-master:/root/ docker cp player.csv spark-master:/root/ 其中 player.csv 的例子： 1100,Tim Duncan,42 1101,Tony Parker,36 1102,LaMarcus Aldridge,33 1103,Rudy Gay,32 1104,Marco Belinelli,32 1105,Danny Green,31 1106,Kyle Anderson,25 1107,Aron Baynes,32 1108,Boris Diaw,36 1109,Tiago Splitter,34 1110,Cory Joseph,27 1111,David West,38 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#准备源文件配置文件"},{"categories":["Nebula Graph"],"content":" 5.2 执行 exchange 程序进入 spark-master 容器，提交执行 exchange 应用。 docker exec -it spark-master bash cd /root/ /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange-2.1.0.jar\\ -c exchange-sst.conf 检查执行结果： spark-submit 输出： 21/08/17 03:37:43 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 1093 ms on localhost (executor driver) (32/32) 21/08/17 03:37:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 21/08/17 03:37:43 INFO DAGScheduler: ResultStage 2 (foreachPartition at VerticesProcessor.scala:179) finished in 22.336 s 21/08/17 03:37:43 INFO DAGScheduler: Job 1 finished: foreachPartition at VerticesProcessor.scala:179, took 22.500639 s 21/08/17 03:37:43 INFO Exchange$: SST-Import: failure.player: 0 21/08/17 03:37:43 WARN Exchange$: Edge is not defined 21/08/17 03:37:43 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040 21/08/17 03:37:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! 验证 HDFS 上生成的 SST 文件： docker exec -it namenode /bin/bash root@2db58903fb53:/# hdfs dfs -ls /sst Found 10 items drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/1 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/10 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/2 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/3 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/4 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/5 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/6 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/7 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/8 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/9 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#执行-exchange-程序"},{"categories":["Nebula Graph"],"content":" 6 写入SST到NebulaGraph这里的操作实际上都是参考文档：SST 导入，得来。其中就是从 console 之中执行了两步操作： Download Ingest 其中 Download 实际上是触发 Nebula Graph 从服务端发起 HDFS Client 的 download，获取 HDFS 上的 SST 文件，然后放到 storageD 能访问的本地路径下，这里，需要我们在服务端部署 HDFS 的依赖。因为我们是最小实践，我就偷懒手动做了这个 Download 的操作。 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#写入sst到nebulagraph"},{"categories":["Nebula Graph"],"content":" 6.1 手动下载这里边手动下载我们就要知道 Nebula Graph 服务端下载的路径，实际上是 /data/storage/nebula/\u003cspace_id\u003e/download/，这里的 Space ID 需要手动获取一下： 这个例子里，我们的 Space Name 是 sst，而 Space ID 是 49。 (root@nebula) [sst]\u003e DESC space sst +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | ID | Name | Partition Number | Replica Factor | Charset | Collate | Vid Type | Atomic Edge | Group | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ | 49 | \"sst\" | 10 | 1 | \"utf8\" | \"utf8_bin\" | \"FIXED_STRING(32)\" | \"false\" | \"default\" | +----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ 于是，下边的操作就是手动把 SST 文件从 HDFS 之中 get 下来，再拷贝到 storageD 之中。 docker exec -it namenode /bin/bash $ hdfs dfs -get /sst /sst exit docker cp namenode:/sst . docker exec -it nebula-docker-compose_storaged0_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged1_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged2_1 mkdir -p /data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged0_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged1_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged2_1:/data/storage/nebula/49/download/ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#手动下载"},{"categories":["Nebula Graph"],"content":" 6.2 SST 文件导入 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 执行 INGEST 开始让 StorageD 读取 SST 文件 (root@nebula) [(none)]\u003e use sst (root@nebula) [sst]\u003e INGEST; 我们可以用如下方法实时查看 Nebula Graph 服务端的日志 tail -f ~/.nebula-up/nebula-docker-compose/logs/*/* 成功的 INGEST 日志： I0817 08:03:28.611877 169 EventListner.h:96] Ingest external SST file: column family default, the external file path /data/storage/nebula/49/download/8/8-6.sst, the internal file path /data/storage/nebula/49/data/000023.sst, the properties of the table: # data blocks=1; # entries=1; # deletions=0; # merge operands=0; # range deletions=0; raw key size=48; raw average key size=48.000000; raw value size=40; raw average value size=40.000000; data block size=75; index block size (user-key? 0, delta-value? 0)=66; filter block size=0; (estimated) table size=141; filter policy name=N/A; prefix extractor name=nullptr; column family ID=N/A; column family name=N/A; comparator name=leveldb.BytewiseComparator; merge operator name=nullptr; property collectors names=[]; SST file compression algo=Snappy; SST file compression options=window_bits=-14; level=32767; strategy=0; max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0; ; creation time=0; time stamp of earliest key=0; file creation time=0; E0817 08:03:28.611912 169 StorageHttpIngestHandler.cpp:63] SSTFile ingest successfully 题图版权：Pietro Jeng ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#sst-文件导入"},{"categories":["Nebula Graph"],"content":" 7 附录","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录"},{"categories":["Nebula Graph"],"content":" 7.1 附录一docker-compose.yaml diff --git a/docker-compose.yaml b/docker-compose.yaml index 48854de..cfeaedb 100644 --- a/docker-compose.yaml +++ b/docker-compose.yaml @@ -6,11 +6,13 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=metad0 - --ws_ip=metad0 - --port=9559 - --ws_http_port=19559 + - --ws_storage_http_port=19779 - --data_path=/data/meta - --log_dir=/logs - --v=0 @@ -34,81 +36,14 @@ services: cap_add: - SYS_PTRACE - metad1: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad1 - - --ws_ip=metad1 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad1:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta1:/data/meta - - ./logs/meta1:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - - metad2: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad2 - - --ws_ip=metad2 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad2:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta2:/data/meta - - ./logs/meta2:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - storaged0: image: vesoft/nebula-storaged:v2.0.0 environment: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged0 - --ws_ip=storaged0 - --port=9779 @@ -119,8 +54,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged0:19779/status\"] interval: 30s @@ -146,7 +81,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged1 - --ws_ip=storaged1 - --port=9779 @@ -157,8 +92,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged1:19779/status\"] interval: 30s @@ -184,7 +119,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged2 - --ws_ip=storaged2 - --port=9779 @@ -195,8 +130,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged2:19779/status\"] interval: 30s @@ -222,17 +157,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd:19669/status\"] interval: 30s @@ -257,17 +194,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd1 - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd1:19669/status\"] interval: 30s @@ -292,17 +231,21 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=me","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录一"},{"categories":["Nebula Graph"],"content":" 7.2 附录二https://github.com/big-data-europe/docker-hadoop 的 docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index ed40dc6..66ff1f4 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -14,6 +14,8 @@ services: - CLUSTER_NAME=test env_file: - ./hadoop.env + networks: + - nebula-net datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 @@ -25,6 +27,8 @@ services: SERVICE_PRECONDITION: \"namenode:9870\" env_file: - ./hadoop.env + networks: + - nebula-net resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 @@ -34,6 +38,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\" env_file: - ./hadoop.env + networks: + - nebula-net nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 @@ -43,6 +49,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\" env_file: - ./hadoop.env + networks: + - nebula-net historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 @@ -54,8 +62,14 @@ services: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env + networks: + - nebula-net volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: + +networks: + nebula-net: + external: true ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录二"},{"categories":["Nebula Graph"],"content":" 7.3 附录三nebula-exchange-sst.conf { # Spark relation config spark: { app: { name: Nebula Exchange 2.1 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"192.168.8.128:9669\"] meta:[\"192.168.8.128:49377\"] } user: root pswd: nebula space: sst # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://192.168.8.128:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is sst, just change type.sink to client if you want to use client import mode. { name: player type: { source: csv sink: sst } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录三"},{"categories":["sketches"],"content":"Nebula Operator Explained","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/"},{"categories":["sketches"],"content":" Nebula Graph operator explained This note explained nebula graph’s K8s Operator: Intro 00:00 Nebula K8s Operator Explained 0:25 How do we use Nebula Operator? 02:23 What is the difference between the Operator based Nebula Graph Cluster and the binary-based one? 03:50 How about the Performance impact when it comes to K8s-Operator deployment? 04:55 What is the easiest way to try out the nebula operator? 06:04 Outra 07:30 ref: https://github.com/vesoft-inc/nebula-operator ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:0:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:1:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:2:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Config Explained","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/"},{"categories":["sketches"],"content":" Nebula Graph config explained This note explained nebula graph configurations: Intro 00:00 Nebula Graph Config Explained 0:16 How about Configurations in Nebula Graph Deployed with Docker? 03:01 What about Nebula Graph in K8s Operator Deployment case? 03:55 Should we use Local-Config or Not?(spoiler: Yes!) 05:03 Outra 05:27 ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:0:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:1:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:2:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Index Demystified","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/"},{"categories":["sketches"],"content":" Nebula Graph Native Index Demystified(Chinese only now, English version will be soon uploaded) Index Demystified 0:33 When should we use index? 06:37 Index v.s. Fulltext Index 07:12 Index Performance Impact 08:03 ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:0:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:1:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:2:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Deployment Options","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/"},{"categories":["sketches"],"content":" Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:0:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#"},{"categories":["sketches"],"content":" 1 Bilibili ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:1:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#bilibili"},{"categories":["sketches"],"content":" 2 Youtube ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:2:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Data Import Options","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/"},{"categories":["sketches"],"content":" Nebula Graph comes with multiple Data Import utils and options, how should we choose from them? ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:0:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#"},{"categories":["sketches"],"content":" 1 Youtube ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:1:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#youtube"},{"categories":["Nebula Graph"],"content":"one liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker)","date":"2021-06-09","objectID":"/en/nebula-operator-kind/","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/"},{"categories":["Nebula Graph"],"content":" Nebula-Kind, an one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND (K8s in Docker) ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:0:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#"},{"categories":["Nebula Graph"],"content":" 1 Nebula-Operator-KindAs a Cloud Native Distributed Database, Nebula Graph comes with an open-source K8s Operator to enable boostrap and maintain Nebula Graph Cluster from a K8s CRD. Normally it takes you some time to setup all the dependencies and control plane resources of the Nebula Operator. If you are as lazy as I am, this Nebula-Operator-Kind is made for you to quick start and play with Nebula Graph in KIND. Nebula-Operator-Kind is the one-liner for setup everything for you including: Docker K8s(KIND) PV Provider Nebula-Operator Nebula-Console nodePort for accessing the Cluster Kubectl for playing with KIND and Nebula Operator ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:1:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#nebula-operator-kind"},{"categories":["Nebula Graph"],"content":" 2 How To UseInstall Nebula-Operator-Kind: curl -sL nebula-kind.siwei.io/install.sh | bash You will see this after it’s done You can connect to the cluster via ~/.nebula-kind/bin/console as below: ~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000 ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:2:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#how-to-use"},{"categories":["Nebula Graph"],"content":" 3 MoreIt’s in GitHub with more information you may be intrested in ;-), please try and feedback there~ https://github.com/wey-gu/nebula-operator-kind Updated Sept. 2021 Install on KubeSphere all-in-on cluster： curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash Install on existing K8s cluster: curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash Banner Picture Credit: Maik Hankemann ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:3:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#more"},{"categories":null,"content":" Hi, this is Wey :)I am a developer @vesoft working as Chief Evangelist of NebulaGraph, the open source distributed Graph Database I create toolings and content for NebulaGraph Database to help Developers in the open source community. I am working in open source and consider it is a privilege 1. It took me a couple of my early career years to figure out that my passion lies in helping others with my thoughts \u0026 the tech/magic I have learned. ","date":"2021-06-04","objectID":"/en/about/:0:0","series":null,"tags":null,"title":"","uri":"/en/about/#hi-this-is-wey-"},{"categories":null,"content":" 1 Recent Projects Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-06-04","objectID":"/en/about/:1:0","series":null,"tags":null,"title":"","uri":"/en/about/#recent-projects"},{"categories":null,"content":" 2 Sketches Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option should I use? Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-06-04","objectID":"/en/about/:2:0","series":null,"tags":null,"title":"","uri":"/en/about/#sketches"},{"categories":null,"content":" 3 Hands-on Cources How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-06-04","objectID":"/en/about/:3:0","series":null,"tags":null,"title":"","uri":"/en/about/#hands-on-cources"},{"categories":null,"content":" 4 Talks DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... ","date":"2021-06-04","objectID":"/en/about/:4:0","series":null,"tags":null,"title":"","uri":"/en/about/#talks"},{"categories":null,"content":" 5 Previous workI worked at Ericsson for amost 10 years(2011 to 2021). As the System Manager 2 of Cloud Execution Envrioment (CEE) 3 PDU Cloud, member of CEE 10 core team and CEE System Management team. Helping evolve CEE was my main job: I studied, designed and implemented more than 20 features for CEE 6.6.2 and CEE 10, including area of compute, network, storage, lifecycle management and security. I am also responsible for Ericsson CEE evangelism (internal and external) in China. I used to share my notes and thoughts on note.siwei.info, while from 2021, I will leave more ideas on siwei.io instead. ","date":"2021-06-04","objectID":"/en/about/:5:0","series":null,"tags":null,"title":"","uri":"/en/about/#previous-work"},{"categories":null,"content":" 6 ContactYou can DM me via twitter, or wey.gu@vesoft.com. Book me a zoom call: I share the same idea with Ahmet Alp Balkan’s tweet: Working in open source (and getting paid for it) is a privilege. It’s a career boost, makes you lots of friends across the industry, and gives you a public brand. I am one of the “lucky few” \u0026 thankful to Microsoft and Google who let me work on OSS nearly all my career. — ahmetb (@ahmetb) February 19, 2021  ↩︎ System Manager, PDU Cloud: Job Description ↩︎ Ericsson’s Telco. Infrastructure as a Service product offerring: Cloud Execution Environment ↩︎ ","date":"2021-06-04","objectID":"/en/about/:6:0","series":null,"tags":null,"title":"","uri":"/en/about/#contact"},{"categories":["Nebula Graph"],"content":"本文分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。","date":"2021-05-26","objectID":"/en/nebula-chia/","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/"},{"categories":["Nebula Graph"],"content":" 本文分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。 我把涉及的代码开源在了这里：https://github.com/wey-gu/nebula-chia ","date":"2021-05-26","objectID":"/en/nebula-chia/:0:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#"},{"categories":["Nebula Graph"],"content":" 1 What is Chia Network?Chia Network 是由 BitTorrent 的作者 Bram Cohen 的团队在 2017 年创建的区块链项目。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#what-is-chia-network"},{"categories":["Nebula Graph"],"content":" 1.1 Why yet another Blockchain? 为什么再搞一个区块链?Chia 用了全新的中本聪共识算法，这个算法通过不允许并行计算，让挖矿（Proof of Work）所需算力和能耗降到非常低，这使得超大组织、玩家没法像在其他的区块链项目那样有算力的绝对优势，也一定程度上规避了能源的浪费。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#why-yet-another-blockchain-为什么再搞一个区块链"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-can-i-access-chia-network-如何连接chia"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#安装"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#运行"},{"categories":["Nebula Graph"],"content":" 1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#访问-chia-的数据"},{"categories":["Nebula Graph"],"content":" 2 Inspect the Chia Network, 分析 Chia 的数据如果大家仔细看了上边表结构定义的截图，就能注意到一些表的主要信息是嵌套二进制 KV Byte，所以只从 SQLite 并不能看到所有 Chia 的数据，所以我们需要（用一个编程语言来）读取表里的 Byte。 幸运的是，这件事儿因为 Chia 是开源的，而且是 Python 的代码，使得我们可以直接交互式的做。 我花了一点点时间在 Chia 客户端代码里找到了需要的封装类，借助它，可以比较方便的分析 Chia 客户端在本地的全链数据。 如果您不感兴趣细节，可以直接看我分析的结论。 结论之后，我也给大家演示一下是怎么读取它们的。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#inspect-the-chia-network-分析-chia-的数据"},{"categories":["Nebula Graph"],"content":" 2.1 TL;DR, 结论我们可以从表中读取到区块链记录（Block Record ），Chia 币记录（Coin Record）。 从区块记录中，我们可以看到关键的涉及交易的信息： 关联的 Coin ，关联的 Puzzle（地址），Coin 的值(Amount) 从币记录中，我们可以看到关键的涉及区块的信息： 生成这个 Coin 所在区块链里的索引高度（Confirmed Index） 如果这个记录是花费 Coin 的，花费它的索引高度（Spent Index） ┌──────────────────────┐ ┌────────────────────────────────────────┐ │ │ │ │ │ Coin Record │ │ Block Record │ │ │ │ │ │ Coin Name │ │ Height ◄────────────────────────────┼─┐ │ │ │ │ │ ┌─┼───► Puzzle │ │ Header │ │ │ │ │ │ │ │ ├─┼───► Coin Parent │ │ Prev Header │ │ │ │ │ │ │ │ ├─┼───► Amount │ │ Block Body │ │ │ │ │ │ farmer_puzzle_hash │ │ │ │ Time Stamp │ │ fees │ │ │ │ │ │ pool_puzzle_hash │ └─────┼─┼─┬─ Confirmed Index │ │ prev_transaction_block_hash │ │ │ │ │ │ prev_transaction_block_height │ │ │ └─ Spent Index │ │ transactions_info ───────────────┼───────┘ │ │ │ ┌─── is_transaction_block │ │ Coinbase │ │ │ sub_epoch_summary ────────────────┼───────┐ │ │ │ │ │ │ └─ ────────────────────┘ │ │ is Peak │ │ │ └──is Block │ │ ┌─────────────────────┐ │ │ │ │ │ └────────────────────────────────────────┘ └─┼─► Sub Epoch Segment │ │ │ └─────────────────────┘ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#tldr-结论"},{"categories":["Nebula Graph"],"content":" 2.2 Preperation, 准备因为安装客户端之后，我们本地实际上已经有了相关的 Python 环境和依赖，只需要在里边跑起来就好。 # 注意，我们要 cd 到之前安装客户端时候克隆的仓库。 cd chia-blockchain # source activate 脚本来切换到仓库安装时候创建的 Python 虚拟环境，并进到 IPython 里。 source venv/bin/activate \u0026\u0026 pip install ipython \u0026\u0026 ipython 然后试着导入客户端里边带有的 Python 的 Chia 的封装类试试看。 In [1]: import sqlite3 ...: from chia.consensus.block_record import BlockRecord # 导入成功，没有报错 In [2]: !pwd # 我的安装克隆目录 /Users/weyl/chia-blockchain 恭喜你做好了准备，我们看看 Block Record 里都有什么。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#preperation-准备"},{"categories":["Nebula Graph"],"content":" 2.3 Block Record Chain，区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, #\u003c---- sub_epoch_summary blob, is_peak tinyint, is_block tinyint) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#block-record-chain区块记录"},{"categories":["Nebula Graph"],"content":" 2.4 Coin Record Chain，Chia 币记录类似的，我们可以获取一个 Coin 的记录，这里边，从表的定义可以看到，唯一二进制（不能直接从数据库查询中被人读懂）的字段就是是币值，不存在嵌套的结构，所以也并不需要封装的类才能看清楚里边的信息。 CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) 这里值得注意的信息主要是 spent_index 和 confirmed_index。 from chia.util.ints import uint64 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" con = sqlite3.connect(chia_db_path) cur = con.cursor() rows = list(cur.execute('SELECT * FROM coin_record WHERE confirmed_index = 201645')) coin_amount = uint64.from_bytes(rows[0][7]) In [201]: rows[0] Out[201]: ('cf35da0f595b49dde626d676b511ee62bce886f2216751aa51bb8ff851563d35', # coin_name 201645, # confirmed_index 0, # spent_index，这里没有spent，所以值无效 0, # spent，其实是 bool 1, # coinbase，bool 'bbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', # puzzle_hash 对应到地址 'ccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', b'\\x00\\x00\\x01\\x97t \\xdc\\x00', # uint64 1619662081) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:4","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#coin-record-chainchia-币记录"},{"categories":["Nebula Graph"],"content":" 2.5 Puzzles/ Address，地址我们可以把 Chia 中的 Puzzle 理解成为交易中的地址，为了方便使用，通常会把 Puzzle 的 hash 用bech32m 转换成地址。 Tips: 这里有一个在线双向转换的在线工具推荐一下: https://www.chiaexplorer.com/tools/address-puzzlehash-converter ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:5","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#puzzles-address地址"},{"categories":["Nebula Graph"],"content":" 3 How to explore Chia Network? 如何探索 Chia 链随着我们之前分析的信息，自然地，我们可以把 Chia 区块链中的信息取出来，用图（Graph）来表示，这里的图并不是（Graphic）图形、图画的意思，是数学、图论中的图。 在图的语境下，最主要的两个元素就是顶点（Vertex）和边（Edge）。 顶点表示一个实体，而边表示实体之间的某种关系，这种关系可以是对等的（无方向的）也可以是有方向的。 这里我们可以把这里的信息抽象映射到如图的图模型里： Block 顶点 Coin 顶点 Puzzle 顶点 spends 边（Block 到 Coin） confirms 边 （Block 到 Coin） belongs_to 边（Coin 到 Puzzle） 这里，我们应用的图是一种叫做属性图的形式，除了点和边的关系之外。这两种实体（点、边）还有其他信息只和它们的一个实例相关，所以再定义为顶点、边就不是很适合，这些信息就作为点、边的属性（preperty）存在。 这种为了处理实体之间关联、涉及实体、关联的属性信息的，也就是\"属性图\"的存储信息的方式在计算机领域越来越流行，甚至有专门为此结构而原生开发的数据库——图数据库（Graph Database）。 这里，我们用的就是一个叫做 Nebula Graph 的图数据库，它是一个现代的、为超大规模分部署架构设计的、原生存储、查询、计算图数据的项目，更棒的是，它是产生于社区的开源产品。 Tips: 安装 Nebula Graph 一般来说，面向超大规模数据的分布式系统，天然的都是不容易轻量部署的，大家如果第一次使用的话可以试试我写的一个叫做 nebula-up 的小工具，可以一行指令部署一个用来试用、学习的 Nebula Graph 集群，地址在这里： https://github.com/wey-gu/nebula-up/ 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-to-explore-chia-network-如何探索-chia-链"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#import-the-chia-to-a-graph-database-nebula-graph-导入-chia-数据到图数据库"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-conversion-数据转换"},{"categories":["Nebula Graph"],"content":" 3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-import-数据导入"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph 探索 Chia 的数据 3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#explore-the-chia-graph-探索-chia-的数据"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph 探索 Chia 的数据 3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#graph-db-queries"},{"categories":["Nebula Graph"],"content":" 3.2 Explore the Chia Graph 探索 Chia 的数据 3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO 5 STEPS FROM \\ \"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\" \\ OVER farmer_puzzle,spends,confirms BIDIRECT ... Got 419437 rows (time spent 735120/1170946 us) Wed, 19 May 2021 10:11:28 UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO 1 STEP FROM \"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\" \\ OVER belongs_to REVERSELY \\ YIELD CASE $$.coin.is_spent \\ WHEN true THEN $$.coin.amount \\ WHEN false THEN -$$.coin.amount \\ END AS Amount | YIELD sum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#the-visulized-way-with-nebula-studio"},{"categories":["Nebula Graph"],"content":" 4 Summary 总结这篇文章里，在我们简单介绍了 Chia Network 之后，我们首次的带大家一起从安装一个 Chia 终端，到分析终端同步到本地的 Chia 全网数据，借助于 Chia 终端开源的 Python 代码库，我们分析了全网数据里的重要信息。 之后，我们开源了一个小工具 Nebula-Chia，有了它，就可以把 Chia 的全网数据转换成 CSV 格式，这样，就可以借助 nebula-importer 把所有的数据导入到一个先进的图数据库（Nebula Graph）中。 Nebula Graph 的项目地址是 https://github.com/vesoft-inc/nebula-graph Nebula-Chia 我也开源在 https://github.com/wey-gu/nebula-chia 在图数据库中，我们展示了做基本 Query 的例子和借助图数据库自带的可视化工具，我们可以轻易地获取 Chia 全网数据之间关联关系，有了这个作为基础，这些数据中洞察的潜力和可以尝试的有意思事情可以比较直观和高效地进一步探索了！ 是不是很酷？ ","date":"2021-05-26","objectID":"/en/nebula-chia/:4:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#summary-总结"},{"categories":["Nebula Graph"],"content":" 5 References https://www.chia.net/faq/ https://chialisp.com/docs/ https://www.chiaexplorer.com/chia-coins https://docs.google.com/document/d/1tmRIb7lgi4QfKkNaxuKOBHRmwbVlGL4f7EsBDr_5xZE https://github.com/sipa/bech32/tree/master/ref/python https://github.com/Chia-Network/chia-blockchain/blob/main/README.md https://www.chia.net/assets/ChiaGreenPaper.pdf https://docs.nebula-graph.com.cn Banner Picture Credit: Icons8 Team ","date":"2021-05-26","objectID":"/en/nebula-chia/:5:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#references"},{"categories":null,"content":" How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-05-26","objectID":"/en/cources/:0:0","series":null,"tags":null,"title":"Hands on Courses","uri":"/en/cources/#"},{"categories":null,"content":" Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... Nebula-Chia A exploration(and open-source utility) on extracting and loading Chia Network Blockchain into Nebula Graph. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-05-26","objectID":"/en/projects/:0:0","series":null,"tags":null,"title":"Side Projects","uri":"/en/projects/#"},{"categories":null,"content":" Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-05-26","objectID":"/en/sketch-notes/:0:0","series":null,"tags":null,"title":"Sketch Notes","uri":"/en/sketch-notes/#"},{"categories":["Nebula Graph"],"content":"nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience.","date":"2021-05-05","objectID":"/en/vscode-ngql/","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/"},{"categories":["Nebula Graph"],"content":" nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#"},{"categories":["Nebula Graph"],"content":" VS Code nGQL Syntax Highlight ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#vs-code-ngql-syntax-highlight"},{"categories":["Nebula Graph"],"content":" 1 DownloadSearch ngql from the market or click here. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:1:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#download"},{"categories":["Nebula Graph"],"content":" 2 Features Highlighting all Keywords, Functions of a given .ngql file ","date":"2021-05-05","objectID":"/en/vscode-ngql/:2:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#features"},{"categories":["Nebula Graph"],"content":" 3 Release Notes","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#release-notes"},{"categories":["Nebula Graph"],"content":" 3.1 0.0.1Initial release, only .ngql Syntax is supported. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:1","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#001"},{"categories":["Nebula Graph"],"content":" 3.2 0.0.2Lower supported vscode version till ^1.50.1 ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:2","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#002"},{"categories":["Nebula Graph"],"content":" 4 Reference https://docs.nebula-graph.io/ https://github.com/vesoft-inc/nebula-graph/blob/master/src/parser/scanner.lex ","date":"2021-05-05","objectID":"/en/vscode-ngql/:4:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#reference"},{"categories":["Big Data","Cloud"],"content":"How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub","date":"2021-05-03","objectID":"/en/nebula-insights/","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/"},{"categories":["Big Data","Cloud"],"content":" How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub 这是我首发在 Datawhale 的文章，介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。 引子 我们想要收集一些帮助 Nebula Graph 社区运营的 metrics，希望能从不同来源的数据自动化周期性收集、处理、并方便地展现出来做数据驱动分析的基础设施。 Nebula Graph 是一个现代的开源分布式图数据库(Graph Database)，欢迎同学们从: 官网: https://nebula-graph.com.cn Bilibili: https://space.bilibili.com/472621355 GitHub:https://github.com/vesoft-inc/nebula-graph 了解我们哈。 ","date":"2021-05-03","objectID":"/en/nebula-insights/:0:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#"},{"categories":["Big Data","Cloud"],"content":" 1 需求 方便增加新的数据 数据收集无需人为触发（自动、周期性） 每天数据量不超过1000条 数据可以生成 dashboard，也可以支持统计分期 query 高可用，数据安全 低预算，尽可能不需要运维人力 ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#需求"},{"categories":["Big Data","Cloud"],"content":" 1.1 需求分析我们需要搭建一个系统能实现 一个能周期性触发获取数据的事件的服务: scheduler 一个触发之后，把数据 ETL 到数据库中的服务: ETL worker 一个数据仓库 一个能够把数据库作为源，允许用户 query，展示数据的界面: Data-UI 这个需求的特点是虽然数据量很小、但是要求服务高可用、安全。因为这种情况下自建服务器还需要保证HA和数据安全会一定会消耗昂贵运维人力，所以我们应该尽量避免在自己维护的服务器中搭建 scheduler, 和数据库。 最终，我们选择了尽量使用公有云的 aaS 的方案: ┌──────────────────────────┐ │ │ │ Google Cloud Scheduler │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────┐ │ │ │ ┌────────────▼─────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ├───┤ └─────────────────────┘ │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌─────────▼─────────┐ │ │ │ │ │ │ │ │ │ Google BigQuery │ │ └─────────────────────────┘ │ │ ├───────────► ... └─────────▲─────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌──────────┴───────────┐ │ │ │ │ └──────────────────┘ │ Google Data Studio │ │ ┌──┐ │ │ ┌──┐ │ │ ┌──┐ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──┴──┴─┴──┴─┴──┴──────┘ 因为我个人比较熟悉 Google Cloud Platform(GCP)的原因，加上GCP在大数据处理上比较领先，再加上Google提供的 free tier额度非常大方，以至于在我们这个数据量下，所有workload都会是免费的。 这个方案最后选择了全栈 Google Cloud，然而，这实际上只是一个参考，同学们完全可以在其他公有云提供商那里找到对应的服务。 这里我简单介绍一下， Google Cloud Scheduler是自解释的，不用多介绍了。 而 Google Cloud Functions是GCP的无服务器(serverless)的 Function as a Service服务，它的好处是我们可以把无状态的 event-driven 的 workload 代码放上去，它是按需付费（pay as you go)的，类似的服务还有 Google Cloud Run，后者的区别在于我们提供的是一个docker/container（这使得能支持的运行环境可以使任何能跑在容器里的东西），而 Cloud Functions是把我们的代码文件放上去。他们的效果是类似的，因为我准备用Python来做 ETL的东西，Clouf Functions已经支持了，我就直接选择它了。 在scheduler里边，我定义了每一天它发一个 pub/sub（类似于kafka，这里google可以保证至少发成功一次）消息给 Cloud Functions，然后 Cloud Functions会去做 ETL的工作。 这里，实际上我的设计里这个触发的函数调会把数据从API那里获取下来，在内存里处理好之后，存储到在对象存储里为 JSON 文件，然后再调用 Google BigQuery 的 API让 BigQuery直接从对对象存储里拉取 JSON 文件，导入记录到相应的表之中。 Google BigQuery 作为GCP 特别有竞争力的一个产品，是它数据仓库，BigQuery 可以无限扩容，支持海量数据导入，支持 SQL-like 的 query，还自带ML算法，通过SQL就能调用这些算法。它可以和很多GCP以及第三方的组件可以集成起来。 Google Data Studio 是GCP的数据 Insights产品，如果大家用过 Google Analytics 应该已经用过它了。 ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#需求分析"},{"categories":["Big Data","Cloud"],"content":" 1.2 数据的获取，API我们第一阶段想要收集的数据来源是 GitHub 上，社区项目的统计数据、Docker Hub上，社区镜像的拉取计数，之后，会增加更多维度的数据。 Github API, ref: https://pygithub.readthedocs.io 这里我们利用了一个Github API的一个 Python 封装，下边是在 IDLE/iPython/Jupyter 里尝试的例子 # 实例化一个client g = Github(login_or_token=token, timeout=60, retry=Retry( total=10, status_forcelist=(500, 502, 504), backoff_factor=0.3)) # 配置好要获取的repo的信息 org_str = \"vesoft-inc\" org = g.get_organization(org_str) repos = org.get_repos() # 这里repos是一个迭代器，方便看到里边的东西，我们把它 list 一下可以看到所有的repo: list(repos) [Repository(full_name=\"vesoft-inc/nebula\"), Repository(full_name=\"vesoft-inc/nebula-docs\"), Repository(full_name=\"vesoft-inc/nebula-dev-docker\"), Repository(full_name=\"vesoft-inc/github-statistics\"), Repository(full_name=\"vesoft-inc/nebula-docker-compose\"), Repository(full_name=\"vesoft-inc/nebula-go\"), Repository(full_name=\"vesoft-inc/nebula-java\"), Repository(full_name=\"vesoft-inc/nebula-python\"), Repository(full_name=\"vesoft-inc/nebula-importer\"), Repository(full_name=\"vesoft-inc/nebula-third-party\"), Repository(full_name=\"vesoft-inc/nebula-storage\"), Repository(full_name=\"vesoft-inc/nebula-graph\"), Repository(full_name=\"vesoft-inc/nebula-common\"), Repository(full_name=\"vesoft-inc/nebula-stats-exporter\"), Repository(full_name=\"vesoft-inc/nebula-web-docker\"), Repository(full_name=\"vesoft-inc/nebula-bench\"), Repository(full_name=\"vesoft-inc/nebula-console\"), Repository(full_name=\"vesoft-inc/nebula-docs-cn\"), Repository(full_name=\"vesoft-inc/nebula-chaos\"), Repository(full_name=\"vesoft-inc/nebula-clients\"), Repository(full_name=\"vesoft-inc/nebula-spark-utils\"), Repository(full_name=\"vesoft-inc/nebula-node\"), Repository(full_name=\"vesoft-inc/nebula-rust\"), Repository(full_name=\"vesoft-inc/nebula-cpp\"), Repository(full_name=\"vesoft-inc/nebula-http-gateway\"), Repository(full_name=\"vesoft-inc/nebula-flink-connector\"), Repository(full_name=\"vesoft-inc/nebula-community\"), Repository(full_name=\"vesoft-inc/nebula-br\"), Repository(full_name=\"vesoft-inc/.github\")] # repo0 是 vesoft-inc/nebula 这个repo，我们可以通过 get_clones_traffic，get_views_traffic 来获取过去十几天的 clone，view 统计 In [16]: repo0.get_clones_traffic() Out[16]: {'count': 362, 'uniques': 150, 'clones': [Clones(uniques=5, timestamp=2021-04-06 00:00:00, count=16), Clones(uniques=8, timestamp=2021-04-07 00:00:00, count=23), Clones(uniques=13, timestamp=2021-04-08 00:00:00, count=30), Clones(uniques=33, timestamp=2021-04-09 00:00:00, count=45), Clones(uniques=2, timestamp=2021-04-10 00:00:00, count=13), Clones(uniques=6, timestamp=2021-04-11 00:00:00, count=19), Clones(uniques=15, timestamp=2021-04-12 00:00:00, count=28), Clones(uniques=40, timestamp=2021-04-13 00:00:00, count=54), Clones(uniques=9, timestamp=2021-04-14 00:00:00, count=21), Clones(uniques=10, timestamp=2021-04-15 00:00:00, count=34), Clones(uniques=10, timestamp=2021-04-16 00:00:00, count=23), Clones(uniques=5, timestamp=2021-04-17 00:00:00, count=17), Clones(uniques=2, timestamp=2021-04-18 00:00:00, count=13), Clones(uniques=9, timestamp=2021-04-19 00:00:00, count=23), Clones(uniques=3, timestamp=2021-04-20 00:00:00, count=3)]} In [17]: repo0.get_views_traffic() Out[17]: {'count': 6019, 'uniques': 1134, 'views': [View(uniques=52, timestamp=2021-04-06 00:00:00, count=169), View(uniques=143, timestamp=2021-04-07 00:00:00, count=569), View(uniques=152, timestamp=2021-04-08 00:00:00, count=635), View(uniques=134, timestamp=2021-04-09 00:00:00, count=648), View(uniques=81, timestamp=2021-04-10 00:00:00, count=318), View(uniques=42, timestamp=2021-04-11 00:00:00, count=197), View(uniques=127, timestamp=2021-04-12 00:00:00, count=515), View(uniques=149, timestamp=2021-04-13 00:00:00, count=580), View(uniques=134, timestamp=2021-04-14 00:00:00, count=762), View(uniques=141, timestamp=2021-04-15 00:00:00, count=385), View(uniques=113, timestamp=2021-04-16 00:00:00, count=284), View(uniques=48, timestamp=2021-04-17 00:00:00, count=168), View(uniques=35, timestamp=2021-04-18 00:00:00, count=135), View(uniques=124, timestamp=2021-04-19 00:00:00, co","date":"2021-05-03","objectID":"/en/nebula-insights/:1:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数据的获取api"},{"categories":["Big Data","Cloud"],"content":" 2 实现","date":"2021-05-03","objectID":"/en/nebula-insights/:2:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#实现"},{"categories":["Big Data","Cloud"],"content":" 2.1 计划任务调度 with Cloud Scheduler前边提到，Scheduler --\u003e Functions 中间是通过消息队列实现的可靠事件触发，我们需要在 Google Cloud Pub/Sub里创建一个订阅消息，后边我们会把这个订阅消息从 Scheduler 定期发送，并且在 Function创建的时候定义为触发条件。 $ gcloud pubsub topics create nebula-insights-cron-topic $ gcloud pubsub subscriptions create cron-sub --topic nebula-insights-cron-topic 任务的创建非常直接，在 Scheduler Web Console 上直接图形化操作就可以了，记得要选择触发 Pub/Sub 消息为 cron-sub，消息主题为 nebula-insights-cron-topic ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#计划任务调度-with-cloud-scheduler"},{"categories":["Big Data","Cloud"],"content":" 2.2 ETL Worker with Python + Google Functions当 Scheduler 每天定时发送消息之后，接收方就是我们要定义的 Google Functions了，它的定义如图 第一步，选择它的触发类型为 Pub/Sub，同时要定义消息的主题和名字。 第二步就是把代码放进去: ┌─────────────────────┐ │ │ ┌──────────────────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ◄───► └─────────────────────┘ │ │ │ └────────────▲─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌────────────▼────────────┐ │ │ │ │ │ │ │ │ │ Google Cloud Storage │ │ └─────────────────────────┘ │ │ ... └────────────┬────────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌─────────▼─────────┐ │ │ │ │ └──────────────────┘ │ Google BigQuery │ │ │ └───────────────────┘ 这部分的逻辑就是通过前边分析了的API取得信息，然后组装成需要的格式存到 Cloud Storage(对象存储），然后再导入到 BigQuery（数仓）之中，全部代码在GitHub上: https://github.com/wey-gu/nebula-insights/blob/main/functions/data-fetching-0/main.py 另外，可以参考这个官方教程 https://cloud.google.com/scheduler/docs/tut-pub-sub ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#etl-worker-with-python--google-functions"},{"categories":["Big Data","Cloud"],"content":" 2.3 数仓表结构定义数仓的表结构比较直接，schema的图贴在下边了，值得注意的是，BigQuery支持嵌套的表结构（而不像一般关系型数据库那样需要把这样的逻辑结构用辅助表来表示），在我们这个场景下非常方便，比如release表中的 assets的三个嵌套字段。 更详细的信息可以参考GitHub上的介绍和代码: https://github.com/wey-gu/nebula-insights#data-etl-bigquery-and-gcs ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:3","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数仓表结构定义"},{"categories":["Big Data","Cloud"],"content":" 2.4 数据可视化到这里，我们就可以自动在BigQuery里存有每天收集的不同来源的统计数据啦，有了它，我们可以借助 Data Studio 来生成各式各样的可视化表示。 参考 https://cloud.google.com/bigquery/docs/visualize-data-studio ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:4","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数据可视化"},{"categories":["Big Data","Cloud"],"content":" 3 总结这样，我们实际上不需要任何认为维护的成本和投入，就搭建了一整个数据的流水线，并且只需要按照数据用量付费，在我们的数据量下，及时考虑未来增加数十个新的量度的收集，我们依然没有达到需要付费的用量，是不是很Cool？ 因为数据同时存在于对象存储与数仓里，我们可以方便随时把数据导入任意其他平台上。 BigQuery还有一些非常常用的，自带的机器学习的功能，只需要写一个SQL-Like的query就能触发然后获得预测结果，如果我们用到这些功能的话也会回到 datawhale 为同学们继续分享哈。 第一次做数据工程方面的分享，如果有错误的地方欢迎大家不吝指出哈~~ 谢谢！ ","date":"2021-05-03","objectID":"/en/nebula-insights/:3:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#总结"},{"categories":["Nebula Graph"],"content":"A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command.","date":"2021-04-26","objectID":"/en/nebula-up/","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/"},{"categories":["Nebula Graph"],"content":" Update: the All-in-one mode is introduced! Check here and try it! Nebula-Up is PoC utility to enable developer to bootstrap an nebula-graph cluster with nebula-graph-studio(Web UI) + nebula-graph-console(Command UI) ready out of box in an oneliner run. All required packages will handled with nebula-up as well, including Docker on Linux(Ubuntu/CentOS), Docker Desktop on macOS(including both Intel and M1 chip based), and Docker Desktop Windows. Also, it’s optimized to leverage China Repo Mirrors(docker, brew, gitee, etc…) in case needed enable a smooth deployment for both Mainland China users and others. macOS and Linux with Shell: curl -fsSL nebula-up.siwei.io/install.sh | bash Note: you could specify the version of Nebula Graph like: curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v2.6 ","date":"2021-04-26","objectID":"/en/nebula-up/:0:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#"},{"categories":["Nebula Graph"],"content":" 1 All-in-one modeWith all-in-one mode, you could play with many Nebula Tools in one command, too: Supported tools: Nebula Dashboard Nebula Graph Studio Nebula Graph Console Nebula BR(backup \u0026 restore) Nebula Graph Spark utils Nebula Graph Spark Connector/PySpark Nebula Graph Algorithm Nebula Graph Exchange Nebula Graph Importer Nebula Graph Fulltext Search Nebula Bench ","date":"2021-04-26","objectID":"/en/nebula-up/:1:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#all-in-one-mode"},{"categories":["Nebula Graph"],"content":" 1.1 Install all in one # Install Nebula Core with all-in-one mode curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash ","date":"2021-04-26","objectID":"/en/nebula-up/:1:1","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-all-in-one"},{"categories":["Nebula Graph"],"content":" 1.2 Install Nebula Core and One of the coponent: # Install Core with Backup and Restore with MinIO curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 br # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark # Install Core with Dashboard curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 dashboard ","date":"2021-04-26","objectID":"/en/nebula-up/:1:2","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-nebula-core-and-one-of-the-coponent"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be \u003chost-ip\u003e:9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#how-to-play-with-all-in-one-mode"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#console-and-basketballplayer-dataset-loading"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#monitor-the-whole-cluster-with-nebula-dashboard"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#access-nebula-graph-studio"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#query-data-with-nebula-spark-connector-in-pyspark-shell"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-exchange"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-graph-algorithm"},{"categories":["Nebula Graph"],"content":" 1.3 How to play with all-in-one mode: 1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#try-backup-and-restore-with-minio-as-storage"},{"categories":["Nebula Graph"],"content":"IPython-nGQL is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It's easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded.","date":"2021-03-07","objectID":"/en/ipython-ngql/","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/"},{"categories":["Nebula Graph"],"content":" ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It’s easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded. ipython-ngql is inspired by ipython-sql created by Catherine Devlin ","date":"2021-03-07","objectID":"/en/ipython-ngql/:0:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#"},{"categories":["Nebula Graph"],"content":" 1 Get Started","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-started"},{"categories":["Nebula Graph"],"content":" 1.1 Installationipython-ngql could be installed either via pip or from this git repo itself. Install via pip pip install ipython-ngql Install inside the repo git clone git@github.com:wey-gu/ipython-ngql.git cd ipython-ngql python setup.py install ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:1","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#installation"},{"categories":["Nebula Graph"],"content":" 1.2 Load it in Jupyter Notebook or iPython %load_ext ngql ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:2","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#load-it-in-jupyter-notebook-or-ipython"},{"categories":["Nebula Graph"],"content":" 1.3 Connect to Nebula GraphArguments as below are needed to connect a Nebula Graph DB instance: Argument Description --address or -addr IP address of the Nebula Graph Instance --port or -P Port number of the Nebula Graph Instance --user or -u User name --password or -p Password Below is an exmple on connecting to 127.0.0.1:9669 with username: “user” and password: “password”. %ngql --address 127.0.0.1 --port 9669 --user user --password password ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:3","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#connect-to-nebula-graph"},{"categories":["Nebula Graph"],"content":" 1.4 Make QueriesNow two kind of iPtython Magics are supported: Option 1: The one line stype with %ngql: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id; Option 2: The multiple lines stype with %%ngql %%ngql USE pokemon_club; SHOW TAGS; SHOW HOSTS; There will be other options in future, i.e. from a .ngql file. ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:4","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#make-queries"},{"categories":["Nebula Graph"],"content":" 1.5 Query String with Variablesipython-ngql supports taking variables from the local namespace, with the help of Jinja2 template framework, it’s supported to have queries like the below example. The actual query string should be GO FROM \"Sue\" OVER owns_pokemon ..., and \"{{ trainer }}\" was renderred as \"Sue\" by consuming the local variable trainer: In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:5","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#query-string-with-variables"},{"categories":["Nebula Graph"],"content":" 1.6 Configure ngql_result_styleBy default, ipython-ngql will use pandas dataframe as output style to enable more human readable output, while it’s supported to use the raw thrift data format comes from the nebula2-python itself. This can be done ad-hoc with below one line: %config IPythonNGQL.ngql_result_style=\"raw\" After above line being executed, the output will be like: ResultSet(ExecutionResponse( error_code=0, latency_in_us=2844, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) The result are always stored in variable _ in Jupyter Notebook, thus, to tweak the result, just refer a new var to it like: In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:6","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#configure-ngql_result_style"},{"categories":["Nebula Graph"],"content":" 1.7 Get HelpDon’t remember anything or even relying on the cheatsheet here, oen takeaway for you: the help! In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:7","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-help"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#examples"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#jupyter-notebook"},{"categories":["Nebula Graph"],"content":" 1.8 Examples 1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#ipython"}]