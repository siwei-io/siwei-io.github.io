[{"categories":["Nebula Graph","Amundsen"],"content":"I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/"},{"categories":["Nebula Graph","Amundsen"],"content":" Do I have to create my own graph model and everything to set up a Data Lineage system? Thanks to many great open-source projects, the answer is: No! Today, I would like to share my opinionated reference data infra stack with some of those best open-source projects with modern ETL, Dashboard, Metadata Governance, and Data Lineage Management. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:0:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#"},{"categories":["Nebula Graph","Amundsen"],"content":"1 Metadata Governance systemA Metadata Governance system is a system providing a single view of where and how all the data are formatted, generated, transformed, consumed, presented, and owned. Metadata Governance is like a catalog of all of the data warehouses, databases, tables, dashboards, ETL jobs, etc so that people don’t have to broadcast their queries on “Hi everyone, could I change the schema of this table?”, “Hey, anyone who knows how I could find the raw data of table-view-foo-bar?”, which, explains why we need a Metadata Governance system in a mature data stack with a relatively large scale of data and team(or one to be grown to). For the other term, Data Lineage, is one of the Metadata that needs to be managed, for example, some dashboard is the downstream of a table view, which has an upstream as two other tables from different databases. That information should be managed at best when possible, too, to enable a trust chain on a data-driven team. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:1:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance-system"},{"categories":["Nebula Graph","Amundsen"],"content":"2 The reference solution","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-reference-solution"},{"categories":["Nebula Graph","Amundsen"],"content":"2.1 MotivationThe metadata and data lineage are by nature fitting to the graph model/graph database well, and the relationship-oriented queries, for instance, “finding all n-depth data lineage per given component(i.e. a table)” is a FIND ALL PATH query in a graph database. This also explains one observation of mine as an OSS contributor of Nebula Graph, a distributed graph database: (from their queries/graph modeling in discussions I could tell) a bunch of teams who are already levering Nebula Graph on their tech stack, are setting up a data lineage system on their own, from scratch. A Metadata Governance system needs some of the following components: Metadata Extractor This part is needed to either pull or be pushed from the different parties of the data stack like databases, data warehouses, dashboards, or even from ETL pipeline and applications, etc. Metadata Storage This could be either a database or even large JSON manifest files Metadata Catalog This could be a system providing API and/or a GUI interface to read/write the metadata and data lineage From my special perspective and the context comes from the open-source community is always itching on this entropy increase situation when most of the components could be standardized and jointly contributed rather than being built in-house per each team because most of the sources to parse metadata are just those famous big-data projects and products, and the way to model and CURD those metadata should be in common with a high probability. Then I came to create an opinionated reference data infra stack with some of those best open-source projects put together. Hopefully, those who were gonna define and iterate their own fashion of Graph Model on Nebula Graph and create in-house Metadata and data linage extracting pipelines can benefit from this project to have a relatively polished, beautifully designed, Metadata Governance system out of the box with a fully evolved graph model. To make the reference project self-contained and runnable, I tried to put layers of data infra stack more than just pure metadata related ones, thus, maybe it will help new data engineers who would like to try and see how far had open-source pushed a modern data lab to. This is a diagram of all the components in this reference data stack, where I see most of them as Metadata Sources: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#motivation"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-data-stack"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#database-and-data-warehouse"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dataops"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#etl"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-visualization"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#job-orchestration"},{"categories":["Nebula Graph","Amundsen"],"content":"2.2 The Data StackThen, let’s introduce the components. 2.2.1 Database and Data WarehouseFor processing and consuming raw and intermediate data, one or more databases and/or warehouses should be used. It could be any DB/DW like Hive, Apache Delta, TiDB, Cassandra, MySQL, or Postgres, in this reference project, we simply choose one of the most popular ones: Postgres. And our reference lab comes with the first service: ✅ - Data warehouse: Postgres 2.2.2 DataOpsWe should have some sort of DataOps setup to enable pipelines and environments to be repeatable, testable, and version-controlled. Here, we used Meltano created by GitLab. Meltano is a just-work DataOps platform that connected Singer as the EL and dbt as the T in a magically elegant way, it is also connected to some other dataInfra utilities such as Apache Superset and Apache Airflow, etc. Thus, we have one more thing to be included: ✅ - GitOps: Meltano 2.2.3 ETLAnd under the hood, we will E(extract) and L(load) data from many different data sources to data targets leveraging Singer together with Meltano, and do T(transformation) with dbt. ✅ - EL: Singer ✅ - T: dbt 2.2.4 Data VisualizationHow about creating dashboards, charts, and tables for getting the insights into all the data? Apache Superset is one of the greatest visualization platforms we could choose from, and we just add it to our packet! ✅ - Dashboard: Apache Superset 2.2.5 Job OrchestrationIn most cases, our DataOps jobs grow to the scale to be executed in a long time that needs to be orchestrated, and here comes the Apache Airflow. ✅ - DAG: Apache Airflow 2.2.6 Metadata governanceWith more components and data being introduced to a data infra, there will be massive metadata in all lifecycle of databases, tables, schemas, dashboards, DAGs, applications, and their administrators and teams could be collectively managed, connected, and discovered. Linux Foundation Amundsen is one of the best projects solving this problem. ✅ - Data Discovery: Linux Foundation Amundsen With a graph database as the source of truth to accelerate the multi-hop queries together with elasticsearch as the full-text search engine, Amundsen indexes all the metadata and their lineage smoothly, and beautifully in the next level. By default, neo4j was used as the graph database, while I will be using Nebula Graph instead in this project due to I am more familiar with the latter. ✅ - Full-text Search: elasticsearch ✅ - Graph Database: Nebula Graph Now, with the components in our stack being revealed, let’s have them assembled. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:2:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-governance"},{"categories":["Nebula Graph","Amundsen"],"content":"3 Environment Bootstrap, Component overviewThe reference runnable project is open-source and you could find it here: https://github.com/wey-gu/data-lineage-ref-solution I will try my best to make things clean and isolated. It’s assumed you are running on a UNIX-like system with internet and Docker Compose being installed. Please refer here to install Docker and Docker Compose before moving forward. I am running it on Ubuntu 20.04 LTS X86_64, but there shouldn’t be issues on other distros or versions of Linux. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#environment-bootstrap-component-overview"},{"categories":["Nebula Graph","Amundsen"],"content":"3.1 Run a Data Warehouse/ DatabaseFirst, let’s install Postgres as our data warehouse. This oneliner will help create a Postgres running in the background with docker, and when being stopped it will be cleaned up(--rm). docker run --rm --name postgres \\ -e POSTGRES_PASSWORD=lineage_ref \\ -e POSTGRES_USER=lineage_ref \\ -e POSTGRES_DB=warehouse -d \\ -p 5432:5432 postgres Then we could verify it with Postgres CLI or GUI clients. Hint: You could use VS Code extension: SQL tools to quickly connect to multiple RDBMS(MariaDB, Postgres, etc.) or even Non-SQL DBMS like Cassandra in a GUI fashion. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#run-a-data-warehouse-database"},{"categories":["Nebula Graph","Amundsen"],"content":"3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env meltano init \u003cyourprojectname\u003e “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace \u003cyourprojectname\u003e with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init \u003cyourprojectname\u003e Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke \u003cplugin\u003e to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" \u003e\u003e .env echo PG_USERNAME=\"lineage_ref\" \u003e\u003e .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-dataops-toolchain-for-etl"},{"categories":["Nebula Graph","Amundsen"],"content":"3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" .env echo PG_USERNAME=\"lineage_ref\" .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#installation-of-meltano"},{"categories":["Nebula Graph","Amundsen"],"content":"3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" .env echo PG_USERNAME=\"lineage_ref\" .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#the-meltano-ui"},{"categories":["Nebula Graph","Amundsen"],"content":"3.2 Setup DataOps toolchain for ETLThen, let’s get Meltano with Singler and dbt installed. Meltano helps us manage ETL utilities(as plugins) and all of their configurations(the pipelines). Those meta-information sits in meltano configurations and its system database, where the configurations are file-based(could be managed with git) and by default the system database is SQLite. 3.2.1 Installation of MeltanoThe workflow using Meltano is to initiate a meltano project and start to add E, L, and T into the configuration files. The initiation of a project just requires a CLI command call: meltano init yourprojectname and to do that, we could install Meltano either with Python’s package manager: pip or via a Docker image: Install Meltano with pip in a python virtual env: mkdir .venv # example in a debian flavor Linux distro sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env meltano init “Install” Meltano via Docker docker pull meltano/meltano:latest docker run --rm meltano/meltano --version # init a project mkdir meltano_projects \u0026\u0026 cd meltano_projects # replace with your own one touch .env docker run --rm -v \"$(pwd)\":/projects \\ -w /projects --env-file .env \\ meltano/meltano init Apart from meltano init, there are a couple of other commands like meltano etl to perform ETL executions, and meltano invoke to call plugins’ command, always check the cheatsheet for quick referencing. 3.2.2 The Meltano UIMeltano also comes with a web-based UI, to start it, just run: meltano ui Then it’s listening to http://localhost:5000. For Docker, just run the container with the 5000 port exposed, here we didn’t provide ui in the end due to the container’s default command being meltano ui already. docker run -v \"$(pwd)\":/project \\ -w /project \\ -p 5000:5000 \\ meltano/meltano 3.2.3 Example Meltano projectsWhen writing this article, I noticed that Pat Nadolny had created great examples on an example dataset for Meltano with dbt(And with Airflow and Superset, too!). We will not recreate the examples and use Pat’s great ones. Note that Andrew Stewart had created another one with a slightly older version of configuration files. You could follow here to run a pipeline of: tap-CSV(Singer), extracting data from CSV files target-postgres(Singer), loading data to Postgres dbt, transform the data into aggregated tables or views You should omit the step of running the local Postgres with docker as we had already created one, be sure to change the Postgres user and password in .env. And it’s basically as this(with meltano being installed as above): git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/singer_dbt_jaffle/ meltano install touch .env echo PG_PASSWORD=\"lineage_ref\" .env echo PG_USERNAME=\"lineage_ref\" .env # Extract and Load(with Singer) meltano run tap-csv target-postgres # Trasnform(with dbt) meltano run dbt:run # Generate dbt docs meltano invoke dbt docs generate # Serve generated dbt docs meltano invoke dbt docs to serve # Then visit http://localhost:8080 Now, I assumed you had finished trying out singer_dbt_jaffle following its README.md, and we could connect to the Postgres to see the loaded and transformed data being reflected as follow, the screenshot is from the SQLTool of VS Code: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#example-meltano-projects"},{"categories":["Nebula Graph","Amundsen"],"content":"3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref \u003e\u003e .env echo PG_PASSWORD=lineage_ref \u003e\u003e .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#setup-a-bi-platform-for-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":"3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref .env echo PG_PASSWORD=lineage_ref .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#bootstrap-meltano-and-superset"},{"categories":["Nebula Graph","Amundsen"],"content":"3.3 Setup a BI Platform for DashboardNow, we have the data in data warehouses, with ETL toolchains to pipe different data sources into it. How could those data be consumed? BI tools like the dashboard could be one way to help us get insights from the data. With Apache Superset, dashboards, and charts based on those data sources could be created and managed smoothly and beautifully. The focus of this project was not on Apache Superset itself, thus, we simply reuse examples that Pat Nadolny had created in Superset as a utility if meltano Example. 3.3.1 Bootstrap Meltano and SupersetCreate a python venv with Meltano installed: mkdir .venv python3 -m venv .venv/meltano source .venv/meltano/bin/activate python3 -m pip install wheel python3 -m pip install meltano Following Pat’s guide, with tiny modifications: Clone the repo, enter the jaffle_superset project git clone https://github.com/pnadolny13/meltano_example_implementations.git cd meltano_example_implementations/meltano_projects/jaffle_superset/ Modify the meltano configuration files to let Superset connect to the Postgres we created: vim meltano_projects/jaffle_superset/meltano.yml In my example, I changed the hostname to 10.1.1.111, which is the IP of my current host, while if you are running it on your macOS machine, this should be fine to leave with it, the diff before and after the change would be: --- a/meltano_projects/jaffle_superset/meltano.yml +++ b/meltano_projects/jaffle_superset/meltano.yml @@ -71,7 +71,7 @@ plugins: A list of database driver dependencies can be found here https://superset.apache.org/docs/databases/installing-database-drivers config: database_name: my_postgres - sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@host.docker.internal:${PG_PORT}/${PG_DATABASE} + sqlalchemy_uri: postgresql+psycopg2://${PG_USERNAME}:${PG_PASSWORD}@10.1.1.168:${PG_PORT}/${PG_DATABASE} tables: - model.my_meltano_project.customers - model.my_meltano_project.orders Add Postgres credential to .env file: echo PG_USERNAME=lineage_ref .env echo PG_PASSWORD=lineage_ref .env Install the Meltano project, run ETL pipeline meltano install meltano run tap-csv target-postgres dbt:run Start Superset, please note that the ui is not a meltano command but a user-defined action in the configuration file. meltano invoke superset:ui In another terminal, run the defined command load_datasources meltano invoke superset:load_datasources Access Superset in a web browser via http://localhost:8088/ We should now see Superset Web Interface: 3.3.2 Create a Dashboard!Let’s try to create a Dashboard on the ETL data in Postgres defined in this Meltano project: Click + DASHBOARD, fill a dashboard name, then click SAVE, then clieck + CREATE A NEW CHART In new chart view, we should select a chart type and DATASET. Here, I selected orders table as the data source and Pie Chart chart type: After clicking CREATE NEW CHART, we are in the chart defination view, where, I selected Query of status as DIMENSIONS, and COUNT(amount) as METRIC. Thus, we could see a Pie Chart per order status’s distribution. Click SAVE , it will ask which dashboard this chart should be added to, after it’s selected, click SAVE \u0026 GO TO DASHBOARD. Then, in the dashboard, we coulds see all charts there. You could see that I added another chart showing customer order count distribution, too: We could set the refresh inteval, or download the dashboard as you wish by clicking the ··· button. It’s quite cool, ah? For now, we have a simple but typical data stack like any hobby data lab with everything open-source! Imagine we have 100 datasets in CSV, 200 tables in Data warehouse and a couple of data engineers running different projects that consume, generate different application, dashboard, and databases. When someone would like to discovery some of those table, dataset, dashboard and pipelines running across them, and then even modify some of them, it’s proven to be quite costly in both communicationand engineering.","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#create-a-dashboard"},{"categories":["Nebula Graph","Amundsen"],"content":"3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ └─","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":"3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ └─","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#deploy-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":"3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ └─","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#metadata-service"},{"categories":["Nebula Graph","Amundsen"],"content":"3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ └─","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#data-builder"},{"categories":["Nebula Graph","Amundsen"],"content":"3.4 Metadata DiscoveryThen, we are stepping to deploy the Amundsen with Nebula Graph and Elasticsearch. Note: For the time being, the PR Nebula Graph as the Amundsen backend is not yet merged, I am working with the Amundsen team to make it happen. With Amundsen, we could have all metadata of the whole data stack being discovered and managed in one place. And there are mainly two parts of Amundsen: Metadata Ingestion Amundsen Data builder Metadata Catalog Amundsen Frontend service Amundsen Metadata service Amundsen Search service We will be leveraging Data builder to pull metadata from different sources, and persist metadata into the backend storage of the Meta service and the backend storage of the Search service, then we could search, discover and manage them from the Frontend service or through the API of the Metadata service. 3.4.1 Deploy Amundsen3.4.1.1 Metadata serviceWe are going to deploy a cluster of Amundsen with its docker-compose file. As the Nebula Graph backend support is not yet merged, we are referring to my fork. First, let’s clone the repo with all submodules: git clone -b amundsen_nebula_graph --recursive git@github.com:wey-gu/amundsen.git cd amundsen Then, start all catalog services and their backend storage: docker-compose -f docker-amundsen-nebula.yml up You could add -d to put the containers running in the background: docker-compose -f docker-amundsen-nebula.yml up -d And this will stop the cluster: docker-compose -f docker-amundsen-nebula.yml stop This will remove the cluster: docker-compose -f docker-amundsen-nebula.yml down Due to this docker-compose file is for developers to play and hack Amundsen easily rather than for production deployment, it’s building images from the codebase, which, will take some time for the very first time. After it’s being deployed, please hold on a second before we load some dummy data into its storage with Data builder. 3.4.1.2 Data builderAmundsen Data builder is just like a Meltano but for ETL of Metadata to Metadata service and Search service‘s backend storage: Nebula Graph and Elasticsearch. The Data builder here is only a python module and the ETL job could be either run as a script or orchestrated with a DAG platform like Apache Airflow. With Amundsen Data builder being installed: cd databuilder python3 -m venv .venv source .venv/bin/activate python3 -m pip install wheel python3 -m pip install -r requirements.txt python3 setup.py install Let’s call this sample Data builder ETL script to have some dummy data filled in. python3 example/scripts/sample_data_loader_nebula.py 3.4.1.3 Verify AmundsenBefore accessing Amundsen, we need to create a test user: # run a container with curl attached to amundsenfrontend docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot # Create a user with id test_user_id curl -X PUT -v http://amundsenmetadata:5002/user \\ -H \"Content-Type: application/json\" \\ --data \\ '{\"user_id\":\"test_user_id\",\"first_name\":\"test\",\"last_name\":\"user\", \"email\":\"test_user_id@mail.com\"}' exit Then we could view UI at http://localhost:5000 and try to search test, it should return some results. Then you could click and explore those dummy metadata loaded to Amundsen during the sample_data_loader_nebula.py on your own. Additionally, you could access the Graph Database with Nebula Studio(http://localhost:7001). Note in Nebula Studio, the default fields to log in will be: Hosts: graphd:9669 User: root Password: nebula This diagram shows some more details on the components of Amundsen: ┌────────────────────────┐ ┌────────────────────────────────────────┐ │ Frontend:5000 │ │ Metadata Sources │ ├────────────────────────┤ │ ┌────────┐ ┌─────────┐ ┌─────────────┐ │ │ Metaservice:5001 │ │ │ │ │ │ │ │ │ │ ┌──────────────┐ │ │ │ Foo DB │ │ Bar App │ │ X Dashboard │ │ ┌────┼─┤ Nebula Proxy │ │ │ │ │ │ │ │ │ │ │ │ └──────────────┘ │ │ │ │ │ │ │ │ │ │ ├────────────────────────┤ │ └────────┘ └─────┬───┘ └─────────────┘ │ ┌─┼────┤ Search searvice:5002 │ │ │ │ │ │ └─","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:3:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-amundsen"},{"categories":["Nebula Graph","Amundsen"],"content":"4 Connecting the dots, Metadata DiscoveryWith the basic environment being set up, let’s put everything together. Remember we had ELT some data to PostgreSQL as this? How could we let Amundsen discover metadata regarding those data and ETL? ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#connecting-the-dots-metadata-discovery"},{"categories":["Nebula Graph","Amundsen"],"content":"4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata --\u003e CSV --\u003e Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph --\u003e Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-postgres-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":"4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata -- CSV -- Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph -- Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata -- CSV -- Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph -- Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-postgres-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":"4.1 Extracting Postgres metadataWe started on the data source: Postgres, first. We install the Postgres Client for python3: sudo apt-get install libpq-dev pip3 install Psycopg2 4.1.1 Execution of Postgres metadata ETLRun a script to parse Postgres Metadata: export CREDENTIALS_POSTGRES_USER=lineage_ref export CREDENTIALS_POSTGRES_PASSWORD=lineage_ref export CREDENTIALS_POSTGRES_DATABASE=warehouse python3 example/scripts/sample_postgres_loader_nebula.py If you look into the code of the sample script for loading Postgres metadata to Nebula, the main lines are quite straightforward: # part 1: PostgressMetadata -- CSV -- Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=PostgresMetadataExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph -- Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) The first job was to load data in path:PostgressMetadata -- CSV -- Nebula Graph PostgresMetadataExtractor was used to extract/pull metadata from Postgres, refer here for its documentation. FsNebulaCSVLoader was used to put extracted data intermediately as CSV files NebulaCsvPublisher was used to publish metadata in form of CSV to Nebula Graph The second job was to load in the path: Metadata stored in NebulaGraph -- Elasticsearch NebulaSearchDataExtractor was used to fetch metadata stored in Nebula Graph SearchMetadatatoElasticasearchTask was used to make metadata indexed with Elasticsearch. Note, in production, we could trigger those jobs either in scripts or with an orchestration platform like Apache Airflow. 4.1.2 Verify the Postgres ExtractionSearch payments or directly visit http://localhost:5000/table_detail/warehouse/postgres/public/payments, you could see the metadata from our Postgres like: Then, metadata management actions like adding tags, owners, and descriptions could be done easily as it was in the above screen capture, too. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-postgres-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":"4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest --\u003e CSV --\u003e Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph --\u003e Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-dbt-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":"4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest -- CSV -- Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph -- Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-dbt-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":"4.2 Extracting dbt metadataActually, we could also pull metadata from dbt itself. The Amundsen DbtExtractor, will parse the catalog.json or manifest.json file to load metadata to Amundsen storage(Nebula Graph and Elasticsearch). In above meltano chapter, we had already generated that file with meltano invoke dbt docs generate, and the output like the following is telling us the catalog.json file: 14:23:15 Done. 14:23:15 Building catalog 14:23:15 Catalog written to /home/ubuntu/ref-data-lineage/meltano_example_implementations/meltano_projects/singer_dbt_jaffle/.meltano/transformers/dbt/target/catalog.json 4.2.1 Execution of dbt metadata ETLThere is an example script with a sample dbt output files: The sample dbt files: $ ls -l example/sample_data/dbt/ total 184 -rw-rw-r-- 1 w w 5320 May 15 07:17 catalog.json -rw-rw-r-- 1 w w 177163 May 15 07:17 manifest.json We could load this sample dbt manifest with: python3 example/scripts/sample_dbt_loader_nebula.py From this lines of python code, we could tell those process as: # part 1: Dbt manifest -- CSV -- Nebula Graph job = DefaultJob( conf=job_config, task=DefaultTask( extractor=DbtExtractor(), loader=FsNebulaCSVLoader()), publisher=NebulaCsvPublisher()) ... # part 2: Metadata stored in NebulaGraph -- Elasticsearch extractor = NebulaSearchDataExtractor() task = SearchMetadatatoElasticasearchTask(extractor=extractor) job = DefaultJob(conf=job_config, task=task) And the only differences from the Postgres meta ETL is the extractor=DbtExtractor(), where it comes with following confiugrations to get below information regarding dbt projects: databases_name catalog_json manifest_json job_config = ConfigFactory.from_dict({ 'extractor.dbt.database_name': database_name, 'extractor.dbt.catalog_json': catalog_file_loc, # File 'extractor.dbt.manifest_json': json.dumps(manifest_data), # JSON Dumped objecy 'extractor.dbt.source_url': source_url}) 4.2.2 Verify the dbt ExtractionSearch dbt_demo or visit http://localhost:5000/table_detail/dbt_demo/snowflake/public/raw_inventory_value to see: Tips: we could optionally enable debug logging to see what had been sent to Elasticsearch and Nebula Graph! - logging.basicConfig(level=logging.INFO) + logging.basicConfig(level=logging.DEBUG) Or, alternatively, explore the imported data in Nebula Studio: First, click “Start with Vertices”, fill in the vertex id: snowflake://dbt_demo.public/fact_warehouse_inventory Then, we could see the vertex being shown as the pink dot. Let’s modify the Expand options with: Direction: Bidirect Steps: Single with 3 And double click the vertex(dot), it will expand 3 steps in bidirection: From this graph view, the insight of the metadata is extremely easy to be explored, right? Tips, you may like to click the 👁 icon to select some properties to be shown, which was done by me before capturing the screen as above. And, what we had seen in the Nebula Studio echoes the data model of Amundsen metadata service, too: Finally, remember we had leveraged dbt to transform some data in meltano, and the menifest file path is .meltano/transformers/dbt/target/catalog.json, you can try create a databuilder job to import it. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:2","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-dbt-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":"4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\u003e\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extracting-superset-metadata"},{"categories":["Nebula Graph","Amundsen"],"content":"4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#execution-of-superset-metadata-etl"},{"categories":["Nebula Graph","Amundsen"],"content":"4.3 Extracting Superset metadataDashboards, Charts and the relationships with Tables can be extracted by Amundsen data builder, as we already setup a Superset Dashboard, let’s try ingesting its metadata. 4.3.1 Execution of Superset metadata ETLThe sample superset script will fetch data from Superset and load metadata into Nebula Graph and Elasticsearch. python3 sample_superset_data_loader_nebula.py If we set the logging level to DEBUG, we could actually see lines like: # fetching metadata from superset DEBUG:urllib3.connectionpool:http://localhost:8088 \"POST /api/v1/security/login HTTP/1.1\" 200 280 INFO:databuilder.task.task:Running a task DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8088 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 308 374 DEBUG:urllib3.connectionpool:http://localhost:8088 \"GET /api/v1/dashboard/?q=(page_size:20,page:0,order_direction:desc) HTTP/1.1\" 200 1058 ... # insert Dashboard DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT VERTEX `Dashboard` (`dashboard_url`, `name`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\":(\"http://localhost:8088/superset/dashboard/3/\",\"my_dashboard\",\"unique_tag\",timestamp()); ... # insert a DASHBOARD_WITH_TABLE relationship/edge INFO:databuilder.publisher.nebula_csv_publisher:Importing data in edge files: ['/tmp/amundsen/dashboard/relationships/Dashboard_Table_DASHBOARD_WITH_TABLE.csv'] DEBUG:databuilder.publisher.nebula_csv_publisher:Query: INSERT edge `DASHBOARD_WITH_TABLE` (`END_LABEL`, `START_LABEL`, published_tag, publisher_last_updated_epoch_ms) VALUES \"superset_dashboard://my_cluster.1/3\"-\"postgresql+psycopg2://my_cluster.warehouse/orders\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()), \"superset_dashboard://my_cluster.1/3\"-\"postgresql+psycopg2://my_cluster.warehouse/customers\":(\"Table\",\"Dashboard\",\"unique_tag\", timestamp()); 4.3.2 Verify the Superset Dashboard ExtractionBy searching it in Amundsen, we could the Dashboard info now. And we could verify it from Nebula Studio, too. Note, see also the Dashboard’s model in Amundsen from the dashboard ingestion guide: ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:3","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#verify-the-superset-dashboard-extraction"},{"categories":["Nebula Graph","Amundsen"],"content":"4.4 Preview data with SupersetSuperset could be used to preview Table Data like this. Corresponding documentation could be referred here, where the API of /superset/sql_json/ will be called by Amundsen Frontend. ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:4","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#preview-data-with-superset"},{"categories":["Nebula Graph","Amundsen"],"content":"4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]-\u003e(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()) ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#enable-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":"4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]-(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()) ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#get-lineage-in-nebula-graph"},{"categories":["Nebula Graph","Amundsen"],"content":"4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]-(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()) ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#extract-data-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":"4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]-(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()) ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#dbt"},{"categories":["Nebula Graph","Amundsen"],"content":"4.5 Enable Data lineageBy default, data lineage was not enabled, we could enable it by: Go to the Amundsen repo, that’s also where we run the docker-compose -f docker-amundsen-nebula.yml up command cd amundsen Modify frontend JS configuration: --- a/frontend/amundsen_application/static/js/config/config-default.ts +++ b/frontend/amundsen_application/static/js/config/config-default.ts tableLineage: { - inAppListEnabled: false, - inAppPageEnabled: false, + inAppListEnabled: true, + inAppPageEnabled: true, externalEnabled: false, iconPath: 'PATH_TO_ICON', isBeta: false, Now let’s run again build for docker image, where the frontend image will be rebuilt. docker-compose -f docker-amundsen-nebula.yml build Then, rerun the up -d to ensure frontend container to be recreated with new configuration: docker-compose -f docker-amundsen-nebula.yml up -d We could see something like this: $ docker-compose -f docker-amundsen-nebula.yml up -d ... Recreating amundsenfrontend ... done After that, we could visit http://localhost:5000/lineage/table/gold/hive/test_schema/test_table1 to see the Lineage is shown as: We could click Downstream(if there is) to see downstream resources of this table: Or click Lineage to see the graph: There are API for lineage query, too. Here is an example to query that with cURL, where we leverage the netshoot container as we did before for user creation. docker run -it --rm --net container:amundsenfrontend nicolaka/netshoot curl \"http://amundsenmetadata:5002/table/snowflake://dbt_demo.public/raw_inventory_value/lineage?depth=3\u0026direction=both\" The above API call was to query linage on both upstream and downstream direction, with depth 3 for table snowflake://dbt_demo.public/raw_inventory_value. And the result should be like: { \"depth\": 3, \"downstream_entities\": [ { \"level\": 2, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_daily_expenses\", \"parent\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"badges\": [], \"source\": \"snowflake\" }, { \"level\": 1, \"usage\": 0, \"key\": \"snowflake://dbt_demo.public/fact_warehouse_inventory\", \"parent\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"badges\": [], \"source\": \"snowflake\" } ], \"key\": \"snowflake://dbt_demo.public/raw_inventory_value\", \"direction\": \"both\", \"upstream_entities\": [] } In fact, this lineage data was just extracted and loaded during our DbtExtractor execution, where extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE} by default was True, thus lineage metadata were created and loaded to Amundsen. 4.5.1 Get lineage in Nebula GraphTwo of the advantages to use a Graph Database as Metadata Storage are: The graph query itself is a flexible DSL for lineage API, for example, this query helps us do the equivalent query of the Amundsen metadata API for fetching lineage: MATCH p=(t:Table) -[:HAS_UPSTREAM|:HAS_DOWNSTREAM *1..3]-(x) WHERE id(t) == \"snowflake://dbt_demo.public/raw_inventory_value\" RETURN p We could now even query it in Nebula Graph Studio’s console, and click View Subgraphs to make it rendered in a graph view then. 4.5.2 Extract Data Lineage4.5.2.1 DbtAs mentioned above, DbtExtractor will extract table level lineage, together with other information defined in the dbt ETL pipeline. 4.5.2.2 Open LineageThe other linage extractor out-of-the-box in Amundsen is OpenLineageTableLineageExtractor. Open Lineage is an open framework to collect lineage data from different sources in one place, which can output linage information as JSON files to be extracted by OpenLineageTableLineageExtractor: dict_config = { # ... f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table', f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json', } ... task = DefaultTask( extractor=OpenLineageTableLineageExtractor(), loader=FsNebulaCSVLoader()) ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:4:5","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#open-lineage"},{"categories":["Nebula Graph","Amundsen"],"content":"5 RecapThe whole idea of Metadata Governance/Discovery is to: Put all components in the stack as Metadata Sources(from any DB or DW to dbt, Airflow, Openlineage, Superset, etc.) Run metadata ETL with Databuilder(as a script, or DAG) to store and index with Nebula Graph(or other Graph Database) and Elasticsearch Consume, manage, and discover metadata from Frontend UI(with Superset for preview) or API Have more possibilities, flexibility, and insights on Nebula Graph from queries and UI ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:0","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#recap"},{"categories":["Nebula Graph","Amundsen"],"content":"5.1 Upstream ProjectsAll projects used in this reference project are listed below in lexicographic order. Amundsen Apache Airflow Apache Superset dbt Elasticsearch meltano Nebula Graph Open Lineage singer Feature Image credit to Phil Hearing ","date":"2022-06-09","objectID":"/en/data-lineage-oss-ref-solution/:5:1","series":null,"tags":["Nebula Graph","Amundsen","Data Lineage","Metadata Governance"],"title":"A Data Lineage OSS Reference Solution","uri":"/en/data-lineage-oss-ref-solution/#upstream-projects"},{"categories":["Nebula Graph"],"content":"What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know.","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/"},{"categories":["Nebula Graph"],"content":" What could be done with Spark and PySpark on top of Nebula Graph, this post covers everything we should know. In this article, I am trying to walk you through all three Spark projects of Nebula Graph with some runnable hands-on examples. Also, I managed to make PySpark usable with Nebula Graph Spark Connector, which will be contributed to the Docs later. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:0:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#"},{"categories":["Nebula Graph"],"content":"1 The three Spark projects for Nebula GraphI used to draw a sketch around all data importing methods of Nebula Graph here, where all three of the Spark-based Nebula Graph projects were already briefly introduced. Instead, in this article, a slightly deeper dive into all of them will be made based on my recent work on them. TL;DR Nebula Spark Connector is a Spark Lib to enable spark application reading from and writing to Nebula Graph in form of a dataframe. Nebula Exchange, built on top of Nebula Spark Connector, is a Spark Lib and Application to exchange(for the Open Source version, it’s one way: write, whereas for the enterprise version it’s bidirectional) different data sources like(MySQL, Neo4j, PostgreSQL, Clickhouse, Hive, etc.). Besides writing directly to Nebula Graph, it could optionally generate SST files to be ingested into Nebula Graph to offload the storage computation outside of the Nebula Graph cluster. Nebula Algorithm, built on top of Nebula Spark Connector and GraphX, is a Spark Lib and Application to run de facto graph algorithms(PageRank, LPA, etc…) on a graph from Nebula Graph. Then let’s have the long version of those spark projects more on how-to perspectives. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:1:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#the-three-spark-projects-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"2 Spark-Connector Codebase: https://github.com/vesoft-inc/nebula-spark-connector Documentation: https://docs.nebula-graph.io/3.0.2/nebula-spark-connector/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/ Code Examples: example ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#spark-connector"},{"categories":["Nebula Graph"],"content":"2.1 Nebula Graph Spark ReaderTo read data from Nebula Graph, i.e. vertex, Nebula Spark Connector will scan all storage instances that hold the given label(TAG): withLabel(\"player\"), and we could optionally specify the properties of the vertex: withReturnCols(List(\"name\", \"age\")). With needed configuration being provided, a call of spark.read.nebula.loadVerticesToDF will return dataframe of the Vertex Scan call towards Nebula Graph: def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColumn(false) .withReturnCols(List(\"name\", \"age\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() vertex.printSchema() vertex.show(20) println(\"vertex count: \" + vertex.count()) } It’s similar for the writer part and one big difference here is the writing path is done via GraphD as the underlying Spark Connector is shooting nGQL INSERT queries. Then let’s do the hands-on end-to-end practice. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#nebula-graph-spark-reader"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColu","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-spark-connector"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColu","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#bootstrap-a-nebula-graph-cluster"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColu","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#create-a-spark-playground"},{"categories":["Nebula Graph"],"content":"2.2 Hands-on Spark ConnectorPrerequisites: it’s assumed below the procedure is being run on a Linux Machine with an internet connection, ideally with Docker and Docker-Compose preinstalled. 2.2.1 Bootstrap a Nebula Graph ClusterFirstly, let’s deploy Nebula Graph Core v3.0 and Nebula Studio with Nebula-Up, it will try to install Docker and Docker-Compose for us, in case it failed, please try to install Docker and Docker-Compose on your own first. curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0 After the above script is executed, let’s connect to it with Nebula-Console, the command line client for Nebula Graph. Enter the container with the console ~/.nebula-up/console.sh Connect to the Nebula Graph nebula-console -addr graphd -port 9669 -user root -p nebula Activate Storage Instances, and check the hosts status ref: https://docs.nebula-graph.io/3.0.2/4.deployment-and-installation/manage-storage-host/ ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; Load the test graph data, which will take one or two minutes to finish. :play basketballplayer; 2.2.2 Create a Spark playgroundThanks to Big data europe, it’s quite handly to do so: docker run --name spark-master-0 --network nebula-docker-compose_nebula-net \\ -h spark-master-0 -e ENABLE_INIT_DAEMON=false -d \\ -v ${PWD}/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 In the above one line command, we created a container named spark-master-0 with a built-in Hadoop 2.7 and spark 2.4.5, connected to the Nebula Graph cluster in its docker network named nebula-docker-compose_nebula-net, and it mapped the current path to /root of the spark container. Then, we could access the spark env container with: docker exec -it spark-master-0 bash Optionally, we could install mvn inside the container: docker exec -it spark-master-0 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 Run spark connector exampleLet’s clone the connector and the example code base, and build(or place the connector Jar package) the connector: git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark-master-0 bash cd /root/nebula-spark-connector /usr/lib/mvn/bin/mvn install -Dgpg.skip -Dmaven.javadoc.skip=true -Dmaven.test.skip=true Then we replace the example code: vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala We put the code as the following, where two functions readVertex and readEdges was created on the basketballplayer graph space: package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TCompactProtocol])) val spark = SparkSession .builder() .master(\"local\") .config(sparkConf) .getOrCreate() readVertex(spark) readEdges(spark) spark.close() sys.exit() } def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColu","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#run-spark-connector-example"},{"categories":["Nebula Graph"],"content":"3 Exchange Codebase: https://github.com/vesoft-inc/nebula-exchange/ Documentation: https://docs.nebula-graph.io/3.0.2/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://github.com/vesoft-inc/nebula-exchange/releases Configuration Examples: exchange-common/src/test/resources/application.conf Nebula Exchange is a Spark Lib/App to read data from multiple sources, then, write to either Nebula Graph directly or into Nebula Graph SST Files. The way to leverage Nebula Exchange is only to firstly create the configuration files to let the exchange know how data should be fetched and written, then call the exchange package with the conf file specified. Now let’s do a hands-on test with the same environment created in the last chapter. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#exchange"},{"categories":["Nebula Graph"],"content":"3.1 Hands-on ExchangeHere, we are using Exchange to consume data source from a CSV file, where the first column is Vertex ID, and the second, and third to be properties of “name” and “age”. player800,\"Foo Bar\",23 player801,\"Another Name\",21 Let’s get into the spark container created in the last chapter, and download the Jar package of Nebula Exchange: docker exec -it spark-master bash cd /root/ wget https://github.com/vesoft-inc/nebula-exchange/releases/download/v3.0.0/nebula-exchange_spark_2.4-3.0.0.jar Create a conf file named exchange.conf in format HOCON, where: under .nebula, information regarding Nebula Graph Cluster was configured under .tags, information regarding Vertices like how required fields are reflected our data source(here it’s CSV file) was configured {#Sparkrelationconfigspark:{app:{name:NebulaExchange}master:localdriver:{cores:1maxResultSize:1G}executor:{memory:1G}cores:{max:16}}#NebulaGraphrelationconfignebula:{address:{graph:[\"graphd:9669\"]meta:[\"metad0:9559\",\"metad1:9559\",\"metad2:9559\"]}user:rootpswd:nebulaspace:basketballplayer#parametersforSSTimport,notrequiredpath:{local:\"/tmp\"remote:\"/sst\"hdfs.namenode:\"hdfs://localhost:9000\" }#nebulaclientconnectionparametersconnection{#socketconnect\u0026executetimeout,unit:millisecondtimeout:30000}error:{#maxnumberoffailures,ifthenumberoffailuresisbiggerthanmax,thenexittheapplication.max:32#failedimportjobwillberecordedinoutputpathoutput:/tmp/errors}#usegoogle'sRateLimitertolimittherequestssendtoNebulaGraphrate:{#thestablethroughputofRateLimiterlimit:1024#AcquiresapermitfromRateLimiter,unit:MILLISECONDS#ifitcan'tbeobtainedwithinthespecifiedtimeout,thengiveuptherequest.timeout:1000}}#Processingtags#TherearetagconfigexamplesfordifferentdataSources.tags:[#HDFSCSV#Importmodeisclient,justchangetype.sinktosstifyouwanttouseclientimportmode.{name:playertype:{source:csvsink:client}path:\"file:///root/player.csv\" #ifyourcsvfilehasnoheader,thenuse_c0,_c1,_c2,..toindicatefieldsfields:[_c1,_c2]nebula.fields:[name,age]vertex:{field:_c0}separator:\",\"header:falsebatch:256partition:32}]} Finally, let’s create player.csv and exchange.conf, it should be listed as the following: # ls -l -rw-r--r-- 1 root root 1912 Apr 19 08:21 exchange.conf -rw-r--r-- 1 root root 157814140 Apr 19 08:17 nebula-exchange_spark_2.4-3.0.0.jar -rw-r--r-- 1 root root 52 Apr 19 08:06 player.csv And we could call the exchange as: /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange_spark_2.4-3.0.0.jar \\ -c exchange.conf And the result should be like ... 22/04/19 08:22:08 INFO Exchange$: import for tag player cost time: 1.32 s 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/04/19 08:22:08 INFO Exchange$: Client-Import: batchFailure.player: 0 ... Please refer to the documentation and conf examples for more data sources. For hands-on Exchange writing to SST Files, you could refer to both Documentation and Nebula Exchange SST 2.x Hands-on Guide. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#hands-on-exchange"},{"categories":["Nebula Graph"],"content":"4 Algorithm Codebase: https://github.com/vesoft-inc/nebula-algorithm Documentation: https://docs.nebula-graph.io/3.0.2/nebula-algorithm/ (it’s versioned, as for now, I put the latest released version 3.0.2 here) Jar Package: https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ Code Examples: example/src/main/scala/com/vesoft/nebula/algorithm ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#algorithm"},{"categories":["Nebula Graph"],"content":"4.1 Calling with spark-submitWhen we call Nebula Algorithm with spark-submit, on how to use perspective, it is quite similar to Exchange. This post already showed us how to do that in action. ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-with-spark-submit"},{"categories":["Nebula Graph"],"content":"4.2 Calling as a lib in codeOn the other hand, we could call Nebula Algorithm in spark as a Spark Lib, the gain will be: More control/customization on the output format of the algorithm Possible to perform algorithm for non-numerical vertex ID cases, see here ","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:4:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#calling-as-a-lib-in-code"},{"categories":["Nebula Graph"],"content":"5 PySpark for Nebula GraphPySpark comes with the capability to call java/scala packages inside python, thus it’s also quite easy to use Spark Connector with Python. Here I am doing this from the pyspark entrypoint in /spark/bin/pyspark, with the connector’s Jar package specified with --driver-class-path and --jars docker exec -it spark-master-0 bash cd root wget https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/3.0.0/nebula-spark-connector-3.0.0.jar /spark/bin/pyspark --driver-class-path nebula-spark-connector-3.0.0.jar --jars nebula-spark-connector-3.0.0.jar Then, rather than pass NebulaConnectionConfig and ReadNebulaConfig to spark.read.nebula, we should instead call spark.read.format(\"com.vesoft.nebula.connector.NebulaDataSource\"). Voilà! df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows Below are how I figured out how to make this work with almost zero scala knowledge:-P. How reader should be called def loadVerticesToDF(): DataFrame = { assert(connectionConfig != null \u0026\u0026 readConfig != null, \"nebula config is not set, please call nebula() before loadVerticesToDF\") val dfReader = reader .format(classOf[NebulaDataSource].getName) .option(NebulaOptions.TYPE, DataTypeEnum.VERTEX.toString) .option(NebulaOptions.SPACE_NAME, readConfig.getSpace) .option(NebulaOptions.LABEL, readConfig.getLabel) .option(NebulaOptions.PARTITION_NUMBER, readConfig.getPartitionNum) .option(NebulaOptions.RETURN_COLS, readConfig.getReturnCols.mkString(\",\")) .option(NebulaOptions.NO_COLUMN, readConfig.getNoColumn) .option(NebulaOptions.LIMIT, readConfig.getLimit) .option(NebulaOptions.META_ADDRESS, connectionConfig.getMetaAddress) .option(NebulaOptions.TIMEOUT, connectionConfig.getTimeout) .option(NebulaOptions.CONNECTION_RETRY, connectionConfig.getConnectionRetry) .option(NebulaOptions.EXECUTION_RETRY, connectionConfig.getExecRetry) .option(NebulaOptions.ENABLE_META_SSL, connectionConfig.getEnableMetaSSL) .option(NebulaOptions.ENABLE_STORAGE_SSL, connectionConfig.getEnableStorageSSL) if (connectionConfig.getEnableStorageSSL || connectionConfig.getEnableMetaSSL) { dfReader.option(NebulaOptions.SSL_SIGN_TYPE, connectionConfig.getSignType) SSLSignType.withName(connectionConfig.getSignType) match { case SSLSignType.CA =\u003e dfReader.option(NebulaOptions.CA_SIGN_PARAM, connectionConfig.getCaSignParam) case SSLSignType.SELF =\u003e dfReader.option(NebulaOptions.SELF_SIGN_PARAM, connectionConfig.getSelfSignParam) } } dfReader.load() } How Option String should be like # object NebulaOptions { /** nebula common config */ val SPACE_NAME: String = \"spaceName\" val META_ADDRESS: String = \"metaAddress\" val GRAPH_ADDRESS: String = \"graphAddress\" val TYPE: String = \"type\" val LABEL: String = \"label\" /** connection config */ val TIMEOUT: String = \"timeout\" val CONNECTION_RETRY: String = \"connectionRetry\" val EXECUTION_RETRY: String = \"executionRetry\" val RATE_TIME_OUT: String = \"reteTimeOut\" val USER_NAME: String = \"user\" val PASSWD: String = \"passwd\" val ENABLE_GRAPH_SSL: String = \"enableGraphSSL\" val ENABLE_META_SSL: String = \"enableMetaSSL\" val ENABLE_STORAGE_SSL: String = \"enableStorageSSL\" val SSL_SIGN_TYPE: String = \"sslSignType\" val CA_SIGN_PARAM: String = \"caSignParam\" val SELF_SIGN_PARAM: String = \"selfSignParam\" /** read config */ val RETURN_COLS: String = \"returnCols\" val NO_COLUMN: String = \"noColumn\" val PARTITION_NUMBER: String = \"partitionNumber\" val LIMIT: String = \"limit\" /** write config */ val RATE_LIMIT: String = \"rateLimit\" val VID_POLICY: String = \"vidPolicy\" val SRC_POLICY: String = \"srcPolicy\" val DST_POLIC","date":"2022-04-19","objectID":"/en/spark-on-nebula-graph/:5:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"Spark on Nebula Graph","uri":"/en/spark-on-nebula-graph/#pyspark-for-nebula-graph"},{"categories":["Nebula Graph"],"content":"Running Nebula Graph Database on ARM64 Single Board Computer/Respberry Pi","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/"},{"categories":["Nebula Graph"],"content":" With the ARM64 Docker Image of Nebula Graph, it’s actually quite easy to run it on SBC/Respberry Pi! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:0:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#"},{"categories":["Nebula Graph"],"content":"1 BackgroundRecently, after Yee from Nebula Graph Community fixed ARM build on nebula-third-party#37, we could play with Nebula Graph on M1 Chip Macbook. While, I didn’t get the chance to run it on a SBC/Pi. A couple of weeks before, in a twitter thread with @laixintao and @andelf I decided to purchase a Rock Pi 3A: And it looks nice!(Even come with a NPU inside) ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:1:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#background"},{"categories":["Nebula Graph"],"content":"2 The Guide of running Nebula Graph on a Pi SBC Actually, since v3.0.0, Nebula comes with a standalone version, which suits the deep edge deployment more, but today, I will only setup the cluster version as the Docker Image is out of box to be used. I will share more on standalone version in upcoming weeks. I put the Ubuntu Server installation steps in the appendix, and now I assumed we already have an ARM64 Linux up and running on a Pi SBC. ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#the-guide-of-running-nebula-graph-on-a-pi-sbc"},{"categories":["Nebula Graph"],"content":"2.1 Step 0, Install Docker-Compose on PiI am using debian/ubuntu here, while it should be the same for other distros, referring to https://docs.docker.com/engine/install/. sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release echo \\ \"deb [arch=$(dpkg --print-architecture)signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # follow https://docs.docker.com/engine/install/linux-postinstall/ sudo groupadd docker sudo usermod -aG docker $USER exit # login again newgrp docker After Docker being installed, we install compose here, there could be issues encounterred from the Docker website on Compose installation. While, due to compose is just a python package, let’s do it via python3-pip install: sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:1","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-0-install-docker-compose-on-pi"},{"categories":["Nebula Graph"],"content":"2.2 Step 1, bootstrap Nebula Graph Cluster on PiWe clone the compose file for nebula cluster first: git clone https://github.com/vesoft-inc/nebula-docker-compose.git \u0026\u0026 cd nebula-docker-compose docker-compose up -d Then, let’s download the client: nebula-console, and connect to the GraphD service: wget https://github.com/vesoft-inc/nebula-console/releases/download/v3.0.0/nebula-console-linux-arm64-v3.0.0 chmod +x nebula-console-linux-arm64-v3.0.0 ./nebula-console-linux-arm64-v3.0.0 -addr localhost -port 9669 -u root -p nebula Activate the storageD services: ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:2","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-1-bootstrap-nebula-graph-cluster-on-pi"},{"categories":["Nebula Graph"],"content":"2.3 Step 2, Play Nebula Graph on PiWIth the SHOW HOSTS we should see StorageD services are all ONLINE, then we could run this from the console session to load the test dataset. Referennce: https://docs.nebula-graph.io/3.0.1/nebula-console/#import_a_testing_dataset $:play basketballplayer; The test data will be loaded in around 1 minute. Then, we could query something like: USE basketballplayer; GO FROM \"player100\" OVER follow YIELD dst(edge); Check this out and… Happy Graphing! ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:2:3","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#step-2-play-nebula-graph-on-pi"},{"categories":["Nebula Graph"],"content":"3 Appendix: Installing Ubuntu Server on Rock Pi 3A SBC Get the image from https://wiki.radxa.com/Rock3/downloads decompressing the file into .img Write the image to a micro SD card with etcher Boot it! feature image credit: @_louisreed ","date":"2022-03-23","objectID":"/en/nebula-graph-on-pi/:3:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/en/nebula-graph-on-pi/#appendix-installing-ubuntu-server-on-rock-pi-3a-sbc"},{"categories":["Nebula Graph"],"content":"Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph?","date":"2022-02-28","objectID":"/en/resolve-wordle/","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/"},{"categories":["Nebula Graph"],"content":" Could I create something between the human brain and the game-cheater/ruiner to make it more of fun? With Knowledge Graph? ","date":"2022-02-28","objectID":"/en/resolve-wordle/:0:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#"},{"categories":["Nebula Graph"],"content":"1 BackgroundYou may have seen tweets like this in past months, where the color dots in emoji was shared in SNS randomly. Feel free to Google Wordle first if you don’t know its meaning yet. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#background"},{"categories":["Nebula Graph"],"content":"1.1 Wordle SolverFor all magics being used to solve wordle, I am impressed by Grant Sanderson, who explained us the information theory when solving wordle, in an elegent and delightful way. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#wordle-solver"},{"categories":["Nebula Graph"],"content":"1.2 Chinese wordle: “handle”I am not going create yet another wordle-solver today, instead, it’s more about an intresting variant of wordle. To truly enjoy the fun of wordle, mostly we should be a native speaker, and it is not surprising that there is a Spanish wordle out there, and still tweets on wordle(es) are being shared literially every second now. While for non alphabetic languages like Chineses, do we have the luck to have fun with wordling? The answer is yes, while it’s a bit different. For the reason Chinese charactors, also called Hanzi or Han chactors are in from of Logogram, each charactor in Chinese is made up of radicals in quite different forms, each Chinese word can be 2/3/4 charactors. Most of the crossword games in Chinese are idiom, proverb or ancient poetry based: the slot to be filled are one Chinese. ref: Chinese idiom puzzle Thus, a wordle but in Chinese idiom will be quite strange, or even hilarious as Chee and JinGen discussed in this tweet thread, where you could see the candidate characters are counted in thousands: there is no chance to guess the idiom in 10 attempts! As one of the fun on playing wordle could be the feedback loop of guess --\u003e result in limited attempts, while the scale of Chinese Charactor had pushed the Chinese wordle variant creators leveraging other aspects of the charactor: pronunciation. Each charactor in Chinease is monosyllabic without exceptions, and when it comes to its pronunciation, they are all made up from two parts(initial and final, and they could be written in roman letters), which comes in tens level of possbilities. There are bounch of Chinese wordle varients asking player to guess idiom leveraging pinyin: https://pinyincaichengyu.com/ https://cheeaun.github.io/chengyu-wordle/ https://apps.apple.com/cn/app/id1606194420 While, to me, a native Chinese speaker, it’s either too hard to play with condtions of pronunciation parts(pinyin) or too easy to guess on given around 20 Chinese charactors. Then, the varient stands out here is the “handle/汉兜\"(Hanzi-Wordle) created by Antfu. “Handle” introduced the tones with genius to add an extra dimension of all charactors per each guess attempt, which helped player to have more information on filtering the knowledge in the brain. Note, for each Chinese charactor, there will be a tone in 1 of 4 tones in its pronunciation. Let’s see what it’s like to play the “Handle”: There will be 4 Chinese Charactors to be filled in 10 times of guess Not only the charactor self will be colored in result: For example in first line, the green “门” in position 2 is correct whereas in second line, the orange “仓” is corret while the possition should be all but not the first slot. There will be extra hints on: Pinyin parts for both part1(initial) and part2(final) In third line of the boxes, the green “qiao” refers to the first charactor is ponouced in “qiao” with initial:“q” and final:“iao”, although we filled the wrong charactor in the writing dimension. In third line, the orange “uo” refers to there is one chacarctor in other poisition with the final part of the pinyin as “uo”. Tones of the charactor: In third line, the green “-” stands for the third charactor is in tone-1. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-wordle-handle"},{"categories":["Nebula Graph"],"content":"1.3 The Chinese Wordle HelperAs a non-native English speaker, the way I was playing wordle is to cheating relying on helpers: After knowing on the third letter to be “O”, I googled and got this: 5-letter-words-with-o-in-the-middle and do the searching 😁. The way to play with helpers works for me to have fun yet not ruin it by an automated cheat resolver(it’s only simulating my brain as a native-speaker!), so that I could somehow experience the same as Millions of people out there without cheating. While for Chinese “Handle” players, from my perspective, it’s still a bit harder(to find answers in 10 guesses), and the way my wife and I were playing “Handle” when lining up at the restaurant door ended up googling: idiom list with word ‘foo’, yet still having a lot of fun. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:1:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-chinese-wordle-helper"},{"categories":["Nebula Graph"],"content":"2 Chinese idiom Knowledge GraphCould I create something between the human brain and the game-cheater/ruiner to make it more of fun? The answer is yes: a game extension as a secondary brain. For this helper/secondary brain, the solution for “handle” differenciates from the English wordle, unlike the auto-solver, similar algorithms could help on both cases: In wordle(English), player searches in their brain or from a helper like the web page: 5-letter-words-with-o-in-the-middle. In handle(Chinese), it’s harder to be searching based on hints like tones/initial parts of pinyin in fulltext webpage searching anymore, the reason hehind is that the multidimensional filter condtions are not indexed by normal webpages. As I mentioned, the key of the helper to be leveraged to (not ruining the game) is to be the extension of the brain, then the question is: how does our brain work on handling the knowledge of “handle”(yes, I was preparing for this pun for so long!)? Thus, why not do it in a graph/neural network way? And here we go, let’s create a knowledge graph of Chinese idiom and see how it goes with the “handle” game. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#chinese-idiom-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.1 TL;DRIt’s indeed entertaining to me, and I could write Graph Queries[*] by hand or via Visualization tools[**] to help explore things in this graph, because I can we’re doing the “thinking” process the similar way in our own brain, but not so well-informed. # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC ** ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#tldr"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-long-version-of-playing-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#play-handle-with-knowledge-graph"},{"categories":["Nebula Graph"],"content":"2.2 The long version of playing with Knowledge GraphIf you are intrested in how you could do it from your side, here are some details. Before getting started, if you would like to hands-on do it, please refer to appendix chapter on getting your own Nebula Graph Database cluster ready, which are a couple of lines commands to be run. 2.2.1 Play Handle with Knowledge GraphSo, if we visit the “handle” game 👉🏻 https://handle.antfu.me/. We could start with one guess i.e. “爱憎分明”. We will get hint of the first guess as: Not bad, we have three of the charactor with informative hints! There is one Character not in 1st position, with tone-4, final part as “ai”, but is not “爱” There is one Character in tone-1 not in 2nd position There is one Character with final part as “ing”, not in 4th position The 4th Character is tone-2 Then we just query it from Nebula Graph: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH (char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH (x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH (x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH (x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN x, count(x) as c ORDER BY c DESC And there are 7 results left in this single guess for us! (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) And as a Chinese native speaker, the common one would be “惊世骇俗”, and let’s give a try! 2.2.2 The Visualization of the QueryWe could modify our query to make every part of the subquery returned, thus, they could also be renderred visually: # There is one Character not in 1st position, with tone-4, final part as \"ai\", but is not \"爱\" MATCH p0=(char0:character)(pinyin_0:character_pinyin)-[:with_pinyin_part]-(final_part_0:pinyin_part{part_type: \"final\"}) WHERE id(final_part_0) == \"ai\" AND pinyin_0.character_pinyin.tone == 4 AND with_pinyin_0.position != 0 AND with_char_0.position != 0 AND id(char0) != \"爱\" # There is one Character in tone-1 not in 2nd position MATCH p1=(x:idiom) -[with_pinyin_1:with_pinyin]-(pinyin_1:character_pinyin) WHERE pinyin_1.character_pinyin.tone == 1 AND with_pinyin_1.position != 1 # There is one Character with final part as \"ing\", not in 4th position MATCH p2=(x:idiom) -[with_pinyin_2:with_pinyin]-(:character_pinyin)-[:with_pinyin_part]-(final_part_2:pinyin_part{part_type: \"final\"}) WHERE id(final_part_2) == \"ing\" AND with_pinyin_2.position != 3 # The 4th Character is tone-2 MATCH p3=(x:idiom) -[with_pinyin_3:with_pinyin]-(pinyin_3:character_pinyin) WHERE pinyin_3.character_pinyin.tone == 2 AND with_pinyin_3.position == 3 RETURN p0,p1,p2,p3 And then we query it in Nebula-Studio-Console, and click “View Subgraphs” to see how the searching was done like it was in our own brains: ","date":"2022-02-28","objectID":"/en/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#the-visualization-of-the-query"},{"categories":["Nebula Graph"],"content":"3 What’s NextIf you happened to get hands dirty(or being intrested to) on Graph Database, you could checkout the Nebula Graph project now and its Docs to have more fun of it! Also, here are some only playgrounds if you prefer to try it without deployment on your own envrioment. If you are intrested in the MATCH query syntax and would like to actually do exersices with each daily handle challenge, check below Documents: MATCH https://docs.nebula-graph.io/3.0.1/3.ngql-guide/7.general-query-statements/2.match/ Graph Patterns https://docs.nebula-graph.io/3.0.1/3.ngql-guide/1.nGQL-overview/3.graph-patterns/ nGQL command cheatsheet https://docs.nebula-graph.io/3.0.1/2.quick-start/6.cheatsheet-for-ngql/ Happy Graphing! ","date":"2022-02-28","objectID":"/en/resolve-wordle/:3:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#whats-next"},{"categories":["Nebula Graph"],"content":"4 Appendix: Setting up the Knowledge GraphI put the code and process here: https://github.com/wey-gu/chinese-graph, feel free to check that out. ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:0","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#appendix-setting-up-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.1 Build the Knowledge GraphThe process would be: Modeling the Knowledge Preprocessing the data ETL data to a Graph Database: Nebula Graph Have fun on Nebula Graph ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:1","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#build-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.2 Modeling the Knowledge GraphFor Graph Modeling, it’s actually quite straight forward, the mind model for me was to put the knowledge I cares as vertcies and connect them with their relationships first. You will come back to iterate or optimize the modeling when you are actually playing with the data afterwards, thus, if you could imagine how the graph will be queried in the first place, the graph modeling could be adopted accordingly. Otherwise, don’t over design it, just do it the intuitive way. Here, I put the vertices with properties as: idiom character pinyin tone pinyin_part type The edges with properteis as: with_character with_pinyin with_pinyin_part ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:2","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#modeling-the-knowledge-graph"},{"categories":["Nebula Graph"],"content":"4.3 Deploy Nebula Graph With Nebula-UP, it’s an onliner call curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:3","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#deploy-nebula-graph"},{"categories":["Nebula Graph"],"content":"4.4 Load data # clone the code for data genration and data loading git clone https://github.com/wey-gu/chinese-graph.git \u0026\u0026 cd chinese-graph python3 graph_data_generator.py # generate data # load data with Nebula-Importer docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml ","date":"2022-02-28","objectID":"/en/resolve-wordle/:4:4","series":null,"tags":["Nebula Graph","Knowledge Graph","wordle"],"title":"What happens to Wordle and Chinese idiom Knowledge Graph?","uri":"/en/resolve-wordle/#load-data"},{"categories":["Nebula Graph"],"content":"Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index?","date":"2022-02-20","objectID":"/en/nebula-index-explained/","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/"},{"categories":["Nebula Graph"],"content":" Nebula Graph Native Index explained, why index not found? When should I use Nebula Index and full-text index? The term of Nebula Graph Index is quite similar to the index in RDBMS, while, they are not the same. It’s noticed that when getting started with Nebula Graph, the index confused some of the users in first glance on the following What exactly Nebula Graph Index is. When I should use it. How it impacts the performance. Today I’m gonna walk you through the index in Nebula Graph. Let’s get started! ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:0:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#"},{"categories":["Nebula Graph"],"content":"1 What exactly Nebula Graph Index isTL;DR, Nebula Graph Index is only to be used to enable the graph query to be started from conditions on properties of vertices or edges, instead of vertexID. It’s only used in a starting entry of a graph query. If a query is in pattern: (a-\u003eb-\u003ec, where c in condition-foobar) graph walk, due to the only filtering condition-foobar is on c, this query under the hood will be started to seek c, and then it walks through the reversed -\u003e to b, finally to a. Thus, the Nebula Graph Index will be used and only be possbily used in seeking c, when condition-foobar is not like id(c) == \"foobar\" but c.property_x == \"foobar\". ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#what-exactly-nebula-graph-index-is"},{"categories":["Nebula Graph"],"content":"1.1 Index is used only for starting point seekWe know that in RDBMS, an INDEX is to create a duplicated sorted DATA to enable QUERY with condition filtering on the sorted data, to accelerate the query in read and involves extra writes during the write. Note: in RDBMS/Tabular DB, an INDEX on some columns means to create extra data that are sorted on those columns to make query with those columns’ condition to be scanned faster, rather than scanning from the original table data sorted based on the key only. In Nebula Graph, the INDEX is to create a duplicated sorted Vertex/Edge PROP DATA to enable starting point seek of a QUERY(it’s a prerequisite rather than help accelerate it). Not all of the queries relied on index, here are some examples, let’s call them pure-property-condition-start queries: #### Queries relying on Nebula Graph Index # query 0 pure-property-condition-start query LOOKUP ON tag1 WHERE col1 \u003e 1 AND col2 == \"foo\" \\ YIELD tag1.col1 as col1, tag1.col3 as col3; # query 1 pure-property-condition-start query MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name; In both query 0 and query 1, the pattern is to “Find VID/EDGE only based on given the propertiy condtions”. On the contrary, the starting point are VertexID based instead in query 2 and query 3: #### Queries not based on Nebula Graph Index # query 2, walk query starting from given vertex VID: \"player100\" GO FROM \"player100\" OVER follow REVERSELY \\ YIELD src(edge) AS id | \\ GO FROM $-.id OVER serve \\ WHERE properties($^).age \u003e 20 \\ YIELD properties($^).name AS FriendOf, properties($$).name AS Team; # query 3, walk query starting from given vertex VID: \"player101\" or \"player102\" MATCH (v:player { name: 'Tim Duncan' })--(v2) \\ WHERE id(v2) IN [\"player101\", \"player102\"] \\ RETURN v2.player.name AS Name; If we look into query 1 and query 3, which shared condition on vertex on tag:player are both { name: 'Tim Duncan' } though, they are differenciated in starting points: For query 3 , the index is not required as the query will be started from known vertex ID in [\"player101\", \"player102\"] and thus: It’ll directly fetch vertex Data from v2’s vertex IDs then to GetNeighbors(): walk through edges of v2, GetVertices() for next hop: v and filter based on property: name For query 1 , the query has to start from v due to no known vertex IDs were provided: It’ll do IndexScan() first to find all vertices only with property condtion of { name: 'Tim Duncan' } Then, GetNeighbors(): walk through edges of v, GetVertices() for next hop: v2 Now, we could know the whole point that matters here is on whether to know the vertexID. And the above differences could be shown in their execution plans with PROFILE or EXPLAIN like the follow: query 1, requires index(on tag: player), pure prop condition query as starting point query 3, no index required, query starting from known vertex IDs ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:1","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#index-is-used-only-for-starting-point-seek"},{"categories":["Nebula Graph"],"content":"1.2 Why Nebula Graph index is enabler rather than an acceleraterCan’t those queries be done without indexes? It’s possible in theory with full scan, but disabled without index. The reason is Nebula Graph stores data in a distributed and graph-oriented way, the full scan of data was condiserred too expensive to be allowed. Note: from v3.0, it’s possible to do TopN Scan without INDEX, where the LIMIT \u003cn\u003e is used, this is different from the fullscan case(INDEX is a must), which will be explained later. MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name LIMIT 3; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:2","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-nebula-graph-index-is-enabler-rather-than-an-accelerater"},{"categories":["Nebula Graph"],"content":"1.3 Why starting point onlyIndex data is not used in terversal. It could confuse use to think of index is to sorting data based on properties, does it accelerate the terversal with property condition filtering? The answer is, no. In Nebula Graph, the data is structured in a way to enable fast graph-terversal, which is already indexed/sorted on vertex ID(for both vertex and edge) in raw data, where terversal(underlying in storage, it’s calling GetNeighbors interface) of given vertex is cheap and fast due to the locality/stored continuously(pysically linked). So in summary: Nebula Graph Index is sorted prop data to find the starting vertex or edge on given pure prop conditions. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:1:3","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#why-starting-point-only"},{"categories":["Nebula Graph"],"content":"2 Facts on Nebula Graph IndexTo understand more details/limitations/cost of Nebula, let’s reveal more on its design and here are some facts in short: Index Data is stored and sharded together with Vertex Data It’s Left Match based only: It’s RocksDB Prefix Scan under the hood Effect on write and read path(to see its cost): Write Path: Extra Data written + Extra Read request introduced Read Path: RBO(Rule based optimization), Fan Out(to all shards) Data Full Scan LIMIT Sample(not full scan) is supported without Index LOOKUP ON t YIELD t.name | LIMIT 1 MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name LIMIT 3; The key info can be seen from one of my sketch notes: We should notice that only the left match is supported in pure-property-condition-start queries. For queries like wildcard or reguler-expression, Full-text Index/Search is to be used, where an external elastic search is integrated with nebula: please check Nebula Graph Full text index for more. Within this sketch note, more highlights are: It’s a Local Index Design The index is stored and shared locally together with the graph data. It’s sorting based on prop value, and the index search is underlying a rocksDB prefix scan, that’s why only left match is supported. There is cost in the write path The index enables the RDBMS-like Prop Condition Based Query with cost in the write path including not only the extra write, but also, random read, to ensure the data consistency. Index Data write is done in a sync way For Read path: In pure-property-condition-start queries, in GraphD, the index will be selected with Rule-based-optimization like this example, where, in a rule, the col2 to be sorted first is considered optimal with the condition: col2 equals ‘foo’. After the index was chosen, index-scan request will be fanout to storageD instances, and in the case of filters like LIMIT N, it will be pushed down to the storage side to reduce data payload. Note: it was not shown in the sketch but actually from v3.0, the nebula graph allows LIMIT N Sample Prop condition query like this w/o index, which is underlying pushing down the LIMIT filter to storage side. Take aways: Use index only when we have to, as it’s costly in write cases and if limit N sample is the only needed case and it’s fast enough, we can use that instead. Index is left match composite index order matters, should be created carefully. for full-text search use case, use full-text index instead. ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:2:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#facts-on-nebula-graph-index"},{"categories":["Nebula Graph"],"content":"3 How to use the indexWe should always refer to the documentation, and I just put some highlights on this here: To create an index on a tag or edge type to specify a list of props in the order that we need. CREATE INDEX If an index was created after existing data was inserted, we need to trigger an index async rebuild job, as the index data will be written in sync way only when index is created. REBUILD INDEX We can see the index status after REBUILD INDEX issued. SHOW INDEX STATUS Queries levering index could be LOOKUP, and with the pipeline, in most cases we will do follow-up graph-walk queries like: LOOKUPONplayer\\WHEREplayer.name==\"Kobe Bryant\"\\YIELDid(vertex)ASVertexID,properties(vertex).nameASname|\\GOFROM$-.VertexIDOVERserve\\YIELD$-.name,properties(edge).start_year,properties(edge).end_year,properties($$).name; Or in MATCH query like this, under the hood, v will be searched on index and v2 will be walked by default graph data structure without involving index. MATCH (v:player{name:\"Tim Duncan\"})--\u003e(v2:player) \\ RETURN v2.player.name AS Name; ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:3:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#how-to-use-the-index"},{"categories":["Nebula Graph"],"content":"4 RecapFinally, Let’s Recap INDEX is sorting PROP DATA to enable finding starting point on given PURE PROP CONDITION INDEX is not for Trevsal INDEX is left match, not for full-text search INDEX has cost on WRITE Remember to REBUILD after CREATE INDEX on existing data Happy Graphing! Feture image credit to Alina ","date":"2022-02-20","objectID":"/en/nebula-index-explained/:4:0","series":null,"tags":["Nebula Graph","index","search","graph database"],"title":"Nebula Index Explained","uri":"/en/nebula-index-explained/#recap"},{"categories":["Nebula Graph"],"content":"How to parse nebula graph data in an interactive way and what are the best practices?","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/"},{"categories":["Nebula Graph"],"content":" How to parse nebula graph data in an interactive way and what are the best practices? I will show you an easier way in this article 😁. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:0:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#"},{"categories":["Nebula Graph"],"content":"1 Prepare for the Java REPLThanks to https://github.com/albertlatacz/java-repl/ we could play with/debug this in an interactive way, and all we need is to leverage its docker image to have all the envrioment in a clean and quick way: docker pull albertlatacz/java-repl docker run --rm -it \\ --network=nebula-docker-compose_nebula-net \\ -v ~:/root \\ albertlatacz/java-repl \\ bash apt update -y \u0026\u0026 apt install ca-certificates -y wget https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz --no-check-certificate tar xzvf apache-maven-3.8.4-bin.tar.gz wget https://github.com/vesoft-inc/nebula-java/archive/refs/tags/v2.6.1.tar.gz tar xzvf v2.6.1.tar.gz cd nebula-java-2.6.1/ ../apache-maven-3.8.4/bin/mvn dependency:copy-dependencies ../apache-maven-3.8.4/bin/mvn -B package -Dmaven.test.skip=true java -jar ../javarepl/javarepl.jar Now, after executing java -jar ../javarepl/javarepl.jar we are in a Java Shell(REPL), this enable us to execute Java code in an interactive way without wasting time and patience in the slow path(code –\u003e build –\u003e execute –\u003e add print –\u003e build), isn’t that cool? Like this: root@a2e26ba62bb6:/javarepl/nebula-java-2.6.1# java -jar ../javarepl/javarepl.jar Welcome to JavaREPL version 428 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111) Type expression to evaluate, :help for more options or press tab to auto-complete. Connected to local instance at http://localhost:43707 java\u003e System.out.println(\"Hello, World!\"); Hello, World! java\u003e Now we are in the java REPL, let’s introduce all the class path needed and do the imports in one go: :cp /javarepl/nebula-java-2.6.1/client/target/client-2.6.1.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/fastjson-1.2.78.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-api-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-log4j12-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/commons-pool2-2.2.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/log4j-1.2.17.jar import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.vesoft.nebula.ErrorCode; import com.vesoft.nebula.client.graph.NebulaPoolConfig; import com.vesoft.nebula.client.graph.data.CASignedSSLParam; import com.vesoft.nebula.client.graph.data.HostAddress; import com.vesoft.nebula.client.graph.data.ResultSet; import com.vesoft.nebula.client.graph.data.SelfSignedSSLParam; import com.vesoft.nebula.client.graph.data.ValueWrapper; import com.vesoft.nebula.client.graph.net.NebulaPool; import com.vesoft.nebula.client.graph.net.Session; import java.io.UnsupportedEncodingException; import java.util.Arrays; import java.util.List; import java.util.concurrent.TimeUnit; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.*; And let’s connect it to the nebula graph, please replace your graphD IP and Port here, and execute them under the propmt string of java\u003e: NebulaPoolConfig nebulaPoolConfig = new NebulaPoolConfig(); nebulaPoolConfig.setMaxConnSize(10); List\u003cHostAddress\u003e addresses = Arrays.asList(new HostAddress(\"192.168.8.127\", 9669)); NebulaPool pool = new NebulaPool(); pool.init(addresses, nebulaPoolConfig); Session session = pool.getSession(\"root\", \"nebula\", false); ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:1:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#prepare-for-the-java-repl"},{"categories":["Nebula Graph"],"content":"2 The execute for ResultSetFirst let’s check what we can do with a simple query: ResultSet resp = session.execute(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); Now you could play with it: Reference: client/graph/data/ResultSet.java java\u003e resp.isSucceeded() java.lang.Boolean res9 = true java\u003e resp.rowsSize() java.lang.Integer res16 = 1 java\u003e rows = resp.getRows() java.util.ArrayList rows = [Row ( values : [ \u003cValue vVal:Vertex ( vid : \u003cValue sVal:70 6c 61 79 65 72 31 30 30\u003e, tags : [ Tag ( name : 70 6C 61 79 65 72, props : { [B@5264a468 : \u003cValue iVal:42\u003e [B@496b8e10 : \u003cValue sVal:54 69 6d 20 44 75 6e 63 61 6e\u003e } ) ] )\u003e ] )] java\u003e row0 = resp.rowValues(0) java.lang.Iterable\u003ccom.vesoft.nebula.client.graph.data.ValueWrapper\u003e res10 = ColumnName: [n], Values: [(\"player100\" :player {name: \"Tim Duncan\", age: 42})] Remember our item is actually a vertex? (root@nebula) [basketballplayer]\u003e match (n:player) WHERE n.name == \"Tim Duncan\" return n +----------------------------------------------------+ | n | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2116/44373 us) Let’s see what(methods) can be done towards a value? v = Class.forName(\"com.vesoft.nebula.Value\") v.getDeclaredMethods() We could tell it’s quite Primitive on what com.vesoft.nebula.Value provided, thus we should use the ValueWrapper(or use executeJson actually) instead. To get a row of the result via iteration(as its a java iterable), we just follow how the example looped the result: import java.util.ArrayList; import java.util.List; List\u003cValueWrapper\u003e wrappedValueList = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c resp.rowsSize(); i++) { ResultSet.Record record = resp.rowValues(i); for (ValueWrapper value : record.values()) { wrappedValueList.add(value); if (value.isLong()) { System.out.printf(\"%15s |\", value.asLong()); } if (value.isBoolean()) { System.out.printf(\"%15s |\", value.asBoolean()); } if (value.isDouble()) { System.out.printf(\"%15s |\", value.asDouble()); } if (value.isString()) { System.out.printf(\"%15s |\", value.asString()); } if (value.isTime()) { System.out.printf(\"%15s |\", value.asTime()); } if (value.isDate()) { System.out.printf(\"%15s |\", value.asDate()); } if (value.isDateTime()) { System.out.printf(\"%15s |\", value.asDateTime()); } if (value.isVertex()) { System.out.printf(\"%15s |\", value.asNode()); } if (value.isEdge()) { System.out.printf(\"%15s |\", value.asRelationship()); } if (value.isPath()) { System.out.printf(\"%15s |\", value.asPath()); } if (value.isList()) { System.out.printf(\"%15s |\", value.asList()); } if (value.isSet()) { System.out.printf(\"%15s |\", value.asSet()); } if (value.isMap()) { System.out.printf(\"%15s |\", value.asMap()); } } System.out.println(); } As shown in above, the result value/item could be in properties string/int etc… or in graph semantic vertex, edge, path, we should use correspond asXxxx methods: java\u003e v = wrappedValueList.get(0) com.vesoft.nebula.client.graph.data.ValueWrapper v = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e v.asNode() com.vesoft.nebula.client.graph.data.Node res16 = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e node = v.asNode() com.vesoft.nebula.client.graph.data.Node node = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) Btw, it’s also possible to play with it with reflections(we imported already): Of courese we could also check via client/graph/data/ResultSet.java java\u003e rClass=Class.forName(\"com.vesoft.nebula.client.graph.data.ResultSet\") java.lang.Class r = class com.vesoft.nebula.client.graph.data.ResultSet java\u003e rClass.getDeclaredMethods() java.lang.reflect.Method[] res20 = [public java.util.List com.vesoft.nebula.client.graph.data.ResultSet.getColumnNames(), public int com.vesoft.nebula.client.graph.data.ResultSet.rowsSize(), public com.vesoft.nebula.client.graph.data.ResultSet$Record com.vesoft.nebula.clien","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:2:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-execute-for-resultset"},{"categories":["Nebula Graph"],"content":"3 The executeJson methodSince 2.6, nebule finally supports json string response and we could do this: java\u003e String resp_json = session.executeJson(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); java.lang.String resp_json = \" { \"errors\":[ { \"code\":0 } ], \"results\":[ { \"spaceName\":\"basketballplayer\", \"data\":[ { \"meta\":[ { \"type\":\"vertex\", \"id\":\"player100\" } ], \"row\":[ { \"player.age\":42, \"player.name\":\"Tim Duncan\" } ] } ], \"columns\":[ \"n\" ], \"errors\":{ \"code\":0 }, \"latencyInUs\":4761 } ] } \" java\u003e And I believe you know much better than I do with it. ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:3:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#the-executejson-method"},{"categories":["Nebula Graph"],"content":"4 Conclusion If we go with JSON response, it’ll be easier for you to have everything parsed. If we have to deal with resultSet object, just use the ValueWrapper asNode() if the value is a vertex, use asRelationship if value is an edge and asPath() if the value is a path. With the REPL tool shown together with java reflection and source code, it’s possbile to inspect on how data could be parsed. Happy Graphing! Picture Credit：leunesmedia ","date":"2021-11-25","objectID":"/en/nebula-java-happy-parsing-guide/:4:0","series":null,"tags":["Nebula Graph","Graph Database","java"],"title":"Nebula Java Happy Parsing Guide","uri":"/en/nebula-java-happy-parsing-guide/#conclusion"},{"categories":["Nebula Graph"],"content":"Dialog System With Graph Database Backed Knowledge Graph. 基于图数据库的智能问答助手","date":"2021-09-18","objectID":"/en/nebula-siwi/","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/"},{"categories":["Nebula Graph"],"content":" a PoC of Dialog System With Graph Database Backed Knowledge Graph. Related GitHub Repo: https://github.com/wey-gu/nebula-siwi/ I created the Katacoda Interactive Env for this project 👉🏻 https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#"},{"categories":["Nebula Graph"],"content":"Siwi the voice assistantSiwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. For now, it’s a demo for task-driven(not general purpose) dialog bots with KG(Knowledge Graph) leveraging Nebula Graph with the minimal/sample dataset from Nebula Graph Manual/ NG中文手册. Tips: Now you can play with the graph online without installing yourself! Nebula Playground | Nebula Playground - China Mainland Supported queries: relation: What is the relationship between Yao Ming and Lakers? How does Yao Ming and Lakers connected? serving: Which team had Yao Ming served? friendship: Whom does Tim Duncan follow? Who are Yao Ming’s friends? ","date":"2021-09-18","objectID":"/en/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#siwi-the-voice-assistant"},{"categories":["Nebula Graph"],"content":"1 Deploy and TryTBD (leveraging docker and nebula-up) ","date":"2021-09-18","objectID":"/en/nebula-siwi/:1:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#deploy-and-try"},{"categories":["Nebula Graph"],"content":"2 How does it work?This is one of the most naive pipeline for a specific domain/ single purpose chat bot built on a Knowledge Graph. ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#how-does-it-work"},{"categories":["Nebula Graph"],"content":"2.1 BackendThe Backend(Siwi API) is a Flask based API server: Flask API server takes questions in HTTP POST, and calls the bot API. In bot API part there are classfier(Symentic Parsing, Intent Matching, Slot Filling), and question actors(Call corresponding actions to query Knowledge Graph with intents and slots). Knowledge Graph is built on an Open-Source Graph Database: Nebula Graph ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend"},{"categories":["Nebula Graph"],"content":"2.2 FrontendThe Frontend is a VueJS Single Page Applicaiton(SPA): I reused a Vue Bot UI to showcase a chat window in this human-agent interaction, typing is supported. In addtion, leverating Chrome’s Web Speech API, a button to listen to human voice is introduced ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend"},{"categories":["Nebula Graph"],"content":"2.3 A Query Flow ┌────────────────┬──────────────────────────────────────┐ │ │ │ │ │ Speech │ │ ┌──────────▼──────────┐ │ │ │ Frontend │ Siwi, /ˈsɪwi/ │ │ │ Web_Speech_API │ A PoC of │ │ │ │ Dialog System │ │ │ Vue.JS │ With Graph Database │ │ │ │ Backed Knowledge Graph │ │ └──────────┬──────────┘ │ │ │ Sentence │ │ │ │ │ ┌────────────┼──────────────────────────────┐ │ │ │ │ │ │ │ │ │ Backend │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Web API, Flask │ ./app/ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Sentence ./bot/ │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent matching, │ ./bot/classifier│ │ │ │ │ Symentic Processing │ │ │ │ │ │ │ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Intent, Entities │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent Actor │ ./bot/actions │ │ │ │ │ │ │ │ │ └─┴──────────┬──────────┴───────────────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ Graph Database │ Nebula Graph │ │ │ │ │ │ └─────────────────────┘ │ │ │ │ │ │ │ └───────────────────────────────────────────────────────┘ ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:3","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#a-query-flow"},{"categories":["Nebula Graph"],"content":"2.4 Source Code Tree . ├── README.md ├── src │ ├── siwi # Siwi-API Backend │ │ ├── app # Web Server, take HTTP requests and calls Bot API │ │ └── bot # Bot API │ │ ├── actions # Take Intent, Slots, Query Knowledge Graph here │ │ ├── bot # Entrypoint of the Bot API │ │ ├── classifier # Symentic Parsing, Intent Matching, Slot Filling │ │ └── test # Example Data Source as equivalent/mocked module │ └── siwi_frontend # Browser End │ ├── README.md │ ├── package.json │ └── src │ ├── App.vue # Listening to user and pass Questions to Siwi-API │ └── main.js └── wsgi.py ","date":"2021-09-18","objectID":"/en/nebula-siwi/:2:4","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#source-code-tree"},{"categories":["Nebula Graph"],"content":"3 Manually Run Components","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#manually-run-components"},{"categories":["Nebula Graph"],"content":"3.1 BackendInstall and run. # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 For OpenFunction/ KNative docker build -t weygu/siwi-api . docker run --rm --name siwi-api \\ --env=PORT=5000 \\ --env=NG_ENDPOINTS=127.0.0.1:9669 \\ --net=host \\ weygu/siwi-api Try it out Web API: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"question\": \"What is the relationship between Yao Ming and Lakers?\"}' \\ http://192.168.8.128:5000/query | jq { \"answer\": \"There are at least 23 relations between Yao Ming and Lakers, one relation path is: Yao Ming follows Shaquille O'Neal serves Lakers.\" } Call Bot Python API: from nebula2.gclient.net import ConnectionPool from nebula2.Config import Config # define a config config = Config() config.max_connection_pool_size = 10 # init connection pool connection_pool = ConnectionPool() # if the given servers are ok, return true, else return false ok = connection_pool.init([('127.0.0.1', 9669)], config) # import siwi bot from siwi.bot import bot # instantiate a bot b = bot.SiwiBot(connection_pool) # make the question query b.query(\"Which team had Jonathon Simmons served?\") Then a response will be like this: In [4]: b.query(\"Which team had Jonathon Simmons serv ...: ed?\") [DEBUG] ServeAction intent: {'entities': {'Jonathon Simmons': 'player'}, 'intents': ('serve',)} [DEBUG] query for RelationshipAction: USE basketballplayer; MATCH p=(v)-[e:serve*1]-\u003e(v1) WHERE id(v) == \"player112\" RETURN p LIMIT 100; [2021-07-02 02:59:36,392]:Get connection to ('127.0.0.1', 9669) Out[4]: 'Jonathon Simmons had served 3 teams. Spurs from 2015 to 2015; 76ers from 2019 to 2019; Magic from 2017 to 2017; ' ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-1"},{"categories":["Nebula Graph"],"content":"3.2 FrontendReferring to siwi_frontend ","date":"2021-09-18","objectID":"/en/nebula-siwi/:3:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-1"},{"categories":["Nebula Graph"],"content":"4 Further work Use NBA-API to fallback undefined pattern questions Wrap and manage sessions instead of get and release session per request, this is somehow costly actually. Use NLP methods to implement proper Symentic Parsing, Intent Matching, Slot Filling Build Graph to help with Intent Matching, especially for a general purpose bot Use larger Dataset i.e. from wyattowalsh/basketball ","date":"2021-09-18","objectID":"/en/nebula-siwi/:4:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#further-work"},{"categories":["Nebula Graph"],"content":"5 Thanks to Upstream Projects ❤️","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"5.1 Backend I learnt a lot from the KGQA on MedicalKG created by Huanyong Liu Flask pyahocorasick created by Wojciech Muła PyYaml ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#backend-2"},{"categories":["Nebula Graph"],"content":"5.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design Image credit goes to https://unsplash.com/photos/0E_vhMVqL9g ","date":"2021-09-18","objectID":"/en/nebula-siwi/:5:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi: A Dialog System With Graph Database Backed Knowledge Graph","uri":"/en/nebula-siwi/#frontend-2"},{"categories":["Nebula Graph"],"content":"Setup Nebula Graph Dev Env with CLion and Docker 搭建基于 Docker 的 Nebula Graph CLion 开发环境","date":"2021-09-18","objectID":"/en/nebula-clion/","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/"},{"categories":["Nebula Graph"],"content":" 之前卡比同学向我咨询搭建 CLion 环境，开发 Nebula 的一些问题，我做了一些工作方便利用 Docker 在本地搭建这样一个环境，相关的东西放在：https://github.com/wey-gu/nebula-dev-CLion 。 Related GitHub Repo: https://github.com/wey-gu/nebula-dev-CLion ","date":"2021-09-18","objectID":"/en/nebula-clion/:0:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#"},{"categories":["Nebula Graph"],"content":"1 Run Docker Env for Nebula-Graph with CLionBuild Docker Image git clone https://github.com/wey-gu/nebula-dev-CLion.git cd nebula-dev-CLion docker build -t wey/nebula-dev-clion:v2.0 . Run Docker Container for Nebula-Dev with CLion Integration Readiness(actually mostly Rsync \u0026 SSH). cd \u003cnebula-graph-repo-you-worked-on\u003e export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker run --rm -d \\ --name nebula-dev \\ --security-opt seccomp=unconfined \\ -p 2222:22 -p 2873:873 --cap-add=ALL \\ -v $PWD:/home/nebula \\ -w /home/nebula \\ wey/nebula-dev-clion:v2.0 Verify cmake with SSH. The default password is password ssh -o StrictHostKeyChecking=no root@localhost -p 2222 # in docker cd /home/nebula mkdir build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. Access container w/o SSH. docker exec -it nebula-dev bash mkdir -p build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. ","date":"2021-09-18","objectID":"/en/nebula-clion/:1:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#run-docker-env-for-nebula-graph-with-clion"},{"categories":["Nebula Graph"],"content":"2 Configurations in CLion Ref: https://www.jetbrains.com/help/clion/clion-toolchains-in-docker.html#build-and-run Toolchains Add a remote host root@localhost:2222 password Put /opt/vesoft/toolset/cmake/bin/cmake as CMake CMake Toochain: Select the one created in last step Build directory: /home/nebula/build ","date":"2021-09-18","objectID":"/en/nebula-clion/:2:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#configurations-in-clion"},{"categories":["Nebula Graph"],"content":"3 The appendix","date":"2021-09-18","objectID":"/en/nebula-clion/:3:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#the-appendix"},{"categories":["Nebula Graph"],"content":"3.1 References of CMake output: [root@4c98e3f77ce8 build]# cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. \u003e\u003e\u003e\u003e Options of Nebula Graph \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_BUILD_STORAGE : OFF (Whether to build storage) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_MODULE_FORCE_CHECKOUT : ON (Whether checkout branch of module to same as graph.) -- ENABLE_MODULE_UPDATE : OFF (Automatically update module) -- ENABLE_PACK_ONE : ON (Whether to package into one) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_VERBOSE_BISON : OFF (Enable Bison to report state) -- ENABLE_WERROR : ON (Regard warnings as errors) -- CMAKE_BUILD_TYPE : Release (Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...) -- CMAKE_INSTALL_PREFIX : /usr/local/nebula (Install path prefix, prepended onto install directories.) -- CMAKE_CXX_STANDARD : 17 -- CMAKE_CXX_COMPILER : /opt/vesoft/toolset/clang/9.0.0/bin/c++ (CXX compiler) -- CMAKE_CXX_COMPILER_ID : GNU -- NEBULA_USE_LINKER : bfd -- CCACHE_DIR : /root/.ccache \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' \u003c\u003c\u003c\u003c -- NEBULA_THIRDPARTY_ROOT : /opt/vesoft/third-party/2.0 -- Build info of nebula third party: Package : Nebula Third Party Version : 2.0 Date : Mon Jun 28 15:07:38 UTC 2021 glibc : 2.17 Arch : x86_64 Compiler : GCC 9.2.0 C++ ABI : 11 Vendor : VEsoft Inc. -- CMAKE_INCLUDE_PATH : /opt/vesoft/third-party/2.0/include -- CMAKE_LIBRARY_PATH : /opt/vesoft/third-party/2.0/lib64;/opt/vesoft/third-party/2.0/lib -- CMAKE_PROGRAM_PATH : /opt/vesoft/third-party/2.0/bin -- GLIBC_VERSION : 2.17 -- found krb5-config here /opt/vesoft/third-party/2.0/bin/krb5-config -- Found kerberos 5 headers: /opt/vesoft/third-party/2.0/include -- Found kerberos 5 libs: /opt/vesoft/third-party/2.0/lib/libgssapi_krb5.a;/opt/vesoft/third-party/2.0/lib/libkrb5.a;/opt/vesoft/third-party/2.0/lib/libk5crypto.a;/opt/vesoft/third-party/2.0/lib/libcom_err.a;/opt/vesoft/third-party/2.0/lib/libkrb5support.a \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' done \u003c\u003c\u003c\u003c -- Create the pre-commit hook -- Creating pre-commit hook done \u003e\u003e\u003e\u003e Configuring Nebula Common \u003c\u003c\u003c\u003c \u003e\u003e\u003e\u003e Options of Nebula Common \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_WERROR : ON (Regard warnings as errors) -- Set D_GLIBCXX_USE_CXX11_ABI to 1 -- CMAKE_BUIL","date":"2021-09-18","objectID":"/en/nebula-clion/:3:1","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion: Setup Nebula Graph Dev Env with CLion and Docker","uri":"/en/nebula-clion/#references-of-cmake-output"},{"categories":["courses"],"content":"Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database.","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/"},{"categories":["courses"],"content":"Walk you through in actions to do below sections exercises! Bootstrap a Nebula Graph Cluster and Studio Web App Import a graph of dataset about shareholding Exploring the shareholding data with Nebula Importer Visually Exploring the shareholding data with Nebula Studio Run Graph Algorithm on Nebula Cluster Graph Data The dataset comes from https://github.com/wey-gu/nebula-shareholding-example/tree/main/data_sample The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-04","objectID":"/en/learn/nebula-101-shareholding/:0:0","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-shareholding/#"},{"categories":["courses"],"content":"Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s.","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/"},{"categories":["courses"],"content":"A full solution walkthrough for a Knowledge Graph Dialog System. Boostrap a Nebula Cluster in K8s Scale out the Nebula Cluster in K8s way Import the basketballplayer Dataset Siwi, the Knowledge Graph Dialog System with Nebula Graph Siwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. The code of Siwi is here: https://github.com/wey-gu/nebula-siwi. The course is open sourced here: https://github.com/wey-gu/katacoda-scenarios , please feed and contribute there :) ","date":"2021-09-03","objectID":"/en/learn/nebula-101-siwi-kgqa/:0:0","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"Siwi the Knowledge Graph Dialog System with Nebula Graph","uri":"/en/learn/nebula-101-siwi-kgqa/#"},{"categories":["Nebula Graph"],"content":"A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/"},{"categories":["Nebula Graph"],"content":" A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透 Related GitHub Repo: https://github.com/wey-gu/nebula-shareholding-example 更新：在这个数据集生成的工作基础上，我又做了一个全栈示例项目 👉🏻 https://siwei.io/corp-rel-graph/ I created the Katacoda Interactive Env for this project 👉🏻 https://siwei.io/cources/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ This is a demo of Shareholding Relationship Analysis with Distributed open-source Graph Database: Nebula Graph. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:0:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#"},{"categories":["Nebula Graph"],"content":"1 Data","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data"},{"categories":["Nebula Graph"],"content":"1.1 Data ModelingThere are various kinds of relationships when we checking companies’ shareholding breakthrough, here let’s simplify it with only two kind of entities: person and corp, and with following relationship types. person can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp Below is the lines to reflect this graph modele in Nebula Graph, it’s quite straightforward, right? CREATETAGperson(namestring);CREATETAGcorp(namestring);CREATEEDGErole_as(rolestring);CREATEEDGEis_branch_of();CREATEEDGEhold_share(sharefloat);CREATEEDGEreletive_with(degreeint); ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-modeling"},{"categories":["Nebula Graph"],"content":"1.2 Data GenerationWe just randomly generate some data to help with this demo, you can call data_generator.py directly to generate or reuse what’s already done under data_sample folder. The generated data are records to be fit in above data model from below .csv files. $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:2","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-generation"},{"categories":["Nebula Graph"],"content":"1.3 Data ImportWith those data in .csv files, we can easily import them into a Nebula Graph Cluster with the help of Nebula-Importer. nebula-importer.yaml in this repo describes rules and configurations on how this import will be done by the importer. For Nebula Graph Database, plesae refer to Doc , Doc-CN to deploy on any Linux Servers, for study and test, you can run it via Docker following the Quick Start Chapter of the documentation. For Nebula-Importer, if you already have Docker env, you can run it as the following without installing anything. Or, if you prefer to install it, it’s quite easy as it’s written in Golang and you can run its single file binary quite easily, go check both Documentation and Nebula-Importer Repo: https://github.com/vesoft-inc/nebula-importer. Let’s start! Below is the commands I used to import our data into a Nebula Graph Database. # put generated data \u0026 nebula-importor.yaml to nebula-importer server $ scp -r data nebula_graph_host:~ $ scp nebula-importer.yaml data nebula_graph_host:~/data $ ssh nebula_graph_host $ ls -l ${HOME}/data total 756 -rw-r--r--. 1 wei.gu wei.gu 23941 Jul 14 05:44 corp.csv -rw-r--r--. 1 wei.gu wei.gu 1277 Jul 14 05:44 corp_rel.csv -rw-r--r--. 1 wei.gu wei.gu 3048 Jul 14 05:44 corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 3893 Jul 14 05:44 nebula-importer.yaml -rw-r--r--. 1 wei.gu wei.gu 211661 Jul 14 05:44 person.csv -rw-r--r--. 1 wei.gu wei.gu 179770 Jul 14 05:44 person_corp_role.csv -rw-r--r--. 1 wei.gu wei.gu 322965 Jul 14 05:44 person_corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 17689 Jul 14 05:44 person_rel.csv # import data into our nebula graph database $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${HOME}/data/nebula-importer.yaml:/root/nebula-importer.yaml \\ -v ${HOME}/data:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-importer.yaml 2021/07/14 05:49:32 --- START OF NEBULA IMPORTER --- 2021/07/14 05:49:32 [WARN] config.go:491: Not set files[0].schema.vertex.vid.Type, reset to default value `string' ... 2021/07/14 05:49:43 [INFO] reader.go:180: Total lines of file(/root/person_corp_role.csv) is: 5000, error lines: 1287 2021/07/14 05:49:43 [INFO] statsmgr.go:61: Done(/root/person_corp_role.csv): Time(11.39s), Finished(12523), Failed(0), Latency AVG(1514us), Batches Req AVG(1824us), Rows AVG(1099.43/s) 2021/07/14 05:49:47 [INFO] statsmgr.go:61: Tick: Time(15.00s), Finished(25807), Failed(0), Latency AVG(1500us), Batches Req AVG(1805us), Rows AVG(1720.46/s) 2021/07/14 05:49:48 [INFO] reader.go:180: Total lines of file(/root/person.csv) is: 10000, error lines: 0 2021/07/14 05:49:48 [INFO] statsmgr.go:61: Done(/root/person.csv): Time(16.10s), Finished(29731), Failed(0), Latency AVG(1505us), Batches Req AVG(1810us), Rows AVG(1847.17/s) 2021/07/14 05:49:50 [INFO] reader.go:180: Total lines of file(/root/person_corp_share.csv) is: 20000, error lines: 0 2021/07/14 05:49:50 [INFO] statsmgr.go:61: Done(/root/person_corp_share.csv): Time(17.74s), Finished(36013), Failed(0), Latency AVG(1531us), Batches Req AVG(1844us), Rows AVG(2030.29/s) 2021/07/14 05:49:50 Finish import data, consume time: 18.25s 2021/07/14 05:49:51 --- END OF NEBULA IMPORTER --- ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:1:3","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#data-import"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#corporation-sharehold-relationship-breakthrough"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#query-in-ngql"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#in-a-visual-way"},{"categories":["Nebula Graph"],"content":"3 Thanks to Upstream Projects ❤️ Python Faker https://github.com/joke2k/faker/ pydbgen https://github.com/tirthajyoti/pydbgen Nebula Graph https://github.com/vesoft-inc/nebula-graph ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"3.1 Tips: You can deploy nebula graph in one line with: Nebula-UP, it helps install a nebula graph with Docker Nebula-operator-KIND , it helps setup all dependencies of Nebula-K8s-Operator including a K8s in Docker, PV Provider and then install a Nebula Graph with Nebula-Operator in K8s. Image Credit goes to https://unsplash.com/photos/3fPXt37X6UQ ","date":"2021-08-28","objectID":"/en/nebula-holdshare-dataset/:3:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset: A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph.","uri":"/en/nebula-holdshare-dataset/#tips"},{"categories":null,"content":" DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... K8s Community Day 2021 Openfunction + GraphDB Read more... COScon 2021 我的开源之路 Read more... PyCon China 2021 图数据库解谜与 Python 的图库应用实践 Read more... nMeetup: Nebula 应用上手实操 从头实操 Nebula 的部署，股权穿透，图算法运算，语音智能助手。 Read more... How to Train your Dragon 如何成为开源开发者（布道师）。 Read more... ","date":"2021-08-26","objectID":"/en/talk/:0:0","series":null,"tags":null,"title":"My Talks","uri":"/en/talk/#"},{"categories":["Nebula Graph"],"content":"Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm 导入 Livejournal 数据集到 Nebula 并运行 Nebula Algorithm 图算法","date":"2021-08-24","objectID":"/en/nebula-livejournal/","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/"},{"categories":["Nebula Graph"],"content":" 一个导入 Livejournal 数据集到 Nebula Graph 图数据库，并执行 Nebula Algorithm 图算法的过程分享。 Related GitHub Repo: https://github.com/wey-gu/nebula-LiveJournal ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#"},{"categories":["Nebula Graph"],"content":"nebula-LiveJournalLiveJournal Dataset is a Social Network Dataset in one file with two columns(FromNodeId, ToNodeId). $ head soc-LiveJournal1.txt # Directed graph (each unordered pair of nodes is saved once): soc-LiveJournal1.txt # Directed LiveJournal friednship social network # Nodes: 4847571 Edges: 68993773 # FromNodeId ToNodeId 0 1 0 2 0 3 0 4 0 5 0 6 It could be accessed in https://snap.stanford.edu/data/soc-LiveJournal1.html. Dataset statistics Nodes 4847571 Edges 68993773 Nodes in largest WCC 4843953 (0.999) Edges in largest WCC 68983820 (1.000) Nodes in largest SCC 3828682 (0.790) Edges in largest SCC 65825429 (0.954) Average clustering coefficient 0.2742 Number of triangles 285730264 Fraction of closed triangles 0.04266 Diameter (longest shortest path) 16 90-percentile effective diameter 6.5 ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#nebula-livejournal"},{"categories":["Nebula Graph"],"content":"1 Dataset Download and Preprocessing","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#dataset-download-and-preprocessing"},{"categories":["Nebula Graph"],"content":"1.1 DownloadIt is accesissiable from the official web page: $ cd nebula-livejournal/data $ wget https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz Comments in data file should be removed to make the data import tool happy. ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#download"},{"categories":["Nebula Graph"],"content":"1.2 Preprocessing $ gzip -d soc-LiveJournal1.txt.gz $ sed -i '1,4d' soc-LiveJournal1.txt ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:1:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#preprocessing"},{"categories":["Nebula Graph"],"content":"2 Import dataset to Nebula Graph","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#import-dataset-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"2.1 With Nebula ImporterNebula-Importer is a Golang Headless import tool for Nebula Graph. You may need to edit the config file under nebula-importer/importer.yaml on Nebula Graph’s address and credential。 Then, Nebula-Importer could be called in Docker as follow: $ cd nebula-livejournal $ docker run --rm -ti \\ --network=nebula-net \\ -v nebula-importer/importer.yaml:/root/importer.yaml \\ -v data/:/root \\ vesoft/nebula-importer:v2 \\ --config /root/importer.yaml Or if you have the binary nebula-importer locally: $ cd data $ \u003cpath_to_nebula-importer_binary\u003e --config ../nebula-importer/importer.yaml ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-importer"},{"categories":["Nebula Graph"],"content":"2.2 With Nebula ExchangeNebula-Exchange is a Spark Application to enable batch and streaming data import from multiple data sources to Nebula Graph. To be done. (You can refer to https://siwei.io/nebula-exchange-sst-2.x/) ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:2:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#with-nebula-exchange"},{"categories":["Nebula Graph"],"content":"3 Run Algorithms with Nebula GraphNebula-Algorithm is a Spark/GraphX Application to run Graph Algorithms with data consumed from files or a Nebula Graph Cluster. Supported Algorithms for now: Name Use Case PageRank page ranking, important node digging Louvain community digging, hierarchical clustering KCore community detection, financial risk control LabelPropagation community detection, consultation propagation, advertising recommendation ConnectedComponent community detection, isolated island detection StronglyConnectedComponent community detection ShortestPath path plan, network plan TriangleCount network structure analysis BetweennessCentrality important node digging, node influence calculation DegreeStatic graph structure analysis ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms-with-nebula-graph"},{"categories":["Nebula Graph"],"content":"3.1 Ad-hoc Spark Env setupHere I assume the Nebula Graph was bootstraped with Nebula-Up, thus nebula is running in a Docker Network named nebula-docker-compose_nebula-net. Then let’s start a single server spark: docker run --name spark-master --network nebula-docker-compose_nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ -v nebula-algorithm/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 Thus we could make spark application submt inside this container: docker exec -it spark-master bash cd /root/ # download Nebula-Algorithm Jar Packagem, 2.0.0 for example, for other versions, refer to nebula-algorithm github repo and documentations. wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.0.0/nebula-algorithm-2.0.0.jar ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#ad-hoc-spark-env-setup"},{"categories":["Nebula Graph"],"content":"3.2 Run AlgorithmsThere are many altorithms supported by Nebula-Algorithm, here some of their configuration files were put under nebula-algorithm as an example. Before using them, please first edit and change Nebula Graph Cluster Addresses and credentials. vim nebula-altorithm/algo-pagerank.conf Then we could enter the spark container and call corresponding algorithms as follow. Please adjust your --driver-memeory accordingly, i.e. pagerank altorithm: /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 16g nebula-algorithm-2.0.0.jar \\ -p pagerank.conf After the algorithm finished, the output will be under the path insdie the container defined in conf file: write:{ resultPath:/output/ } 题图版权：@sigmund ","date":"2021-08-24","objectID":"/en/nebula-livejournal/:3:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal, Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm","uri":"/en/nebula-livejournal/#run-algorithms"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:0:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#"},{"categories":["Nebula Graph"],"content":"1 什么是 Nebula Exchange ?之前我在 Nebula Data Import Options 之中介绍过，Nebula Exchange 是一个 Nebula Graph 社区开源的 Spark Applicaiton，它专门用来支持批量或者流式地把数据导入 Nebula Graph Database 之中。 Nebula Exchange 支持多种多样的数据源（从 Apache Parquet, ORC, JSON, CSV, HBase, Hive MaxCompute 到 Neo4j, MySQL, ClickHouse, 再有 Kafka, Pulsar，更多的数据源也在不断增加之中）。 如上图所示，在 Exchange 内部，从除了不同 Reader 可以读取不同数据源之外，在数据经过 Processor 处理之后通过 Writer写入（sink） Nebula Graph 图数据库的时候，除了走正常的 ServerBaseWriter 的写入流程之外，它还可以绕过整个写入流程，利用 Spark 的计算能力并行生成底层 RocksDB 的 SST 文件，从而实现超高性能的数据导入，这个 SST 文件导入的场景就是本文带大家上手熟悉的部分。 详细信息请参阅：Nebula Graph 手册:什么是 Nebula Exchange Nebula Graph 官方博客也有更多 Nebula Exchange 的实践文章 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:1:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#什么是-nebula-exchange-"},{"categories":["Nebula Graph"],"content":"2 步骤概观 实验环境 配置 Exchange 生成 SST 文件 写入 SST 文件到 Nebula Graph ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:2:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#步骤概观"},{"categories":["Nebula Graph"],"content":"3 实验环境准备为了最小化使用 Nebula Exchange 的 SST 功能，我们需要： 搭建一个 Nebula Graph 集群，创建导入数据的 Schema，我们选择使用 Docker-Compose 方式、利用 Nebula-Up 快速部署，并简单修改其网络，以方便同样容器化的 Exchange 程序对其访问。 搭建容器化的 Spark 运行环境 搭建容器化的 HDFS ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#实验环境准备"},{"categories":["Nebula Graph"],"content":"3.1 搭建 Nebula Graph 集群借助于 Nebula-Up 我们可以在 Linux 环境下一键部署一套 Nebula Graph 集群： curl -fsSL nebula-up.siwei.io/install.sh | bash 待部署成功之后，我们需要对环境做一些修改，这里我做的修改其实就是两点： 只保留一个 metaD 服务 起用 Docker 的外部网络 详细修改的部分参考附录一 应用 docker-compose 的修改： cd ~/.nebula-up/nebula-docker-compose vim docker-compose.yaml # 参考附录一 docker network create nebula-net # 需要创建外部网络 docker-compose up -d --remove-orphans 之后，我们来创建要测试的图空间，并创建图的 Schema，为此，我们可以利用 nebula-console ，同样，Nebula-Up 里自带了容器化的 nebula-console。 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 创建图空间（我们起名字叫 sst ），以及 schema createspacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));:sleep20usesstcreatetagplayer(namestring,ageint); 示例输出 (root@nebula)[(none)]\u003ecreatespacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));Executionsucceeded(timespent1468/1918us)(root@nebula)[(none)]\u003e:sleep20(root@nebula)[(none)]\u003eusesstExecutionsucceeded(timespent1253/1566us)Wed,18Aug202108:18:13UTC(root@nebula)[sst]\u003ecreatetagplayer(namestring,ageint);Executionsucceeded(timespent1312/1735us)Wed,18Aug202108:18:23UTC ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建-nebula-graph-集群"},{"categories":["Nebula Graph"],"content":"3.2 搭建容器化的 Spark 环境利用 big data europe 做的工作，这个过程非常容易。 值得注意的是： 现在的 Nebula Exchange 对 Spark 的版本有要求，在现在的 2021 年 8 月，我是用了 spark-2.4.5-hadoop-2.7 的版本。 为了方便，我让 Spark 运行在 Nebula Graph 相同的机器上，并且指定了运行在同一个 Docker 网络下 docker run --name spark-master --network nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ bde2020/spark-master:2.4.5-hadoop2.7 然后，我们就可以进入到环境中了： docker exec -it spark-master bash 进到 Spark 容器中之后，可以像这样安装 maven: export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 还可以这样在容器里下载 nebula-exchange 的 jar 包： cd ~ wget https://repo1.maven.org/maven2/com/vesoft/nebula-exchange/2.1.0/nebula-exchange-2.1.0.jar ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建容器化的-spark-环境"},{"categories":["Nebula Graph"],"content":"3.3 搭建容器化的 HDFS同样借助 big-data-euroupe 的工作，这非常简单，不过我们要做一点修改，让它的 docker-compose.yml 文件里使用 nebula-net 这个之前创建的 Docker 网络。 详细修改的部分参考附录二 git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop vim docker-compose.yml docker-compose up -d ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:3:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#搭建容器化的-hdfs"},{"categories":["Nebula Graph"],"content":"4 配置Exchange这个配置主要填入的信息就是 Nebula Graph 集群本身和将要写入数据的 Space Name，以及数据源相关的配置（这里我们用 csv 作为例子），最后再配置输出（sink）为 sst Nebula Graph GraphD 地址 MetaD 地址 credential Space Name 数据源 source: csv path fields etc. ink: sst 详细的配置参考附录二 注意，这里 metaD 的地址可以这样获取，可以看到 0.0.0.0:49377-\u003e9559 表示 49377 是外部的地址。 $ docker ps | grep meta 887740c15750 vesoft/nebula-metad:v2.0.0 \"./bin/nebula-metad …\" 6 hours ago Up 6 hours (healthy) 9560/tcp, 0.0.0.0:49377-\u003e9559/tcp, :::49377-\u003e9559/tcp, 0.0.0.0:49376-\u003e19559/tcp, :::49376-\u003e19559/tcp, 0.0.0.0:49375-\u003e19560/tcp, :::49375-\u003e19560/tcp nebula-docker-compose_metad0_1 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:4:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#配置exchange"},{"categories":["Nebula Graph"],"content":"5 生成SST文件","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#生成sst文件"},{"categories":["Nebula Graph"],"content":"5.1 准备源文件、配置文件 docker cp exchange-sst.conf spark-master:/root/ docker cp player.csv spark-master:/root/ 其中 player.csv 的例子： 1100,Tim Duncan,42 1101,Tony Parker,36 1102,LaMarcus Aldridge,33 1103,Rudy Gay,32 1104,Marco Belinelli,32 1105,Danny Green,31 1106,Kyle Anderson,25 1107,Aron Baynes,32 1108,Boris Diaw,36 1109,Tiago Splitter,34 1110,Cory Joseph,27 1111,David West,38 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#准备源文件配置文件"},{"categories":["Nebula Graph"],"content":"5.2 执行 exchange 程序进入 spark-master 容器，提交执行 exchange 应用。 docker exec -it spark-master bash cd /root/ /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange-2.1.0.jar\\ -c exchange-sst.conf 检查执行结果： spark-submit 输出： 21/08/17 03:37:43 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 1093 ms on localhost (executor driver) (32/32) 21/08/17 03:37:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 21/08/17 03:37:43 INFO DAGScheduler: ResultStage 2 (foreachPartition at VerticesProcessor.scala:179) finished in 22.336 s 21/08/17 03:37:43 INFO DAGScheduler: Job 1 finished: foreachPartition at VerticesProcessor.scala:179, took 22.500639 s 21/08/17 03:37:43 INFO Exchange$: SST-Import: failure.player: 0 21/08/17 03:37:43 WARN Exchange$: Edge is not defined 21/08/17 03:37:43 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040 21/08/17 03:37:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! 验证 HDFS 上生成的 SST 文件： docker exec -it namenode /bin/bash root@2db58903fb53:/# hdfs dfs -ls /sst Found 10 items drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/1 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/10 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/2 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/3 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/4 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/5 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/6 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/7 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/8 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/9 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:5:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#执行-exchange-程序"},{"categories":["Nebula Graph"],"content":"6 写入SST到NebulaGraph这里的操作实际上都是参考文档：SST 导入，得来。其中就是从 console 之中执行了两步操作： Download Ingest 其中 Download 实际上是触发 Nebula Graph 从服务端发起 HDFS Client 的 download，获取 HDFS 上的 SST 文件，然后放到 storageD 能访问的本地路径下，这里，需要我们在服务端部署 HDFS 的依赖。因为我们是最小实践，我就偷懒手动做了这个 Download 的操作。 ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#写入sst到nebulagraph"},{"categories":["Nebula Graph"],"content":"6.1 手动下载这里边手动下载我们就要知道 Nebula Graph 服务端下载的路径，实际上是 /data/storage/nebula/\u003cspace_id\u003e/download/，这里的 Space ID 需要手动获取一下： 这个例子里，我们的 Space Name 是 sst，而 Space ID 是 49。 (root@nebula)[sst]\u003eDESCspacesst+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |ID|Name|PartitionNumber|ReplicaFactor|Charset|Collate|VidType|AtomicEdge|Group|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |49|\"sst\"|10|1|\"utf8\"|\"utf8_bin\"|\"FIXED_STRING(32)\"|\"false\"|\"default\"|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ 于是，下边的操作就是手动把 SST 文件从 HDFS 之中 get 下来，再拷贝到 storageD 之中。 docker exec -it namenode /bin/bash $ hdfs dfs -get /sst /sst exit docker cp namenode:/sst . docker exec -it nebula-docker-compose_storaged0_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged1_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged2_1 mkdir -p /data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged0_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged1_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged2_1:/data/storage/nebula/49/download/ ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#手动下载"},{"categories":["Nebula Graph"],"content":"6.2 SST 文件导入 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 执行 INGEST 开始让 StorageD 读取 SST 文件 (root@nebula) [(none)]\u003e use sst (root@nebula) [sst]\u003e INGEST; 我们可以用如下方法实时查看 Nebula Graph 服务端的日志 tail -f ~/.nebula-up/nebula-docker-compose/logs/*/* 成功的 INGEST 日志： I0817 08:03:28.611877 169 EventListner.h:96] Ingest external SST file: column family default, the external file path /data/storage/nebula/49/download/8/8-6.sst, the internal file path /data/storage/nebula/49/data/000023.sst, the properties of the table: # data blocks=1; # entries=1; # deletions=0; # merge operands=0; # range deletions=0; raw key size=48; raw average key size=48.000000; raw value size=40; raw average value size=40.000000; data block size=75; index block size (user-key? 0, delta-value? 0)=66; filter block size=0; (estimated) table size=141; filter policy name=N/A; prefix extractor name=nullptr; column family ID=N/A; column family name=N/A; comparator name=leveldb.BytewiseComparator; merge operator name=nullptr; property collectors names=[]; SST file compression algo=Snappy; SST file compression options=window_bits=-14; level=32767; strategy=0; max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0; ; creation time=0; time stamp of earliest key=0; file creation time=0; E0817 08:03:28.611912 169 StorageHttpIngestHandler.cpp:63] SSTFile ingest successfully 题图版权：Pietro Jeng ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:6:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#sst-文件导入"},{"categories":["Nebula Graph"],"content":"7 附录","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录"},{"categories":["Nebula Graph"],"content":"7.1 附录一docker-compose.yaml diff --git a/docker-compose.yaml b/docker-compose.yaml index 48854de..cfeaedb 100644 --- a/docker-compose.yaml +++ b/docker-compose.yaml @@ -6,11 +6,13 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=metad0 - --ws_ip=metad0 - --port=9559 - --ws_http_port=19559 + - --ws_storage_http_port=19779 - --data_path=/data/meta - --log_dir=/logs - --v=0 @@ -34,81 +36,14 @@ services: cap_add: - SYS_PTRACE - metad1: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad1 - - --ws_ip=metad1 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad1:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta1:/data/meta - - ./logs/meta1:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - - metad2: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad2 - - --ws_ip=metad2 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad2:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta2:/data/meta - - ./logs/meta2:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - storaged0: image: vesoft/nebula-storaged:v2.0.0 environment: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged0 - --ws_ip=storaged0 - --port=9779 @@ -119,8 +54,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged0:19779/status\"] interval: 30s @@ -146,7 +81,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged1 - --ws_ip=storaged1 - --port=9779 @@ -157,8 +92,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged1:19779/status\"] interval: 30s @@ -184,7 +119,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged2 - --ws_ip=storaged2 - --port=9779 @@ -195,8 +130,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged2:19779/status\"] interval: 30s @@ -222,17 +157,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd:19669/status\"] interval: 30s @@ -257,17 +194,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd1 - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd1:19669/status\"] interval: 30s @@ -292,17 +231,21 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=met","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录一"},{"categories":["Nebula Graph"],"content":"7.2 附录二https://github.com/big-data-europe/docker-hadoop 的 docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index ed40dc6..66ff1f4 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -14,6 +14,8 @@ services: - CLUSTER_NAME=test env_file: - ./hadoop.env + networks: + - nebula-net datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 @@ -25,6 +27,8 @@ services: SERVICE_PRECONDITION: \"namenode:9870\" env_file: - ./hadoop.env + networks: + - nebula-net resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 @@ -34,6 +38,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\" env_file: - ./hadoop.env + networks: + - nebula-net nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 @@ -43,6 +49,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\" env_file: - ./hadoop.env + networks: + - nebula-net historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 @@ -54,8 +62,14 @@ services: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env + networks: + - nebula-net volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: + +networks: + nebula-net: + external: true ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录二"},{"categories":["Nebula Graph"],"content":"7.3 附录三nebula-exchange-sst.conf { # Spark relation config spark: { app: { name: Nebula Exchange 2.1 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"192.168.8.128:9669\"] meta:[\"192.168.8.128:49377\"] } user: root pswd: nebula space: sst # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://192.168.8.128:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is sst, just change type.sink to client if you want to use client import mode. { name: player type: { source: csv sink: sst } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } ","date":"2021-08-18","objectID":"/en/nebula-exchange-sst-2.x/:7:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/en/nebula-exchange-sst-2.x/#附录三"},{"categories":["sketches"],"content":"Nebula Operator Explained","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/"},{"categories":["sketches"],"content":" Nebula Graph operator explained This note explained nebula graph’s K8s Operator: Intro 00:00 Nebula K8s Operator Explained 0:25 How do we use Nebula Operator? 02:23 What is the difference between the Operator based Nebula Graph Cluster and the binary-based one? 03:50 How about the Performance impact when it comes to K8s-Operator deployment? 04:55 What is the easiest way to try out the nebula operator? 06:04 Outra 07:30 ref: https://github.com/vesoft-inc/nebula-operator ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:0:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:1:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-08-06","objectID":"/en/sketches/nebula-operator-explained/:2:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula Operator Explained","uri":"/en/sketches/nebula-operator-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Config Explained","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/"},{"categories":["sketches"],"content":" Nebula Graph config explained This note explained nebula graph configurations: Intro 00:00 Nebula Graph Config Explained 0:16 How about Configurations in Nebula Graph Deployed with Docker? 03:01 What about Nebula Graph in K8s Operator Deployment case? 03:55 Should we use Local-Config or Not?(spoiler: Yes!) 05:03 Outra 05:27 ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:0:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:1:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-26","objectID":"/en/sketches/nebula-config-explained/:2:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/en/sketches/nebula-config-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Index Demystified","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/"},{"categories":["sketches"],"content":" Nebula Graph Native Index Demystified(Chinese only now, English version will be soon uploaded) Index Demystified 0:33 When should we use index? 06:37 Index v.s. Fulltext Index 07:12 Index Performance Impact 08:03 ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:0:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:1:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-13","objectID":"/en/sketches/nebula-index-demystified/:2:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Index Demystified","uri":"/en/sketches/nebula-index-demystified/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Deployment Options","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/"},{"categories":["sketches"],"content":" Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:0:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:1:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-06-25","objectID":"/en/sketches/nebula-deployment-options/:2:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/en/sketches/nebula-deployment-options/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Data Import Options","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/"},{"categories":["sketches"],"content":" Nebula Graph comes with multiple Data Import utils and options, how should we choose from them? ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:0:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#"},{"categories":["sketches"],"content":"1 Youtube ","date":"2021-06-15","objectID":"/en/sketches/nebula-data-import-options/:1:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/en/sketches/nebula-data-import-options/#youtube"},{"categories":["Nebula Graph"],"content":"one liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker)","date":"2021-06-09","objectID":"/en/nebula-operator-kind/","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/"},{"categories":["Nebula Graph"],"content":" Nebula-Kind, an one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND (K8s in Docker) ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:0:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#"},{"categories":["Nebula Graph"],"content":"1 Nebula-Operator-KindAs a Cloud Native Distributed Database, Nebula Graph comes with an open-source K8s Operator to enable boostrap and maintain Nebula Graph Cluster from a K8s CRD. Normally it takes you some time to setup all the dependencies and control plane resources of the Nebula Operator. If you are as lazy as I am, this Nebula-Operator-Kind is made for you to quick start and play with Nebula Graph in KIND. Nebula-Operator-Kind is the one-liner for setup everything for you including: Docker K8s(KIND) PV Provider Nebula-Operator Nebula-Console nodePort for accessing the Cluster Kubectl for playing with KIND and Nebula Operator ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:1:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#nebula-operator-kind"},{"categories":["Nebula Graph"],"content":"2 How To UseInstall Nebula-Operator-Kind: curl -sL nebula-kind.siwei.io/install.sh | bash You will see this after it’s done You can connect to the cluster via ~/.nebula-kind/bin/console as below: ~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000 ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:2:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#how-to-use"},{"categories":["Nebula Graph"],"content":"3 MoreIt’s in GitHub with more information you may be intrested in ;-), please try and feedback there~ https://github.com/wey-gu/nebula-operator-kind Updated Sept. 2021 Install on KubeSphere all-in-on cluster： curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash Install on existing K8s cluster: curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash Banner Picture Credit: Maik Hankemann ","date":"2021-06-09","objectID":"/en/nebula-operator-kind/:3:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground ","uri":"/en/nebula-operator-kind/#more"},{"categories":null,"content":"Hi, this is Wey :)I am a developer @vesoft working as Developer Advocate of Nebula Graph, the open source distributed Graph Database I create toolings and content for Nebula Graph Database to help Developers in the open source community. I am working in open source and consider it is a privilege 1. It took me a couple of my early career years to figure out that my passion lies in helping others with my thoughts \u0026 the tech/magic I have learned. ","date":"2021-06-04","objectID":"/en/about/:0:0","series":null,"tags":null,"title":"","uri":"/en/about/#hi-this-is-wey-"},{"categories":null,"content":"1 Recent Projects Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-06-04","objectID":"/en/about/:1:0","series":null,"tags":null,"title":"","uri":"/en/about/#recent-projects"},{"categories":null,"content":"2 Sketches Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option should I use? Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-06-04","objectID":"/en/about/:2:0","series":null,"tags":null,"title":"","uri":"/en/about/#sketches"},{"categories":null,"content":"3 Hands-on Cources How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-06-04","objectID":"/en/about/:3:0","series":null,"tags":null,"title":"","uri":"/en/about/#hands-on-cources"},{"categories":null,"content":"4 Talks DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB Read more... Data on K8s Community 2021 GraphDB on Kubesphere Read more... ","date":"2021-06-04","objectID":"/en/about/:4:0","series":null,"tags":null,"title":"","uri":"/en/about/#talks"},{"categories":null,"content":"5 Previous workI worked at Ericsson for amost 10 years(2011 to 2021). As the System Manager 2 of Cloud Execution Envrioment (CEE) 3 PDU Cloud, member of CEE 10 core team and CEE System Management team. Helping evolve CEE was my main job: I studied, designed and implemented more than 20 features for CEE 6.6.2 and CEE 10, including area of compute, network, storage, lifecycle management and security. I am also responsible for Ericsson CEE evangelism (internal and external) in China. I used to share my notes and thoughts on note.siwei.info, while from 2021, I will leave more ideas on siwei.io instead. ","date":"2021-06-04","objectID":"/en/about/:5:0","series":null,"tags":null,"title":"","uri":"/en/about/#previous-work"},{"categories":null,"content":"6 ContactYou can DM me via twitter, or wey.gu@vesoft.com. I share the same idea with Ahmet Alp Balkan’s tweet: Working in open source (and getting paid for it) is a privilege. It’s a career boost, makes you lots of friends across the industry, and gives you a public brand. I am one of the “lucky few” \u0026 thankful to Microsoft and Google who let me work on OSS nearly all my career. — ahmetb (@ahmetb) February 19, 2021  ↩︎ System Manager, PDU Cloud: Job Description ↩︎ Ericsson’s Telco. Infrastructure as a Service product offerring: Cloud Execution Environment ↩︎ ","date":"2021-06-04","objectID":"/en/about/:6:0","series":null,"tags":null,"title":"","uri":"/en/about/#contact"},{"categories":["Nebula Graph"],"content":"本文分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。","date":"2021-05-26","objectID":"/en/nebula-chia/","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/"},{"categories":["Nebula Graph"],"content":" 本文分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。 我把涉及的代码开源在了这里：https://github.com/wey-gu/nebula-chia ","date":"2021-05-26","objectID":"/en/nebula-chia/:0:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#"},{"categories":["Nebula Graph"],"content":"1 What is Chia Network?Chia Network 是由 BitTorrent 的作者 Bram Cohen 的团队在 2017 年创建的区块链项目。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#what-is-chia-network"},{"categories":["Nebula Graph"],"content":"1.1 Why yet another Blockchain? 为什么再搞一个区块链?Chia 用了全新的中本聪共识算法，这个算法通过不允许并行计算，让挖矿（Proof of Work）所需算力和能耗降到非常低，这使得超大组织、玩家没法像在其他的区块链项目那样有算力的绝对优势，也一定程度上规避了能源的浪费。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#why-yet-another-blockchain-为什么再搞一个区块链"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-can-i-access-chia-network-如何连接chia"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#安装"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#运行"},{"categories":["Nebula Graph"],"content":"1.2 How Can I access Chia Network? 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#访问-chia-的数据"},{"categories":["Nebula Graph"],"content":"2 Inspect the Chia Network, 分析 Chia 的数据如果大家仔细看了上边表结构定义的截图，就能注意到一些表的主要信息是嵌套二进制 KV Byte，所以只从 SQLite 并不能看到所有 Chia 的数据，所以我们需要（用一个编程语言来）读取表里的 Byte。 幸运的是，这件事儿因为 Chia 是开源的，而且是 Python 的代码，使得我们可以直接交互式的做。 我花了一点点时间在 Chia 客户端代码里找到了需要的封装类，借助它，可以比较方便的分析 Chia 客户端在本地的全链数据。 如果您不感兴趣细节，可以直接看我分析的结论。 结论之后，我也给大家演示一下是怎么读取它们的。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#inspect-the-chia-network-分析-chia-的数据"},{"categories":["Nebula Graph"],"content":"2.1 TL;DR, 结论我们可以从表中读取到区块链记录（Block Record ），Chia 币记录（Coin Record）。 从区块记录中，我们可以看到关键的涉及交易的信息： 关联的 Coin ，关联的 Puzzle（地址），Coin 的值(Amount) 从币记录中，我们可以看到关键的涉及区块的信息： 生成这个 Coin 所在区块链里的索引高度（Confirmed Index） 如果这个记录是花费 Coin 的，花费它的索引高度（Spent Index） ┌──────────────────────┐ ┌────────────────────────────────────────┐ │ │ │ │ │ Coin Record │ │ Block Record │ │ │ │ │ │ Coin Name │ │ Height ◄────────────────────────────┼─┐ │ │ │ │ │ ┌─┼───► Puzzle │ │ Header │ │ │ │ │ │ │ │ ├─┼───► Coin Parent │ │ Prev Header │ │ │ │ │ │ │ │ ├─┼───► Amount │ │ Block Body │ │ │ │ │ │ farmer_puzzle_hash │ │ │ │ Time Stamp │ │ fees │ │ │ │ │ │ pool_puzzle_hash │ └─────┼─┼─┬─ Confirmed Index │ │ prev_transaction_block_hash │ │ │ │ │ │ prev_transaction_block_height │ │ │ └─ Spent Index │ │ transactions_info ───────────────┼───────┘ │ │ │ ┌─── is_transaction_block │ │ Coinbase │ │ │ sub_epoch_summary ────────────────┼───────┐ │ │ │ │ │ │ └─ ────────────────────┘ │ │ is Peak │ │ │ └──is Block │ │ ┌─────────────────────┐ │ │ │ │ │ └────────────────────────────────────────┘ └─┼─► Sub Epoch Segment │ │ │ └─────────────────────┘ ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#tldr-结论"},{"categories":["Nebula Graph"],"content":"2.2 Preperation, 准备因为安装客户端之后，我们本地实际上已经有了相关的 Python 环境和依赖，只需要在里边跑起来就好。 # 注意，我们要 cd 到之前安装客户端时候克隆的仓库。 cd chia-blockchain # source activate 脚本来切换到仓库安装时候创建的 Python 虚拟环境，并进到 IPython 里。 source venv/bin/activate \u0026\u0026 pip install ipython \u0026\u0026 ipython 然后试着导入客户端里边带有的 Python 的 Chia 的封装类试试看。 In [1]: import sqlite3 ...: from chia.consensus.block_record import BlockRecord # 导入成功，没有报错 In [2]: !pwd # 我的安装克隆目录 /Users/weyl/chia-blockchain 恭喜你做好了准备，我们看看 Block Record 里都有什么。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#preperation-准备"},{"categories":["Nebula Graph"],"content":"2.3 Block Record Chain，区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#\u003c---- sub_epoch_summaryblob,is_peaktinyint,is_blocktinyint) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#block-record-chain区块记录"},{"categories":["Nebula Graph"],"content":"2.4 Coin Record Chain，Chia 币记录类似的，我们可以获取一个 Coin 的记录，这里边，从表的定义可以看到，唯一二进制（不能直接从数据库查询中被人读懂）的字段就是是币值，不存在嵌套的结构，所以也并不需要封装的类才能看清楚里边的信息。 CREATETABLEcoin_record(coin_nametextPRIMARYKEY,confirmed_indexbigint,spent_indexbigint,spentint,coinbaseint,puzzle_hashtext,coin_parenttext,amountblob,timestampbigint) 这里值得注意的信息主要是 spent_index 和 confirmed_index。 from chia.util.ints import uint64 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" con = sqlite3.connect(chia_db_path) cur = con.cursor() rows = list(cur.execute('SELECT * FROM coin_record WHERE confirmed_index = 201645')) coin_amount = uint64.from_bytes(rows[0][7]) In [201]: rows[0] Out[201]: ('cf35da0f595b49dde626d676b511ee62bce886f2216751aa51bb8ff851563d35', # coin_name 201645, # confirmed_index 0, # spent_index，这里没有spent，所以值无效 0, # spent，其实是 bool 1, # coinbase，bool 'bbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', # puzzle_hash 对应到地址 'ccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', b'\\x00\\x00\\x01\\x97t \\xdc\\x00', # uint64 1619662081) ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:4","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#coin-record-chainchia-币记录"},{"categories":["Nebula Graph"],"content":"2.5 Puzzles/ Address，地址我们可以把 Chia 中的 Puzzle 理解成为交易中的地址，为了方便使用，通常会把 Puzzle 的 hash 用bech32m 转换成地址。 Tips: 这里有一个在线双向转换的在线工具推荐一下: https://www.chiaexplorer.com/tools/address-puzzlehash-converter ","date":"2021-05-26","objectID":"/en/nebula-chia/:2:5","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#puzzles-address地址"},{"categories":["Nebula Graph"],"content":"3 How to explore Chia Network? 如何探索 Chia 链随着我们之前分析的信息，自然地，我们可以把 Chia 区块链中的信息取出来，用图（Graph）来表示，这里的图并不是（Graphic）图形、图画的意思，是数学、图论中的图。 在图的语境下，最主要的两个元素就是顶点（Vertex）和边（Edge）。 顶点表示一个实体，而边表示实体之间的某种关系，这种关系可以是对等的（无方向的）也可以是有方向的。 这里我们可以把这里的信息抽象映射到如图的图模型里： Block 顶点 Coin 顶点 Puzzle 顶点 spends 边（Block 到 Coin） confirms 边 （Block 到 Coin） belongs_to 边（Coin 到 Puzzle） 这里，我们应用的图是一种叫做属性图的形式，除了点和边的关系之外。这两种实体（点、边）还有其他信息只和它们的一个实例相关，所以再定义为顶点、边就不是很适合，这些信息就作为点、边的属性（preperty）存在。 这种为了处理实体之间关联、涉及实体、关联的属性信息的，也就是\"属性图\"的存储信息的方式在计算机领域越来越流行，甚至有专门为此结构而原生开发的数据库——图数据库（Graph Database）。 这里，我们用的就是一个叫做 Nebula Graph 的图数据库，它是一个现代的、为超大规模分部署架构设计的、原生存储、查询、计算图数据的项目，更棒的是，它是产生于社区的开源产品。 Tips: 安装 Nebula Graph 一般来说，面向超大规模数据的分布式系统，天然的都是不容易轻量部署的，大家如果第一次使用的话可以试试我写的一个叫做 nebula-up 的小工具，可以一行指令部署一个用来试用、学习的 Nebula Graph 集群，地址在这里： https://github.com/wey-gu/nebula-up/ 。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#how-to-explore-chia-network-如何探索-chia-链"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#import-the-chia-to-a-graph-database-nebula-graph-导入-chia-数据到图数据库"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-conversion-数据转换"},{"categories":["Nebula Graph"],"content":"3.1 Import the Chia to a Graph Database, Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 Data conversion 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 Data import 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#data-import-数据导入"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph 探索 Chia 的数据3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#explore-the-chia-graph-探索-chia-的数据"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph 探索 Chia 的数据3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#graph-db-queries"},{"categories":["Nebula Graph"],"content":"3.2 Explore the Chia Graph 探索 Chia 的数据3.2.1 Graph DB Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 The Visulized way with Nebula StudioNebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/en/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#the-visulized-way-with-nebula-studio"},{"categories":["Nebula Graph"],"content":"4 Summary 总结这篇文章里，在我们简单介绍了 Chia Network 之后，我们首次的带大家一起从安装一个 Chia 终端，到分析终端同步到本地的 Chia 全网数据，借助于 Chia 终端开源的 Python 代码库，我们分析了全网数据里的重要信息。 之后，我们开源了一个小工具 Nebula-Chia，有了它，就可以把 Chia 的全网数据转换成 CSV 格式，这样，就可以借助 nebula-importer 把所有的数据导入到一个先进的图数据库（Nebula Graph）中。 Nebula Graph 的项目地址是 https://github.com/vesoft-inc/nebula-graph Nebula-Chia 我也开源在 https://github.com/wey-gu/nebula-chia 在图数据库中，我们展示了做基本 Query 的例子和借助图数据库自带的可视化工具，我们可以轻易地获取 Chia 全网数据之间关联关系，有了这个作为基础，这些数据中洞察的潜力和可以尝试的有意思事情可以比较直观和高效地进一步探索了！ 是不是很酷？ ","date":"2021-05-26","objectID":"/en/nebula-chia/:4:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#summary-总结"},{"categories":["Nebula Graph"],"content":"5 References https://www.chia.net/faq/ https://chialisp.com/docs/ https://www.chiaexplorer.com/chia-coins https://docs.google.com/document/d/1tmRIb7lgi4QfKkNaxuKOBHRmwbVlGL4f7EsBDr_5xZE https://github.com/sipa/bech32/tree/master/ref/python https://github.com/Chia-Network/chia-blockchain/blob/main/README.md https://www.chia.net/assets/ChiaGreenPaper.pdf https://docs.nebula-graph.com.cn Banner Picture Credit: Icons8 Team ","date":"2021-05-26","objectID":"/en/nebula-chia/:5:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"Explore Chia Network in a Visualized Way","uri":"/en/nebula-chia/#references"},{"categories":null,"content":" How to Analysis shareholding ownership Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database. Read more... How I built Siwi, the Voice assistant Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s. Read more... ","date":"2021-05-26","objectID":"/en/cources/:0:0","series":null,"tags":null,"title":"Hands on Courses","uri":"/en/cources/#"},{"categories":null,"content":" Nebula-Siwi Nebula-Siwi, a Dialog System With Graph Database Backed Knowledge Graph. Read more... Nebula-Holdshare Nebula-Holdshare, a demo/ dataset of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. Read more... Nebula-KIND Nebula-Kind, one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND(K8s in Docker) Read more... Nebula-Up A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command on Windows, Linux, and macOS(x86_64 and aarch64), w/o dealing with dependencies. Read more... VSCode-nGQL nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. Read more... Nebula-Chia A exploration(and open-source utility) on extracting and loading Chia Network Blockchain into Nebula Graph. Read more... IPython-nGQL ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or IPython. Read more... nebula-insights We leveraged Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights, including Google Cloud Scheduler, Google Cloud Functions and BigQuery, which is all open-sourced in GitHub. Read more... ","date":"2021-05-26","objectID":"/en/projects/:0:0","series":null,"tags":null,"title":"Side Projects","uri":"/en/projects/#"},{"categories":null,"content":" Nebula Operator Explained Nebula Graph K8s Operator Explained Read more... Nebula Config Explained Nebula Graph Config Explained Read more... Nebula Index Demystified Nebula Graph Native Index Demystified Read more... Nebula Data Import Options Nebula Graph comes with multiple Data Import utils, how should we choose from them? Read more... Nebula Deployment Options Nebula Graph is distributed and deployed in different options, here is which deployment option we should use. Read more... Nebula Intro Intro session for Nebula Graph for beginners. Read more... ","date":"2021-05-26","objectID":"/en/sketch-notes/:0:0","series":null,"tags":null,"title":"Sketch Notes","uri":"/en/sketch-notes/#"},{"categories":["Nebula Graph"],"content":"nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience.","date":"2021-05-05","objectID":"/en/vscode-ngql/","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/"},{"categories":["Nebula Graph"],"content":" nGQL extension VSCode is built to integrate the Nebula Graph with VSCode for an awesome developer experience. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#"},{"categories":["Nebula Graph"],"content":"VS Code nGQL Syntax Highlight","date":"2021-05-05","objectID":"/en/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#vs-code-ngql-syntax-highlight"},{"categories":["Nebula Graph"],"content":"1 DownloadSearch ngql from the market or click here. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:1:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#download"},{"categories":["Nebula Graph"],"content":"2 Features Highlighting all Keywords, Functions of a given .ngql file ","date":"2021-05-05","objectID":"/en/vscode-ngql/:2:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#features"},{"categories":["Nebula Graph"],"content":"3 Release Notes","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#release-notes"},{"categories":["Nebula Graph"],"content":"3.1 0.0.1Initial release, only .ngql Syntax is supported. ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:1","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#001"},{"categories":["Nebula Graph"],"content":"3.2 0.0.2Lower supported vscode version till ^1.50.1 ","date":"2021-05-05","objectID":"/en/vscode-ngql/:3:2","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#002"},{"categories":["Nebula Graph"],"content":"4 Reference https://docs.nebula-graph.io/ https://github.com/vesoft-inc/nebula-graph/blob/master/src/parser/scanner.lex ","date":"2021-05-05","objectID":"/en/vscode-ngql/:4:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-nGQL, nGQL extension for VSCode","uri":"/en/vscode-ngql/#reference"},{"categories":["Big Data","Cloud"],"content":"How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub","date":"2021-05-03","objectID":"/en/nebula-insights/","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/"},{"categories":["Big Data","Cloud"],"content":" How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights. We used Google Cloud Scheduler, Google Cloud Functions, BigQuery, and codes are shared in GitHub 这是我首发在 Datawhale 的文章，介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。 引子 我们想要收集一些帮助 Nebula Graph 社区运营的 metrics，希望能从不同来源的数据自动化周期性收集、处理、并方便地展现出来做数据驱动分析的基础设施。 Nebula Graph 是一个现代的开源分布式图数据库(Graph Database)，欢迎同学们从: 官网: https://nebula-graph.com.cn Bilibili: https://space.bilibili.com/472621355 GitHub:https://github.com/vesoft-inc/nebula-graph 了解我们哈。 ","date":"2021-05-03","objectID":"/en/nebula-insights/:0:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#"},{"categories":["Big Data","Cloud"],"content":"1 需求 方便增加新的数据 数据收集无需人为触发（自动、周期性） 每天数据量不超过1000条 数据可以生成 dashboard，也可以支持统计分期 query 高可用，数据安全 低预算，尽可能不需要运维人力 ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#需求"},{"categories":["Big Data","Cloud"],"content":"1.1 需求分析我们需要搭建一个系统能实现 一个能周期性触发获取数据的事件的服务: scheduler 一个触发之后，把数据 ETL 到数据库中的服务: ETL worker 一个数据仓库 一个能够把数据库作为源，允许用户 query，展示数据的界面: Data-UI 这个需求的特点是虽然数据量很小、但是要求服务高可用、安全。因为这种情况下自建服务器还需要保证HA和数据安全会一定会消耗昂贵运维人力，所以我们应该尽量避免在自己维护的服务器中搭建 scheduler, 和数据库。 最终，我们选择了尽量使用公有云的 aaS 的方案: ┌──────────────────────────┐ │ │ │ Google Cloud Scheduler │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────┐ │ │ │ ┌────────────▼─────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ├───┤ └─────────────────────┘ │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌─────────▼─────────┐ │ │ │ │ │ │ │ │ │ Google BigQuery │ │ └─────────────────────────┘ │ │ ├───────────► ... └─────────▲─────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌──────────┴───────────┐ │ │ │ │ └──────────────────┘ │ Google Data Studio │ │ ┌──┐ │ │ ┌──┐ │ │ ┌──┐ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──┴──┴─┴──┴─┴──┴──────┘ 因为我个人比较熟悉 Google Cloud Platform(GCP)的原因，加上GCP在大数据处理上比较领先，再加上Google提供的 free tier额度非常大方，以至于在我们这个数据量下，所有workload都会是免费的。 这个方案最后选择了全栈 Google Cloud，然而，这实际上只是一个参考，同学们完全可以在其他公有云提供商那里找到对应的服务。 这里我简单介绍一下， Google Cloud Scheduler是自解释的，不用多介绍了。 而 Google Cloud Functions是GCP的无服务器(serverless)的 Function as a Service服务，它的好处是我们可以把无状态的 event-driven 的 workload 代码放上去，它是按需付费（pay as you go)的，类似的服务还有 Google Cloud Run，后者的区别在于我们提供的是一个docker/container（这使得能支持的运行环境可以使任何能跑在容器里的东西），而 Cloud Functions是把我们的代码文件放上去。他们的效果是类似的，因为我准备用Python来做 ETL的东西，Clouf Functions已经支持了，我就直接选择它了。 在scheduler里边，我定义了每一天它发一个 pub/sub（类似于kafka，这里google可以保证至少发成功一次）消息给 Cloud Functions，然后 Cloud Functions会去做 ETL的工作。 这里，实际上我的设计里这个触发的函数调会把数据从API那里获取下来，在内存里处理好之后，存储到在对象存储里为 JSON 文件，然后再调用 Google BigQuery 的 API让 BigQuery直接从对对象存储里拉取 JSON 文件，导入记录到相应的表之中。 Google BigQuery 作为GCP 特别有竞争力的一个产品，是它数据仓库，BigQuery 可以无限扩容，支持海量数据导入，支持 SQL-like 的 query，还自带ML算法，通过SQL就能调用这些算法。它可以和很多GCP以及第三方的组件可以集成起来。 Google Data Studio 是GCP的数据 Insights产品，如果大家用过 Google Analytics 应该已经用过它了。 ","date":"2021-05-03","objectID":"/en/nebula-insights/:1:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#需求分析"},{"categories":["Big Data","Cloud"],"content":"1.2 数据的获取，API我们第一阶段想要收集的数据来源是 GitHub 上，社区项目的统计数据、Docker Hub上，社区镜像的拉取计数，之后，会增加更多维度的数据。 Github API, ref: https://pygithub.readthedocs.io 这里我们利用了一个Github API的一个 Python 封装，下边是在 IDLE/iPython/Jupyter 里尝试的例子 # 实例化一个client g = Github(login_or_token=token, timeout=60, retry=Retry( total=10, status_forcelist=(500, 502, 504), backoff_factor=0.3)) # 配置好要获取的repo的信息 org_str = \"vesoft-inc\" org = g.get_organization(org_str) repos = org.get_repos() # 这里repos是一个迭代器，方便看到里边的东西，我们把它 list 一下可以看到所有的repo: list(repos) [Repository(full_name=\"vesoft-inc/nebula\"), Repository(full_name=\"vesoft-inc/nebula-docs\"), Repository(full_name=\"vesoft-inc/nebula-dev-docker\"), Repository(full_name=\"vesoft-inc/github-statistics\"), Repository(full_name=\"vesoft-inc/nebula-docker-compose\"), Repository(full_name=\"vesoft-inc/nebula-go\"), Repository(full_name=\"vesoft-inc/nebula-java\"), Repository(full_name=\"vesoft-inc/nebula-python\"), Repository(full_name=\"vesoft-inc/nebula-importer\"), Repository(full_name=\"vesoft-inc/nebula-third-party\"), Repository(full_name=\"vesoft-inc/nebula-storage\"), Repository(full_name=\"vesoft-inc/nebula-graph\"), Repository(full_name=\"vesoft-inc/nebula-common\"), Repository(full_name=\"vesoft-inc/nebula-stats-exporter\"), Repository(full_name=\"vesoft-inc/nebula-web-docker\"), Repository(full_name=\"vesoft-inc/nebula-bench\"), Repository(full_name=\"vesoft-inc/nebula-console\"), Repository(full_name=\"vesoft-inc/nebula-docs-cn\"), Repository(full_name=\"vesoft-inc/nebula-chaos\"), Repository(full_name=\"vesoft-inc/nebula-clients\"), Repository(full_name=\"vesoft-inc/nebula-spark-utils\"), Repository(full_name=\"vesoft-inc/nebula-node\"), Repository(full_name=\"vesoft-inc/nebula-rust\"), Repository(full_name=\"vesoft-inc/nebula-cpp\"), Repository(full_name=\"vesoft-inc/nebula-http-gateway\"), Repository(full_name=\"vesoft-inc/nebula-flink-connector\"), Repository(full_name=\"vesoft-inc/nebula-community\"), Repository(full_name=\"vesoft-inc/nebula-br\"), Repository(full_name=\"vesoft-inc/.github\")] # repo0 是 vesoft-inc/nebula 这个repo，我们可以通过 get_clones_traffic，get_views_traffic 来获取过去十几天的 clone，view 统计 In [16]: repo0.get_clones_traffic() Out[16]: {'count': 362, 'uniques': 150, 'clones': [Clones(uniques=5, timestamp=2021-04-06 00:00:00, count=16), Clones(uniques=8, timestamp=2021-04-07 00:00:00, count=23), Clones(uniques=13, timestamp=2021-04-08 00:00:00, count=30), Clones(uniques=33, timestamp=2021-04-09 00:00:00, count=45), Clones(uniques=2, timestamp=2021-04-10 00:00:00, count=13), Clones(uniques=6, timestamp=2021-04-11 00:00:00, count=19), Clones(uniques=15, timestamp=2021-04-12 00:00:00, count=28), Clones(uniques=40, timestamp=2021-04-13 00:00:00, count=54), Clones(uniques=9, timestamp=2021-04-14 00:00:00, count=21), Clones(uniques=10, timestamp=2021-04-15 00:00:00, count=34), Clones(uniques=10, timestamp=2021-04-16 00:00:00, count=23), Clones(uniques=5, timestamp=2021-04-17 00:00:00, count=17), Clones(uniques=2, timestamp=2021-04-18 00:00:00, count=13), Clones(uniques=9, timestamp=2021-04-19 00:00:00, count=23), Clones(uniques=3, timestamp=2021-04-20 00:00:00, count=3)]} In [17]: repo0.get_views_traffic() Out[17]: {'count': 6019, 'uniques': 1134, 'views': [View(uniques=52, timestamp=2021-04-06 00:00:00, count=169), View(uniques=143, timestamp=2021-04-07 00:00:00, count=569), View(uniques=152, timestamp=2021-04-08 00:00:00, count=635), View(uniques=134, timestamp=2021-04-09 00:00:00, count=648), View(uniques=81, timestamp=2021-04-10 00:00:00, count=318), View(uniques=42, timestamp=2021-04-11 00:00:00, count=197), View(uniques=127, timestamp=2021-04-12 00:00:00, count=515), View(uniques=149, timestamp=2021-04-13 00:00:00, count=580), View(uniques=134, timestamp=2021-04-14 00:00:00, count=762), View(uniques=141, timestamp=2021-04-15 00:00:00, count=385), View(uniques=113, timestamp=2021-04-16 00:00:00, count=284), View(uniques=48, timestamp=2021-04-17 00:00:00, count=168), View(uniques=35, timestamp=2021-04-18 00:00:00, count=135), View(uniques=124, timestamp=2021-04-19 00:00:00, cou","date":"2021-05-03","objectID":"/en/nebula-insights/:1:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数据的获取api"},{"categories":["Big Data","Cloud"],"content":"2 实现","date":"2021-05-03","objectID":"/en/nebula-insights/:2:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#实现"},{"categories":["Big Data","Cloud"],"content":"2.1 计划任务调度 with Cloud Scheduler前边提到，Scheduler --\u003e Functions 中间是通过消息队列实现的可靠事件触发，我们需要在 Google Cloud Pub/Sub里创建一个订阅消息，后边我们会把这个订阅消息从 Scheduler 定期发送，并且在 Function创建的时候定义为触发条件。 $ gcloud pubsub topics create nebula-insights-cron-topic $ gcloud pubsub subscriptions create cron-sub --topic nebula-insights-cron-topic 任务的创建非常直接，在 Scheduler Web Console 上直接图形化操作就可以了，记得要选择触发 Pub/Sub 消息为 cron-sub，消息主题为 nebula-insights-cron-topic ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#计划任务调度-with-cloud-scheduler"},{"categories":["Big Data","Cloud"],"content":"2.2 ETL Worker with Python + Google Functions当 Scheduler 每天定时发送消息之后，接收方就是我们要定义的 Google Functions了，它的定义如图 第一步，选择它的触发类型为 Pub/Sub，同时要定义消息的主题和名字。 第二步就是把代码放进去: ┌─────────────────────┐ │ │ ┌──────────────────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ◄───► └─────────────────────┘ │ │ │ └────────────▲─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌────────────▼────────────┐ │ │ │ │ │ │ │ │ │ Google Cloud Storage │ │ └─────────────────────────┘ │ │ ... └────────────┬────────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌─────────▼─────────┐ │ │ │ │ └──────────────────┘ │ Google BigQuery │ │ │ └───────────────────┘ 这部分的逻辑就是通过前边分析了的API取得信息，然后组装成需要的格式存到 Cloud Storage(对象存储），然后再导入到 BigQuery（数仓）之中，全部代码在GitHub上: https://github.com/wey-gu/nebula-insights/blob/main/functions/data-fetching-0/main.py 另外，可以参考这个官方教程 https://cloud.google.com/scheduler/docs/tut-pub-sub ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#etl-worker-with-python--google-functions"},{"categories":["Big Data","Cloud"],"content":"2.3 数仓表结构定义数仓的表结构比较直接，schema的图贴在下边了，值得注意的是，BigQuery支持嵌套的表结构（而不像一般关系型数据库那样需要把这样的逻辑结构用辅助表来表示），在我们这个场景下非常方便，比如release表中的 assets的三个嵌套字段。 更详细的信息可以参考GitHub上的介绍和代码: https://github.com/wey-gu/nebula-insights#data-etl-bigquery-and-gcs ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:3","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数仓表结构定义"},{"categories":["Big Data","Cloud"],"content":"2.4 数据可视化到这里，我们就可以自动在BigQuery里存有每天收集的不同来源的统计数据啦，有了它，我们可以借助 Data Studio 来生成各式各样的可视化表示。 参考 https://cloud.google.com/bigquery/docs/visualize-data-studio ","date":"2021-05-03","objectID":"/en/nebula-insights/:2:4","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#数据可视化"},{"categories":["Big Data","Cloud"],"content":"3 总结这样，我们实际上不需要任何认为维护的成本和投入，就搭建了一整个数据的流水线，并且只需要按照数据用量付费，在我们的数据量下，及时考虑未来增加数十个新的量度的收集，我们依然没有达到需要付费的用量，是不是很Cool？ 因为数据同时存在于对象存储与数仓里，我们可以方便随时把数据导入任意其他平台上。 BigQuery还有一些非常常用的，自带的机器学习的功能，只需要写一个SQL-Like的query就能触发然后获得预测结果，如果我们用到这些功能的话也会回到 datawhale 为同学们继续分享哈。 第一次做数据工程方面的分享，如果有错误的地方欢迎大家不吝指出哈~~ 谢谢！ ","date":"2021-05-03","objectID":"/en/nebula-insights/:3:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights, How we leverage Serverless Cloud infra to setup Data Pipeline for Nebula Graph Community Insights","uri":"/en/nebula-insights/#总结"},{"categories":["Nebula Graph"],"content":"A PoC utility for the newcomers or developers to bootstrap a nebula-graph playground in a oneliner command.","date":"2021-04-26","objectID":"/en/nebula-up/","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/"},{"categories":["Nebula Graph"],"content":" Update: the All-in-one mode is introduced! Check here and try it! Nebula-Up is PoC utility to enable developer to bootstrap an nebula-graph cluster with nebula-graph-studio(Web UI) + nebula-graph-console(Command UI) ready out of box in an oneliner run. All required packages will handled with nebula-up as well, including Docker on Linux(Ubuntu/CentOS), Docker Desktop on macOS(including both Intel and M1 chip based), and Docker Desktop Windows. Also, it’s optimized to leverage China Repo Mirrors(docker, brew, gitee, etc…) in case needed enable a smooth deployment for both Mainland China users and others. macOS and Linux with Shell: curl -fsSL nebula-up.siwei.io/install.sh | bash Note: you could specify the version of Nebula Graph like: curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v2.6 ","date":"2021-04-26","objectID":"/en/nebula-up/:0:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#"},{"categories":["Nebula Graph"],"content":"1 All-in-one modeWith all-in-one mode, you could play with many Nebula Tools in one command, too: Supported tools: Nebula Dashboard Nebula Graph Studio Nebula Graph Console Nebula BR(backup \u0026 restore) Nebula Graph Spark utils Nebula Graph Spark Connector/PySpark Nebula Graph Algorithm Nebula Graph Exchange Nebula Graph Importer Nebula Graph Fulltext Search Nebula Bench ","date":"2021-04-26","objectID":"/en/nebula-up/:1:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#all-in-one-mode"},{"categories":["Nebula Graph"],"content":"1.1 Install all in one # Install Nebula Core with all-in-one mode curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash ","date":"2021-04-26","objectID":"/en/nebula-up/:1:1","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-all-in-one"},{"categories":["Nebula Graph"],"content":"1.2 Install Nebula Core and One of the coponent: # Install Core with Backup and Restore with MinIO curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 br # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark # Install Core with Dashboard curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 dashboard ","date":"2021-04-26","objectID":"/en/nebula-up/:1:2","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#install-nebula-core-and-one-of-the-coponent"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be \u003chost-ip\u003e:9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#how-to-play-with-all-in-one-mode"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#console-and-basketballplayer-dataset-loading"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#monitor-the-whole-cluster-with-nebula-dashboard"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#access-nebula-graph-studio"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#query-data-with-nebula-spark-connector-in-pyspark-shell"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-exchange"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#run-nebula-graph-algorithm"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/en/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up, oneliner installer for Nebula Graph Playground","uri":"/en/nebula-up/#try-backup-and-restore-with-minio-as-storage"},{"categories":["Nebula Graph"],"content":"IPython-nGQL is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It's easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded.","date":"2021-03-07","objectID":"/en/ipython-ngql/","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/"},{"categories":["Nebula Graph"],"content":" ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It’s easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded. ipython-ngql is inspired by ipython-sql created by Catherine Devlin ","date":"2021-03-07","objectID":"/en/ipython-ngql/:0:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#"},{"categories":["Nebula Graph"],"content":"1 Get Started","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-started"},{"categories":["Nebula Graph"],"content":"1.1 Installationipython-ngql could be installed either via pip or from this git repo itself. Install via pip pip install ipython-ngql Install inside the repo git clone git@github.com:wey-gu/ipython-ngql.git cd ipython-ngql python setup.py install ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:1","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#installation"},{"categories":["Nebula Graph"],"content":"1.2 Load it in Jupyter Notebook or iPython %load_ext ngql ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:2","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#load-it-in-jupyter-notebook-or-ipython"},{"categories":["Nebula Graph"],"content":"1.3 Connect to Nebula GraphArguments as below are needed to connect a Nebula Graph DB instance: Argument Description --address or -addr IP address of the Nebula Graph Instance --port or -P Port number of the Nebula Graph Instance --user or -u User name --password or -p Password Below is an exmple on connecting to 127.0.0.1:9669 with username: “user” and password: “password”. %ngql --address 127.0.0.1 --port 9669 --user user --password password ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:3","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#connect-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"1.4 Make QueriesNow two kind of iPtython Magics are supported: Option 1: The one line stype with %ngql: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id; Option 2: The multiple lines stype with %%ngql %%ngql USE pokemon_club; SHOW TAGS; SHOW HOSTS; There will be other options in future, i.e. from a .ngql file. ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:4","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#make-queries"},{"categories":["Nebula Graph"],"content":"1.5 Query String with Variablesipython-ngql supports taking variables from the local namespace, with the help of Jinja2 template framework, it’s supported to have queries like the below example. The actual query string should be GO FROM \"Sue\" OVER owns_pokemon ..., and \"{{ trainer }}\" was renderred as \"Sue\" by consuming the local variable trainer: In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:5","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#query-string-with-variables"},{"categories":["Nebula Graph"],"content":"1.6 Configure ngql_result_styleBy default, ipython-ngql will use pandas dataframe as output style to enable more human readable output, while it’s supported to use the raw thrift data format comes from the nebula2-python itself. This can be done ad-hoc with below one line: %config IPythonNGQL.ngql_result_style=\"raw\" After above line being executed, the output will be like: ResultSet(ExecutionResponse( error_code=0, latency_in_us=2844, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) The result are always stored in variable _ in Jupyter Notebook, thus, to tweak the result, just refer a new var to it like: In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:6","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#configure-ngql_result_style"},{"categories":["Nebula Graph"],"content":"1.7 Get HelpDon’t remember anything or even relying on the cheatsheet here, oen takeaway for you: the help! In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:7","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#get-help"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#examples"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#jupyter-notebook"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/en/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph Jupyter Plugin","uri":"/en/ipython-ngql/#ipython"}]