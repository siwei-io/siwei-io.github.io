[{"categories":["Nebula Graph"],"content":"Nebula Graph 生态中有哪些 Spark 项目？ 本文为大家介绍 Spark-connector（包括 PySpark）， Nebula Algorithm 和 Nebula Exchange。","date":"2022-06-06","objectID":"/spark-on-nebula-graph/","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/"},{"categories":["Nebula Graph"],"content":" Nebula Graph 生态中有哪些 Spark 项目？ 本文为大家介绍 Spark-connector（包括 PySpark）， Nebula Algorithm 和 Nebula Exchange。 最近我试着搭建了方便大家一键试玩的 Nebula Graph 中的 Spark 相关的项目，今天就把它们整理成文分享给大家。而且，我趟出来了 PySpark 下的 Nebula Spark Connector 的使用方式，后边也会一并贡献到文档里。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:0:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#"},{"categories":["Nebula Graph"],"content":"1 Nebula Graph 的三个 Spark 子项目我曾经围绕 Nebula Graph 的所有数据导入方法画过一个草图，其中已经包含了 Spark Connector，Nebula Exchange 的简单介绍。在这篇文章中我将它们和另外的 Nebula Algorithm 进行稍微深入的探讨。 TL;DR Nebula Spark Connector 是一个 Spark Lib，它能让 Spark 应用程序能够以 dataframe 的形式从 Nebula Graph 中读取和写入图数据。 Nebula Exchange 建立在 Nebula Spark Connector 之上，作为一个 Spark Lib 同时可以直接被 Spark 提交 JAR 包执行的应用程序，它的设计目标是和 Nebula Graph 交换不同的数据源（对于开源版本，它是单向的：写入，而对于企业版本，它是双向的）。Nebula Exchange 支持的很多不同类型的数据源如：MySQL、Neo4j、PostgreSQL、ClickHouse、Hive 等。除了直接写入 Nebula Graph，它还可以选择生成 SST 文件，并将其注入 Nebula Graph，以便使用 Nebula Graph 集群之外算力帮助排序底层。 Nebula Algorithm，建立在 Nebula Spark Connector 和 GraphX 之上，也是一个Spark Lib 和 Spark 上的应用程序，它用来在 Nebula Graph 的图上运行常用的图算法（pagerank，LPA等）。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:1:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#nebula-graph-的三个-spark-子项目"},{"categories":["Nebula Graph"],"content":"2 Nebula Spark Connector 代码：https://github.com/vesoft-inc/nebula-spark-connector 文档：https://docs.nebula-graph.io/3.1.0/nebula-spark-connector/ JAR 包：https://repo1.maven.org/maven2/com/vesoft/nebula-spark-connector/ 代码例子：example ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#nebula-spark-connector"},{"categories":["Nebula Graph"],"content":"2.1 Nebula Graph Spark Reader为了从 Nebula Graph 中读取数据，比如读 vertex，Nebula Spark Connector 将扫描所有带有给定 TAG 的 Nebula StorageD，比如这样表示扫描 player 这个 TAG ：withLabel(\"player\")，我们还可以指定 vertex 的属性：withReturnCols(List(\"name\", \"age\"))。 指定好所有的读 TAG 相关的配置之后，调用 spark.read.nebula.loadVerticesToDF 返回得到的就是扫描 Nebula Graph 之后转换为 Dataframe 的图数据，像这样： def readVertex(spark: SparkSession): Unit = { LOG.info(\"start to read nebula vertices\") val config = NebulaConnectionConfig .builder() .withMetaAddress(\"metad0:9559,metad1:9559,metad2:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"basketballplayer\") .withLabel(\"player\") .withNoColumn(false) .withReturnCols(List(\"name\", \"age\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() vertex.printSchema() vertex.show(20) println(\"vertex count: \" + vertex.count()) } 写入的例子我这里不列出，不过，前边给出的代码示例的链接里是有更详细的例子，这里值得一提的是，Spark Connector 读数据为了满足图分析、图计算的大量数据场景，和大部分其他客户端非常不同，它直接绕过了 GraphD，通过扫描 MetaD 和 StorageD 获得数据，但是写入的情况则是通过 GraphD 发起 nGQL DML 语句写入的。 接下来我们来做一个上手练习吧。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#nebula-graph-spark-reader"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo \u003e example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#上手-nebula-spark-connector"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#拉起环境"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#进入-spark-环境"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#跑-spark-connector-的例子"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#选项-1推荐通过-pyspark"},{"categories":["Nebula Graph"],"content":"2.2 上手 Nebula Spark Connector先决条件：假设下面的程序是在一台有互联网连接的 Linux 机器上运行的，最好是预装了 Docker 和 Docker-Compose。 2.2.1 拉起环境首先，让我们用 Nebula-Up 部署基于容器的 Nebula Graph Core v3、Nebula Studio、Nebula Console 和 Spark、Hadoop 环境，如果还没安装好它也会尝试为我们安装 Docker 和 Docker-Compose。 # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark 你知道吗 Nebula-UP 可以一键装更多东西，如果你的环境配置大一点（比如 8 GB RAM）curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash 可以装更多东西，但是请注意 Nebula-UP 不是为生产环境准备的。 上述边脚本执行后，让我们用 Nebula-Console（Nebula Graph 的命令行客户端）来连接它。 # Connect to nebula with console ~/.nebula-up/console.sh # Execute any queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" 加载一份数据进去，并执行一个图查询： # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # 等一分钟左右 # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 2.2.2 进入 Spark 环境执行下面这一行，我们就可以进入到 Spark 环境： docker exec -it spark_master_1 bash 如果我们想执行编译，可以在里边安装 mvn： docker exec -it spark_master_1 bash # in the container shell export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 2.2.3 跑 Spark Connector 的例子2.2.3.1 选项 1（推荐）：通过 PySpark 进入 PySpark Shell ~/.nebula-up/nebula-pyspark.sh 调用 Nebula Spark Reader # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit of 2 df.show(n=2) 返回结果例子 ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 2.2.3.2 选项 2：编译、提交示例 JAR 包 先克隆 Spark Connector 和它示例代码的代码仓库，然后编译： 注意，我们使用了 master 分支，因为当下 master 分支是兼容 3.x 的，一定要保证 spark connector 和数据库内核版本是匹配的，版本对应关系参考代码仓库的 README.md 。 cd ~/.nebula-up/nebula-up/spark git clone https://github.com/vesoft-inc/nebula-spark-connector.git docker exec -it spark_master_1 bash cd /root/nebula-spark-connector 替换示例项目的代码 echo example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala vi example/src/main/scala/com/vesoft/nebula/examples/connector/NebulaSparkReaderExample.scala 把如下的代码粘贴进去，这里边我们对前边加载的图： basketballplayer 上做了顶点和边的读操作：分别调用 readVertex 和 readEdges。 package com.vesoft.nebula.examples.connector import com.facebook.thrift.protocol.TCompactProtocol import com.vesoft.nebula.connector.connector.NebulaDataFrameReader import com.vesoft.nebula.connector.{NebulaConnectionConfig, ReadNebulaConfig} import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory object NebulaSparkReaderExample { private val LOG = LoggerFactory.getLogger(this.getClass) def main(args: Array[String]): Unit = { val sparkConf = new SparkConf sparkConf .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array[Class[_]](classOf[TComp","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:2:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#选项-2编译提交示例-jar-包"},{"categories":["Nebula Graph"],"content":"3 Nebula Exchange 代码：https://github.com/vesoft-inc/nebula-exchange/ 文档：https://docs.nebula-graph.com.cn/3.1.0/nebula-exchange/about-exchange/ex-ug-what-is-exchange/ JAR 包：https://github.com/vesoft-inc/nebula-exchange/releases 配置例子： exchange-common/src/test/resources/application.conf Nebula Exchange 是一个 Spark Lib，也是一个可以直接提交执行的 Spark 应用，它被用来从多个数据源读取数据写入 Nebula Graph 或者输出 Nebula Graph SST 文件。 通过 spark-submit 的方式使用 Nebula Exchange 的方法很直接： 首先创建配置文件，让 Exchange 知道应该如何获取和写入数据 然后用指定的配置文件调用 Exchange 包 现在，让我们用上一章中创建的相同环境做一个实际测试。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:3:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#nebula-exchange"},{"categories":["Nebula Graph"],"content":"3.1 一键试玩 Exchange3.1.1 先跑起来看看吧 请参考前边拉起环境这一章节，先一键装好环境。 一键执行： ~/.nebula-up/nebula-exchange-example.sh 恭喜你，已经第一次执行成功一个 Exchange 的数据导入任务啦！ 3.1.2 再看看一些细节这个例子里，我们实际上是用 Exchange 从 CSV 文件这一其中支持的数据源中读取数据写入 Nebula Graph 集群的。这个 CSV 文件中第一列是顶点 ID，第二和第三列是 “姓名 “和 “年龄 “的属性： player800,\"Foo Bar\",23 player801,\"Another Name\",21 咱们可以进到 Spark 环境里看看 docker exec -it spark_master_1 bash cd /root 可以看到我们提交 Exchange 任务时候指定的配置文件 exchange.conf 它是一个 HOCON 格式的文件： 在 .nebula 中描述了 Nebula Graph 集群的相关信息 在 .tags 中描述了如何将必填字段对应到我们的数据源（这里是 CSV 文件）等有关 Vertecies 的信息。 {#Sparkrelationconfigspark:{app:{name:NebulaExchange}master:localdriver:{cores:1maxResultSize:1G}executor:{memory:1G}cores:{max:16}}#NebulaGraphrelationconfignebula:{address:{graph:[\"graphd:9669\"]meta:[\"metad0:9559\",\"metad1:9559\",\"metad2:9559\"]}user:rootpswd:nebulaspace:basketballplayer#parametersforSSTimport,notrequiredpath:{local:\"/tmp\"remote:\"/sst\"hdfs.namenode:\"hdfs://localhost:9000\" }#nebulaclientconnectionparametersconnection{#socketconnect\u0026executetimeout,unit:millisecondtimeout:30000}error:{#maxnumberoffailures,ifthenumberoffailuresisbiggerthanmax,thenexittheapplication.max:32#failedimportjobwillberecordedinoutputpathoutput:/tmp/errors}#usegoogle'sRateLimitertolimittherequestssendtoNebulaGraphrate:{#thestablethroughputofRateLimiterlimit:1024#AcquiresapermitfromRateLimiter,unit:MILLISECONDS#ifitcan'tbeobtainedwithinthespecifiedtimeout,thengiveuptherequest.timeout:1000}}#Processingtags#TherearetagconfigexamplesfordifferentdataSources.tags:[#HDFScsv#Importmodeisclient,justchangetype.sinktosstifyouwanttouseclientimportmode.{name:playertype:{source:csvsink:client}path:\"file:///root/player.csv\" #ifyourcsvfilehasnoheader,thenuse_c0,_c1,_c2,..toindicatefieldsfields:[_c1,_c2]nebula.fields:[name,age]vertex:{field:_c0}separator:\",\"header:falsebatch:256partition:32}]} 我们应该能看到那个 CSV 数据源和这个配置文件都在同一个目录下了： bash-5.0# ls -l total 24 drwxrwxr-x 2 1000 1000 4096 Jun 1 04:26 download -rw-rw-r-- 1 1000 1000 1908 Jun 1 04:23 exchange.conf -rw-rw-r-- 1 1000 1000 2593 Jun 1 04:23 hadoop.env drwxrwxr-x 7 1000 1000 4096 Jun 6 03:27 nebula-spark-connector -rw-rw-r-- 1 1000 1000 51 Jun 1 04:23 player.csv 然后，实际上我们可以手动再次提交一下这个 Exchange 任务 /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange download/nebula-exchange.jar \\ -c exchange.conf 部分返回结果 22/06/06 03:56:26 INFO Exchange$: Processing Tag player 22/06/06 03:56:26 INFO Exchange$: field keys: _c1, _c2 22/06/06 03:56:26 INFO Exchange$: nebula keys: name, age 22/06/06 03:56:26 INFO Exchange$: Loading CSV files from file:///root/player.csv ... 22/06/06 03:56:41 INFO Exchange$: import for tag player cost time: 3.35 s 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchFailure.player: 0 ... 更多的数据源，请参考文档和配置的例子。 关于 Exchange 输出 SST 文件的实践，你可以参考文档和我的旧文 Nebula Exchange SST 2.x实践指南。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#一键试玩-exchange"},{"categories":["Nebula Graph"],"content":"3.1 一键试玩 Exchange3.1.1 先跑起来看看吧 请参考前边拉起环境这一章节，先一键装好环境。 一键执行： ~/.nebula-up/nebula-exchange-example.sh 恭喜你，已经第一次执行成功一个 Exchange 的数据导入任务啦！ 3.1.2 再看看一些细节这个例子里，我们实际上是用 Exchange 从 CSV 文件这一其中支持的数据源中读取数据写入 Nebula Graph 集群的。这个 CSV 文件中第一列是顶点 ID，第二和第三列是 “姓名 “和 “年龄 “的属性： player800,\"Foo Bar\",23 player801,\"Another Name\",21 咱们可以进到 Spark 环境里看看 docker exec -it spark_master_1 bash cd /root 可以看到我们提交 Exchange 任务时候指定的配置文件 exchange.conf 它是一个 HOCON 格式的文件： 在 .nebula 中描述了 Nebula Graph 集群的相关信息 在 .tags 中描述了如何将必填字段对应到我们的数据源（这里是 CSV 文件）等有关 Vertecies 的信息。 {#Sparkrelationconfigspark:{app:{name:NebulaExchange}master:localdriver:{cores:1maxResultSize:1G}executor:{memory:1G}cores:{max:16}}#NebulaGraphrelationconfignebula:{address:{graph:[\"graphd:9669\"]meta:[\"metad0:9559\",\"metad1:9559\",\"metad2:9559\"]}user:rootpswd:nebulaspace:basketballplayer#parametersforSSTimport,notrequiredpath:{local:\"/tmp\"remote:\"/sst\"hdfs.namenode:\"hdfs://localhost:9000\" }#nebulaclientconnectionparametersconnection{#socketconnect\u0026executetimeout,unit:millisecondtimeout:30000}error:{#maxnumberoffailures,ifthenumberoffailuresisbiggerthanmax,thenexittheapplication.max:32#failedimportjobwillberecordedinoutputpathoutput:/tmp/errors}#usegoogle'sRateLimitertolimittherequestssendtoNebulaGraphrate:{#thestablethroughputofRateLimiterlimit:1024#AcquiresapermitfromRateLimiter,unit:MILLISECONDS#ifitcan'tbeobtainedwithinthespecifiedtimeout,thengiveuptherequest.timeout:1000}}#Processingtags#TherearetagconfigexamplesfordifferentdataSources.tags:[#HDFScsv#Importmodeisclient,justchangetype.sinktosstifyouwanttouseclientimportmode.{name:playertype:{source:csvsink:client}path:\"file:///root/player.csv\" #ifyourcsvfilehasnoheader,thenuse_c0,_c1,_c2,..toindicatefieldsfields:[_c1,_c2]nebula.fields:[name,age]vertex:{field:_c0}separator:\",\"header:falsebatch:256partition:32}]} 我们应该能看到那个 CSV 数据源和这个配置文件都在同一个目录下了： bash-5.0# ls -l total 24 drwxrwxr-x 2 1000 1000 4096 Jun 1 04:26 download -rw-rw-r-- 1 1000 1000 1908 Jun 1 04:23 exchange.conf -rw-rw-r-- 1 1000 1000 2593 Jun 1 04:23 hadoop.env drwxrwxr-x 7 1000 1000 4096 Jun 6 03:27 nebula-spark-connector -rw-rw-r-- 1 1000 1000 51 Jun 1 04:23 player.csv 然后，实际上我们可以手动再次提交一下这个 Exchange 任务 /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange download/nebula-exchange.jar \\ -c exchange.conf 部分返回结果 22/06/06 03:56:26 INFO Exchange$: Processing Tag player 22/06/06 03:56:26 INFO Exchange$: field keys: _c1, _c2 22/06/06 03:56:26 INFO Exchange$: nebula keys: name, age 22/06/06 03:56:26 INFO Exchange$: Loading CSV files from file:///root/player.csv ... 22/06/06 03:56:41 INFO Exchange$: import for tag player cost time: 3.35 s 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchFailure.player: 0 ... 更多的数据源，请参考文档和配置的例子。 关于 Exchange 输出 SST 文件的实践，你可以参考文档和我的旧文 Nebula Exchange SST 2.x实践指南。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#先跑起来看看吧"},{"categories":["Nebula Graph"],"content":"3.1 一键试玩 Exchange3.1.1 先跑起来看看吧 请参考前边拉起环境这一章节，先一键装好环境。 一键执行： ~/.nebula-up/nebula-exchange-example.sh 恭喜你，已经第一次执行成功一个 Exchange 的数据导入任务啦！ 3.1.2 再看看一些细节这个例子里，我们实际上是用 Exchange 从 CSV 文件这一其中支持的数据源中读取数据写入 Nebula Graph 集群的。这个 CSV 文件中第一列是顶点 ID，第二和第三列是 “姓名 “和 “年龄 “的属性： player800,\"Foo Bar\",23 player801,\"Another Name\",21 咱们可以进到 Spark 环境里看看 docker exec -it spark_master_1 bash cd /root 可以看到我们提交 Exchange 任务时候指定的配置文件 exchange.conf 它是一个 HOCON 格式的文件： 在 .nebula 中描述了 Nebula Graph 集群的相关信息 在 .tags 中描述了如何将必填字段对应到我们的数据源（这里是 CSV 文件）等有关 Vertecies 的信息。 {#Sparkrelationconfigspark:{app:{name:NebulaExchange}master:localdriver:{cores:1maxResultSize:1G}executor:{memory:1G}cores:{max:16}}#NebulaGraphrelationconfignebula:{address:{graph:[\"graphd:9669\"]meta:[\"metad0:9559\",\"metad1:9559\",\"metad2:9559\"]}user:rootpswd:nebulaspace:basketballplayer#parametersforSSTimport,notrequiredpath:{local:\"/tmp\"remote:\"/sst\"hdfs.namenode:\"hdfs://localhost:9000\" }#nebulaclientconnectionparametersconnection{#socketconnect\u0026executetimeout,unit:millisecondtimeout:30000}error:{#maxnumberoffailures,ifthenumberoffailuresisbiggerthanmax,thenexittheapplication.max:32#failedimportjobwillberecordedinoutputpathoutput:/tmp/errors}#usegoogle'sRateLimitertolimittherequestssendtoNebulaGraphrate:{#thestablethroughputofRateLimiterlimit:1024#AcquiresapermitfromRateLimiter,unit:MILLISECONDS#ifitcan'tbeobtainedwithinthespecifiedtimeout,thengiveuptherequest.timeout:1000}}#Processingtags#TherearetagconfigexamplesfordifferentdataSources.tags:[#HDFScsv#Importmodeisclient,justchangetype.sinktosstifyouwanttouseclientimportmode.{name:playertype:{source:csvsink:client}path:\"file:///root/player.csv\" #ifyourcsvfilehasnoheader,thenuse_c0,_c1,_c2,..toindicatefieldsfields:[_c1,_c2]nebula.fields:[name,age]vertex:{field:_c0}separator:\",\"header:falsebatch:256partition:32}]} 我们应该能看到那个 CSV 数据源和这个配置文件都在同一个目录下了： bash-5.0# ls -l total 24 drwxrwxr-x 2 1000 1000 4096 Jun 1 04:26 download -rw-rw-r-- 1 1000 1000 1908 Jun 1 04:23 exchange.conf -rw-rw-r-- 1 1000 1000 2593 Jun 1 04:23 hadoop.env drwxrwxr-x 7 1000 1000 4096 Jun 6 03:27 nebula-spark-connector -rw-rw-r-- 1 1000 1000 51 Jun 1 04:23 player.csv 然后，实际上我们可以手动再次提交一下这个 Exchange 任务 /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange download/nebula-exchange.jar \\ -c exchange.conf 部分返回结果 22/06/06 03:56:26 INFO Exchange$: Processing Tag player 22/06/06 03:56:26 INFO Exchange$: field keys: _c1, _c2 22/06/06 03:56:26 INFO Exchange$: nebula keys: name, age 22/06/06 03:56:26 INFO Exchange$: Loading CSV files from file:///root/player.csv ... 22/06/06 03:56:41 INFO Exchange$: import for tag player cost time: 3.35 s 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchSuccess.player: 2 22/06/06 03:56:41 INFO Exchange$: Client-Import: batchFailure.player: 0 ... 更多的数据源，请参考文档和配置的例子。 关于 Exchange 输出 SST 文件的实践，你可以参考文档和我的旧文 Nebula Exchange SST 2.x实践指南。 ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:3:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#再看看一些细节"},{"categories":["Nebula Graph"],"content":"4 Nebula Algorithm 代码仓库： https://github.com/vesoft-inc/nebula-algorithm 文档：https://docs.nebula-graph.com.cn/3.1.0/nebula-algorithm/ JAR 包：https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ 示例代码：example/src/main/scala/com/vesoft/nebula/algorithm ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:4:0","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#nebula-algorithm"},{"categories":["Nebula Graph"],"content":"4.1 通过 spark-submit 提交任务 我在这个代码仓库里给出了例子，今天我们借助 Nebula-UP 可以更方便体验它。 参考前边拉起环境这一章节，先一键装好环境。 在如上通过 Nebula-UP 的 Spark 模式部署了需要的依赖之后 加载 LiveJournal 数据集 ~/.nebula-up/load-LiveJournal-dataset.sh 在 LiveJournal 数据集上执行一个 PageRank 算法，结果输出到 CSV 文件中 ~/.nebula-up/nebula-algo-pagerank-example.sh 检查输出结果： docker exec -it spark_master_1 bash head /output/part*000.csv _id,pagerank 637100,0.9268620883822242 108150,1.1855749056722755 957460,0.923720299211093 257320,0.9967932799358413 4.1.1 配置文件解读完整文件在这里，这里，我们介绍一下主要的字段： .data 指定了源是 Nebula，表示从集群获取图数据，输出sink是 csv，表示写到本地文件里。 data: { # data source. optional of nebula,csv,json source: nebula # data sink, means the algorithm result will be write into this sink. optional of nebula,csv,text sink: csv # if your algorithm needs weight hasWeight: false } .nebula.read 规定了读 Nebula Graph 集群的对应关系，这里是读取所有 edge type: follow 的边数据为一整张图 nebula: { # algo's data source from Nebula. If data.source is nebula, then this nebula.read config can be valid. read: { # Nebula metad server address, multiple addresses are split by English comma metaAddress: \"metad0:9559\" # Nebula space space: livejournal # Nebula edge types, multiple labels means that data from multiple edges will union together labels: [\"follow\"] # Nebula edge property name for each edge type, this property will be as weight col for algorithm. # Make sure the weightCols are corresponding to labels. weightCols: [] } .algorithm 里配置了我们要调用的算法，和算法的配置 algorithm: { executeAlgo: pagerank # PageRank parameter pagerank: { maxIter: 10 resetProb: 0.15 # default 0.15 } ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#通过-spark-submit-提交任务"},{"categories":["Nebula Graph"],"content":"4.1 通过 spark-submit 提交任务 我在这个代码仓库里给出了例子，今天我们借助 Nebula-UP 可以更方便体验它。 参考前边拉起环境这一章节，先一键装好环境。 在如上通过 Nebula-UP 的 Spark 模式部署了需要的依赖之后 加载 LiveJournal 数据集 ~/.nebula-up/load-LiveJournal-dataset.sh 在 LiveJournal 数据集上执行一个 PageRank 算法，结果输出到 CSV 文件中 ~/.nebula-up/nebula-algo-pagerank-example.sh 检查输出结果： docker exec -it spark_master_1 bash head /output/part*000.csv _id,pagerank 637100,0.9268620883822242 108150,1.1855749056722755 957460,0.923720299211093 257320,0.9967932799358413 4.1.1 配置文件解读完整文件在这里，这里，我们介绍一下主要的字段： .data 指定了源是 Nebula，表示从集群获取图数据，输出sink是 csv，表示写到本地文件里。 data: { # data source. optional of nebula,csv,json source: nebula # data sink, means the algorithm result will be write into this sink. optional of nebula,csv,text sink: csv # if your algorithm needs weight hasWeight: false } .nebula.read 规定了读 Nebula Graph 集群的对应关系，这里是读取所有 edge type: follow 的边数据为一整张图 nebula: { # algo's data source from Nebula. If data.source is nebula, then this nebula.read config can be valid. read: { # Nebula metad server address, multiple addresses are split by English comma metaAddress: \"metad0:9559\" # Nebula space space: livejournal # Nebula edge types, multiple labels means that data from multiple edges will union together labels: [\"follow\"] # Nebula edge property name for each edge type, this property will be as weight col for algorithm. # Make sure the weightCols are corresponding to labels. weightCols: [] } .algorithm 里配置了我们要调用的算法，和算法的配置 algorithm: { executeAlgo: pagerank # PageRank parameter pagerank: { maxIter: 10 resetProb: 0.15 # default 0.15 } ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:4:1","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#配置文件解读"},{"categories":["Nebula Graph"],"content":"4.2 作为一个库在 Spark 中调用 Nebula Algoritm请注意另一方面，我们可以将 Nebula Algoritm 作为一个库调用，它的好处在于： 对算法的输出格式有更多的控制/定制功能 可以对非数字 ID 的情况进行转换，见这里 这里我先不给出例子了，如果大家感兴趣可以给 Nebula-UP 提需求，我也会增加相应的例子。 题图来源： Sander ","date":"2022-06-06","objectID":"/spark-on-nebula-graph/:4:2","series":null,"tags":["Nebula Graph","Spark","PySpark"],"title":"一文了解 Nebula Graph 上的 Spark 项目","uri":"/spark-on-nebula-graph/#作为一个库在-spark-中调用-nebula-algoritm"},{"categories":["Nebula Graph"],"content":"得益于 Nebula 的原生 ARM64v8 的支持，在树莓派等 ARM 单板上跑 Nebula Graph 也非常容易。","date":"2022-03-23","objectID":"/nebula-graph-on-pi/","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/"},{"categories":["Nebula Graph"],"content":" 得益于 Nebula 的原生 ARM64v8 的支持，在树莓派等 ARM 单板上跑 Nebula Graph 非常容易。 ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:0:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#"},{"categories":["Nebula Graph"],"content":"1 背景最近，在 Nebula Graph 社区 Yee 老师的（再）一次修复了 Nebula Graph 的构建依赖的 ARM 支持问题（nebula-third-party#37）之后，我们又可以愉快地在 M1 Mac 上玩这个分布式开源图数据库了。 苦于树莓派的价格，一直没找机会把 Nebula 跑在小板子上玩玩。至于为什么要跑在树莓派上我的回答当然是 Because I can 在非常非常边缘计算的场景下（这里挖个坑，我一定要找一个这样的场景分享出来）。 终于，一周多之前在 @laixintao 和 @andelf 的一个讨论下我决定找一个树莓派的 alternative，最后下单了 Rock Pi 3A，在因为深圳疫情影响下拖到了这个礼拜才终于发货了！ 它看起来真的很棒！ ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:1:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#背景"},{"categories":["Nebula Graph"],"content":"2 在 ARM64 板子上装 Nebula Graph 图数据库 实际上 Nebula Graph 在 3.0 之后提供了一个单机版，这使得 Nebula 在边缘计算情况下有了更小的 footprint，不过这次我还没有使用这个版本，下次试试再给大家分享。 我在附录列出了安装 Ubuntu Server 的步骤，这里假设大家已经在树莓派或者其他单板 ARM 电脑里拉起来了 64 位的 Linux Server。 ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:2:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#在-arm64-板子上装-nebula-graph-图数据库"},{"categories":["Nebula Graph"],"content":"2.1 第 0 步，安装 Docker 和 Docker-Compose这里，我假设是 Debian/Ubuntu，其他分发版直接参考这里就好。 sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release echo \\ \"deb [arch=$(dpkg --print-architecture)signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # follow https://docs.docker.com/engine/install/linux-postinstall/ sudo groupadd docker sudo usermod -aG docker $USER exit # login again newgrp docker 安装好了 Docker 之后，安装 Compose，它 Docker 官方的步骤是有问题的，因为它其实是一个 Python 的包，我们通过 PIP 去装就好了。 sudo apt-get install -y python3 python3-pip sudo pip3 install docker-compose ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:2:1","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#第-0-步安装-docker-和-docker-compose"},{"categories":["Nebula Graph"],"content":"2.2 第 1 步，拉起 Nebula Graph首先，我们克隆 Nebula Docker Compose 这个 Repo，在 Master Branch，用 Compose 把服务拉起来。 git clone https://github.com/vesoft-inc/nebula-docker-compose.git \u0026\u0026 cd nebula-docker-compose docker-compose up -d 然后，我们下载 Console，连上 GraphD 服务。 wget https://github.com/vesoft-inc/nebula-console/releases/download/v3.0.0/nebula-console-linux-arm64-v3.0.0 chmod +x nebula-console-linux-arm64-v3.0.0 ./nebula-console-linux-arm64-v3.0.0 -addr localhost -port 9669 -u root -p nebula 并激活 Storage 服务。 ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779; SHOW HOSTS; ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:2:2","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#第-1-步拉起-nebula-graph"},{"categories":["Nebula Graph"],"content":"2.3 第 2 步，玩转 Nebula Graph on Pi这时候，透过 SHOW HOSTS 看到三个 StorageD 服务都是 ONLINE 之后，我们可以给 Nebula 里加载进去测试数据集。 $:play basketballplayer; 差不多一分钟之后，数据库加载成功，我们进入这个图空间，玩一下吧！ USE basketballplayer; GO FROM \"player100\" OVER follow YIELD dst(edge); Check this out and… Happy Graphing! ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:2:3","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#第-2-步玩转-nebula-graph-on-pi"},{"categories":["Nebula Graph"],"content":"3 附录：安装 Ubuntu Server 在 Rock Pi 3A 上 准备一个 micro SD card，在 https://wiki.radxa.com/Rock3/downloads 下载镜像，解压为 .img 文件 把镜像写进 SD card，比如用 etcher 插入电源（5V，3A）启动！ feature image credit: @_louisreed ","date":"2022-03-23","objectID":"/nebula-graph-on-pi/:3:0","series":null,"tags":["Nebula Graph","ARM","Respberry Pi"],"title":"Nebula Graph on Pi","uri":"/nebula-graph-on-pi/#附录安装-ubuntu-server-在-rock-pi-3a-上"},{"categories":["Nebula Graph"],"content":"我发现用 Nebula Graph 的图查询解 Antfu 的汉兜特别有意思！","date":"2022-02-28","objectID":"/resolve-wordle/","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/"},{"categories":["Nebula Graph"],"content":" 我发现用 Nebula Graph 的图查询解 Antfu 的汉兜（最好的中文成语版 wordle 👉🏻 handle.antfu.me）特别有意思，很适合每天写图库语句的体操练习，本文揭示如何用知识图谱作弊解汉兜😁 ","date":"2022-02-28","objectID":"/resolve-wordle/:0:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#"},{"categories":["Nebula Graph"],"content":"1 什么是汉兜？汉兜（https://handle.antfu.me）是由 Vue/Vite 核心团队的 Antfu 的又一个非常酷的作品，一个非常精致的汉字版的 Wordle，他是是一个每日挑战的填字游戏的中文成语版。 每天，汉兜会发起一个猜成语挑战，人们要在十次内才对它才能获胜，每一步之后都会收到相应的文字、声母、韵母、声调的匹配情况的提示，其中：绿色表示这个因素存在并且位置匹配、橘色表示这个元素存在但是位置不对，详细的规则可见如下的网页截图： 汉兜的乐趣就我们在于在有限的尝试过程中，在大脑中搜寻可能的答案，不断去逼近真理，任何试图作弊、讨巧去泄漏结果的行为都是很无趣、倒胃口的（比如从开源的汉兜代码里窃取信息），这个过程就像在做大脑的体操。 说到大脑的成语词汇量体操，我突然想到，为什么我们不能在大脑之外造一个汉语成语知识图谱，然后基于这个图谱去做图数据库查询语法体操呢？ ","date":"2022-02-28","objectID":"/resolve-wordle/:1:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#什么是汉兜"},{"categories":["Nebula Graph"],"content":"2 构造解决汉兜的成语知识图谱","date":"2022-02-28","objectID":"/resolve-wordle/:2:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#构造解决汉兜的成语知识图谱"},{"categories":["Nebula Graph"],"content":"2.1 什么是知识图谱？简单来说，知识图谱是一个连接实体之间关联关系的网络，它最初由 Google 提出并用来满足搜索引擎中基于知识推理才可获得（而不是网页倒排索引）的搜索问题，比如：”姚明妻子的年龄？“、”火箭队得过几次总冠军？“ 这里边，我们关注的条件。到 2022 年的现在，知识图谱已经被广泛应用在推荐系统、问答系统、安全风控等等更多搜索之外的领域。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:1","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#什么是知识图谱"},{"categories":["Nebula Graph"],"content":"2.2 为什么需要用知识图谱解决汉兜？原因就是：because I can 实际上，我们在大脑中解决字谜游戏的过程像极了图谱网络中的信息搜寻的过程，汉兜的解谜反馈提示条件天然适合被用图谱的语义来进行表达。在本文后边，你们会发现解谜条件翻译成图语义是非常非常自然的，这个问题就像是一个天然的为图谱而存在的练习一样，我相信这和知识图谱的结构和人脑中的知识结构非常接近有很大的关系。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:2","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#为什么需要用知识图谱解决汉兜"},{"categories":["Nebula Graph"],"content":"2.3 如何构建面向汉兜解谜的知识图谱？知识图谱是由实体（顶点）和关系（边）组成的，用图数据库管理系统（Graph Database MS）可以很方便进行知识的入库、更改、查询、甚至可视化探索。 在本文里，我将利用开源的分布式图数据库 Nebula Graph 开实践这个过程，具体图谱系统的搭建我都会放在文末。 在本章，我们只讨论图谱的建模：如何面向汉兜的解谜去设计“实体”与“关系”。 2.3.1 图建模2.3.1.1 最初的想法首先，一定存在的实体是： 成语 汉字 成语-[包含]-\u003e汉字，每个汉字-[读作]-\u003e读音。 其次，因为解谜过程中涉及到了声母、韵母以及声调的条件，考虑到图谱本身的量级非常小（千级别），而且字的读音是一对多的关系，我把读音和声母（包涵声母-initial和韵母-final）也作为实体，他们之间的关系则是顺理成章了： 2.3.1.2 最终的版本然而，我在后边基于图谱进行查询的时候发现最初的建模会使得(成语)–\u003e(字)–\u003e(读音)查询过程中丢失了这个字特定的读法的条件，所以我最终的建模是： 这样，纯文字的条件只涉及了(成语)--\u003e(字) 这一跳，而读音、声母、声调的条件则是另一条关系路径，既没有最初版本条件的冗余，又可以在一个路径模式匹配里带上两种条件（后边的例子里会涉及这样的表达）。 2.3.2 构建成语知识图谱有了建模、这么简单的图谱的构建就剩下了数据的收集、清洗和入库。 对于所有成语数据和他们的读音，我一方面直接抽取了汉兜代码内部的数据、另一方面利用 PyPinyin 这个开源的 Python 库将汉兜数据中没有读音的数据获得读音，同时，我也用到了 PyPinyin 里的很多方便的函数比如获取一个拼音的声母、韵母。 构建工具的代码在这里：https://github.com/wey-gu/chinese-graph 更多信息我也放在文末的附录之中。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:3","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#如何构建面向汉兜解谜的知识图谱"},{"categories":["Nebula Graph"],"content":"2.3 如何构建面向汉兜解谜的知识图谱？知识图谱是由实体（顶点）和关系（边）组成的，用图数据库管理系统（Graph Database MS）可以很方便进行知识的入库、更改、查询、甚至可视化探索。 在本文里，我将利用开源的分布式图数据库 Nebula Graph 开实践这个过程，具体图谱系统的搭建我都会放在文末。 在本章，我们只讨论图谱的建模：如何面向汉兜的解谜去设计“实体”与“关系”。 2.3.1 图建模2.3.1.1 最初的想法首先，一定存在的实体是： 成语 汉字 成语-[包含]-汉字，每个汉字-[读作]-读音。 其次，因为解谜过程中涉及到了声母、韵母以及声调的条件，考虑到图谱本身的量级非常小（千级别），而且字的读音是一对多的关系，我把读音和声母（包涵声母-initial和韵母-final）也作为实体，他们之间的关系则是顺理成章了： 2.3.1.2 最终的版本然而，我在后边基于图谱进行查询的时候发现最初的建模会使得(成语)–(字)–(读音)查询过程中丢失了这个字特定的读法的条件，所以我最终的建模是： 这样，纯文字的条件只涉及了(成语)--(字) 这一跳，而读音、声母、声调的条件则是另一条关系路径，既没有最初版本条件的冗余，又可以在一个路径模式匹配里带上两种条件（后边的例子里会涉及这样的表达）。 2.3.2 构建成语知识图谱有了建模、这么简单的图谱的构建就剩下了数据的收集、清洗和入库。 对于所有成语数据和他们的读音，我一方面直接抽取了汉兜代码内部的数据、另一方面利用 PyPinyin 这个开源的 Python 库将汉兜数据中没有读音的数据获得读音，同时，我也用到了 PyPinyin 里的很多方便的函数比如获取一个拼音的声母、韵母。 构建工具的代码在这里：https://github.com/wey-gu/chinese-graph 更多信息我也放在文末的附录之中。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:3","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#图建模"},{"categories":["Nebula Graph"],"content":"2.3 如何构建面向汉兜解谜的知识图谱？知识图谱是由实体（顶点）和关系（边）组成的，用图数据库管理系统（Graph Database MS）可以很方便进行知识的入库、更改、查询、甚至可视化探索。 在本文里，我将利用开源的分布式图数据库 Nebula Graph 开实践这个过程，具体图谱系统的搭建我都会放在文末。 在本章，我们只讨论图谱的建模：如何面向汉兜的解谜去设计“实体”与“关系”。 2.3.1 图建模2.3.1.1 最初的想法首先，一定存在的实体是： 成语 汉字 成语-[包含]-汉字，每个汉字-[读作]-读音。 其次，因为解谜过程中涉及到了声母、韵母以及声调的条件，考虑到图谱本身的量级非常小（千级别），而且字的读音是一对多的关系，我把读音和声母（包涵声母-initial和韵母-final）也作为实体，他们之间的关系则是顺理成章了： 2.3.1.2 最终的版本然而，我在后边基于图谱进行查询的时候发现最初的建模会使得(成语)–(字)–(读音)查询过程中丢失了这个字特定的读法的条件，所以我最终的建模是： 这样，纯文字的条件只涉及了(成语)--(字) 这一跳，而读音、声母、声调的条件则是另一条关系路径，既没有最初版本条件的冗余，又可以在一个路径模式匹配里带上两种条件（后边的例子里会涉及这样的表达）。 2.3.2 构建成语知识图谱有了建模、这么简单的图谱的构建就剩下了数据的收集、清洗和入库。 对于所有成语数据和他们的读音，我一方面直接抽取了汉兜代码内部的数据、另一方面利用 PyPinyin 这个开源的 Python 库将汉兜数据中没有读音的数据获得读音，同时，我也用到了 PyPinyin 里的很多方便的函数比如获取一个拼音的声母、韵母。 构建工具的代码在这里：https://github.com/wey-gu/chinese-graph 更多信息我也放在文末的附录之中。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:3","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#最初的想法"},{"categories":["Nebula Graph"],"content":"2.3 如何构建面向汉兜解谜的知识图谱？知识图谱是由实体（顶点）和关系（边）组成的，用图数据库管理系统（Graph Database MS）可以很方便进行知识的入库、更改、查询、甚至可视化探索。 在本文里，我将利用开源的分布式图数据库 Nebula Graph 开实践这个过程，具体图谱系统的搭建我都会放在文末。 在本章，我们只讨论图谱的建模：如何面向汉兜的解谜去设计“实体”与“关系”。 2.3.1 图建模2.3.1.1 最初的想法首先，一定存在的实体是： 成语 汉字 成语-[包含]-汉字，每个汉字-[读作]-读音。 其次，因为解谜过程中涉及到了声母、韵母以及声调的条件，考虑到图谱本身的量级非常小（千级别），而且字的读音是一对多的关系，我把读音和声母（包涵声母-initial和韵母-final）也作为实体，他们之间的关系则是顺理成章了： 2.3.1.2 最终的版本然而，我在后边基于图谱进行查询的时候发现最初的建模会使得(成语)–(字)–(读音)查询过程中丢失了这个字特定的读法的条件，所以我最终的建模是： 这样，纯文字的条件只涉及了(成语)--(字) 这一跳，而读音、声母、声调的条件则是另一条关系路径，既没有最初版本条件的冗余，又可以在一个路径模式匹配里带上两种条件（后边的例子里会涉及这样的表达）。 2.3.2 构建成语知识图谱有了建模、这么简单的图谱的构建就剩下了数据的收集、清洗和入库。 对于所有成语数据和他们的读音，我一方面直接抽取了汉兜代码内部的数据、另一方面利用 PyPinyin 这个开源的 Python 库将汉兜数据中没有读音的数据获得读音，同时，我也用到了 PyPinyin 里的很多方便的函数比如获取一个拼音的声母、韵母。 构建工具的代码在这里：https://github.com/wey-gu/chinese-graph 更多信息我也放在文末的附录之中。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:3","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#最终的版本"},{"categories":["Nebula Graph"],"content":"2.3 如何构建面向汉兜解谜的知识图谱？知识图谱是由实体（顶点）和关系（边）组成的，用图数据库管理系统（Graph Database MS）可以很方便进行知识的入库、更改、查询、甚至可视化探索。 在本文里，我将利用开源的分布式图数据库 Nebula Graph 开实践这个过程，具体图谱系统的搭建我都会放在文末。 在本章，我们只讨论图谱的建模：如何面向汉兜的解谜去设计“实体”与“关系”。 2.3.1 图建模2.3.1.1 最初的想法首先，一定存在的实体是： 成语 汉字 成语-[包含]-汉字，每个汉字-[读作]-读音。 其次，因为解谜过程中涉及到了声母、韵母以及声调的条件，考虑到图谱本身的量级非常小（千级别），而且字的读音是一对多的关系，我把读音和声母（包涵声母-initial和韵母-final）也作为实体，他们之间的关系则是顺理成章了： 2.3.1.2 最终的版本然而，我在后边基于图谱进行查询的时候发现最初的建模会使得(成语)–(字)–(读音)查询过程中丢失了这个字特定的读法的条件，所以我最终的建模是： 这样，纯文字的条件只涉及了(成语)--(字) 这一跳，而读音、声母、声调的条件则是另一条关系路径，既没有最初版本条件的冗余，又可以在一个路径模式匹配里带上两种条件（后边的例子里会涉及这样的表达）。 2.3.2 构建成语知识图谱有了建模、这么简单的图谱的构建就剩下了数据的收集、清洗和入库。 对于所有成语数据和他们的读音，我一方面直接抽取了汉兜代码内部的数据、另一方面利用 PyPinyin 这个开源的 Python 库将汉兜数据中没有读音的数据获得读音，同时，我也用到了 PyPinyin 里的很多方便的函数比如获取一个拼音的声母、韵母。 构建工具的代码在这里：https://github.com/wey-gu/chinese-graph 更多信息我也放在文末的附录之中。 ","date":"2022-02-28","objectID":"/resolve-wordle/:2:3","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#构建成语知识图谱"},{"categories":["Nebula Graph"],"content":"3 开始知识图谱查询体操至此，我假设咱们都已经有了我帮大家搭建的成语作弊知识图谱了，开始我们的图谱查询体操吧！ 首先，打开汉兜 👉🏻 https://handle.antfu.me/ 假设我们想从一个成语开始，如果你没有想法的话可以试试这个： # 匹配成语中的一个结果MATCH(x:idiom)RETURNxLIMIT1# 返回结果(\"爱憎分明\":idiom{pinyin:\"['ai4', 'zeng1', 'fen1', 'ming2']\"}) 然后我们把它填到汉兜之中，获得第一次尝试的提示条件： 我们运气不错，得到了三个位置上的条件！ 有一个非第一个位置的字，拼音是 4 声，韵母是 ai，但不是爱（爱） 有一个一声的字，不在第二个位置（憎） 有一个字韵母是 ing，不在第四个位置（明） 第四个字是二声（明） 下面，我们开始图数据库语句体操！ # 有一个非第一个位置的字，拼音是 4 声，韵母是 ai，但不是爱MATCH(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type:\"final\"})WHEREid(final_part_0)==\"ai\"ANDpinyin_0.character_pinyin.tone==4ANDwith_pinyin_0.position!=0ANDwith_char_0.position!=0ANDid(char0)!=\"爱\"# 有一个一声的字，不在第二个位置MATCH(x:idiom)-[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin)WHEREpinyin_1.character_pinyin.tone==1ANDwith_pinyin_1.position!=1# 有一个字韵母是 ing，不在第四个位置MATCH(x:idiom)-[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type:\"final\"})WHEREid(final_part_2)==\"ing\"ANDwith_pinyin_2.position!=3# 第四个字是二声MATCH(x:idiom)-[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin)WHEREpinyin_3.character_pinyin.tone==2ANDwith_pinyin_3.position==3RETURNx,count(x)ascORDERBYcDESC 在图数据库之中运行，得到了 7 个答案： (\"惊愚骇俗\" :idiom{pinyin: \"['jing1', 'yu2', 'hai4', 'su2']\"}) (\"惊世骇俗\" :idiom{pinyin: \"['jing1', 'shi4', 'hai4', 'su2']\"}) (\"惊见骇闻\" :idiom{pinyin: \"['jing1', 'jian4', 'hai4', 'wen2']\"}) (\"沽名卖直\" :idiom{pinyin: \"['gu1', 'ming2', 'mai4', 'zhi2']\"}) (\"惊心骇神\" :idiom{pinyin: \"['jing1', 'xin1', 'hai4', 'shen2']\"}) (\"荆棘载途\" :idiom{pinyin: \"['jing1', 'ji2', 'zai4', 'tu2']\"}) (\"出卖灵魂\" :idiom{pinyin: \"['chu1', 'mai4', 'ling2', 'hun2']\"}) 看起来 惊世骇俗 比较主流，试试！ 我们很幸运，借助于成语作弊知识图谱，居然一次就找到了答案，当然这实际上得益于第一次随机选取的词带来的限制条件的个数，不过在大部分情况下，两次尝试获得最终答案的可能性还是非常大的！ 注，这中间很长的253分钟是因为我在查询中发现之前代码里构造的图谱有点 bug，是“披枷带锁”这个词引起的读音图谱的错误数据，还好后来被修复了。 大家知道“披枷带锁”的正确读音么？😭 接下来，我给大家详细解释一下这个语句的意思。 ","date":"2022-02-28","objectID":"/resolve-wordle/:3:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#开始知识图谱查询体操"},{"categories":["Nebula Graph"],"content":"3.1 语句的含义我们从第一个字的条件开始，这是一个既有声音、又有字形信息的条件。 声音信息：存在一个韵母为 ai4 的发音，位置不在第一个字 文字信息：这个韵母为 ai4 的字，不是爱字 对于声音信息条件，转换为图模式匹配为：(成语)-一个字发音-(拼音) -包含声母-(韵母) WHERE 拼音韵母为 ai4 AND 位置不是第一个。 因为建模的时候，属性名称我用的是英文（其实中文也是支持的），实际上的语句为： # 有一个非第一个位置的字，拼音是 4 声，韵母是 aiMATCH(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type:\"final\"})WHEREid(final_part_0)==\"ai\"ANDpinyin_0.character_pinyin.tone==4ANDwith_pinyin_0.position!=0# ...RETURNx 类似的，表示非第一个位置的字，不是爱 的表达是： # 有一个非第一个位置的字，拼音是 4 声，韵母是 ai，但不是爱MATCH(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)WHEREwith_char_0.position!=0ANDid(char0)!=\"爱\"# ...RETURNx,count(x)ascORDERBYcDESC 而因为这两个条件最终描述的是同一个字，所以它们是可以被写在一个路径下的： # 有一个非第一个位置的字，拼音是 4 声，韵母是 ai，但不是爱MATCH(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type:\"final\"})WHEREid(final_part_0)==\"ai\"ANDpinyin_0.character_pinyin.tone==4ANDwith_pinyin_0.position!=0ANDwith_char_0.position!=0ANDid(char0)!=\"爱\"# ...RETURNx 更多的 MATCH 语法和例子细节，请大家参考文档： MATCH https://docs.nebula-graph.com.cn/3.0.0/3.ngql-guide/7.general-query-statements/2.match/ 图模式 https://docs.nebula-graph.com.cn/3.0.0/3.ngql-guide/1.nGQL-overview/3.graph-patterns/ nGQL 命令 cheatsheet https://docs.nebula-graph.com.cn/3.0.0/2.quick-start/6.cheatsheet-for-ngql-command/ ","date":"2022-02-28","objectID":"/resolve-wordle/:3:1","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#语句的含义"},{"categories":["Nebula Graph"],"content":"4 可视化展示线索我们把每一个条件的匹配路径作为输出，利用 Nebula Graph 的可视化能力，可以得到： # 有一个非第一个位置的字，拼音是 4 声，韵母是 ai，但不是爱MATCHp0=(char0:character)\u003c-[with_char_0:with_character]-(x:idiom)-[with_pinyin_0:with_pinyin]-\u003e(pinyin_0:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_0:pinyin_part{part_type:\"final\"})WHEREid(final_part_0)==\"ai\"ANDpinyin_0.character_pinyin.tone==4ANDwith_pinyin_0.position!=0ANDwith_char_0.position!=0ANDid(char0)!=\"爱\"# 有一个一声的字，不在第二个位置MATCHp1=(x:idiom)-[with_pinyin_1:with_pinyin]-\u003e(pinyin_1:character_pinyin)WHEREpinyin_1.character_pinyin.tone==1ANDwith_pinyin_1.position!=1# 有一个字韵母是 ing，不在第四个位置MATCHp2=(x:idiom)-[with_pinyin_2:with_pinyin]-\u003e(:character_pinyin)-[:with_pinyin_part]-\u003e(final_part_2:pinyin_part{part_type:\"final\"})WHEREid(final_part_2)==\"ing\"ANDwith_pinyin_2.position!=3# 第四个字是二声MATCHp3=(x:idiom)-[with_pinyin_3:with_pinyin]-\u003e(pinyin_3:character_pinyin)WHEREpinyin_3.character_pinyin.tone==2ANDwith_pinyin_3.position==3RETURNp0,p1,p2,p3 在可视化工具的 Console 控制台里执行上边的语句之后，选择导入图探索，就可以看到 ","date":"2022-02-28","objectID":"/resolve-wordle/:4:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#可视化展示线索"},{"categories":["Nebula Graph"],"content":"5 下一步如果大家是从本文第一次了解到 Nebula Graph 图数据库，那么大家可以下一步从 Nebula Graph 项目和 Nebula Graph 社区的官方 Bilibili 站点 👉🏻 https://space.bilibili.com/472621355 了解更多有意思的入门知识。 另外，这里是 Nebula Graph 的官方线上试玩环境，大家可以照着文档，利用试玩环境尝鲜。 后边，Nebula Graph 会开展每天的汉兜 nGQL 体操活动，敬请关注哈！ Happy Graphing! ","date":"2022-02-28","objectID":"/resolve-wordle/:5:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#下一步"},{"categories":["Nebula Graph"],"content":"6 附录：搭建成语知识图谱","date":"2022-02-28","objectID":"/resolve-wordle/:6:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#附录搭建成语知识图谱"},{"categories":["Nebula Graph"],"content":"6.1 收集、生成图谱数据 $ python3 graph_data_generator.py ","date":"2022-02-28","objectID":"/resolve-wordle/:6:1","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#收集生成图谱数据"},{"categories":["Nebula Graph"],"content":"6.2 导入数据到 Nebula Graph 图数据库6.2.1 部署图数据库 借助于 Nebula-Up https://github.com/wey-gu/nebula-up/ ，一行就可以了。 $ curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 部署成功的话，会看到这样的结果： ┌────────────────────────────────────────┐ │ 🌌 Nebula-Graph Playground is Up now! │ ├────────────────────────────────────────┤ │ │ │ 🎉 Congrats! Your Nebula is Up now! │ │ $ cd ~/.nebula-up │ │ │ │ 🌏 You can access it from browser: │ │ http://127.0.0.1:7001 │ │ http://\u003cother_interface\u003e:7001 │ │ │ │ 🔥 Or access via Nebula Console: │ │ $ ~/.nebula-up/console.sh │ │ │ │ To remove the playground: │ │ $ ~/.nebula-up/uninstall.sh │ │ │ │ 🚀 Have Fun! │ │ │ └────────────────────────────────────────┘ 6.2.2 图谱入库 借助于 Nebula-Importer https://github.com/vesoft-inc/nebula-importer/ ，一行就可以了。 $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml 大概一两分钟数据就导入成功了，命令也会正常退出。 连到图数据库的 console 进入 Console 的容器执行下边的命令： $ ~/.nebula-up/console.sh # nebula-console -addr graphd -port 9669 -user root -p nebula 检查一下导入的数据： (root@nebula)[(none)]\u003eshowspaces+--------------------+ |Name|+--------------------+ |\"chinese_idiom\"|+--------------------+ (root@nebula)[(none)]\u003eusechinese_idiomExecutionsucceeded(timespent1510/2329us)Fri,25Feb202208:53:11UTC(root@nebula)[chinese_idiom]\u003ematchp=(成语:idiom)returnplimit2+------------------------------------------------------------------+ |p|+------------------------------------------------------------------+ |\u003c(\"一丁不识\":idiom{pinyin:\"['yi1', 'ding1', 'bu4', 'shi2']\"})\u003e||\u003c(\"一丝不挂\":idiom{pinyin:\"['yi1', 'si1', 'bu4', 'gua4']\"})\u003e|+------------------------------------------------------------------+ (root@nebula)[chinese_idiom]\u003eSUBMITJOBSTATS+------------+ |NewJobId|+------------+ |11|+------------+ (root@nebula)[chinese_idiom]\u003eSHOWSTATS+---------+--------------------+--------+ |Type|Name|Count|+---------+--------------------+--------+ |\"Tag\"|\"character\"|4847||\"Tag\"|\"character_pinyin\"|1336||\"Tag\"|\"idiom\"|29503||\"Tag\"|\"pinyin_part\"|57||\"Edge\"|\"with_character\"|116090||\"Edge\"|\"with_pinyin\"|5943||\"Edge\"|\"with_pinyin_part\"|3290||\"Space\"|\"vertices\"|35739||\"Space\"|\"edges\"|125323|+---------+--------------------+--------+ ","date":"2022-02-28","objectID":"/resolve-wordle/:6:2","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#导入数据到-nebula-graph-图数据库"},{"categories":["Nebula Graph"],"content":"6.2 导入数据到 Nebula Graph 图数据库6.2.1 部署图数据库 借助于 Nebula-Up https://github.com/wey-gu/nebula-up/ ，一行就可以了。 $ curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 部署成功的话，会看到这样的结果： ┌────────────────────────────────────────┐ │ 🌌 Nebula-Graph Playground is Up now! │ ├────────────────────────────────────────┤ │ │ │ 🎉 Congrats! Your Nebula is Up now! │ │ $ cd ~/.nebula-up │ │ │ │ 🌏 You can access it from browser: │ │ http://127.0.0.1:7001 │ │ http://:7001 │ │ │ │ 🔥 Or access via Nebula Console: │ │ $ ~/.nebula-up/console.sh │ │ │ │ To remove the playground: │ │ $ ~/.nebula-up/uninstall.sh │ │ │ │ 🚀 Have Fun! │ │ │ └────────────────────────────────────────┘ 6.2.2 图谱入库 借助于 Nebula-Importer https://github.com/vesoft-inc/nebula-importer/ ，一行就可以了。 $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml 大概一两分钟数据就导入成功了，命令也会正常退出。 连到图数据库的 console 进入 Console 的容器执行下边的命令： $ ~/.nebula-up/console.sh # nebula-console -addr graphd -port 9669 -user root -p nebula 检查一下导入的数据： (root@nebula)[(none)]showspaces+--------------------+ |Name|+--------------------+ |\"chinese_idiom\"|+--------------------+ (root@nebula)[(none)]usechinese_idiomExecutionsucceeded(timespent1510/2329us)Fri,25Feb202208:53:11UTC(root@nebula)[chinese_idiom]matchp=(成语:idiom)returnplimit2+------------------------------------------------------------------+ |p|+------------------------------------------------------------------+ ||||+------------------------------------------------------------------+ (root@nebula)[chinese_idiom]SUBMITJOBSTATS+------------+ |NewJobId|+------------+ |11|+------------+ (root@nebula)[chinese_idiom]SHOWSTATS+---------+--------------------+--------+ |Type|Name|Count|+---------+--------------------+--------+ |\"Tag\"|\"character\"|4847||\"Tag\"|\"character_pinyin\"|1336||\"Tag\"|\"idiom\"|29503||\"Tag\"|\"pinyin_part\"|57||\"Edge\"|\"with_character\"|116090||\"Edge\"|\"with_pinyin\"|5943||\"Edge\"|\"with_pinyin_part\"|3290||\"Space\"|\"vertices\"|35739||\"Space\"|\"edges\"|125323|+---------+--------------------+--------+ ","date":"2022-02-28","objectID":"/resolve-wordle/:6:2","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#部署图数据库"},{"categories":["Nebula Graph"],"content":"6.2 导入数据到 Nebula Graph 图数据库6.2.1 部署图数据库 借助于 Nebula-Up https://github.com/wey-gu/nebula-up/ ，一行就可以了。 $ curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v3.0.0 部署成功的话，会看到这样的结果： ┌────────────────────────────────────────┐ │ 🌌 Nebula-Graph Playground is Up now! │ ├────────────────────────────────────────┤ │ │ │ 🎉 Congrats! Your Nebula is Up now! │ │ $ cd ~/.nebula-up │ │ │ │ 🌏 You can access it from browser: │ │ http://127.0.0.1:7001 │ │ http://:7001 │ │ │ │ 🔥 Or access via Nebula Console: │ │ $ ~/.nebula-up/console.sh │ │ │ │ To remove the playground: │ │ $ ~/.nebula-up/uninstall.sh │ │ │ │ 🚀 Have Fun! │ │ │ └────────────────────────────────────────┘ 6.2.2 图谱入库 借助于 Nebula-Importer https://github.com/vesoft-inc/nebula-importer/ ，一行就可以了。 $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${PWD}/importer_conf.yaml:/root/importer_conf.yaml \\ -v ${PWD}/output:/root \\ vesoft/nebula-importer:v3.0.0 \\ --config /root/importer_conf.yaml 大概一两分钟数据就导入成功了，命令也会正常退出。 连到图数据库的 console 进入 Console 的容器执行下边的命令： $ ~/.nebula-up/console.sh # nebula-console -addr graphd -port 9669 -user root -p nebula 检查一下导入的数据： (root@nebula)[(none)]showspaces+--------------------+ |Name|+--------------------+ |\"chinese_idiom\"|+--------------------+ (root@nebula)[(none)]usechinese_idiomExecutionsucceeded(timespent1510/2329us)Fri,25Feb202208:53:11UTC(root@nebula)[chinese_idiom]matchp=(成语:idiom)returnplimit2+------------------------------------------------------------------+ |p|+------------------------------------------------------------------+ ||||+------------------------------------------------------------------+ (root@nebula)[chinese_idiom]SUBMITJOBSTATS+------------+ |NewJobId|+------------+ |11|+------------+ (root@nebula)[chinese_idiom]SHOWSTATS+---------+--------------------+--------+ |Type|Name|Count|+---------+--------------------+--------+ |\"Tag\"|\"character\"|4847||\"Tag\"|\"character_pinyin\"|1336||\"Tag\"|\"idiom\"|29503||\"Tag\"|\"pinyin_part\"|57||\"Edge\"|\"with_character\"|116090||\"Edge\"|\"with_pinyin\"|5943||\"Edge\"|\"with_pinyin_part\"|3290||\"Space\"|\"vertices\"|35739||\"Space\"|\"edges\"|125323|+---------+--------------------+--------+ ","date":"2022-02-28","objectID":"/resolve-wordle/:6:2","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#图谱入库"},{"categories":["Nebula Graph"],"content":"7 附录：图建模的 Schema nGQL CREATESPACEIFNOTEXISTSchinese_idiom(partition_num=5,replica_factor=1,vid_type=FIXED_STRING(24));USEchinese_idiom;#创建点的类型CREATETAGidiom(pinyinstring);#成语CREATETAGcharacter();#汉字CREATETAGcharacter_pinyin(toneint);#单字的拼音CREATETAGpinyin_part(part_typestring);#拼音的声部#创建边的类型CREATEEDGEwith_character(positionint);#包含汉字CREATEEDGEwith_pinyin(positionint);#读作CREATEEDGEwith_pinyin_part(part_typestring);#包含声部 ","date":"2022-02-28","objectID":"/resolve-wordle/:7:0","series":null,"tags":["Nebula Graph","图数据库","知识图谱","wordle","汉兜"],"title":"图数据库体操：用 Nebula Graph 搭成语图谱解汉兜","uri":"/resolve-wordle/#附录图建模的-schema-ngql"},{"categories":["Nebula Graph"],"content":"找不到索引？为什么我要创建 Nebula Graph 索引？什么时候要用到 Nebula Graph 原生索引，一文把这些搞清楚。","date":"2022-02-20","objectID":"/nebula-index-explained/","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/"},{"categories":["Nebula Graph"],"content":" index not found？找不到索引？为什么我要创建 Nebula Graph 索引？什么时候要用到 Nebula Graph 原生索引，一文把这些搞清楚。 Nebula Graph 的索引其实和传统的关系型数据库中的索引很像，但是又有一些容易让人疑惑的区别。刚开始了解 Nebula 的同学会疑惑于： 不清楚 Nebula Graph 图数据库中的索引到的是什么概念 我应该什么时候使用 Nebula Graph 索引 Nebula Graph 索引怎么影响到写入性能 这篇文章里，我们就把这些问题回答好。 ","date":"2022-02-20","objectID":"/nebula-index-explained/:0:0","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#"},{"categories":["Nebula Graph"],"content":"1 到底 Nebula Graph 索引是什么简而言之，Nebula Graph 索引是用来，且只用来针对纯属性条件出发查询场景的 图游走（walk）查询中的属性条件过滤不需要它 纯属性条件出发查询（注：非采样情况）必须创建索引 ","date":"2022-02-20","objectID":"/nebula-index-explained/:1:0","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#到底-nebula-graph-索引是什么"},{"categories":["Nebula Graph"],"content":"1.1 纯属性条件出发查询我们知道在传统关系型数据库中，索引是对表数据的一个或多个针对特定列重排序的副本，它用来加速特定列过滤条件的读查询并带来了额外的数据写入（加速而非这样查询的必须前提）。 在 Nebula Graph 图数据库里，索引则是对点、边特定属性数据重排序的副本，用来提供纯属性条件出发查询（如下边的查询：从只给定了点边属性条件，而非点的 ID 出发去获取图数据） ####必须NebulaGraph索引存在的查询#query0纯属性条件出发查询LOOKUPONtag1WHEREcol1\u003e1ANDcol2==\"foo\"\\YIELDtag1.col1ascol1,tag1.col3ascol3;#query1纯属性条件出发查询MATCH(v:player{name:'Tim Duncan'})--\u003e(v2:player) \\ RETURNv2.player.nameASName; 上边这两个纯属性条件出发查询就是字面意思的”根据给定的属性条件获取点或者边本身“ ，反面的例子则是给定了点的 ID： ####不基于索引的查询#query2,从给定的点做的游走查询vertexVID:\"player100\"GOFROM\"player100\"OVERfollowREVERSELY\\YIELDsrc(edge)ASid|\\GOFROM$-.idOVERserve\\WHEREproperties($^).age\u003e20\\YIELDproperties($^).nameASFriendOf,properties($$).nameASTeam;#query3,从给定的点做的游走查询vertexVID:\"player101\"或者\"player102\"MATCH(v:player{name:'Tim Duncan'})--(v2) \\ WHEREid(v2)IN[\"player101\",\"player102\"]\\RETURNv2.player.nameASName; 我们仔细看前边的 query 1 和 query 3，尽管语句中条件都有针对 tag 为 player 的过滤： { name: 'Tim Duncan' } ： query 3之中不需要索引，因为它可以： 更直接的从已知的 v2 顶点： [\"player101\", \"player102\"] 向外扩展、游走（GetNeighbors() 获得边的另一端的点，然后GetVertices() 得到下一跳的 v），根据 v.player.name 过滤掉不要的数据 query 1 则不同，它因为没有任何给定的顶点 ID： 只能从属性条件入手，{ name: 'Tim Duncan' }，在按照 name 排序了的索引数据中先找到符合的点：IndexScan() 得到 v 然后再从 v 做 GetNeighbors() 获得边的另一端 的 v2 ，在通过 GetVertices() 去获得下一跳 v2 中的数据 其实，这里的关键就是在于是查询是否存在给定的顶点 ID（Vertex ID），下边两个查询的执行计划里更清晰地比较了他们的区别： query 1, 需要基于索引，纯属性条件出发查询 query 3, 从已知 VID，不需要索引 ","date":"2022-02-20","objectID":"/nebula-index-explained/:1:1","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#纯属性条件出发查询"},{"categories":["Nebula Graph"],"content":"1.2 为什么纯属性条件出发查询里必须要索引呢？因为 Nebula Graph 在存储数据的时候，它的结构是面向分布式与关联关系设计的，类似表结构数据库中无索引的全扫描条件搜索实际上更加昂贵，所以设计上被有意禁止了。 注: 如果不追求全部数据，只要采样一部分，3.0 里之后是支持不强制索引 LIMIT 的情况的，如下查询（有 LIMIT）不需要索引： MATCH (v:player { name: 'Tim Duncan' })--\u003e(v2:player) \\ RETURN v2.player.name AS Name LIMIT 3; ","date":"2022-02-20","objectID":"/nebula-index-explained/:1:2","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#为什么纯属性条件出发查询里必须要索引呢"},{"categories":["Nebula Graph"],"content":"1.3 为什么只有纯属性条件出发查询我们比较一下正常的图查询 graph-queries 和纯属性条件出发查询 pure-prop-condition queries： graph-queries： 如 query 2 、 query 3是沿着边一路找到特定路径条件的扩展游走 pure-prop-condition queries：如 query 0 and query 1 是只通过一定属性条件（或者是无限制条件）找到满足的点、边 而在 Nebula Graph 里，graph-queries 在扩展的时候，图的原始数据已经按照 VID（点和边都是）排序过了（或者说在数据里已经索引过了），这个排序带来连续存储（物理上临接）使得扩展游走本身就是优化、很快的。 ","date":"2022-02-20","objectID":"/nebula-index-explained/:1:3","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#为什么只有纯属性条件出发查询"},{"categories":["Nebula Graph"],"content":"1.4 总结：索引是什么，索引不是什么？索引是什么？ Nebula Graph 索引是为了从给定属性条件查点、边的一份属性数据的排序，它用写入的代价是的这种读查询模式成为可能。 索引不是什么？ Nebula Graph 索引不是用来加速一般图查询的：从一个点开始向外拓展的查询（即使是过滤属性条件的）不会依赖原生索引，因为 Nebula 数据自身的存储就是面向这种查询优化、排序的。 ","date":"2022-02-20","objectID":"/nebula-index-explained/:1:4","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#总结索引是什么索引不是什么"},{"categories":["Nebula Graph"],"content":"2 一些 Nebula Graph 索引的设计细节为了更好理解索引的限制、代价、能力，咱们来解释更多他的细节 Nebula Graph 索引是在本地（不是分开、中心化）和点数据被一起存储、分片的。 它只支持左匹配 因为底层是 RocksDB Prefix Scan 性能代价: 写入时候的路径：不只是多一分数据，为了保证一致性，还有昂贵的读操作 读路径：基于规则的优化选择索引，Fan Out 到所有 StorageD 这些信息也在我的手绘图和视频里可以看到： 因为左匹配的设计，在有更复杂的针对纯属性条件出发查询里涉及到通配、REGEXP这样的全文搜索情况，Nebula Graph 提供了全文索引的功能，它是利用 Raft Listener 去异步将数据写到外部 Elasticsearch 集群之中，并在查询的时候去查 ES 去做到的，见文档。 在这个手绘图中，我们还可以看出 Write path 写入索引数据是同步操作的 Read path 这部分画了一个 RBO 的例子，查询里的规则假设 col2 相等匹配排在左边的情况下，性能优于 col1 的大小比较匹配，所以选择了第二个索引 选好了索引之后，扫描索引的请求被 fan out 到存储节点上，这其中有些过滤条件比如 top n 是可以下推的 结论： 因为写入的代价，只有必须用索引的时候采用，如果采样查询能满足读的要求，可以不创建索引而用 LIMIT 。 索引有左匹配的限制 符合查询的顺序要仔细设计 有时候需要使用全文索引 full-text index。 ","date":"2022-02-20","objectID":"/nebula-index-explained/:2:0","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#一些-nebula-graph-索引的设计细节"},{"categories":["Nebula Graph"],"content":"3 索引的使用具体要参考索引文档一些要点是： 在 tag 或者 edge type 上针对想要被条件反查点边的属性创建索引 CREATE INDEX 创建索引之后的索引部分数据会同步写入，但是如果创建索引之前已经有的点边数据对应的索引是需要明确指定去创建的，这是一个异步的 job： REBUILD INDEX 触发了异步的 REBUILD INDEX 之后，可以查询状态： SHOW INDEX STATUS 利用到索引的查询可以是 LOOKUP，并且常常可以借助管道符在此之上做拓展查询（Graph Query）： LOOKUPONplayer\\WHEREplayer.name==\"Kobe Bryant\"\\YIELDid(vertex)ASVertexID,properties(vertex).nameASname|\\GOFROM$-.VertexIDOVERserve\\YIELD$-.name,properties(edge).start_year,properties(edge).end_year,properties($$).name; 也可以是 MATCH ，这里边 v 是通过索引得到的，而 v2 则是在数据（非索引）部分拓展查询获得的。 MATCH (v:player{name:\"Tim Duncan\"})--(v2:player) \\ RETURN v2.player.name AS Name; 复合索引的能力与限制 理解原生索引的匹配是左匹配能让我们知道对于超过一个属性的索引：复合索引，并且能帮助我们理解它的能力有限制，这里说几个结论： 我们创建针对多个属性的复合索引是顺序有关的 比如，我们创建一个双属性复合索引 index_a: (isRisky: bool, age: int)，和 index_b: (age: int, isRisky: bool) 在根据 WHERE n.user.isRisky == true AND n.user.age \u003e 18 这个条件查询时候，index_a 因为左匹配一个相等的短字段，显然效率更高。 只有复合左匹配的被复合索引的属性真子集的过滤条件才能被只支持 比如，index_a: (isRisky: bool, age: int)，和 index_b: (age: int, isRisky: bool) 在查询 WHERE n.user.age \u003e 18 这个语句的时候, 只有 index_b 复合最左匹配，能满足这个查询。 针对一些从属性作为查询的起点，找点、边的情况，原生索引是不能满足全文搜索的匹配场景的，这时候，我们应该考虑使用 Nebula 全文索引，它是 Nebula 社区支持的开箱即用的外置 Elastic Search，通过配置，创建了全文索引的数据会通过 Raft listener 异步更新到 Elastic 集群中，他的查询入口也是 LOOKUP，详细的信息请参考文档。 ","date":"2022-02-20","objectID":"/nebula-index-explained/:3:0","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#索引的使用"},{"categories":["Nebula Graph"],"content":"4 回顾 Nebula Graph 索引在只提供属性条件情况下通过对属性的排序副本扫描查点、边 Nebula Graph 索引不是用来图拓展查询的 Nebula Graph 索引是左匹配，不是用来做模糊全文搜索的 Nebula Graph 索引在写入时候有性能代价 记得如果创建 Nebula Graph 索引之前已经有相应点边上的数据，要重建索引 Happy Graphing! Feture image credit to Alina ","date":"2022-02-20","objectID":"/nebula-index-explained/:4:0","series":null,"tags":["Nebula Graph","索引","全文搜索","图数据库"],"title":"Nebula Graph 索引详解","uri":"/nebula-index-explained/#回顾"},{"categories":["Nebula Graph"],"content":"一文了解 K8s 部署的 Nebula Graph 集群的 Nebula-Algorithm 使用方法。","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/"},{"categories":["Nebula Graph"],"content":" 一文了解 K8s 部署的 Nebula Graph 集群的 Nebula-Algorithm 使用方法。 ","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/:0:0","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/#"},{"categories":["Nebula Graph"],"content":"1 步骤最方便的方法是将 Nebula Algorithm/ Spark 运行在与 Nebula-Operator 相同的网络命名空间里，将 show hosts meta 的 MetaD 域名:端口 格式的地址填进配置里就可以了。 注：需要 2.6.2 或者更新的版本，Spark-Connector/Algorithm 才支持域名形式的 MetaD 地址。 获取 MetaD 地址 (root@nebula) [(none)]\u003e show hosts meta +------------------------------------------------------------------+------+----------+--------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +------------------------------------------------------------------+------+----------+--------+--------------+---------+ | \"nebula-metad-0.nebula-metad-headless.default.svc.cluster.local\" | 9559 | \"ONLINE\" | \"META\" | \"d113f4a\" | \"2.6.2\" | +------------------------------------------------------------------+------+----------+--------+--------------+---------+ Got 1 rows (time spent 1378/2598 us) Mon, 14 Feb 2022 08:22:33 UTC 填写 Algorithm 的配置文件 Ref: https://github.com/vesoft-inc/nebula-algorithm/blob/master/nebula-algorithm/src/main/resources/application.conf # ... nebula: { # algo's data source from Nebula. If data.source is nebula, then this nebula.read config can be valid. read: { # Nebula metad server address, multiple addresses are split by English comma metaAddress: \"nebula-metad-0.nebula-metad-headless.default.svc.cluster.local:9559\" #... 或者是调用 spark-connector 的代码 Ref: https://github.com/vesoft-inc/nebula-spark-connector val config = NebulaConnectionConfig .builder() .withMetaAddress(\"nebula-metad-0.nebula-metad-headless.default.svc.cluster.local:9559\") .withConenctionRetry(2) .build() val nebulaReadVertexConfig: ReadNebulaConfig = ReadNebulaConfig .builder() .withSpace(\"foo_bar_space\") .withLabel(\"person\") .withNoColumn(false) .withReturnCols(List(\"birthday\")) .withLimit(10) .withPartitionNum(10) .build() val vertex = spark.read.nebula(config, nebulaReadVertexConfig).loadVerticesToDF() 看起来非常简单，那么，为什么这么简单的过程却值得一篇文章呢？ ","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/:1:0","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/#步骤"},{"categories":["Nebula Graph"],"content":"1.1 容易忽略的问题这里的问题在于： a. 它隐含地需要保证 StorageD 的地址能被 spark 环境访问； b. 但是这些 StorageD 地址是从 MetaD 获取的； c. Nebula K8s Operator 里，MetaD 中存储的 StorageD 地址（服务发现）的来源是 StorageD 的配置文件，而它是 k8s 的内部地址。 ","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/:1:1","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/#容易忽略的问题"},{"categories":["Nebula Graph"],"content":"1.2 背景知识a. 的理由比较直接，和 nebula 的架构有关：图的数据都存在 Storage Service 之中，通常用语句的查询是透过 Graph Service 来透传，只需要 GraphD 的连接就足够，而 Spark-Connector 使用 Nebula Graph 的场景是扫描全图或者子图，这时候计算存储分离的设计使得我们可以绕过查询、计算层直接高效读取图数据。 那么问题来了，为什么需要、且只需要 MetaD 的地址呢？ 这也是和架构有关，Meta Service 里包含了全图的分布数据与分布式的 Storage Service 的各个分片和实例的分布，所以一方面因为只有 Meta 才有全图的信息（需要），另一方面因为从 Meta 可以获得这部分信息（只需要）。到这里 b. 的答案也有了。 详细的 Nebula Graph 架构信息可以参考博客系列文章：nebula-graph.com.cn/tags/架构设计 另外请确保您已经读了架构介绍的文档哦：Nebula 架构总览 下面我们看看 c. c. Nebula K8s Operator 里，MetaD 中存储的 StorageD 地址（服务发现）的来源是 StorageD 的配置文件，而它是 k8s 的内部地址。 这和 Nebula Graph 里的服务发现机制有关：在 Nebula Graph 集群中，Graph Service 和 Storage Service 都是通过心跳将自己的信息上报给 Meta Service 的，而这其中服务自身的地址的来源则来自于他们相应的配置文件中的网络配置。 关于服务自身的地址配置请参考文档：Storage networking 配置 关于服务发现详细的信息请参考四王的文章：图数据库 Nebula Graph 集群通信：从心跳说起。 最后，我们知道 Nebula Operator 是一个在 K8s 集群中按照配置，自动创建、维护、扩缩容 Nebula 集群的 K8s 控制面的应用，它需要抽象一部分内部资源相关的配置，这就包括了 GraphD 和 StorageD 实例的实际地址，他们是被配置的地址实际上是 headless service 地址。 而这些地址（如下）默认是没法被 k8s 外部网络访问的，所以针对 GraphD、MetaD 我们可以方便创建服务将其暴露出来。 (root@nebula)[(none)]\u003eshowhostsmeta+------------------------------------------------------------------+------+----------+--------+--------------+---------+ |Host|Port|Status|Role|GitInfoSha|Version|+------------------------------------------------------------------+------+----------+--------+--------------+---------+ |\"nebula-metad-0.nebula-metad-headless.default.svc.cluster.local\"|9559|\"ONLINE\"|\"META\"|\"d113f4a\"|\"2.6.2\"|+------------------------------------------------------------------+------+----------+--------+--------------+---------+ Got1rows(timespent1378/2598us)Mon,14Feb202209:22:33UTC(root@nebula)[(none)]\u003eshowhostsgraph+---------------------------------------------------------------+------+----------+---------+--------------+---------+ |Host|Port|Status|Role|GitInfoSha|Version|+---------------------------------------------------------------+------+----------+---------+--------------+---------+ |\"nebula-graphd-0.nebula-graphd-svc.default.svc.cluster.local\"|9669|\"ONLINE\"|\"GRAPH\"|\"d113f4a\"|\"2.6.2\"|+---------------------------------------------------------------+------+----------+---------+--------------+---------+ Got1rows(timespent2072/3403us)Mon,14Feb202210:03:58UTC(root@nebula)[(none)]\u003eshowhostsstorage+------------------------------------------------------------------------+------+----------+-----------+--------------+---------+ |Host|Port|Status|Role|GitInfoSha|Version|+------------------------------------------------------------------------+------+----------+-----------+--------------+---------+ |\"nebula-storaged-0.nebula-storaged-headless.default.svc.cluster.local\"|9779|\"ONLINE\"|\"STORAGE\"|\"d113f4a\"|\"2.6.2\"||\"nebula-storaged-1.nebula-storaged-headless.default.svc.cluster.local\"|9779|\"ONLINE\"|\"STORAGE\"|\"d113f4a\"|\"2.6.2\"||\"nebula-storaged-2.nebula-storaged-headless.default.svc.cluster.local\"|9779|\"ONLINE\"|\"STORAGE\"|\"d113f4a\"|\"2.6.2\"|+------------------------------------------------------------------------+------+----------+-----------+--------------+---------+ Got3rows(timespent1603/2979us)Mon,14Feb202210:05:24UTC 然而，因为前边提到的 Spark-Connector 通过 Meta Service 去获取 StorageD 的地址，且这个地址是服务发现而得，所以 Spark-Connector 实际上获取的 StorageD 地址就是上边的这种 headless 的服务地址，没法直接从外部访问。 所以，我们在有条件的情况下，只需要让 Spark 运行在和 Nebula Cluster 相同的 K8s 网络里，一切就迎刃而解了，否则，我们需要： 将 MetaD 和 StorageD 的地址利用 Ingress 等方式将其 L4（TCP）暴露出来。 可以参考 Nebula Operator 的文档：doc/user/client_service.md 通过反向代理和DNS让这些 headless 服务能被解析到相应的 StorageD。 参考：TBD 那么，有没有更方便的方式？ 非常抱歉的是，目前最方便的方式依然是如文章最开头所介绍：让 Spark 运行在 Nebula Cluster 内部。实际上，我在努力推进 Nebula Spark 社区去支持可以配置的 StorageAddresses 选项，有了它之后，前边提到的 2. 就是不必要的了。 ","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/:1:2","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/#背景知识"},{"categories":["Nebula Graph"],"content":"2 Bonus：一键体验 nebula-algorithm + nebula-operator为了方便在 k8s 上尝鲜 nebula-graph nebula-algorithm 的同学，这里我要再次安利一下我写的一个小工具 Neubla-Operator-KinD，它是一个一键在 Docker 环境内部单独部署一个 k8s 集群并在其中部署 Nebula Operator 以及所有依赖（包括 storage provider）并部署一个小 Nebula Cluster 的工具。有点绕，不过可以看下边的步骤哈： 第一步，部署 K8s+Nebula-Operator+Nebula Cluster： curl -sL nebula-kind.siwei.io/install.sh | bash 第二步，照着工具文档里的 what’s next a. 用 console 连接集群，并加载示例数据集 b. 在这个 k8s 里跑一个图算法 创建一个 Spark 环境 kubectl create -f http://nebula-kind.siwei.io/deployment/spark.yaml kubectl wait pod --timeout=-1s --for=condition=Ready -l '!job-name' 等上边的 wait 都 ready 之后，进入 spark 的 pod。 kubectl exec -it deploy/spark-deployment -- bash 下载 nebula-algorithm 比如 2.6.2 这个版本，更多版本请参考 https://github.com/vesoft-inc/nebula-algorithm/。 注意： 官方发布的版本在这里可以获取：https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/ 因为这个问题 https://github.com/vesoft-inc/nebula-algorithm/issues/42 只有 2.6.2 或者更新的版本才支持域名访问 MetaD。 # 下载 nebula-algorithm-2.6.2.jar wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.6.2/nebula-algorithm-2.6.2.jar # 下载 nebula-algorthm 配置文件 wget https://github.com/vesoft-inc/nebula-algorithm/raw/v2.6/nebula-algorithm/src/main/resources/application.conf 修改Then we could change the config file of nebula-algorithm on meta and graph addresses: sed -i '/^ metaAddress/c\\ metaAddress: \\\"nebula-metad-0.nebula-metad-headless.default.svc.cluster.local:9559\\\"' application.conf sed -i '/^ graphAddress/c\\ graphAddress: \\\"nebula-graphd-0.nebula-graphd-svc.default.svc.cluster.local:9669\\\"' application.conf ##### change space sed -i '/^ space/c\\ space: basketballplayer' application.conf ##### read data from nebula graph sed -i '/^ source/c\\ source: nebula' application.conf ##### execute algorithm: labelpropagation sed -i '/^ executeAlgo/c\\ executeAlgo: labelpropagation' application.conf 执行 LPA 算法在 basketballplayer 图空间 /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ nebula-algorithm-2.6.2.jar \\ -p application.conf 结果如下： bash-5.0# ls /tmp/count/ _SUCCESS part-00000-5475f9f4-66b9-426b-b0c2-704f946e54d3-c000.csv bash-5.0# head /tmp/count/part-00000-5475f9f4-66b9-426b-b0c2-704f946e54d3-c000.csv _id,lpa 1100,1104 2200,2200 2201,2201 1101,1104 2202,2202 Happy Graphing! Picture Credit: Timelab Pro ","date":"2022-02-14","objectID":"/nebula-algo-spark-k8s/:2:0","series":null,"tags":["Nebula Graph","图数据库","Spark-Connector","Nebula-Algorithm","k8s","nebula-operator"],"title":"为什么我的 Nebula-Spark-Connector、Nebula-Algorithm 连不上 K8s 部署的 Nebula Graph 集群？","uri":"/nebula-algo-spark-k8s/#bonus一键体验-nebula-algorithm--nebula-operator"},{"categories":["Nebula Graph","Mini Project"],"content":"如何利用图数据库从0-1构建一个特定领域问答助手？本文手把手带你构建一个简易版的篮球领域智能问答机器人。","date":"2021-12-30","objectID":"/siwi/","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/"},{"categories":["Nebula Graph","Mini Project"],"content":" 如何利用图数据库从0-1构建一个特定领域问答助手？本文手把手带你构建一个简易版的篮球领域智能问答机器人。 ","date":"2021-12-30","objectID":"/siwi/:0:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#"},{"categories":["Nebula Graph","Mini Project"],"content":"1 前言「问答机器人」在我们日常生活中并不少见到 ：比如在一些电商客服、智能问诊、技术支持等人工输入与沟通界面的场景下，机器人“智能”问答系统一定程度上可以在无需人力、不需要耗费终端用户心智去做知识库、商品搜索、科室选择等等的情况下实时给出问题答案。 问答机器人系统背后的技术有多重可能： 基于检索，全文搜索接近的问题 基于机器学习阅读理解 基于知识图谱（Knowledge-Based Question Answering system: KBQA） 其他 基于知识图谱构建问答系统在以下三个情况下很有优势： 对于领域类型是结构化数据场景：电商、医药、系统运维（微服务、服务器、事件）、产品支持系统等，其中作为问答系统的参考对象已经是结构化数据； 问题的解答过程涉及多跳查询，比如“姚明的妻子今年是本命年吗？”，“你们家的产品 A 和 A+ 的区别是什么？”； 为了解决其他需求（风控、推荐、管理），已经构建了图结构数据、知识图谱的情况。 为了方便读者最快速了解如何构建 KBQA 系统，我写了非常简陋的小 KBQA 项目，在本文中，我会带领大家从头到尾把它搭起来。 💡：这个小项目叫做 Siwi，它的代码就在 GitHub 上：github.com/wey-gu/nebula-siwi Siwi 的发音是：/ˈsɪwi/ 或者叫：思二为 ，它是一个能解答 NBA 相关问题的机器人。 我们开始吧。 ","date":"2021-12-30","objectID":"/siwi/:1:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#前言"},{"categories":["Nebula Graph","Mini Project"],"content":"2 鸟瞰 TL;DRKBQA 用一句话说就是把问题解析、转换成在知识图谱中的查询，查询得到结果之后进行筛选、翻译成结果（句子、卡片或者任何方便人理解的答案格式）。 💡：知识图谱的构建实际上是非常重要的过程，在本文中，我们专注在串起来 KBQA 系统的骨架，我们假设需求是基于一个已经有的图谱之上，为其增加一个 QA 系统。 「问题到图谱查询的转换」有不同的方法可以实现。 可以是对语义进行分析：理解问题的意图，针对不同意图匹配最可能的问题类型，从而构建这个类型问题的图谱查询，查得结果； 也可以是基于信息的抽取：从问题中抽取主要的实体，在图谱中获取实体的所有知识、关系条目（子图），再对结果根据问题中的约束条件匹配、排序选择结果。 💡：美团技术团队在这篇文章里分享了他们的真实世界实践，下图是美团结合了机器学习和 NLP 的方案。 而在 Siwi 里，我们一切从简，单独选择了语义分析这条路，它的特点是需要人为去标注或者编码一些问题类型的查询方式，但实际上在大多数场景下，尤其单一领域图谱的场景下反而是轻量却效果不差的方案，也是一个便于新手理解 KBQA 的合适的入门方式。 除了核心的问答部分，我还为 Siwi 增加了语音识别和语音回答（感谢浏览器接口标准的发展）的功能，于是，这个项目的结构和问答调用流程就是这样的了：一个语音问题自上而下分别经过三个部分： 基于网页的 Siwi Frontend 语音、文字问答界面 Python Flask 实现的 Siwi Backend/API 系统 Nebula Graph 开源分布式高性能图数据库之上的知识图谱 ┌────────────────┬──────────────────────────────────────┐ │ │ │ │ │ Speech │ │ ┌──────────▼──────────┐ │ │ │ Frontend │ Siwi, /ˈsɪwi/ │ │ │ Web_Speech_API │ A PoC of │ │ │ │ Dialog System │ │ │ Vue.JS │ With Graph Database │ │ │ │ Backed Knowledge Graph │ │ └──────────┬──────────┘ │ │ │ Sentence │ │ ┌────────────┼──────────────────────────────┐ │ │ │ │ Backend │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Web API, Flask │ ./app/ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Sentence ./bot/ │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Intent matching, │ ./bot/classifier│ │ │ │ │ Symentic Processing │ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Intent, Entities │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Intent Actor │ ./bot/actions │ │ │ └─┴──────────┬──────────┴───────────────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ Graph Database │ Nebula Graph │ │ └─────────────────────┘ │ └───────────────────────────────────────────────────────┘ 💡：图数据库相比于其他知识图谱存储系统来说，因为其设计专注于数据内的数据关系，非常擅长实时获取海量数据下实体之间的复杂关联关系。 Nebula Graph 的原生分布式设计和 share-nothing 架构使得它擅长于巨大数据量和高并发读写的场景，加上它的开源社区特别活跃，已经被国内很多团队用于支撑生产上的各种业务，这里有一些他们分享的选型、落地实践。 ","date":"2021-12-30","objectID":"/siwi/:2:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#鸟瞰-tldr"},{"categories":["Nebula Graph","Mini Project"],"content":"3 知识图谱Siwi 构建于一个篮球相关的知识图谱之上，它其实是 Siwi 采用的开源分布式图数据库 Nebula Graph 社区的官方文档里的示例数据集。 在这个非常简单的图谱之中，只有两种点： player，球员 team，球队 两种关系： serve 服役于（比如：姚明 -服役于-\u003e 休斯顿火箭） follow 关注 （比如：姚明 -关注-\u003e 奥尼尔） 💡：这个数据集在 Nebula 社区上有一个 在线体验 环境，任何人都无需登录，通过Nebula Graph Studio 可视化探索篮球图谱。 下图就是这个图谱的可视化探索截图，可以看到左边的中心节点勇士队（Warriors）有杜兰特（Durant）还有其他几个队员在其中服役（serve）；除了服役之外，还可以看到队员和队员之中也有关注（follow）的关系存在。 有了这个知识图谱，咱们接下来就在它之上搭一个简单的基于语法解析的 QA 系统吧😁。 ","date":"2021-12-30","objectID":"/siwi/:3:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#知识图谱"},{"categories":["Nebula Graph","Mini Project"],"content":"4 Siwi-backend ┌────────────┼──────────────────────────────┐ │ │ Backend │ │ ┌──────────▼──────────┐ │ │ │ Web API, Flask │ ./app/ │ │ └──────────┬──────────┘ │ │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ │ Graph Query ┌──────────▼──────────┐ │ Graph Database │ Nebula Graph └─────────────────────┘ 如上图的设计流程，Siwi 的后端部分需要接收问句，处理之后访问知识图谱（图数据库），然后将处理结果返回给用户。 ","date":"2021-12-30","objectID":"/siwi/:4:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#siwi-backend"},{"categories":["Nebula Graph","Mini Project"],"content":"4.1 接收 HTTP 请求(app)对于请求，就简单地用 Flask 作为 web server 来接收 HTTP 的 POST 请求： 💡：还不熟悉 Flask 的同学，可以在 freeCodeCamp 上搜索一下，有一些不错的课程哈。 下边的代码就是告诉 Flask ： 如果用户发过来 http://\u003cserver\u003e/query 的 POST 请求，提的问题就在请求的 body 里的 question 的 Key 之下。 取得问题之后，调用把请求传给 siwi_bot 的 query()，得到 answer 。 代码段：src/siwi/app/__init__.py #... from siwi.bot import bot #... @app.route(\"/query\", methods=[\"POST\"]) def query(): request_data = request.get_json() question = request_data.get(\"question\", \"\") # \u003c----- 1. if question: answer = siwi_bot.query( request_data.get(\"question\", \"\")) # \u003c----- 2. else: answer = \"Sorry, what did you say?\" return jsonify({\"answer\": answer}) 接下来我们来实现 siwi_bot，真正处理提问的逻辑。 ","date":"2021-12-30","objectID":"/siwi/:4:1","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#接收-http-请求app"},{"categories":["Nebula Graph","Mini Project"],"content":"4.2 处理请求(bot) │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ 前边提到过，KBQA 基本上是 a. 把问题解析、转换成在知识图谱中的查询 b. 查询得到结果之后进行筛选、翻译成结果 这里，我们把 a. 的逻辑放在 classifier 里，b. 的逻辑放在 actions(actor) 里。 a. HTTP 请求的问题句子 sentence 传过来，用 classifier 解析它的意图和句子实体 b. 用意图和句子实体构造 action，并链接图数据库执行，获取结果。 代码段：src/siwi/bot/bot/__init__.py from siwi.bot.actions import SiwiActions from siwi.bot.classifier import SiwiClassifier class SiwiBot(): def __init__(self, connection_pool) -\u003e None: self.classifier = SiwiClassifier() self.actions = SiwiActions() self.connection_pool = connection_pool def query(self, sentence): intent = self.classifier.get(sentence) # \u003c--- a. action = self.actions.get(intent) # \u003c--- b. return action.execute(self.connection_pool) 首先咱们来进一步实现一下 SiwiClassifier 吧。 4.2.1 语义解析(classifier)classifier 需要在 get(sentence) 方法里将句子中的实体和句子的意图解析、分类出来。通常来说，这里是需要借助机器学习、NLP去分词、分类实现的，这里只是为了展示这个过程实际上只是各种 if/ else。 我们这里实现了三类意图的问题： 关系（A，B）：获得 A 和 B 在图谱中的关系路径，比如姚明和湖人队的关系是？ 服役情况：比如乔纳森在哪里服役？ 关注情况：比如邓肯关注了谁？ ❓ 开放问题： 如果看教程的你觉得这几个问题太没意思了，这里留一个开放问题，你可以在 Siwi 里帮我们实现：「共同好友（A，B）获得 A 和 B 的一度共同好友」这个意图（或者更酷的其他句子）么？欢迎来 Github：github.com/wey-gu/nebula-siwi/ 提 PR 哦，看看谁先实现。 代码片段：src/siwi/bot/classfier/__init__.py class SiwiClassifier(): def get(self, sentence: str) -\u003e dict: \"\"\" Classify Sentences and Fill Slots. This should be done by NLP, here we fake one to demostrate the intent Actor --\u003e Graph DB work flow. sentense: relation: - What is the relationship between Yao Ming and Lakers? - How does Tracy McGrady and Lakers connected? serving: - Which team had Jonathon Simmons served? friendship: - Whom does Tim Duncan follow? - Who are Tracy McGrady's friends? returns: { \"entities\": entities, \"intents\": intents } \"\"\" entities = self.get_matched_entities(sentence) intents = self.get_matched_intents(sentence) return { \"entities\": entities, \"intents\": intents } 这里，我把匹配的规则（等价于 if else…）写在了 src/siwi/bot/test/data 之下的 YAML 文件里，这样增加 classifier 之中新的规则只需要更新这个文件就可以了： 4.2.1.1 意图识别(intent) def load_entity_data(self) -\u003e None: # load data from yaml files module_path = f\"{ siwi.__path__[0] }/bot/test/data\" #... with open(f\"{ module_path }/intents.yaml\", \"r\") as file: self.intents = yaml.safe_load(file)[\"intents\"] 对于每一个意图来说： intents.\u003c名字\u003e 代表名字 名字之后的 action 代表后边在要实现的相应的 xxxAction 的类 比如 RelationshipAction 将是用来处理查询关系（A，B）这样的问题的 Action 类 keywords 代表在句子之中匹配的关键词 比如问句里出现 serve，served，serving 的字眼的时候，将会匹配服役的问题 💡：写 if else 条件来对应意图是不容易的，因为不同意图不可能没有关键词相交的情况，我们的实现只是一个非常简陋、不严谨的方式。在实际场景下，训练模型去做匹配效果会更好，有意思的是，那些做的比较好的模型的输入和我们的 YAML 的格式是很类似的。 ---intents:fallback:action:FallbackActionkeywords:[]relationship:action:RelationshipActionkeywords:- between- relation- relationship- related- connect- correlateserve:action:ServeActionkeywords:- serve- served- servingfriend:action:FollowActionkeywords:- follows- followed- follow- friend- friends 4.2.1.2 实体识别(entity)类似的，实体识别的部分本质上也是 if else，只不过这里利用到了**Aho–Corasick算法**来帮助搜索实体，在生产（非玩具）的情况下，应该用 NLP 里的分词的方法来做。 💡：大家可以去了解一下这个 AC自动机算法 def setup_entity_tree(self) -\u003e None: self.entity_type_map.update({ key: \"player\" for key in self.players.keys() }) self.entity_type_map.update({ key: \"team\" for key in self.teams.keys() }) self.entity_tree = ahocorasick.Automaton() for index, entity in enumerate(self.entity_type_map.keys()): self.entity_tree.add_word(entity, (index, entity)) self.entity_tree.make_automaton() #... def get_matched_entities(self, sentence: str) -\u003e dict: \"\"\" Consume a sentence to be matched with ahocorasick Returns a dict: {entity: entity_type} \"\"\" _matched = [] for item in self.entity_tree.iter(sentence): entities_matched.append(item[1][1]) return { entity: self.entity_type_map[entity] for entity in _matched } 至此，我们的 SiwiClassifier.get(sentence) 已经能返回解析、分类出来的意图和实体了，这时候，它们会被传给 Actions 来让 siwi bot 知道如何去执行知识图谱的查询啦！ 4.2.2 构造图谱查询(action)还记得前边的 bot 代码里，最后一步，图谱查询的","date":"2021-12-30","objectID":"/siwi/:4:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#处理请求bot"},{"categories":["Nebula Graph","Mini Project"],"content":"4.2 处理请求(bot) │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ 前边提到过，KBQA 基本上是 a. 把问题解析、转换成在知识图谱中的查询 b. 查询得到结果之后进行筛选、翻译成结果 这里，我们把 a. 的逻辑放在 classifier 里，b. 的逻辑放在 actions(actor) 里。 a. HTTP 请求的问题句子 sentence 传过来，用 classifier 解析它的意图和句子实体 b. 用意图和句子实体构造 action，并链接图数据库执行，获取结果。 代码段：src/siwi/bot/bot/__init__.py from siwi.bot.actions import SiwiActions from siwi.bot.classifier import SiwiClassifier class SiwiBot(): def __init__(self, connection_pool) - None: self.classifier = SiwiClassifier() self.actions = SiwiActions() self.connection_pool = connection_pool def query(self, sentence): intent = self.classifier.get(sentence) # dict: \"\"\" Classify Sentences and Fill Slots. This should be done by NLP, here we fake one to demostrate the intent Actor -- Graph DB work flow. sentense: relation: - What is the relationship between Yao Ming and Lakers? - How does Tracy McGrady and Lakers connected? serving: - Which team had Jonathon Simmons served? friendship: - Whom does Tim Duncan follow? - Who are Tracy McGrady's friends? returns: { \"entities\": entities, \"intents\": intents } \"\"\" entities = self.get_matched_entities(sentence) intents = self.get_matched_intents(sentence) return { \"entities\": entities, \"intents\": intents } 这里，我把匹配的规则（等价于 if else…）写在了 src/siwi/bot/test/data 之下的 YAML 文件里，这样增加 classifier 之中新的规则只需要更新这个文件就可以了： 4.2.1.1 意图识别(intent) def load_entity_data(self) - None: # load data from yaml files module_path = f\"{ siwi.__path__[0] }/bot/test/data\" #... with open(f\"{ module_path }/intents.yaml\", \"r\") as file: self.intents = yaml.safe_load(file)[\"intents\"] 对于每一个意图来说： intents. 代表名字 名字之后的 action 代表后边在要实现的相应的 xxxAction 的类 比如 RelationshipAction 将是用来处理查询关系（A，B）这样的问题的 Action 类 keywords 代表在句子之中匹配的关键词 比如问句里出现 serve，served，serving 的字眼的时候，将会匹配服役的问题 💡：写 if else 条件来对应意图是不容易的，因为不同意图不可能没有关键词相交的情况，我们的实现只是一个非常简陋、不严谨的方式。在实际场景下，训练模型去做匹配效果会更好，有意思的是，那些做的比较好的模型的输入和我们的 YAML 的格式是很类似的。 ---intents:fallback:action:FallbackActionkeywords:[]relationship:action:RelationshipActionkeywords:- between- relation- relationship- related- connect- correlateserve:action:ServeActionkeywords:- serve- served- servingfriend:action:FollowActionkeywords:- follows- followed- follow- friend- friends 4.2.1.2 实体识别(entity)类似的，实体识别的部分本质上也是 if else，只不过这里利用到了**Aho–Corasick算法**来帮助搜索实体，在生产（非玩具）的情况下，应该用 NLP 里的分词的方法来做。 💡：大家可以去了解一下这个 AC自动机算法 def setup_entity_tree(self) - None: self.entity_type_map.update({ key: \"player\" for key in self.players.keys() }) self.entity_type_map.update({ key: \"team\" for key in self.teams.keys() }) self.entity_tree = ahocorasick.Automaton() for index, entity in enumerate(self.entity_type_map.keys()): self.entity_tree.add_word(entity, (index, entity)) self.entity_tree.make_automaton() #... def get_matched_entities(self, sentence: str) - dict: \"\"\" Consume a sentence to be matched with ahocorasick Returns a dict: {entity: entity_type} \"\"\" _matched = [] for item in self.entity_tree.iter(sentence): entities_matched.append(item[1][1]) return { entity: self.entity_type_map[entity] for entity in _matched } 至此，我们的 SiwiClassifier.get(sentence) 已经能返回解析、分类出来的意图和实体了，这时候，它们会被传给 Actions 来让 siwi bot 知道如何去执行知识图谱的查询啦！ 4.2.2 构造图谱查询(action)还记得前边的 bot 代码里，最后一步，图谱查询的","date":"2021-12-30","objectID":"/siwi/:4:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#语义解析classifier"},{"categories":["Nebula Graph","Mini Project"],"content":"4.2 处理请求(bot) │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ 前边提到过，KBQA 基本上是 a. 把问题解析、转换成在知识图谱中的查询 b. 查询得到结果之后进行筛选、翻译成结果 这里，我们把 a. 的逻辑放在 classifier 里，b. 的逻辑放在 actions(actor) 里。 a. HTTP 请求的问题句子 sentence 传过来，用 classifier 解析它的意图和句子实体 b. 用意图和句子实体构造 action，并链接图数据库执行，获取结果。 代码段：src/siwi/bot/bot/__init__.py from siwi.bot.actions import SiwiActions from siwi.bot.classifier import SiwiClassifier class SiwiBot(): def __init__(self, connection_pool) - None: self.classifier = SiwiClassifier() self.actions = SiwiActions() self.connection_pool = connection_pool def query(self, sentence): intent = self.classifier.get(sentence) # dict: \"\"\" Classify Sentences and Fill Slots. This should be done by NLP, here we fake one to demostrate the intent Actor -- Graph DB work flow. sentense: relation: - What is the relationship between Yao Ming and Lakers? - How does Tracy McGrady and Lakers connected? serving: - Which team had Jonathon Simmons served? friendship: - Whom does Tim Duncan follow? - Who are Tracy McGrady's friends? returns: { \"entities\": entities, \"intents\": intents } \"\"\" entities = self.get_matched_entities(sentence) intents = self.get_matched_intents(sentence) return { \"entities\": entities, \"intents\": intents } 这里，我把匹配的规则（等价于 if else…）写在了 src/siwi/bot/test/data 之下的 YAML 文件里，这样增加 classifier 之中新的规则只需要更新这个文件就可以了： 4.2.1.1 意图识别(intent) def load_entity_data(self) - None: # load data from yaml files module_path = f\"{ siwi.__path__[0] }/bot/test/data\" #... with open(f\"{ module_path }/intents.yaml\", \"r\") as file: self.intents = yaml.safe_load(file)[\"intents\"] 对于每一个意图来说： intents. 代表名字 名字之后的 action 代表后边在要实现的相应的 xxxAction 的类 比如 RelationshipAction 将是用来处理查询关系（A，B）这样的问题的 Action 类 keywords 代表在句子之中匹配的关键词 比如问句里出现 serve，served，serving 的字眼的时候，将会匹配服役的问题 💡：写 if else 条件来对应意图是不容易的，因为不同意图不可能没有关键词相交的情况，我们的实现只是一个非常简陋、不严谨的方式。在实际场景下，训练模型去做匹配效果会更好，有意思的是，那些做的比较好的模型的输入和我们的 YAML 的格式是很类似的。 ---intents:fallback:action:FallbackActionkeywords:[]relationship:action:RelationshipActionkeywords:- between- relation- relationship- related- connect- correlateserve:action:ServeActionkeywords:- serve- served- servingfriend:action:FollowActionkeywords:- follows- followed- follow- friend- friends 4.2.1.2 实体识别(entity)类似的，实体识别的部分本质上也是 if else，只不过这里利用到了**Aho–Corasick算法**来帮助搜索实体，在生产（非玩具）的情况下，应该用 NLP 里的分词的方法来做。 💡：大家可以去了解一下这个 AC自动机算法 def setup_entity_tree(self) - None: self.entity_type_map.update({ key: \"player\" for key in self.players.keys() }) self.entity_type_map.update({ key: \"team\" for key in self.teams.keys() }) self.entity_tree = ahocorasick.Automaton() for index, entity in enumerate(self.entity_type_map.keys()): self.entity_tree.add_word(entity, (index, entity)) self.entity_tree.make_automaton() #... def get_matched_entities(self, sentence: str) - dict: \"\"\" Consume a sentence to be matched with ahocorasick Returns a dict: {entity: entity_type} \"\"\" _matched = [] for item in self.entity_tree.iter(sentence): entities_matched.append(item[1][1]) return { entity: self.entity_type_map[entity] for entity in _matched } 至此，我们的 SiwiClassifier.get(sentence) 已经能返回解析、分类出来的意图和实体了，这时候，它们会被传给 Actions 来让 siwi bot 知道如何去执行知识图谱的查询啦！ 4.2.2 构造图谱查询(action)还记得前边的 bot 代码里，最后一步，图谱查询的","date":"2021-12-30","objectID":"/siwi/:4:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#意图识别intent"},{"categories":["Nebula Graph","Mini Project"],"content":"4.2 处理请求(bot) │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ 前边提到过，KBQA 基本上是 a. 把问题解析、转换成在知识图谱中的查询 b. 查询得到结果之后进行筛选、翻译成结果 这里，我们把 a. 的逻辑放在 classifier 里，b. 的逻辑放在 actions(actor) 里。 a. HTTP 请求的问题句子 sentence 传过来，用 classifier 解析它的意图和句子实体 b. 用意图和句子实体构造 action，并链接图数据库执行，获取结果。 代码段：src/siwi/bot/bot/__init__.py from siwi.bot.actions import SiwiActions from siwi.bot.classifier import SiwiClassifier class SiwiBot(): def __init__(self, connection_pool) - None: self.classifier = SiwiClassifier() self.actions = SiwiActions() self.connection_pool = connection_pool def query(self, sentence): intent = self.classifier.get(sentence) # dict: \"\"\" Classify Sentences and Fill Slots. This should be done by NLP, here we fake one to demostrate the intent Actor -- Graph DB work flow. sentense: relation: - What is the relationship between Yao Ming and Lakers? - How does Tracy McGrady and Lakers connected? serving: - Which team had Jonathon Simmons served? friendship: - Whom does Tim Duncan follow? - Who are Tracy McGrady's friends? returns: { \"entities\": entities, \"intents\": intents } \"\"\" entities = self.get_matched_entities(sentence) intents = self.get_matched_intents(sentence) return { \"entities\": entities, \"intents\": intents } 这里，我把匹配的规则（等价于 if else…）写在了 src/siwi/bot/test/data 之下的 YAML 文件里，这样增加 classifier 之中新的规则只需要更新这个文件就可以了： 4.2.1.1 意图识别(intent) def load_entity_data(self) - None: # load data from yaml files module_path = f\"{ siwi.__path__[0] }/bot/test/data\" #... with open(f\"{ module_path }/intents.yaml\", \"r\") as file: self.intents = yaml.safe_load(file)[\"intents\"] 对于每一个意图来说： intents. 代表名字 名字之后的 action 代表后边在要实现的相应的 xxxAction 的类 比如 RelationshipAction 将是用来处理查询关系（A，B）这样的问题的 Action 类 keywords 代表在句子之中匹配的关键词 比如问句里出现 serve，served，serving 的字眼的时候，将会匹配服役的问题 💡：写 if else 条件来对应意图是不容易的，因为不同意图不可能没有关键词相交的情况，我们的实现只是一个非常简陋、不严谨的方式。在实际场景下，训练模型去做匹配效果会更好，有意思的是，那些做的比较好的模型的输入和我们的 YAML 的格式是很类似的。 ---intents:fallback:action:FallbackActionkeywords:[]relationship:action:RelationshipActionkeywords:- between- relation- relationship- related- connect- correlateserve:action:ServeActionkeywords:- serve- served- servingfriend:action:FollowActionkeywords:- follows- followed- follow- friend- friends 4.2.1.2 实体识别(entity)类似的，实体识别的部分本质上也是 if else，只不过这里利用到了**Aho–Corasick算法**来帮助搜索实体，在生产（非玩具）的情况下，应该用 NLP 里的分词的方法来做。 💡：大家可以去了解一下这个 AC自动机算法 def setup_entity_tree(self) - None: self.entity_type_map.update({ key: \"player\" for key in self.players.keys() }) self.entity_type_map.update({ key: \"team\" for key in self.teams.keys() }) self.entity_tree = ahocorasick.Automaton() for index, entity in enumerate(self.entity_type_map.keys()): self.entity_tree.add_word(entity, (index, entity)) self.entity_tree.make_automaton() #... def get_matched_entities(self, sentence: str) - dict: \"\"\" Consume a sentence to be matched with ahocorasick Returns a dict: {entity: entity_type} \"\"\" _matched = [] for item in self.entity_tree.iter(sentence): entities_matched.append(item[1][1]) return { entity: self.entity_type_map[entity] for entity in _matched } 至此，我们的 SiwiClassifier.get(sentence) 已经能返回解析、分类出来的意图和实体了，这时候，它们会被传给 Actions 来让 siwi bot 知道如何去执行知识图谱的查询啦！ 4.2.2 构造图谱查询(action)还记得前边的 bot 代码里，最后一步，图谱查询的","date":"2021-12-30","objectID":"/siwi/:4:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#实体识别entity"},{"categories":["Nebula Graph","Mini Project"],"content":"4.2 处理请求(bot) │ │ Sentence ./bot/ │ │ ┌──────────▼──────────┐ │ │ │ Intent matching, │ ./bot/classifier│ │ │ Symentic Processing │ │ │ └──────────┬──────────┘ │ │ │ Intent, Entities │ │ ┌──────────▼──────────┐ │ │ │ Intent Actor │ ./bot/actions │ └─┴──────────┬──────────┴───────────────────┘ 前边提到过，KBQA 基本上是 a. 把问题解析、转换成在知识图谱中的查询 b. 查询得到结果之后进行筛选、翻译成结果 这里，我们把 a. 的逻辑放在 classifier 里，b. 的逻辑放在 actions(actor) 里。 a. HTTP 请求的问题句子 sentence 传过来，用 classifier 解析它的意图和句子实体 b. 用意图和句子实体构造 action，并链接图数据库执行，获取结果。 代码段：src/siwi/bot/bot/__init__.py from siwi.bot.actions import SiwiActions from siwi.bot.classifier import SiwiClassifier class SiwiBot(): def __init__(self, connection_pool) - None: self.classifier = SiwiClassifier() self.actions = SiwiActions() self.connection_pool = connection_pool def query(self, sentence): intent = self.classifier.get(sentence) # dict: \"\"\" Classify Sentences and Fill Slots. This should be done by NLP, here we fake one to demostrate the intent Actor -- Graph DB work flow. sentense: relation: - What is the relationship between Yao Ming and Lakers? - How does Tracy McGrady and Lakers connected? serving: - Which team had Jonathon Simmons served? friendship: - Whom does Tim Duncan follow? - Who are Tracy McGrady's friends? returns: { \"entities\": entities, \"intents\": intents } \"\"\" entities = self.get_matched_entities(sentence) intents = self.get_matched_intents(sentence) return { \"entities\": entities, \"intents\": intents } 这里，我把匹配的规则（等价于 if else…）写在了 src/siwi/bot/test/data 之下的 YAML 文件里，这样增加 classifier 之中新的规则只需要更新这个文件就可以了： 4.2.1.1 意图识别(intent) def load_entity_data(self) - None: # load data from yaml files module_path = f\"{ siwi.__path__[0] }/bot/test/data\" #... with open(f\"{ module_path }/intents.yaml\", \"r\") as file: self.intents = yaml.safe_load(file)[\"intents\"] 对于每一个意图来说： intents. 代表名字 名字之后的 action 代表后边在要实现的相应的 xxxAction 的类 比如 RelationshipAction 将是用来处理查询关系（A，B）这样的问题的 Action 类 keywords 代表在句子之中匹配的关键词 比如问句里出现 serve，served，serving 的字眼的时候，将会匹配服役的问题 💡：写 if else 条件来对应意图是不容易的，因为不同意图不可能没有关键词相交的情况，我们的实现只是一个非常简陋、不严谨的方式。在实际场景下，训练模型去做匹配效果会更好，有意思的是，那些做的比较好的模型的输入和我们的 YAML 的格式是很类似的。 ---intents:fallback:action:FallbackActionkeywords:[]relationship:action:RelationshipActionkeywords:- between- relation- relationship- related- connect- correlateserve:action:ServeActionkeywords:- serve- served- servingfriend:action:FollowActionkeywords:- follows- followed- follow- friend- friends 4.2.1.2 实体识别(entity)类似的，实体识别的部分本质上也是 if else，只不过这里利用到了**Aho–Corasick算法**来帮助搜索实体，在生产（非玩具）的情况下，应该用 NLP 里的分词的方法来做。 💡：大家可以去了解一下这个 AC自动机算法 def setup_entity_tree(self) - None: self.entity_type_map.update({ key: \"player\" for key in self.players.keys() }) self.entity_type_map.update({ key: \"team\" for key in self.teams.keys() }) self.entity_tree = ahocorasick.Automaton() for index, entity in enumerate(self.entity_type_map.keys()): self.entity_tree.add_word(entity, (index, entity)) self.entity_tree.make_automaton() #... def get_matched_entities(self, sentence: str) - dict: \"\"\" Consume a sentence to be matched with ahocorasick Returns a dict: {entity: entity_type} \"\"\" _matched = [] for item in self.entity_tree.iter(sentence): entities_matched.append(item[1][1]) return { entity: self.entity_type_map[entity] for entity in _matched } 至此，我们的 SiwiClassifier.get(sentence) 已经能返回解析、分类出来的意图和实体了，这时候，它们会被传给 Actions 来让 siwi bot 知道如何去执行知识图谱的查询啦！ 4.2.2 构造图谱查询(action)还记得前边的 bot 代码里，最后一步，图谱查询的","date":"2021-12-30","objectID":"/siwi/:4:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#构造图谱查询action"},{"categories":["Nebula Graph","Mini Project"],"content":"4.3 测试一下4.3.1 启动图数据库我们在 Nebula Graph 里建立（导入数据）一个篮球的知识图谱。 💡：在导入数据之前，请先部署一个 Nebula Graph 集群。最简便的部署方式是使用 Nebula-UP 这个小工具，只需要一行命令就能在 Linux 机器上同时启动一个 Nebula Graph 核心和可视化图探索工具 Nebula Graph Studio。如果你更愿意用 Docker 部署，请参考这个文档。 本文假设我们使用 Nebula-UP 来部署一个 Nebula Graph： curl -fsSL nebula-up.siwei.io/install.sh | bash 之后，我们会看到这样的提示： 按照提示，我们可以通过这个命令进入到有 Nebula Console 的容器里： 💡：Nebula Console 是命令行访问 Nebula Graph 图数据库的客户端，支持 Linux，Windows 和 macOS，下载地址：这里 ~/.nebula-up/console.sh 然后，在 # 的提示符下就表示我们进来了，我们在里边可以执行： nebula-console -addr graphd -port 9669 -user root -p nebula 这样就表示我们连接上了 Nebula Graph 图数据库： /# nebula-console -addr graphd -port 9669 -user root -p nebula WelcometoNebulaGraph!(root@nebula)[(none)]\u003e 在这里，我们就可以通过 nGQL 去操作 Nebula Graph，不过我们先退出来，执行 exit： (root@nebula)[(none)]\u003eexitByeroot!Fri,31Dec202104:11:28UTC 我们在这个容器内把基于 nGQL 语句的数据下载下来： / # wget https://docs.nebula-graph.io/2.0/basketballplayer-2.X.ngql 然后通过 Nebula Console 的 -f \u003cfile_path\u003e 把数据导入进去： nebula-console -addr graphd -port 9669 -user root -p nebula -f basketballplayer-2.X.ngql 至此，我们就启动了一个 Nebula Graph 图数据库，还在里边加载了篮球的知识图谱！ 💡：还记得前边我们提到的 在线体验 环境么？现在，我们可以在这个利用 Nebula-UP 部署了 Nebula 的环境里启动自己的 Nebula Studio 啦，按照上边 Nebula-UP 的提示：http://\u003c本机IP\u003e:7001 就是它的地址，然后大家可以参考文档和在线体验介绍去了解更多。 4.3.2 启动 Siwi-backend大家可以直接 clone 我的代码：git clone https://github.com/wey-gu/nebula-siwi/ 然后安装、启动 Siwi Backend： cd nebula-siwi # Install dependencies python3 -m pip install -r src/requirements.txt # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 启动之后，我们可以另外开窗口，通过 cURL 去发起问题给 backend，更多细节大家可以参考 GitHub 上的 README： 至此，我们已经写好了 QA 系统的重要的代码啦，大家是不是对一个 KBQA 的构成有了更清晰的概念了呢？ 接下来，我们为它增加一个界面！ ","date":"2021-12-30","objectID":"/siwi/:4:3","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#测试一下"},{"categories":["Nebula Graph","Mini Project"],"content":"4.3 测试一下4.3.1 启动图数据库我们在 Nebula Graph 里建立（导入数据）一个篮球的知识图谱。 💡：在导入数据之前，请先部署一个 Nebula Graph 集群。最简便的部署方式是使用 Nebula-UP 这个小工具，只需要一行命令就能在 Linux 机器上同时启动一个 Nebula Graph 核心和可视化图探索工具 Nebula Graph Studio。如果你更愿意用 Docker 部署，请参考这个文档。 本文假设我们使用 Nebula-UP 来部署一个 Nebula Graph： curl -fsSL nebula-up.siwei.io/install.sh | bash 之后，我们会看到这样的提示： 按照提示，我们可以通过这个命令进入到有 Nebula Console 的容器里： 💡：Nebula Console 是命令行访问 Nebula Graph 图数据库的客户端，支持 Linux，Windows 和 macOS，下载地址：这里 ~/.nebula-up/console.sh 然后，在 # 的提示符下就表示我们进来了，我们在里边可以执行： nebula-console -addr graphd -port 9669 -user root -p nebula 这样就表示我们连接上了 Nebula Graph 图数据库： /# nebula-console -addr graphd -port 9669 -user root -p nebula WelcometoNebulaGraph!(root@nebula)[(none)] 在这里，我们就可以通过 nGQL 去操作 Nebula Graph，不过我们先退出来，执行 exit： (root@nebula)[(none)]exitByeroot!Fri,31Dec202104:11:28UTC 我们在这个容器内把基于 nGQL 语句的数据下载下来： / # wget https://docs.nebula-graph.io/2.0/basketballplayer-2.X.ngql 然后通过 Nebula Console 的 -f 把数据导入进去： nebula-console -addr graphd -port 9669 -user root -p nebula -f basketballplayer-2.X.ngql 至此，我们就启动了一个 Nebula Graph 图数据库，还在里边加载了篮球的知识图谱！ 💡：还记得前边我们提到的 在线体验 环境么？现在，我们可以在这个利用 Nebula-UP 部署了 Nebula 的环境里启动自己的 Nebula Studio 啦，按照上边 Nebula-UP 的提示：http://:7001 就是它的地址，然后大家可以参考文档和在线体验介绍去了解更多。 4.3.2 启动 Siwi-backend大家可以直接 clone 我的代码：git clone https://github.com/wey-gu/nebula-siwi/ 然后安装、启动 Siwi Backend： cd nebula-siwi # Install dependencies python3 -m pip install -r src/requirements.txt # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 启动之后，我们可以另外开窗口，通过 cURL 去发起问题给 backend，更多细节大家可以参考 GitHub 上的 README： 至此，我们已经写好了 QA 系统的重要的代码啦，大家是不是对一个 KBQA 的构成有了更清晰的概念了呢？ 接下来，我们为它增加一个界面！ ","date":"2021-12-30","objectID":"/siwi/:4:3","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#启动图数据库"},{"categories":["Nebula Graph","Mini Project"],"content":"4.3 测试一下4.3.1 启动图数据库我们在 Nebula Graph 里建立（导入数据）一个篮球的知识图谱。 💡：在导入数据之前，请先部署一个 Nebula Graph 集群。最简便的部署方式是使用 Nebula-UP 这个小工具，只需要一行命令就能在 Linux 机器上同时启动一个 Nebula Graph 核心和可视化图探索工具 Nebula Graph Studio。如果你更愿意用 Docker 部署，请参考这个文档。 本文假设我们使用 Nebula-UP 来部署一个 Nebula Graph： curl -fsSL nebula-up.siwei.io/install.sh | bash 之后，我们会看到这样的提示： 按照提示，我们可以通过这个命令进入到有 Nebula Console 的容器里： 💡：Nebula Console 是命令行访问 Nebula Graph 图数据库的客户端，支持 Linux，Windows 和 macOS，下载地址：这里 ~/.nebula-up/console.sh 然后，在 # 的提示符下就表示我们进来了，我们在里边可以执行： nebula-console -addr graphd -port 9669 -user root -p nebula 这样就表示我们连接上了 Nebula Graph 图数据库： /# nebula-console -addr graphd -port 9669 -user root -p nebula WelcometoNebulaGraph!(root@nebula)[(none)] 在这里，我们就可以通过 nGQL 去操作 Nebula Graph，不过我们先退出来，执行 exit： (root@nebula)[(none)]exitByeroot!Fri,31Dec202104:11:28UTC 我们在这个容器内把基于 nGQL 语句的数据下载下来： / # wget https://docs.nebula-graph.io/2.0/basketballplayer-2.X.ngql 然后通过 Nebula Console 的 -f 把数据导入进去： nebula-console -addr graphd -port 9669 -user root -p nebula -f basketballplayer-2.X.ngql 至此，我们就启动了一个 Nebula Graph 图数据库，还在里边加载了篮球的知识图谱！ 💡：还记得前边我们提到的 在线体验 环境么？现在，我们可以在这个利用 Nebula-UP 部署了 Nebula 的环境里启动自己的 Nebula Studio 啦，按照上边 Nebula-UP 的提示：http://:7001 就是它的地址，然后大家可以参考文档和在线体验介绍去了解更多。 4.3.2 启动 Siwi-backend大家可以直接 clone 我的代码：git clone https://github.com/wey-gu/nebula-siwi/ 然后安装、启动 Siwi Backend： cd nebula-siwi # Install dependencies python3 -m pip install -r src/requirements.txt # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 启动之后，我们可以另外开窗口，通过 cURL 去发起问题给 backend，更多细节大家可以参考 GitHub 上的 README： 至此，我们已经写好了 QA 系统的重要的代码啦，大家是不是对一个 KBQA 的构成有了更清晰的概念了呢？ 接下来，我们为它增加一个界面！ ","date":"2021-12-30","objectID":"/siwi/:4:3","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#启动-siwi-backend"},{"categories":["Nebula Graph","Mini Project"],"content":"5 Siwi-frontend","date":"2021-12-30","objectID":"/siwi/:5:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#siwi-frontend"},{"categories":["Nebula Graph","Mini Project"],"content":"5.1 聊天界面我们利用 Vue Bot UI 这个可爱的机器人界面的 Vue 实现可以很容易构造一个 代码段：src/siwi/frontend/src/App.vue \u003ctemplate\u003e \u003cdiv id=\"app\"\u003e \u003cVueBotUI :messages=\"msg\" :options=\"botOptions\" :bot-typing=\"locking\" :input-disable=\"locking\" @msg-send=\"msgSender\" /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript\u003e import { VueBotUI } from 'vue-bot-ui' 注意到那个小飞机按钮了吧，它是发出问题请求的按键，我们要在按下它的时候对后端做出请求。 ","date":"2021-12-30","objectID":"/siwi/:5:1","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#聊天界面"},{"categories":["Nebula Graph","Mini Project"],"content":"5.2 访问后端这部分用到了Axios，它是浏览器里访问其他地址的 HTTP 客户端。 在按下的时候，@msg-send=\"msgSender\" 会触发 msgSender() msgSender()去构造axios.post(this.apiEndpoint, { \"question\": data.text }) 的请求给 Siwi 的后端 后端的结果被 push() 到界面的聊天消息里，渲染出来 this.msg.push() 代码段：src/siwi/frontend/src/App.vue \u003ctemplate\u003e \u003cdiv id=\"app\"\u003e \u003cVueBotUI :messages=\"msg\" :options=\"botOptions\" :bot-typing=\"locking\" :input-disable=\"locking\" @msg-send=\"msgSender\" ---------------- 1. /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript\u003e import { VueBotUI } from 'vue-bot-ui' import axios from \"axios\"; export default { name: 'App', components: { VueBotUI, }, methods: { msgSender(data) { this.msg.push({ agent: \"user\", type: \"text\", text: data.text, }); this.locking = true; axios.post(this.apiEndpoint, { \"question\": data.text }).then((response) =\u003e { console.log(response); ----------------- 2. this.msg.push({ ----------------- 3. agent: \"bot\", type: \"text\", text: response.data.answer, }); this.synthText = response.data.answer; this.agentSpeak = true; this.locking = false; }); }, } } 现在，我们已经有了一个图形界面的机器人啦，不过，更进一步，我们可以利用现代浏览器的接口，实现语音识别和机器人说话！ ","date":"2021-12-30","objectID":"/siwi/:5:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#访问后端"},{"categories":["Nebula Graph","Mini Project"],"content":"5.3 语音识别我们借助于 Vue Web Speech, 这个语音 API 的 VueJS 的绑定，可以很容易在按下 🎙️ 的时候接收人的语音，并把语音转换成文字发出去，在回答被返回之后，它（还是他/她😁？）也会把回答的句子读出来给用户。 record 在 🎙️ 被按下之后，变成 👂 触发 onResults() 监听 把返回结果发给 this.synthText 合成器，准备读出 \u003cvue-web-speech-synth\u003e 把语音读出 代码段：src/siwi/frontend/src/App.vue \u003ctemplate\u003e \u003cdiv id=\"app\"\u003e \u003cbutton id=\"mic_btn\" @click=\"record = !record\"\u003e {{record?'👂':'🎙️'}} --------------------------\u003e 1. \u003c/button\u003e \u003cvue-web-speech v-model=\"record\" @results=\"onResults\" --------------------------\u003e 1. @unrecognized=\"unrecognized\" \u003e \u003c/vue-web-speech\u003e ... \u003cvue-web-speech-synth v-model=\"agentSpeak\" :voice=\"synthVoice\" :text=\"synthText\" @list-voices=\"listVoices\" --------------------------\u003e 4. /\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript\u003e import { VueBotUI } from 'vue-bot-ui' import axios from \"axios\"; export default { name: 'App', components: { VueBotUI, }, onResults (data) { -------------------------\u003e 2. this.results = data; this.locking = true; this.msg.push({ agent: \"user\", type: \"text\", text: data[0], }); this.locking = true; console.log(data[0]); axios.post(this.apiEndpoint, { \"question\": data[0] }).then((response) =\u003e { console.log(response.data); this.msg.push({ agent: \"bot\", type: \"text\", text: response.data.answer, }); this.synthText = response.data.answer; ----------\u003e 3. this.agentSpeak = true; }); this.locking = false; }, } } \u003c/script\u003e ","date":"2021-12-30","objectID":"/siwi/:5:3","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#语音识别"},{"categories":["Nebula Graph","Mini Project"],"content":"6 总结至此，我们已经学会了搭建自己的第一个 KBQA：知识图谱驱动的问答系统。 回顾下它的代码结构： src/siwi 对应后端 App 是 Flask API 处理的部分 Bot 是处理请求、访问 Nebula Graph 的部分 src/siwi_frontend 是前端 希望大家在这个简陋的基础之上，多多探索，做出来更加成熟的聊天机器人，欢迎你来给我邮件、留言告诉我呀，这里：https://siwei.io/about 有我的联系方式。 . ├── README.md ├── src │ ├── siwi # Siwi-API Backend │ │ ├── app # Web Server, take HTTP requests and calls Bot API │ │ └── bot # Bot API │ │ ├── actions # Take Intent, Slots, Query Knowledge Graph here │ │ ├── bot # Entrypoint of the Bot API │ │ ├── classifier # Symentic Parsing, Intent Matching, Slot Filling │ │ └── test # Example Data Source as equivalent/mocked module │ └── siwi_frontend # Browser End │ ├── README.md │ ├── package.json │ └── src │ ├── App.vue # Listening to user and pass Questions to Siwi-API │ └── main.js └── wsgi.py 如果你很喜欢这样的小项目，欢迎来看看我之前的分享： 「从0-1：如何构建一个企业股权图谱系统？」哦。 💡：你知道吗，我其实借助于 Katacoda 已经为大家搭建了一个交互式体验 Siwi + Nebula 的部署的环境，如果您的网络条件够快（Katacoda服务器在国外），可以在这里点点鼠标就交互式体验它。 视频介绍 ","date":"2021-12-30","objectID":"/siwi/:6:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#总结"},{"categories":["Nebula Graph","Mini Project"],"content":"7 感谢用到的开源项目 ❤️这个小项目里我们用到了好多开源的项目，非常感谢这些贡献者们的慷慨与无私，开源是不是很酷呢？ ","date":"2021-12-30","objectID":"/siwi/:7:0","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#感谢用到的开源项目-"},{"categories":["Nebula Graph","Mini Project"],"content":"7.1 Backend KGQA on MedicalKG by Huanyong Liu Flask pyahocorasick created by Wojciech Muła PyYaml ","date":"2021-12-30","objectID":"/siwi/:7:1","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#backend"},{"categories":["Nebula Graph","Mini Project"],"content":"7.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design ","date":"2021-12-30","objectID":"/siwi/:7:2","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#frontend"},{"categories":["Nebula Graph","Mini Project"],"content":"7.3 Graph Database Nebula Graph 高性能、云原生的开源分布式图数据库 ","date":"2021-12-30","objectID":"/siwi/:7:3","series":null,"tags":["Nebula Graph","图数据库","智能问答","语音助手","KBQA"],"title":"从零到一：如何构建一个基于知识图谱的智能问答助手？","uri":"/siwi/#graph-database"},{"categories":["Nebula Graph"],"content":"Docker 部署情况下使用 Python Storage Client 指南","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/"},{"categories":["Nebula Graph"],"content":" Python Storage Client 连不上第一次安装的 Nebula Graph？ Docker 部署情况下使用 Python Storage Client 指南 ","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:0:0","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#"},{"categories":["Nebula Graph"],"content":"1 前置条件 注意：一个重要的前置条件是我们真的需要 Storage Client，如果我们只需要在 Python Client 里通过 nGQL 来请求数据，那么 GraphClient 才是你需要的，可以跳过本文。 对于刚接触 Nebula 的同学，部署集群最快速的方式是用 Docker Compose，安装部署可以参考文档：deploy-nebula-graph-with-docker-compose。 在 docker compose up -d 之后，我们的 nebula 集群就起来了，我们可以用 docker ps 看看运行着的容器： ❯ docker ps --format \"table {{.Names}}\\t{{.Ports}}\" NAMES PORTS nebula-docker-compose-graphd1-1 0.0.0.0:61852-\u003e9669/tcp, 0.0.0.0:61850-\u003e19669/tcp, 0.0.0.0:61851-\u003e19670/tcp nebula-docker-compose-graphd-1 0.0.0.0:9669-\u003e9669/tcp, 0.0.0.0:61858-\u003e19669/tcp, 0.0.0.0:61859-\u003e19670/tcp nebula-docker-compose-graphd2-1 0.0.0.0:61855-\u003e9669/tcp, 0.0.0.0:61853-\u003e19669/tcp, 0.0.0.0:61854-\u003e19670/tcp nebula-docker-compose-storaged2-1 9777-9778/tcp, 9780/tcp, 0.0.0.0:61868-\u003e9779/tcp, 0.0.0.0:61869-\u003e19779/tcp, 0.0.0.0:61870-\u003e19780/tcp nebula-docker-compose-storaged1-1 9777-9778/tcp, 9780/tcp, 0.0.0.0:61865-\u003e9779/tcp, 0.0.0.0:61866-\u003e19779/tcp, 0.0.0.0:61864-\u003e19780/tcp nebula-docker-compose-storaged0-1 9777-9778/tcp, 9780/tcp, 0.0.0.0:61845-\u003e9779/tcp, 0.0.0.0:61843-\u003e19779/tcp, 0.0.0.0:61844-\u003e19780/tcp nebula-docker-compose-metad1-1 9560/tcp, 0.0.0.0:61705-\u003e9559/tcp, 0.0.0.0:61706-\u003e19559/tcp, 0.0.0.0:61707-\u003e19560/tcp nebula-docker-compose-metad2-1 9560/tcp, 0.0.0.0:61699-\u003e9559/tcp, 0.0.0.0:61700-\u003e19559/tcp, 0.0.0.0:61701-\u003e19560/tcp nebula-docker-compose-metad0-1 9560/tcp, 0.0.0.0:61704-\u003e9559/tcp, 0.0.0.0:61702-\u003e19559/tcp, 0.0.0.0:61703-\u003e19560/tcp 这里边有三种容器 ： GrpahD，查询引擎，也是我们用户进行登录、连接、发 Query 请求直接访问的唯一一种服务、接口。 MetaD，元数据服务，它一般不会暴露给外部，只有 GraphD 、StorageD 会直接访问它。 StorageD，存储引擎，它一般不会暴露给外部，只有 GraphD、StorageD 会直接访问它。 我们可以从 Stuido Console，或者 Nebula Console （连接到 GraphD 之后）里通过 SHOW HOSTS \u003cType\u003e，来获取每种服务的信息，其中第一列的信息就是他们的 IP 或者 Host，下边的例子是 Docker Compose 默认配置下的情况。 (root@nebula) [(none)]\u003e SHOW HOSTS GRAPH +-----------+------+----------+---------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-----------+------+----------+---------+--------------+---------+ | \"graphd\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"2.6.0\" | +-----------+------+----------+---------+--------------+---------+ | \"graphd1\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"2.6.0\" | +-----------+------+----------+---------+--------------+---------+ | \"graphd2\" | 9669 | \"ONLINE\" | \"GRAPH\" | \"3ba41bd\" | \"2.6.0\" | +-----------+------+----------+---------+--------------+---------+ (root@nebula) [(none)]\u003e SHOW HOSTS META +----------+------+----------+--------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +----------+------+----------+--------+--------------+---------+ | \"metad1\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"2.6.0\" | +----------+------+----------+--------+--------------+---------+ | \"metad0\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"2.6.0\" | +----------+------+----------+--------+--------------+---------+ | \"metad2\" | 9559 | \"ONLINE\" | \"META\" | \"3ba41bd\" | \"2.6.0\" | +----------+------+----------+--------+--------------+---------+ (root@nebula) [(none)]\u003e SHOW HOSTS STORAGE +-------------+------+----------+-----------+--------------+---------+ | Host | Port | Status | Role | Git Info Sha | Version | +-------------+------+----------+-----------+--------------+---------+ | \"storaged0\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"2.6.0\" | +-------------+------+----------+-----------+--------------+---------+ | \"storaged1\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"2.6.0\" | +-------------+------+----------+-----------+--------------+---------+ | \"storaged2\" | 9779 | \"ONLINE\" | \"STORAGE\" | \"3ba41bd\" | \"2.6.0\" | +-------------+------+----------+-----------+--------------+---------+ 在默认的 nebula-docker-compose 的配置下各个服务为graphd, metad1, storaged0 这种域名的 Host 格式，这其实是假设了我们使用 Docker 启动的服务一般是作为单机测试之用，除了其中的 graphd之外，其他的服务都没有指定固定的外部映射端口（见前边的 docker ps 的结果里，它暴露在0.0.0.0:9669)。 这意味着，如果客户端不是运行在本机，访问其他服务的端口都是动态的，这会让很多第一次想用 Python Storage Client 连接服务的同学卡住。 所以我这里给大家分享一个快速用 Python 去调试 Storage Client 的方法： 本质上我们可以通过修改 Compose 的配置文件、通过其他部署或者配置的方式安装 Nebula 来保证 Python Client 能够访","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:1:0","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#前置条件"},{"categories":["Nebula Graph"],"content":"1.1 第一步，把 Python 容器在 nebula-docker-compose_nebula-net 这个容器网络启动一个 Jupyter 的容器。 这里采用了 https://github.com/jupyter/docker-stacks 维护的 Docker Image。 docker run \\ -p 8888:8888 \\ --network nebula-docker-compose_nebula-net \\ jupyter/scipy-notebook:33add21fab64 可以看到容器了，并且在监听端口：8888。 [I 07:06:31.060 NotebookApp] Jupyter Notebook 6.3.0 is running at: [I 07:06:31.060 NotebookApp] http://e170a5eb4858:8888/?token=5beafb26fc6995b081c611d5d2cc96d557897b74bfdaac53 [I 07:06:31.060 NotebookApp] or http://127.0.0.1:8888/?token=5beafb26fc6995b081c611d5d2cc96d557897b74bfdaac53 这时候我们可在浏览器打开 http://127.0.0.1:8888/?token=5beafb26fc6995b081c611d5d2cc96d557897b74bfdaac53. 在里边新建一个 Notebook。 ","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:1:1","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#第一步把-python-容器"},{"categories":["Nebula Graph"],"content":"1.2 第二步，安装 Nebula Python SDK我们只需要在 Jupyter 里执行 !pip install nebula2-python==2.6.0 就可以了，具体的版本要根据 Nebula Python 的 README 里的版本对应关系来给定。 ","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:1:2","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#第二步安装-nebula-python-sdk"},{"categories":["Nebula Graph"],"content":"1.3 第三步，实例化 MetaCache 和 GraphStorageClient from nebula2.mclient import MetaCache, HostAddr from nebula2.sclient.GraphStorageClient import GraphStorageClient # Docker Compose 下默认 meta 的地址是 metad1 metad0 metad2 meta_cache = MetaCache([('metad1',9559),('metad0',9559),('metad2',9559)]) graph_storage_client = GraphStorageClient(meta_cache) ","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:1:3","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#第三步实例化-metacache-和-graphstorageclient"},{"categories":["Nebula Graph"],"content":"1.4 第四步，扫数据 按空间、点类型扫点 resp = graph_storage_client.scan_vertex( space_name='basketballplayer', tag_name='player') while resp.has_next(): result = resp.next() for vertex_data in result: print(vertex_data) 按空间、边类型扫边 resp = graph_storage_client.scan_edge( space_name='basketballplayer', edge_name='follow') while resp.has_next(): result = resp.next() for edge_data in result: print(edge_data) Jupyter 的过程我也记录在这个 notebook 里方便大家参考。 Happy Graphing! Picture Credit：Borderpolar Photographer ","date":"2021-12-10","objectID":"/nebula-python-storage-docker-guide/:1:4","series":null,"tags":["Nebula Graph","图数据库","python"],"title":"快速搭建调试 Nebula Graph Python Storage 客户端的环境","uri":"/nebula-python-storage-docker-guide/#第四步扫数据"},{"categories":["Nebula Graph"],"content":"如何快速、即时、符合直觉地去处理 Nebula Java Client 中的数据解析？","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/"},{"categories":["Nebula Graph"],"content":" 如何快速、即时、符合直觉地去处理 Nebula Java Client 中的数据解析？读这一篇就够了。 ","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/:0:0","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/#"},{"categories":["Nebula Graph"],"content":"1 关键步骤：几行准备一个干净的交互式 Nebula Java REPL 环境多亏了 Java-REPL 我们可以很方便地（像 iPython 那样）去实时交互地调试、分析 Nebula Java 客户端，我们用它的 Docker 镜像可以很干净的去搞定： docker pull albertlatacz/java-repl docker run --rm -it \\ --network=nebula-docker-compose_nebula-net \\ -v ~:/root \\ albertlatacz/java-repl \\ bash apt update -y \u0026\u0026 apt install ca-certificates -y wget https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz --no-check-certificate tar xzvf apache-maven-3.8.4-bin.tar.gz wget https://github.com/vesoft-inc/nebula-java/archive/refs/tags/v2.6.1.tar.gz tar xzvf v2.6.1.tar.gz cd nebula-java-2.6.1/ ../apache-maven-3.8.4/bin/mvn dependency:copy-dependencies ../apache-maven-3.8.4/bin/mvn -B package -Dmaven.test.skip=true java -jar ../javarepl/javarepl.jar 这时候，在执行完 java -jar ../javarepl/javarepl.jar 之后，我们就进入了交互式的 Java Shell（REPL），我们可以无需做编译，执行，print 这样的慢反馈来调试和研究我们的代码了，是不是很方便？ root@a2e26ba62bb6:/javarepl/nebula-java-2.6.1# java -jar ../javarepl/javarepl.jar Welcome to JavaREPL version 428 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111) Type expression to evaluate, :help for more options or press tab to auto-complete. Connected to local instance at http://localhost:43707 java\u003e System.out.println(\"Hello, World!\"); Hello, World! java\u003e 首先我们在 java\u003e 提示符下，这些来把必须的类路径和导入： :cp /javarepl/nebula-java-2.6.1/client/target/client-2.6.1.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/fastjson-1.2.78.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-api-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/slf4j-log4j12-1.7.25.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/commons-pool2-2.2.jar :cp /javarepl/nebula-java-2.6.1/client/target/dependency/log4j-1.2.17.jar import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.vesoft.nebula.ErrorCode; import com.vesoft.nebula.client.graph.NebulaPoolConfig; import com.vesoft.nebula.client.graph.data.CASignedSSLParam; import com.vesoft.nebula.client.graph.data.HostAddress; import com.vesoft.nebula.client.graph.data.ResultSet; import com.vesoft.nebula.client.graph.data.SelfSignedSSLParam; import com.vesoft.nebula.client.graph.data.ValueWrapper; import com.vesoft.nebula.client.graph.net.NebulaPool; import com.vesoft.nebula.client.graph.net.Session; import java.io.UnsupportedEncodingException; import java.util.Arrays; import java.util.List; import java.util.concurrent.TimeUnit; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.*; 我们可以从这 Java 环境连接到 Nebula Graph 里，下边的例子里我用了自己的 GraphD 的 IP 和端口作为例子： NebulaPoolConfig nebulaPoolConfig = new NebulaPoolConfig(); nebulaPoolConfig.setMaxConnSize(10); List\u003cHostAddress\u003e addresses = Arrays.asList(new HostAddress(\"192.168.8.127\", 9669)); NebulaPool pool = new NebulaPool(); pool.init(addresses, nebulaPoolConfig); Session session = pool.getSession(\"root\", \"nebula\", false); ","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/:1:0","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/#关键步骤几行准备一个干净的交互式-nebula-java-repl-环境"},{"categories":["Nebula Graph"],"content":"2 通过调用 \u003ccode\u003eexecute\u003c/code\u003e 方法获得不太容易懂的 ResultSet 对象刚接触这里的大家一定对这个 ResultSet 对象有些愁，借助我们的环境，咱们来十分钟把它搞通吧，这里我们执行一个简单的返回 Vertex 顶点的结果看看： ResultSet resp = session.execute(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); 这里我们可以参考 ResultSet 的代码： Reference: client/graph/data/ResultSet.java 好吧，其实可以先不看，跟着我的教程往下走吧，我们知道结果都是二维的表，ResultSet 提供了常见的针对行、列的一些方法，通常，我们是获取每一行，然后解析它，而关键的问题是每一个值要怎么处理，对吧。 java\u003e resp.isSucceeded() java.lang.Boolean res9 = true java\u003e resp.rowsSize() java.lang.Integer res16 = 1 java\u003e rows = resp.getRows() java.util.ArrayList rows = [Row ( values : [ \u003cValue vVal:Vertex ( vid : \u003cValue sVal:70 6c 61 79 65 72 31 30 30\u003e, tags : [ Tag ( name : 70 6C 61 79 65 72, props : { [B@5264a468 : \u003cValue iVal:42\u003e [B@496b8e10 : \u003cValue sVal:54 69 6d 20 44 75 6e 63 61 6e\u003e } ) ] )\u003e ] )] java\u003e row0 = resp.rowValues(0) java.lang.Iterable\u003ccom.vesoft.nebula.client.graph.data.ValueWrapper\u003e res10 = ColumnName: [n], Values: [(\"player100\" :player {name: \"Tim Duncan\", age: 42})] 我们其实回到这次的 query ，其实是返回一个 vertex：顶点： (root@nebula) [basketballplayer]\u003e match (n:player) WHERE n.name == \"Tim Duncan\" return n +----------------------------------------------------+ | n | +----------------------------------------------------+ | (\"player100\" :player{age: 42, name: \"Tim Duncan\"}) | +----------------------------------------------------+ Got 1 rows (time spent 2116/44373 us) 通过上边的几个方法，我们其实能够获得这个顶点的值： v = Class.forName(\"com.vesoft.nebula.Value\") v.getDeclaredMethods() 然而，我们可以看出来这个 com.vesoft.nebula.Value 的值的类提供的方法特别特别原始，这也是让大家犯愁的原因，在这个教程里最重要的一个带走的经验（除了利用 REPL之外）就是：除非必要，不要去取这个原始的类，我们应该去取得 ValueWrapper 封装之后的值！！！ 注意：其实我们有更轻松地方法，就是用 executeJson 直接获得 JSON string，别担心，会在后边提到，不过这个方法要 2.6 之后才支持。 那么问题来了，如何使用 ValueWrapper 封装呢？其实答案已经在上边了，大家可以回去看看，resp.rowValues(0) 的类型正是 ValueWrapper 的可迭代对象！ 所以，正确打开方式是迭它！迭它！迭它！其实这个就是代码库里的 GraphClientExample 的一部分例子了，我们把它迭代取出来，放到 wrappedValueList 里慢慢把玩： import java.util.ArrayList; import java.util.List; List\u003cValueWrapper\u003e wrappedValueList = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c resp.rowsSize(); i++) { ResultSet.Record record = resp.rowValues(i); for (ValueWrapper value : record.values()) { wrappedValueList.add(value); if (value.isLong()) { System.out.printf(\"%15s |\", value.asLong()); } if (value.isBoolean()) { System.out.printf(\"%15s |\", value.asBoolean()); } if (value.isDouble()) { System.out.printf(\"%15s |\", value.asDouble()); } if (value.isString()) { System.out.printf(\"%15s |\", value.asString()); } if (value.isTime()) { System.out.printf(\"%15s |\", value.asTime()); } if (value.isDate()) { System.out.printf(\"%15s |\", value.asDate()); } if (value.isDateTime()) { System.out.printf(\"%15s |\", value.asDateTime()); } if (value.isVertex()) { System.out.printf(\"%15s |\", value.asNode()); } if (value.isEdge()) { System.out.printf(\"%15s |\", value.asRelationship()); } if (value.isPath()) { System.out.printf(\"%15s |\", value.asPath()); } if (value.isList()) { System.out.printf(\"%15s |\", value.asList()); } if (value.isSet()) { System.out.printf(\"%15s |\", value.asSet()); } if (value.isMap()) { System.out.printf(\"%15s |\", value.asMap()); } } System.out.println(); } 上边这些很丑的 if 就是关键了，我们知道 query 的返回值可能是多种类型的，他们分为： 图语义的：点、边、路径 数据类型：String，日期，列表，集合 等等等 这里的关键是，我们要使用 ValueWrapper 为我们准备好的 asXxx 方法，如果这个值是一个顶点，么这个 Xxx 就是 Node，同理如果是边的话，这个 Xxx 就是 Relationship。 所以，我给大家看看咱们这个返回点结果的情况下的 asNode() 方法： java\u003e v = wrappedValueList.get(0) com.vesoft.nebula.client.graph.data.ValueWrapper v = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e v.asNode() com.vesoft.nebula.client.graph.data.Node res16 = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) java\u003e node = v.asNode() com.vesoft.nebula.client.graph.data.Node node = (\"player100\" :player {name: \"Tim Duncan\", age: 42}) 顺便说一下，借助于 Java 的 reflection ，我们可以在这个交互程序里做类似于 Python 里 dir() 的事情，实时地去获取一个类支持的方法，像这样，省去了查代码。 java\u003e rClass=Class.forName(\"com.vesoft.nebula.client.graph.data.ResultSet\") java.lang.Class r = class com.vesoft.nebula.client.graph.data.ResultSet java\u003e rClass.getDeclaredMethods() java.l","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/:2:0","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/#通过调用-execute-方法获得不太容易懂的-resultset-对象"},{"categories":["Nebula Graph"],"content":"3 直接返回 JSON 的 \u003ccode\u003eexecuteJson\u003c/code\u003e 方法最后，好消息是，从 2.6 开始，nebula 可以直接返回 JSON 的 String 了，我们上边的纠结也都不是必要的了： java\u003e String resp_json = session.executeJson(\"USE basketballplayer;MATCH (n:player) WHERE n.name==\\\"Tim Duncan\\\" RETURN n\"); java.lang.String resp_json = \" { \"errors\":[ { \"code\":0 } ], \"results\":[ { \"spaceName\":\"basketballplayer\", \"data\":[ { \"meta\":[ { \"type\":\"vertex\", \"id\":\"player100\" } ], \"row\":[ { \"player.age\":42, \"player.name\":\"Tim Duncan\" } ] } ], \"columns\":[ \"n\" ], \"errors\":{ \"code\":0 }, \"latencyInUs\":4761 } ] } \" 我相信大家肯定比我更擅长处理 JSON 的结果了哈~~ ","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/:3:0","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/#直接返回-json-的-executejson-方法"},{"categories":["Nebula Graph"],"content":"4 结论 如果我们有条件（2.6以后）用 JSON，情况会很容易，可能大家不太需要本文的方法（不过有交互环境还是很方便吧？） 如果我们不得不和 resultSet 打交道，记得用 ValueWrapper ，因为我们可以用 asNode()，asRelationship() 和 asPath() ，封装之后的值比原始的值可爱太多了！ 通过 REPL 工具，结合 Java 的 reflection 加上 源代码本身，分析数据的处理将变得异常顺滑 Happy Graphing! Picture Credit：leunesmedia ","date":"2021-11-25","objectID":"/nebula-java-happy-parsing-guide/:4:0","series":null,"tags":["Nebula Graph","图数据库","java"],"title":"Nebula Graph 的 Java 数据解析实践与指导","uri":"/nebula-java-happy-parsing-guide/#结论"},{"categories":["Nebula Graph","Mini Project"],"content":"如何利用图数据库从零到一构建一个具有股权分析的图谱与线上系统呢？本文手把手带你构建一个简易版的股权穿透图谱系统。","date":"2021-11-24","objectID":"/corp-rel-graph/","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/"},{"categories":["Nebula Graph","Mini Project"],"content":" 如何构建一个具有股权分析的图谱与线上系统呢？本文里，我将利用图数据库从零到一带你构建一个简易版的股权穿透图谱系统。 我们知道无论是监管部门、企业还是个人，都有需求去针对一个企业、法人做一些背景调查，这些调查可以是法律诉讼、公开持股、企业任职等等多种多样的信息。这些背景信息可以辅助我们做商业上的重要决策，规避风险：比如根据公司的股权关系，了解是否存在利益冲突比如是否选择与一家公司进行商业往来。 在满足这样的关系分析需求的时候，我们往往面临一些挑战，比如： 如何将这些数据的关联关系体现在系统之中？使得它们可以被挖掘、利用 多种异构数据、数据源之间的关系可能随着业务的发展引申出更多的变化，在结构数据库中，这意味着 Schema 变更 分析系统需要尽可能实时获取需要的查询结果，这通常涉及到多跳关系查询 领域专家能否快速灵活、可视化获取分享信息 那么如何构建这样一个系统解决以上挑战呢？ ","date":"2021-11-24","objectID":"/corp-rel-graph/:0:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#"},{"categories":["Nebula Graph","Mini Project"],"content":"1 数据存在哪里？ 前提：数据集准备，为了更好的给大家演示解决这个问题，我写了一个轮子能随机生成股权结构相关的数据，生成的数据的例子在这里。 这里，我们有法人、公司的数据，更有公司与子公司之间的关系，公司持有公司股份，法人任职公司，法人持有公司股份和法人之间亲密度的关系数据。 数据存在哪里？这是一个关键的问题，这里我们剧透一下，答案是：图数据库。然后我们再简单解释一下为什么这样一个股权图谱系统跑在图数据库上是更好的。 在这样一个简单的数据模型之下，我们可以很直接的在关系型数据库中这么建模： 而这么建模的问题在于：这种逻辑关联的方式使得无论数据的关联关系查询表达、存储、还是引入新的关联关系都不是很高效。 查询表达不高效是因为关系型数据库是面向表结构设计的，这决定了关系查询要写嵌套的 JOIN。 这就是前边提到的挑战 1：能够表达，但是比较勉强，遇到稍微复杂的情况就变得很难。 存储不高效是因为表结构被设计的模式是面向数据记录，而非数据之间的关系：我们虽然习惯了将数据中实体（比如法人）和实体关联（比如持有股权 hold_sharing_relationship）以另外一个表中的记录来表达、存储起来，这逻辑上完全行得通，但是到了多跳、大量需要请求数据关系跳转的情况下，这样跨表 JOIN 的代价就成为了瓶颈。 这就是前边提到的挑战 3：无法应对多条查询的性能需要。 引入新的关联关系代价大，还是前边提到的，表结构下，用新的表来表达持有股权 hold_sharing_relationship这个关联关系是可行的，但是这非常不灵活、而且昂贵，它意味着我们在引入这个关系的时候限定了起点终点的类型，比如股权持有的关系可能是法人-\u003e公司，也可能是公司-\u003e公司，随着业务的演进，我们可能还需要引入政府-\u003e公司的新关系，而这些变化都需要做有不小代价的工作：改动 Schema。 这就是前边提到的挑战 2：无法应对业务上对数据关系上灵活多变的要求。 当一个通用系统无法满足不可忽视的具体需求的时候，一个新的系统就会诞生，这就是图数据库，针对这样的场景，图数据库很自然地特别针对关联关系场景去设计整个数据库： 面向关联关系表达的语义。（挑战 1） 如下表，我列举了一个等价的一跳查询在表结构数据库与图数据库中，查询语句的区别。大家应该可以看出“找到所有持有和 p_100 共同持有公司股份的人”这样的查询表达可以在图数据库如何自然表达，这仅仅是一条查询的区别，如果是多跳的话，他们的复杂度区分还会更明显一些。 表结构数据库 图数据库（属性图） 将关联关系存储为物理连接，从而使得跳转查询代价最小。（挑战 3、2） 图数据之中，从点拓展（找到一个或者多个关系的另一头）出去的代价是非常小的，这因为图数据库是一个专有的系统，得益于它主要关心“图”结构的设计，查找确定的实体（比如和一个法人 A ）所有关联（可能是任职、亲戚、持有、等等关系）其他所有实体（公司、法人）这个查找的代价是 O(1) 的，因为它们在图数据库的数据机构里是真的链接在一起的。 大家可以从下表的定量参考数据一窥图数据库在这种查询下的优势，这种优势在多跳高并发情况下的区别是“能”与”不能“作为线上系统的区别，是“实时”与“离线”的区别。 在面向关联关系的数据建模和数据结构之下，引入新的实体、关联关系的代价要小很多，还是前边提到的例子： 在 Nebula Graph 图数据中引入一个新的“政府机构”类型的实体，并增加政府机构-\u003e公司的“持有股份”的关联关系相比于在非图模型的数据库中的代价小很多。 表结构数据库 图数据库（属性图） 4 跳查询时延 1544 秒 4 跳查询时延 1.36 秒 建模符合直觉；图数据库有面向数据连接的数据可视化能力（挑战 4） 大家在下表第二列中可以对比我们本文中进行的股权分析数据在两种数据库之中的建模的区别，尤其是在关心关联关系的场景下，我们可以感受到属性图的模型建立是很符合人类大脑直觉的，而这和大脑之中神经元的结构可能也有一些关系。 图数据库中内置的可视化工具提供了一般用户便捷理解数据关系的能力，也给领域专家用户提供了表达请求复杂数据关系的直观接口。 表结构数据库 图数据库（属性图） 表结构数据库与图数据库的总体比较： 表结构数据库 图数据库（属性图） 查询 建模 性能 4 跳查询时延 1544 秒 4 跳查询时延 1.36 秒 综上，在本教程里，我们将利用图数据库来进行数据存储。 ","date":"2021-11-24","objectID":"/corp-rel-graph/:1:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#数据存在哪里"},{"categories":["Nebula Graph","Mini Project"],"content":"2 图数据建模前面在讨论数据存在哪里的时候，我们已经揭示了在图数据库中建模的方式：本质上，在这张图中，将会有两种实体： 人 公司 四种关系： 人 –作为亲人–\u003e人 人 –作为角色–\u003e 公司 人 或者 公司 –持有股份–\u003e 公司 公司 –作为子机构–\u003e 公司 这里面，实体与关系本身都可以包含更多的信息，这些信息在图数据库里就是实体、关系自身的属性。如下图表示： 人的属性包括 name，age 公司的属性包括 name，location 持有股份 这个关系有属性 share(份额) 任职这个关系有属性 role，level ","date":"2021-11-24","objectID":"/corp-rel-graph/:2:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#图数据建模"},{"categories":["Nebula Graph","Mini Project"],"content":"3 数据入库本教程中，我们使用的图数据库叫做 Nebula Graph（星云图数据库），它是一个以 Apache 2.0 许可证开源的分布式图数据库。 Nebula Graph in Github: https://github.com/vesoft-inc/nebula 在向 Nebula Graph 导入数据的时候，关于如何选择工具，请参考这篇文档和这个视频。 这里，由于数据格式是 csv 文件并且利用单机的客户端资源就足够了，我们可以选择使用 nebula-importer 来完成这个工作。 提示：在导入数据之前，请先部署一个 Nebula Graph 集群，最简便的部署方式是使用 nebula-up 这个小工具，只需要一行命令就能在 Linux 机器上同时启动一个 Nebula Graph 核心和可视化图探索工具 Nebula Graph Studio。如果你更愿意用 Docker 部署，请参考这个文档。 本文假设我们使用 Nebula-UP 来部署： curl -fsSL nebula-up.siwei.io/install.sh | bash 这里的数据是生成器生成的，你可以按需生成任意规模随机数据集，或者选择一份生成好了的数据在这里 有了这些数据，我们可以开始导入了。 $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv 导入工具 nebula-importer 是一个 golang 的二进制文件，使用方式就是将导入的 Nebula Graph 连接信息、数据源中字段的含义的信息写进 YAML 格式的配置文件里，然后通过命令行调用它。可以参考文档或者它的 GitHub 仓库里的例子。 这里我已经写好了准备好了一份 nebula-importer 的配置文件，在数据生成器同一个 repo 之下的这里。 最后，只需要执行如下命令就可以开始数据导入了： 注意，在写本文的时候，nebula 的新版本是 2.6.1，这里对应的 nebula-importer 是 v2.6.0，如果您出现导入错误可能是版本不匹配，可以相应调整下边命令中的版本号。 git clone https://github.com/wey-gu/nebula-shareholding-example cp -r data_sample /tmp/data cp nebula-importer.yaml /tmp/data/ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /tmp/data:/root \\ vesoft/nebula-importer:v2.6.0 \\ --config /root/nebula-importer.yaml 你知道吗？TL;DR 实际上，这份 importer 的配置里帮我们做了 Nebula Graph 之中的图建模的操作，它们的指令在下边，我们不需要手动去执行了。 CREATESPACEIFNOTEXISTSshareholding(partition_num=5,replica_factor=1,vid_type=FIXED_STRING(10));USEshareholding;CREATETAGperson(namestring);CREATETAGcorp(namestring);CREATETAGINDEXperson_nameonperson(name(20));CREATETAGINDEXcorp_nameoncorp(name(20));CREATEEDGErole_as(rolestring);CREATEEDGEis_branch_of();CREATEEDGEhold_share(sharefloat);CREATEEDGEreletive_with(degreeint); ","date":"2021-11-24","objectID":"/corp-rel-graph/:3:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#数据入库"},{"categories":["Nebula Graph","Mini Project"],"content":"4 图库中查询数据 Tips: 你知道吗，你也可以无需部署安装，通过 Nebula-Playground 之中，找到股权穿透来在线访问同一份数据集。 我们可以借助 Nebula Graph Studio 来访问数据，访问我们部署 Nebula-UP 的服务器地址的 7001 端口就可以了： 假设服务器地址为 192.168.8.127，则有： Nebula Studio 地址：192.168.8.127:7001 Nebula Graph 地址：192.168.8.127:9669 默认用户名：root 默认密码：nebula 访问 Nebula Studio： 选择图空间: Shareholding 之后，我们就可以在里边探索比如一个公司的三跳以内的股权穿透，具体的操作可以参考：股权穿透在线 Playground 的介绍： ","date":"2021-11-24","objectID":"/corp-rel-graph/:4:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#图库中查询数据"},{"categories":["Nebula Graph","Mini Project"],"content":"5 构建一个图谱系统 这部分的代码开源在 GitHub 上： https://github.com/wey-gu/nebula-corp-rel-search 本项目的 Demo 也在 PyCon China 2021 上的演讲中有过展示：视频地址 在此基础之上，我们可以构建一个提供给终端用户来使用的股权查询系统了，我们已经有了图数据库作为这个图谱的存储引擎，理论上，如果业务允许，我们可以直接使用或者封装 Nebula Graph Studio 来提供服务，这完全是可行也是合规的，不过，有一些情况下，我们需要自己去实现界面、或者我们需要封装出一个 API 给上游（多端）提供图谱查询的功能。 为此，我为大家写了一个简单的实例项目，提供这样的服务，他的架构也很直接： 前端接受用户要查询的穿透法人、公司，按需发请求给后端，并用 D3.js 将返回结果渲染为关系图 后端接受前端的 API 请求，将请求转换为 Graph DB 的查询，并返回前端期待的结果 ┌───────────────┬───────────────┐ │ │ Frontend │ │ │ │ │ ┌──────────▼──────────┐ │ │ │ Vue.JS │ │ │ │ D3.JS │ │ │ └──────────┬──────────┘ │ │ │ Backend │ │ ┌──────────┴──────────┐ │ │ │ Flask │ │ │ │ Nebula-Python │ │ │ └──────────┬──────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ Graph Database │ │ │ └─────────────────────┘ │ │ │ └───────────────────────────────┘ ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#构建一个图谱系统"},{"categories":["Nebula Graph","Mini Project"],"content":"5.1 后端服务\u0026ndash;\u0026gt;图数据库 详细的数据格式分析大家可以参考这里 5.1.1 查询语句我们假设用户请求的实体是 c_132 ，那么请求 1 到 3 步的关系穿透的语法是： MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 这里边 ()包裹的是图之中的点，而[] 包裹的则是点之间的关系：边，所以： (v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) 之中的： (v)-[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]-(v2)应该比较好理解，意思是从 v 到v2 做拓展。 现在我们介绍中间[]包裹的部分，这里，它的语义是：经由四种类型的边（:之后的是边的类型，|代表或者）通过可变的跳数：*1..3 （一跳到三跳）。 所以，简单来说整理看开，我们的拓展的路径是：从点 v 开始，经由四种关系一到三跳拓展到点v2，返回整个拓展路径 p，限制 100 个路径结果，其中 v 是 c_132。 5.1.2 Nebula Python Client/ SDK我们已经知道了查询语句的语法，那么就只需要在后端程序里根据请求、通过图数据库的客户端来发出查询请求，并处理返回结构就好了。在今天的例子中，我选择使用 Python 来实现后端的逻辑，所以我用了 Nebula-python 这个库，它是 Nebula 的 Python Client。 你知道么？截至到现在，Nebula 在 GitHub 上有 Java，GO，Python，C++，Spark，Flink，Rust（未GA），NodeJS（未GA） 的客户端支持，更多的语言的客户端也会慢慢被发布哦。 下边是一个 Python Client 执行一个查询并返回结果的例子，值得注意的是，在我实现这个代码的时候，Nebula Python 尚未支持返回 JSON （通过session.execute_json()）结果，如果你要实现自己的代码，我非常推荐试试 JSON 哈，就可以不用从对象中一点点取数据了，不过借助 iPython/IDLE 这种 REPL，快速了解返回对象的结构也没有那么麻烦。 $ python3 -m pip install nebula2-python==2.5.0 # 注意这里我引用旧的记录，它是 2.5.0， $ ipython In [1]: from nebula2.gclient.net import ConnectionPool In [2]: from nebula2.Config import Config In [3]: config = Config() ...: config.max_connection_pool_size = 10 ...: # init connection pool ...: connection_pool = ConnectionPool() ...: # if the given servers are ok, return true, else return false ...: ok = connection_pool.init([('192.168.8.137', 9669)], config) ...: session = connection_pool.get_session('root', 'nebula') [2021-10-13 13:44:24,242]:Get connection to ('192.168.8.137', 9669) In [4]: resp = session.execute(\"use shareholding\") In [5]: query = ''' ...: MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ ...: WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 ...: ''' In [6]: resp = session.execute(query) # Note: after nebula graph 2.6.0, we could use execute_json as well In [7]: resp.col_size() Out[7]: 1 In [9]: resp.row_size() Out[10]: 100 我们往下分析看看，我们知道这个请求本质上结果是路径，它有一个 .nodes() 方法和 .relationships()方法来获得路径上的点和边： In [11]: p=resp.row_values(22)[0].as_path() In [12]: p.nodes() Out[12]: [(\"c_132\" :corp{name: \"Chambers LLC\"}), (\"p_4000\" :person{name: \"Colton Bailey\"})] In [13]: p.relationships() Out[13]: [(\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-\u003e(\"c_132\")] 对于边来说有这些方法 .edge_name(), .properties(), .start_vertex_id(), .end_vertex_id()，这里 edge_name 是获得边的类型。 In [14]: rel=p.relationships()[0] In [15]: rel Out[15]: (\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-\u003e(\"c_132\") In [16]: rel.edge_name() Out[16]: 'role_as' In [17]: rel.properties() Out[17]: {'role': \"Editorial assistant\"} In [18]: rel.start_vertex_id() Out[18]: \"p_4000\" In [19]: rel.end_vertex_id() Out[19]: \"c_132\" 对于点来说，可以用到这些方法 .tags(), properties, get_id()，这里边 tags 是获得点的类型，它在 Nebula 里叫标签tag。 这些概念可以在文档里获得更详细的解释。 In [20]: node=p.nodes()[0] In [21]: node.tags() Out[21]: ['corp'] In [22]: node.properties('corp') Out[22]: {'name': \"Chambers LLC\"} In [23]: node.get_id() Out[23]: \"c_132\" ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:1","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#后端服务--图数据库"},{"categories":["Nebula Graph","Mini Project"],"content":"5.1 后端服务–\u003e图数据库 详细的数据格式分析大家可以参考这里 5.1.1 查询语句我们假设用户请求的实体是 c_132 ，那么请求 1 到 3 步的关系穿透的语法是： MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 这里边 ()包裹的是图之中的点，而[] 包裹的则是点之间的关系：边，所以： (v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) 之中的： (v)-[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]-(v2)应该比较好理解，意思是从 v 到v2 做拓展。 现在我们介绍中间[]包裹的部分，这里，它的语义是：经由四种类型的边（:之后的是边的类型，|代表或者）通过可变的跳数：*1..3 （一跳到三跳）。 所以，简单来说整理看开，我们的拓展的路径是：从点 v 开始，经由四种关系一到三跳拓展到点v2，返回整个拓展路径 p，限制 100 个路径结果，其中 v 是 c_132。 5.1.2 Nebula Python Client/ SDK我们已经知道了查询语句的语法，那么就只需要在后端程序里根据请求、通过图数据库的客户端来发出查询请求，并处理返回结构就好了。在今天的例子中，我选择使用 Python 来实现后端的逻辑，所以我用了 Nebula-python 这个库，它是 Nebula 的 Python Client。 你知道么？截至到现在，Nebula 在 GitHub 上有 Java，GO，Python，C++，Spark，Flink，Rust（未GA），NodeJS（未GA） 的客户端支持，更多的语言的客户端也会慢慢被发布哦。 下边是一个 Python Client 执行一个查询并返回结果的例子，值得注意的是，在我实现这个代码的时候，Nebula Python 尚未支持返回 JSON （通过session.execute_json()）结果，如果你要实现自己的代码，我非常推荐试试 JSON 哈，就可以不用从对象中一点点取数据了，不过借助 iPython/IDLE 这种 REPL，快速了解返回对象的结构也没有那么麻烦。 $ python3 -m pip install nebula2-python==2.5.0 # 注意这里我引用旧的记录，它是 2.5.0， $ ipython In [1]: from nebula2.gclient.net import ConnectionPool In [2]: from nebula2.Config import Config In [3]: config = Config() ...: config.max_connection_pool_size = 10 ...: # init connection pool ...: connection_pool = ConnectionPool() ...: # if the given servers are ok, return true, else return false ...: ok = connection_pool.init([('192.168.8.137', 9669)], config) ...: session = connection_pool.get_session('root', 'nebula') [2021-10-13 13:44:24,242]:Get connection to ('192.168.8.137', 9669) In [4]: resp = session.execute(\"use shareholding\") In [5]: query = ''' ...: MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ ...: WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 ...: ''' In [6]: resp = session.execute(query) # Note: after nebula graph 2.6.0, we could use execute_json as well In [7]: resp.col_size() Out[7]: 1 In [9]: resp.row_size() Out[10]: 100 我们往下分析看看，我们知道这个请求本质上结果是路径，它有一个 .nodes() 方法和 .relationships()方法来获得路径上的点和边： In [11]: p=resp.row_values(22)[0].as_path() In [12]: p.nodes() Out[12]: [(\"c_132\" :corp{name: \"Chambers LLC\"}), (\"p_4000\" :person{name: \"Colton Bailey\"})] In [13]: p.relationships() Out[13]: [(\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-(\"c_132\")] 对于边来说有这些方法 .edge_name(), .properties(), .start_vertex_id(), .end_vertex_id()，这里 edge_name 是获得边的类型。 In [14]: rel=p.relationships()[0] In [15]: rel Out[15]: (\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-(\"c_132\") In [16]: rel.edge_name() Out[16]: 'role_as' In [17]: rel.properties() Out[17]: {'role': \"Editorial assistant\"} In [18]: rel.start_vertex_id() Out[18]: \"p_4000\" In [19]: rel.end_vertex_id() Out[19]: \"c_132\" 对于点来说，可以用到这些方法 .tags(), properties, get_id()，这里边 tags 是获得点的类型，它在 Nebula 里叫标签tag。 这些概念可以在文档里获得更详细的解释。 In [20]: node=p.nodes()[0] In [21]: node.tags() Out[21]: ['corp'] In [22]: node.properties('corp') Out[22]: {'name': \"Chambers LLC\"} In [23]: node.get_id() Out[23]: \"c_132\" ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:1","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#查询语句"},{"categories":["Nebula Graph","Mini Project"],"content":"5.1 后端服务–图数据库 详细的数据格式分析大家可以参考这里 5.1.1 查询语句我们假设用户请求的实体是 c_132 ，那么请求 1 到 3 步的关系穿透的语法是： MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 这里边 ()包裹的是图之中的点，而[] 包裹的则是点之间的关系：边，所以： (v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) 之中的： (v)-[xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]-(v2)应该比较好理解，意思是从 v 到v2 做拓展。 现在我们介绍中间[]包裹的部分，这里，它的语义是：经由四种类型的边（:之后的是边的类型，|代表或者）通过可变的跳数：*1..3 （一跳到三跳）。 所以，简单来说整理看开，我们的拓展的路径是：从点 v 开始，经由四种关系一到三跳拓展到点v2，返回整个拓展路径 p，限制 100 个路径结果，其中 v 是 c_132。 5.1.2 Nebula Python Client/ SDK我们已经知道了查询语句的语法，那么就只需要在后端程序里根据请求、通过图数据库的客户端来发出查询请求，并处理返回结构就好了。在今天的例子中，我选择使用 Python 来实现后端的逻辑，所以我用了 Nebula-python 这个库，它是 Nebula 的 Python Client。 你知道么？截至到现在，Nebula 在 GitHub 上有 Java，GO，Python，C++，Spark，Flink，Rust（未GA），NodeJS（未GA） 的客户端支持，更多的语言的客户端也会慢慢被发布哦。 下边是一个 Python Client 执行一个查询并返回结果的例子，值得注意的是，在我实现这个代码的时候，Nebula Python 尚未支持返回 JSON （通过session.execute_json()）结果，如果你要实现自己的代码，我非常推荐试试 JSON 哈，就可以不用从对象中一点点取数据了，不过借助 iPython/IDLE 这种 REPL，快速了解返回对象的结构也没有那么麻烦。 $ python3 -m pip install nebula2-python==2.5.0 # 注意这里我引用旧的记录，它是 2.5.0， $ ipython In [1]: from nebula2.gclient.net import ConnectionPool In [2]: from nebula2.Config import Config In [3]: config = Config() ...: config.max_connection_pool_size = 10 ...: # init connection pool ...: connection_pool = ConnectionPool() ...: # if the given servers are ok, return true, else return false ...: ok = connection_pool.init([('192.168.8.137', 9669)], config) ...: session = connection_pool.get_session('root', 'nebula') [2021-10-13 13:44:24,242]:Get connection to ('192.168.8.137', 9669) In [4]: resp = session.execute(\"use shareholding\") In [5]: query = ''' ...: MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \\ ...: WHERE id(v) IN [\"c_132\"] RETURN p LIMIT 100 ...: ''' In [6]: resp = session.execute(query) # Note: after nebula graph 2.6.0, we could use execute_json as well In [7]: resp.col_size() Out[7]: 1 In [9]: resp.row_size() Out[10]: 100 我们往下分析看看，我们知道这个请求本质上结果是路径，它有一个 .nodes() 方法和 .relationships()方法来获得路径上的点和边： In [11]: p=resp.row_values(22)[0].as_path() In [12]: p.nodes() Out[12]: [(\"c_132\" :corp{name: \"Chambers LLC\"}), (\"p_4000\" :person{name: \"Colton Bailey\"})] In [13]: p.relationships() Out[13]: [(\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-(\"c_132\")] 对于边来说有这些方法 .edge_name(), .properties(), .start_vertex_id(), .end_vertex_id()，这里 edge_name 是获得边的类型。 In [14]: rel=p.relationships()[0] In [15]: rel Out[15]: (\"p_4000\")-[:role_as@0{role: \"Editorial assistant\"}]-(\"c_132\") In [16]: rel.edge_name() Out[16]: 'role_as' In [17]: rel.properties() Out[17]: {'role': \"Editorial assistant\"} In [18]: rel.start_vertex_id() Out[18]: \"p_4000\" In [19]: rel.end_vertex_id() Out[19]: \"c_132\" 对于点来说，可以用到这些方法 .tags(), properties, get_id()，这里边 tags 是获得点的类型，它在 Nebula 里叫标签tag。 这些概念可以在文档里获得更详细的解释。 In [20]: node=p.nodes()[0] In [21]: node.tags() Out[21]: ['corp'] In [22]: node.properties('corp') Out[22]: {'name': \"Chambers LLC\"} In [23]: node.get_id() Out[23]: \"c_132\" ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:1","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#nebula-python-client-sdk"},{"categories":["Nebula Graph","Mini Project"],"content":"5.2 前端渲染点边为图 详细的分析大家也可以参考这里 为了方便实现，我们采用了 Vue.js 和 vue-network-d3（D3 的 Vue Binding）。 通过 vue-network-d3 的抽象，能看出来喂给他这样的数据，就可以把点边信息渲染成很好看的图 nodes: [ {\"id\": \"c_132\", \"name\": \"Chambers LLC\", \"tag\": \"corp\"}, {\"id\": \"p_4000\", \"name\": \"Colton Bailey\", \"tag\": \"person\"}], relationships: [ {\"source\": \"p_4000\", \"target\": \"c_132\", \"properties\": { \"role\": \"Editorial assistant\" }, \"edge\": \"role_as\"}] ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:2","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#前端渲染点边为图"},{"categories":["Nebula Graph","Mini Project"],"content":"5.3 前端\u0026lt;\u0026ndash;后端 详细信息可以参考这里 我们从 D3 的初步研究上可以知道，后端只需要返回如下的 JSON 格式数据就好了 Nodes: [{\"id\": \"c_132\", \"name\": \"Chambers LLC\", \"tag\": \"corp\"}, {\"id\": \"p_4000\", \"name\": \"Colton Bailey\", \"tag\": \"person\"}] Relationships: [{\"source\": \"p_4000\", \"target\": \"c_132\", \"properties\": { \"role\": \"Editorial assistant\" }, \"edge\": \"role_as\"}, {\"source\": \"p_1039\", \"target\": \"c_132\", \"properties\": { \"share\": \"3.0\" }, \"edge\": \"hold_share\"}] 于是，，结合前边我们用 iPython 分析 Python 返回结果看，这个逻辑大概是： def make_graph_response(resp) -\u003e dict: nodes, relationships = list(), list() for row_index in range(resp.row_size()): path = resp.row_values(row_index)[0].as_path() _nodes = [ { \"id\": node.get_id(), \"tag\": node.tags()[0], \"name\": node.properties(node.tags()[0]).get(\"name\", \"\") } for node in path.nodes() ] nodes.extend(_nodes) _relationships = [ { \"source\": rel.start_vertex_id(), \"target\": rel.end_vertex_id(), \"properties\": rel.properties(), \"edge\": rel.edge_name() } for rel in path.relationships() ] relationships.extend(_relationships) return {\"nodes\": nodes, \"relationships\": relationships} 前端到后端的通信是 HTTP ，所以我们可以借助 Flask，把这个函数封装成一个 RESTful API： 前端程序通过 HTTP POST 到 /api 参考这里 from flask import Flask, jsonify, request app = Flask(__name__) @app.route(\"/\") def root(): return \"Hey There?\" @app.route(\"/api\", methods=[\"POST\"]) def api(): request_data = request.get_json() entity = request_data.get(\"entity\", \"\") if entity: resp = query_shareholding(entity) data = make_graph_response(resp) else: data = dict() # tbd return jsonify(data) def parse_nebula_graphd_endpoint(): ng_endpoints_str = os.environ.get( 'NG_ENDPOINTS', '127.0.0.1:9669,').split(\",\") ng_endpoints = [] for endpoint in ng_endpoints_str: if endpoint: parts = endpoint.split(\":\") # we dont consider IPv6 now ng_endpoints.append((parts[0], int(parts[1]))) return ng_endpoints def query_shareholding(entity): query_string = ( f\"USE shareholding; \" f\"MATCH p=(v)-[e:hold_share|:is_branch_of|:reletive_with|:role_as*1..3]-(v2) \" f\"WHERE id(v) IN ['{ entity }'] RETURN p LIMIT 100\" ) session = connection_pool.get_session('root', 'nebula') resp = session.execute(query_string) return resp 这个请求的结果则是前边前端期待的 JSON，像这样： curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"entity\": \"c_132\"}' \\ http://192.168.10.14:5000/api | jq { \"nodes\": [ { \"id\": \"c_132\", \"name\": \"\\\"Chambers LLC\\\"\", \"tag\": \"corp\" }, { \"id\": \"c_245\", \"name\": \"\\\"Thompson-King\\\"\", \"tag\": \"corp\" }, { \"id\": \"c_132\", \"name\": \"\\\"Chambers LLC\\\"\", \"tag\": \"corp\" }, ... } ], \"relationships\": [ { \"edge\": \"hold_share\", \"properties\": \"{'share': 0.0}\", \"source\": \"c_245\", \"target\": \"c_132\" { \"edge\": \"hold_share\", \"properties\": \"{'share': 9.0}\", \"source\": \"p_1767\", \"target\": \"c_132\" }, { \"edge\": \"hold_share\", \"properties\": \"{'share': 11.0}\", \"source\": \"p_1997\", \"target\": \"c_132\" }, ... }, { \"edge\": \"reletive_with\", \"properties\": \"{'degree': 51}\", \"source\": \"p_7283\", \"target\": \"p_4723\" } ] } ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:3","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#前端--后端"},{"categories":["Nebula Graph","Mini Project"],"content":"5.4 放到一起项目的代码都在 GitHub 上，最后其实只有一两百行的代码，把所有东西拼起来之后的代码是： ├── README.md # You could find Design Logs here ├── corp-rel-backend │ └── app.py # Flask App to handle Requst and calls GDB ├── corp-rel-frontend │ └── src │ ├── App.vue │ └── main.js # Vue App to call Flask App and Renders Graph └── requirements.txt ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:4","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#放到一起"},{"categories":["Nebula Graph","Mini Project"],"content":"5.5 最终效果我们做出来了一个简陋但是足够具有参考性的小系统，它接受一个用户输入的实体的 ID，再回车之后： 前端程序把请求发给后端 后端拼接 Nebula Graph 的查询语句，通过 Nebula Python 客户端请求 Nebula Graph Nebula Graph 接受请求做出穿透查询，返回结构给后端 后端将结果构建成前端 D3 接受的格式，传给前端 前端接收到图结构的数据，渲染股权穿透的数据如下： ","date":"2021-11-24","objectID":"/corp-rel-graph/:5:5","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#最终效果"},{"categories":["Nebula Graph","Mini Project"],"content":"6 总结现在，我们知道得益于图数据库的设计，在它上边构建一个方便的股权分析系统非常自然、高效，我们或者利用图数据库的图探索可视化能力、或者自己搭建，可以为用户提供非常高效、直观的多跳股权穿透分析。 如果你想了解更多关于分布式图数据库的知识，欢迎关注 Nebula Graph 这个开源项目，它已经被国内很多团队、公司认可选为图时代数据技术存储层的利器，大家可以访问这里，或者这里，了解更多相关的分享和文章。 未来，我会给大家分享更多图数据库相关的文章、视频和开源示例项目思路分享和教程，欢迎大家关注我的网站: siwei.io。 题图版权：fabioha ","date":"2021-11-24","objectID":"/corp-rel-graph/:6:0","series":null,"tags":["Nebula Graph","图数据库","股权穿透","知识图谱"],"title":"从零到一：如何构建一个企业股权图谱系统？","uri":"/corp-rel-graph/#总结"},{"categories":["Nebula Graph"],"content":"Dialog System With Graph Database Backed Knowledge Graph. 基于图数据库的智能问答助手","date":"2021-09-18","objectID":"/nebula-siwi/","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/"},{"categories":["Nebula Graph"],"content":" 一个基于图数据库的智能问答助手项目。 GitHub Repo: https://github.com/wey-gu/nebula-siwi/ 这个项目我也做成了互动教程，可以按照这里的步骤搭建起来 👉🏻 https://siwei.io/cources/ update: 写了一篇完整介绍 Siwi 设计的文章 👉🏻 https://siwei.io/siwi 您也可以在 Nebula Playground 上直接玩这个数据集啦：https://nebula-graph.com.cn/demo/ ","date":"2021-09-18","objectID":"/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#"},{"categories":["Nebula Graph"],"content":"Siwi the voice assistantSiwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. For now, it’s a demo for task-driven(not general purpose) dialog bots with KG(Knowledge Graph) leveraging Nebula Graph with the minimal/sample dataset from Nebula Graph Manual/ NG中文手册. Tips: Now you can play with the graph online without installing yourself! Nebula Playground | Nebula Playground - China Mainland Supported queries: relation: What is the relationship between Yao Ming and Lakers? How does Yao Ming and Lakers connected? serving: Which team had Yao Ming served? friendship: Whom does Tim Duncan follow? Who are Yao Ming’s friends? ","date":"2021-09-18","objectID":"/nebula-siwi/:0:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#siwi-the-voice-assistant"},{"categories":["Nebula Graph"],"content":"1 Deploy and TryTBD (leveraging docker and nebula-up) ","date":"2021-09-18","objectID":"/nebula-siwi/:1:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#deploy-and-try"},{"categories":["Nebula Graph"],"content":"2 How does it work?This is one of the most naive pipeline for a specific domain/ single purpose chat bot built on a Knowledge Graph. ","date":"2021-09-18","objectID":"/nebula-siwi/:2:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#how-does-it-work"},{"categories":["Nebula Graph"],"content":"2.1 BackendThe Backend(Siwi API) is a Flask based API server: Flask API server takes questions in HTTP POST, and calls the bot API. In bot API part there are classfier(Symentic Parsing, Intent Matching, Slot Filling), and question actors(Call corresponding actions to query Knowledge Graph with intents and slots). Knowledge Graph is built on an Open-Source Graph Database: Nebula Graph ","date":"2021-09-18","objectID":"/nebula-siwi/:2:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#backend"},{"categories":["Nebula Graph"],"content":"2.2 FrontendThe Frontend is a VueJS Single Page Applicaiton(SPA): I reused a Vue Bot UI to showcase a chat window in this human-agent interaction, typing is supported. In addtion, leverating Chrome’s Web Speech API, a button to listen to human voice is introduced ","date":"2021-09-18","objectID":"/nebula-siwi/:2:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#frontend"},{"categories":["Nebula Graph"],"content":"2.3 A Query Flow ┌────────────────┬──────────────────────────────────────┐ │ │ │ │ │ Speech │ │ ┌──────────▼──────────┐ │ │ │ Frontend │ Siwi, /ˈsɪwi/ │ │ │ Web_Speech_API │ A PoC of │ │ │ │ Dialog System │ │ │ Vue.JS │ With Graph Database │ │ │ │ Backed Knowledge Graph │ │ └──────────┬──────────┘ │ │ │ Sentence │ │ │ │ │ ┌────────────┼──────────────────────────────┐ │ │ │ │ │ │ │ │ │ Backend │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ Web API, Flask │ ./app/ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Sentence ./bot/ │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent matching, │ ./bot/classifier│ │ │ │ │ Symentic Processing │ │ │ │ │ │ │ │ │ │ │ └──────────┬──────────┘ │ │ │ │ │ Intent, Entities │ │ │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ │ │ │ │ Intent Actor │ ./bot/actions │ │ │ │ │ │ │ │ │ └─┴──────────┬──────────┴───────────────────┘ │ │ │ Graph Query │ │ ┌──────────▼──────────┐ │ │ │ │ │ │ │ Graph Database │ Nebula Graph │ │ │ │ │ │ └─────────────────────┘ │ │ │ │ │ │ │ └───────────────────────────────────────────────────────┘ ","date":"2021-09-18","objectID":"/nebula-siwi/:2:3","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#a-query-flow"},{"categories":["Nebula Graph"],"content":"2.4 Source Code Tree . ├── README.md ├── src │ ├── siwi # Siwi-API Backend │ │ ├── app # Web Server, take HTTP requests and calls Bot API │ │ └── bot # Bot API │ │ ├── actions # Take Intent, Slots, Query Knowledge Graph here │ │ ├── bot # Entrypoint of the Bot API │ │ ├── classifier # Symentic Parsing, Intent Matching, Slot Filling │ │ └── test # Example Data Source as equivalent/mocked module │ └── siwi_frontend # Browser End │ ├── README.md │ ├── package.json │ └── src │ ├── App.vue # Listening to user and pass Questions to Siwi-API │ └── main.js └── wsgi.py ","date":"2021-09-18","objectID":"/nebula-siwi/:2:4","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#source-code-tree"},{"categories":["Nebula Graph"],"content":"3 Manually Run Components","date":"2021-09-18","objectID":"/nebula-siwi/:3:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#manually-run-components"},{"categories":["Nebula Graph"],"content":"3.1 BackendInstall and run. # Install siwi backend python3 -m build # Configure Nebula Graph Endpoint export NG_ENDPOINTS=127.0.0.1:9669 # Run Backend API server gunicorn --bind :5000 wsgi --workers 1 --threads 1 --timeout 60 For OpenFunction/ KNative docker build -t weygu/siwi-api . docker run --rm --name siwi-api \\ --env=PORT=5000 \\ --env=NG_ENDPOINTS=127.0.0.1:9669 \\ --net=host \\ weygu/siwi-api Try it out Web API: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"question\": \"What is the relationship between Yao Ming and Lakers?\"}' \\ http://192.168.8.128:5000/query | jq { \"answer\": \"There are at least 23 relations between Yao Ming and Lakers, one relation path is: Yao Ming follows Shaquille O'Neal serves Lakers.\" } Call Bot Python API: from nebula2.gclient.net import ConnectionPool from nebula2.Config import Config # define a config config = Config() config.max_connection_pool_size = 10 # init connection pool connection_pool = ConnectionPool() # if the given servers are ok, return true, else return false ok = connection_pool.init([('127.0.0.1', 9669)], config) # import siwi bot from siwi.bot import bot # instantiate a bot b = bot.SiwiBot(connection_pool) # make the question query b.query(\"Which team had Jonathon Simmons served?\") Then a response will be like this: In [4]: b.query(\"Which team had Jonathon Simmons serv ...: ed?\") [DEBUG] ServeAction intent: {'entities': {'Jonathon Simmons': 'player'}, 'intents': ('serve',)} [DEBUG] query for RelationshipAction: USE basketballplayer; MATCH p=(v)-[e:serve*1]-\u003e(v1) WHERE id(v) == \"player112\" RETURN p LIMIT 100; [2021-07-02 02:59:36,392]:Get connection to ('127.0.0.1', 9669) Out[4]: 'Jonathon Simmons had served 3 teams. Spurs from 2015 to 2015; 76ers from 2019 to 2019; Magic from 2017 to 2017; ' ","date":"2021-09-18","objectID":"/nebula-siwi/:3:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#backend-1"},{"categories":["Nebula Graph"],"content":"3.2 FrontendReferring to siwi_frontend ","date":"2021-09-18","objectID":"/nebula-siwi/:3:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#frontend-1"},{"categories":["Nebula Graph"],"content":"4 Further work Use NBA-API to fallback undefined pattern questions Wrap and manage sessions instead of get and release session per request, this is somehow costly actually. Use NLP methods to implement proper Symentic Parsing, Intent Matching, Slot Filling Build Graph to help with Intent Matching, especially for a general purpose bot Use larger Dataset i.e. from wyattowalsh/basketball ","date":"2021-09-18","objectID":"/nebula-siwi/:4:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#further-work"},{"categories":["Nebula Graph"],"content":"5 Thanks to Upstream Projects ❤️","date":"2021-09-18","objectID":"/nebula-siwi/:5:0","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"5.1 Backend I learnt a lot from the KGQA on MedicalKG created by Huanyong Liu Flask pyahocorasick created by Wojciech Muła PyYaml ","date":"2021-09-18","objectID":"/nebula-siwi/:5:1","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#backend-2"},{"categories":["Nebula Graph"],"content":"5.2 Frontend VueJS for frontend framework Vue Bot UI, as a lovely bot UI in vue Vue Web Speech, for speech API vue wrapper Axios for browser http client Solarized for color scheme Vitesome for landing page design Image credit goes to https://unsplash.com/photos/0E_vhMVqL9g ","date":"2021-09-18","objectID":"/nebula-siwi/:5:2","series":null,"tags":["Nebula Graph","Know How","Intelligent assistant","图数据库应用","智能助手"],"title":"Nebula Siwi，基于图数据库的智能问答助手","uri":"/nebula-siwi/#frontend-2"},{"categories":["Nebula Graph"],"content":"Setup Nebula Graph Dev Env with CLion and Docker 搭建基于 Docker 的 Nebula Graph CLion 开发环境","date":"2021-09-18","objectID":"/nebula-clion/","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/"},{"categories":["Nebula Graph"],"content":" 之前卡比同学向我咨询搭建 CLion 环境，开发 Nebula 的一些问题，我做了一些工作方便利用 Docker 在本地搭建这样一个环境，相关的东西放在：https://github.com/wey-gu/nebula-dev-CLion 。 Related GitHub Repo: https://github.com/wey-gu/nebula-dev-CLion ","date":"2021-09-18","objectID":"/nebula-clion/:0:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/#"},{"categories":["Nebula Graph"],"content":"1 Run Docker Env for Nebula-Graph with CLionBuild Docker Image git clone https://github.com/wey-gu/nebula-dev-CLion.git cd nebula-dev-CLion docker build -t wey/nebula-dev-clion:v2.0 . Run Docker Container for Nebula-Dev with CLion Integration Readiness(actually mostly Rsync \u0026 SSH). cd \u003cnebula-graph-repo-you-worked-on\u003e export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker run --rm -d \\ --name nebula-dev \\ --security-opt seccomp=unconfined \\ -p 2222:22 -p 2873:873 --cap-add=ALL \\ -v $PWD:/home/nebula \\ -w /home/nebula \\ wey/nebula-dev-clion:v2.0 Verify cmake with SSH. The default password is password ssh -o StrictHostKeyChecking=no root@localhost -p 2222 # in docker cd /home/nebula mkdir build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. Access container w/o SSH. docker exec -it nebula-dev bash mkdir -p build \u0026\u0026 cd build cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. ","date":"2021-09-18","objectID":"/nebula-clion/:1:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/#run-docker-env-for-nebula-graph-with-clion"},{"categories":["Nebula Graph"],"content":"2 Configurations in CLion Ref: https://www.jetbrains.com/help/clion/clion-toolchains-in-docker.html#build-and-run Toolchains Add a remote host root@localhost:2222 password Put /opt/vesoft/toolset/cmake/bin/cmake as CMake CMake Toochain: Select the one created in last step Build directory: /home/nebula/build ","date":"2021-09-18","objectID":"/nebula-clion/:2:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/#configurations-in-clion"},{"categories":["Nebula Graph"],"content":"3 The appendix","date":"2021-09-18","objectID":"/nebula-clion/:3:0","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/#the-appendix"},{"categories":["Nebula Graph"],"content":"3.1 References of CMake output: [root@4c98e3f77ce8 build]# cmake -DENABLE_TESTING=OFF -DCMAKE_BUILD_TYPE=Release .. \u003e\u003e\u003e\u003e Options of Nebula Graph \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_BUILD_STORAGE : OFF (Whether to build storage) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_MODULE_FORCE_CHECKOUT : ON (Whether checkout branch of module to same as graph.) -- ENABLE_MODULE_UPDATE : OFF (Automatically update module) -- ENABLE_PACK_ONE : ON (Whether to package into one) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_VERBOSE_BISON : OFF (Enable Bison to report state) -- ENABLE_WERROR : ON (Regard warnings as errors) -- CMAKE_BUILD_TYPE : Release (Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...) -- CMAKE_INSTALL_PREFIX : /usr/local/nebula (Install path prefix, prepended onto install directories.) -- CMAKE_CXX_STANDARD : 17 -- CMAKE_CXX_COMPILER : /opt/vesoft/toolset/clang/9.0.0/bin/c++ (CXX compiler) -- CMAKE_CXX_COMPILER_ID : GNU -- NEBULA_USE_LINKER : bfd -- CCACHE_DIR : /root/.ccache \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' \u003c\u003c\u003c\u003c -- NEBULA_THIRDPARTY_ROOT : /opt/vesoft/third-party/2.0 -- Build info of nebula third party: Package : Nebula Third Party Version : 2.0 Date : Mon Jun 28 15:07:38 UTC 2021 glibc : 2.17 Arch : x86_64 Compiler : GCC 9.2.0 C++ ABI : 11 Vendor : VEsoft Inc. -- CMAKE_INCLUDE_PATH : /opt/vesoft/third-party/2.0/include -- CMAKE_LIBRARY_PATH : /opt/vesoft/third-party/2.0/lib64;/opt/vesoft/third-party/2.0/lib -- CMAKE_PROGRAM_PATH : /opt/vesoft/third-party/2.0/bin -- GLIBC_VERSION : 2.17 -- found krb5-config here /opt/vesoft/third-party/2.0/bin/krb5-config -- Found kerberos 5 headers: /opt/vesoft/third-party/2.0/include -- Found kerberos 5 libs: /opt/vesoft/third-party/2.0/lib/libgssapi_krb5.a;/opt/vesoft/third-party/2.0/lib/libkrb5.a;/opt/vesoft/third-party/2.0/lib/libk5crypto.a;/opt/vesoft/third-party/2.0/lib/libcom_err.a;/opt/vesoft/third-party/2.0/lib/libkrb5support.a \u003e\u003e\u003e\u003e Configuring third party for 'Nebula Graph' done \u003c\u003c\u003c\u003c -- Create the pre-commit hook -- Creating pre-commit hook done \u003e\u003e\u003e\u003e Configuring Nebula Common \u003c\u003c\u003c\u003c \u003e\u003e\u003e\u003e Options of Nebula Common \u003c\u003c\u003c\u003c -- ENABLE_ASAN : OFF (Build with AddressSanitizer) -- ENABLE_CCACHE : ON (Use ccache to speed up compiling) -- ENABLE_CLANG_TIDY : OFF (Enable clang-tidy if present) -- ENABLE_COMPRESSED_DEBUG_INFO : ON (Compress debug info to reduce binary size) -- ENABLE_COVERAGE : OFF (Build with coverage report) -- ENABLE_FRAME_POINTER : OFF (Build with frame pointer) -- ENABLE_FUZZY_TESTING : OFF (Enable Fuzzy tests) -- ENABLE_GDB_SCRIPT_SECTION : OFF (Add .debug_gdb_scripts section) -- ENABLE_JEMALLOC : ON (Use jemalloc as memory allocator) -- ENABLE_PIC : OFF (Build with -fPIC) -- ENABLE_STATIC_ASAN : OFF (Statically link against libasan) -- ENABLE_STATIC_UBSAN : OFF (Statically link against libubsan) -- ENABLE_STRICT_ALIASING : OFF (Build with -fstrict-aliasing) -- ENABLE_TESTING : OFF (Build unit tests) -- ENABLE_TSAN : OFF (Build with ThreadSanitizer) -- ENABLE_UBSAN : OFF (Build with UndefinedBehaviourSanitizer) -- ENABLE_WERROR : ON (Regard warnings as errors) -- Set D_GLIBCXX_USE_CXX11_ABI to 1 -- CMAKE_BUIL","date":"2021-09-18","objectID":"/nebula-clion/:3:1","series":null,"tags":["Nebula Graph","Dev Guide"],"title":"Nebula CLion，搭建基于 Docker 的 Nebula Graph CLion 开发环境","uri":"/nebula-clion/#references-of-cmake-output"},{"categories":["courses"],"content":"Hands-on Course: Breakdown multistage relationship of Persons and Corporations leverating the Nebula Graph Database.","date":"2021-09-04","objectID":"/learn/nebula-101-shareholding/","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"上手实战图数据库股权关系穿透","uri":"/learn/nebula-101-shareholding/"},{"categories":["courses"],"content":"Walk you through in actions to do below sections exercises! Bootstrap a Nebula Graph Cluster and Studio Web App Import a graph of dataset about shareholding Exploring the shareholding data with Nebula Importer Visually Exploring the shareholding data with Nebula Studio Run Graph Algorithm on Nebula Cluster Graph Data The dataset comes from https://github.com/wey-gu/nebula-shareholding-example/tree/main/data_sample 课程开源在 https://github.com/wey-gu/katacoda-scenarios ，欢迎来反馈，贡献 ","date":"2021-09-04","objectID":"/learn/nebula-101-shareholding/:0:0","series":null,"tags":["Nebula Graph","katacoda","shareholding","Nebula Solution","Knowledge Graph"],"title":"上手实战图数据库股权关系穿透","uri":"/learn/nebula-101-shareholding/#"},{"categories":null,"content":" 您可以通过这个导图开始了解、入门图数据库和 Nebula Graph，上边的区域是入门的部分，下边还有进阶的部分，我们会持续更新、维护这个导图的~ Tips: 加载成功之后，点击 [See the board] 可以开始浏览，双击放大缩小。 或者点导图区域右上角的箭头在新的窗口打开，可以更方面浏览器中内容。 ","date":"2021-09-04","objectID":"/path/:0:0","series":null,"tags":null,"title":"图数据库学习路径","uri":"/path/#"},{"categories":["courses"],"content":"Hands-on Course: Setup a KGQA system from scratch with Nebula Graph, VueJS, Flask on K8s.","date":"2021-09-03","objectID":"/learn/nebula-101-siwi-kgqa/","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"上手实战从0制作一个基于图谱的语音智能助手","uri":"/learn/nebula-101-siwi-kgqa/"},{"categories":["courses"],"content":"A full solution walkthrough for a Knowledge Graph Dialog System. Boostrap a Nebula Cluster in K8s Scale out the Nebula Cluster in K8s way Import the basketballplayer Dataset Siwi, the Knowledge Graph Dialog System with Nebula Graph Siwi (/ˈsɪwi/) is a PoC of Dialog System With Graph Database Backed Knowledge Graph. The code of Siwi is here: https://github.com/wey-gu/nebula-siwi. 课程开源在 https://github.com/wey-gu/katacoda-scenarios ，欢迎来反馈，贡献 ","date":"2021-09-03","objectID":"/learn/nebula-101-siwi-kgqa/:0:0","series":null,"tags":["Nebula Graph","katacoda","Dialog System","Nebula Solution"],"title":"上手实战从0制作一个基于图谱的语音智能助手","uri":"/learn/nebula-101-siwi-kgqa/#"},{"categories":["Nebula Graph"],"content":"A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/"},{"categories":["Nebula Graph"],"content":" A demo of Shareholding Breakthrough with Distributed open-source Graph Database: Nebula Graph. 图数据库应用示例：股权关系穿透 Related GitHub Repo: https://github.com/wey-gu/nebula-shareholding-example 更新：在这个数据集生成的工作基础上，我又做了一个全栈示例项目 👉🏻 https://siwei.io/corp-rel-graph/ 这个项目我也做成了互动教程，可以按照这里的步骤搭建起来 👉🏻 https://siwei.io/cources/ I created the Katacoda Interactive Env for this project 👉🏻 https://siwei.io/cources/ 您也可以在 Nebula Playground 上直接玩这个数据集啦：https://nebula-graph.com.cn/demo/ Now you can play with the data on Nebula Playground: https://nebula-graph.io/demo/ This is a demo of Shareholding Relationship Analysis with Distributed open-source Graph Database: Nebula Graph. ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:0:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#"},{"categories":["Nebula Graph"],"content":"1 Data","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:1:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#data"},{"categories":["Nebula Graph"],"content":"1.1 Data ModelingThere are various kinds of relationships when we checking companies’ shareholding breakthrough, here let’s simplify it with only two kind of entities: person and corp, and with following relationship types. person can hold a corp in {share} % person can be relative with another person corp can hold another corp in {share} % corp can be a branch of another corp person can be as a role of a corp Below is the lines to reflect this graph modele in Nebula Graph, it’s quite straightforward, right? CREATETAGperson(namestring);CREATETAGcorp(namestring);CREATEEDGErole_as(rolestring);CREATEEDGEis_branch_of();CREATEEDGEhold_share(sharefloat);CREATEEDGEreletive_with(degreeint); ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:1:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#data-modeling"},{"categories":["Nebula Graph"],"content":"1.2 Data GenerationWe just randomly generate some data to help with this demo, you can call data_generator.py directly to generate or reuse what’s already done under data_sample folder. The generated data are records to be fit in above data model from below .csv files. $ pip install Faker==2.0.5 pydbgen==1.0.5 $ python3 data_generator.py $ ls -l data total 1688 -rw-r--r-- 1 weyl staff 23941 Jul 14 13:28 corp.csv -rw-r--r-- 1 weyl staff 1277 Jul 14 13:26 corp_rel.csv -rw-r--r-- 1 weyl staff 3048 Jul 14 13:26 corp_share.csv -rw-r--r-- 1 weyl staff 211661 Jul 14 13:26 person.csv -rw-r--r-- 1 weyl staff 179770 Jul 14 13:26 person_corp_role.csv -rw-r--r-- 1 weyl staff 322965 Jul 14 13:26 person_corp_share.csv -rw-r--r-- 1 weyl staff 17689 Jul 14 13:26 person_rel.csv ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:1:2","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#data-generation"},{"categories":["Nebula Graph"],"content":"1.3 Data ImportWith those data in .csv files, we can easily import them into a Nebula Graph Cluster with the help of Nebula-Importer. nebula-importer.yaml in this repo describes rules and configurations on how this import will be done by the importer. For Nebula Graph Database, plesae refer to Doc , Doc-CN to deploy on any Linux Servers, for study and test, you can run it via Docker following the Quick Start Chapter of the documentation. For Nebula-Importer, if you already have Docker env, you can run it as the following without installing anything. Or, if you prefer to install it, it’s quite easy as it’s written in Golang and you can run its single file binary quite easily, go check both Documentation and Nebula-Importer Repo: https://github.com/vesoft-inc/nebula-importer. Let’s start! Below is the commands I used to import our data into a Nebula Graph Database. # put generated data \u0026 nebula-importor.yaml to nebula-importer server $ scp -r data nebula_graph_host:~ $ scp nebula-importer.yaml data nebula_graph_host:~/data $ ssh nebula_graph_host $ ls -l ${HOME}/data total 756 -rw-r--r--. 1 wei.gu wei.gu 23941 Jul 14 05:44 corp.csv -rw-r--r--. 1 wei.gu wei.gu 1277 Jul 14 05:44 corp_rel.csv -rw-r--r--. 1 wei.gu wei.gu 3048 Jul 14 05:44 corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 3893 Jul 14 05:44 nebula-importer.yaml -rw-r--r--. 1 wei.gu wei.gu 211661 Jul 14 05:44 person.csv -rw-r--r--. 1 wei.gu wei.gu 179770 Jul 14 05:44 person_corp_role.csv -rw-r--r--. 1 wei.gu wei.gu 322965 Jul 14 05:44 person_corp_share.csv -rw-r--r--. 1 wei.gu wei.gu 17689 Jul 14 05:44 person_rel.csv # import data into our nebula graph database $ docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v ${HOME}/data/nebula-importer.yaml:/root/nebula-importer.yaml \\ -v ${HOME}/data:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-importer.yaml 2021/07/14 05:49:32 --- START OF NEBULA IMPORTER --- 2021/07/14 05:49:32 [WARN] config.go:491: Not set files[0].schema.vertex.vid.Type, reset to default value `string' ... 2021/07/14 05:49:43 [INFO] reader.go:180: Total lines of file(/root/person_corp_role.csv) is: 5000, error lines: 1287 2021/07/14 05:49:43 [INFO] statsmgr.go:61: Done(/root/person_corp_role.csv): Time(11.39s), Finished(12523), Failed(0), Latency AVG(1514us), Batches Req AVG(1824us), Rows AVG(1099.43/s) 2021/07/14 05:49:47 [INFO] statsmgr.go:61: Tick: Time(15.00s), Finished(25807), Failed(0), Latency AVG(1500us), Batches Req AVG(1805us), Rows AVG(1720.46/s) 2021/07/14 05:49:48 [INFO] reader.go:180: Total lines of file(/root/person.csv) is: 10000, error lines: 0 2021/07/14 05:49:48 [INFO] statsmgr.go:61: Done(/root/person.csv): Time(16.10s), Finished(29731), Failed(0), Latency AVG(1505us), Batches Req AVG(1810us), Rows AVG(1847.17/s) 2021/07/14 05:49:50 [INFO] reader.go:180: Total lines of file(/root/person_corp_share.csv) is: 20000, error lines: 0 2021/07/14 05:49:50 [INFO] statsmgr.go:61: Done(/root/person_corp_share.csv): Time(17.74s), Finished(36013), Failed(0), Latency AVG(1531us), Batches Req AVG(1844us), Rows AVG(2030.29/s) 2021/07/14 05:49:50 Finish import data, consume time: 18.25s 2021/07/14 05:49:51 --- END OF NEBULA IMPORTER --- ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:1:3","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#data-import"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)]\u003e show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)]\u003e use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding]\u003e GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#corporation-sharehold-relationship-breakthrough"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#query-in-ngql"},{"categories":["Nebula Graph"],"content":"2 Corporation sharehold relationship breakthrough2.0.1 Query in nGQLWe can query from corp: c_132 over *(all relationships) in BIDIRECT: GO1TO3STEPSFROM\"c_132\"over*BIDIRECT Below are lines I call this query in nebula-console: / # nebula-console -addr 192.168.8.128 -port 9669 -user user -password password 2021/07/14 07:07:41 [INFO] connection pool is initialized successfully Welcome to Nebula Graph! (user@nebula) [(none)] show spaces +--------------------+ | Name | +--------------------+ | \"basketballplayer\" | +--------------------+ | \"shareholding\" | +--------------------+ Got 2 rows (time spent 3851/4595 us) Wed, 14 Jul 2021 07:07:57 UTC (user@nebula) [(none)] use shareholding Execution succeeded (time spent 1822/2342 us) Wed, 14 Jul 2021 07:08:02 UTC (user@nebula) [shareholding] GO 1 TO 3 STEPS FROM \"c_132\" over * BIDIRECT +--------------+-------------------+-----------------+--------------------+ | role_as._dst | is_branch_of._dst | hold_share._dst | reletive_with._dst | +--------------+-------------------+-----------------+--------------------+ | \"p_2024\" | | | | +--------------+-------------------+-----------------+--------------------+ | \"p_4000\" | | | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1039\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1399\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"p_1767\" | | ... +--------------+-------------------+-----------------+--------------------+ | | | \"c_132\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_245\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_25\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_649\" | | +--------------+-------------------+-----------------+--------------------+ | | | \"c_753\" | | +--------------+-------------------+-----------------+--------------------+ Got 1519 rows (time spent 12190/14702 us) Wed, 14 Jul 2021 07:08:06 UTC 2.0.2 In a Visual WayOf course, we can also be done in a visual way: Step 1, login from Nebula Graph Studio, explore with one VID: c_132: Step2, click this explored vertex dot, then you can explore from select vertices by selecting: Edge Type Direction Steps Query Limit(Optional) note, you can click the 👁️ icon to add options to show fields of the graph, Step3, after click Expand, you will see all quried relations with c_132 the Chambers LLC. ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:2:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#in-a-visual-way"},{"categories":["Nebula Graph"],"content":"3 Thanks to Upstream Projects ❤️ Python Faker https://github.com/joke2k/faker/ pydbgen https://github.com/tirthajyoti/pydbgen Nebula Graph https://github.com/vesoft-inc/nebula-graph ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:3:0","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#thanks-to-upstream-projects-"},{"categories":["Nebula Graph"],"content":"3.1 Tips: You can deploy nebula graph in one line with: Nebula-UP, it helps install a nebula graph with Docker Nebula-operator-KIND , it helps setup all dependencies of Nebula-K8s-Operator including a K8s in Docker, PV Provider and then install a Nebula Graph with Nebula-Operator in K8s. Image Credit goes to https://unsplash.com/photos/3fPXt37X6UQ ","date":"2021-08-28","objectID":"/nebula-holdshare-dataset/:3:1","series":null,"tags":["Nebula Graph","Know How","Shareholding Relationship Analysis","图数据库应用","股权穿透"],"title":"Nebula Holdshare Dataset，图数据库的股权穿透","uri":"/nebula-holdshare-dataset/#tips"},{"categories":null,"content":" 股权穿透教程 1. 上手实战图数据库股权关系穿透 2. 体验 Nebula-Up 一键部署 Nebula Core + Studio 阅读全文 Siwi 1. 上手实战从 0 制作一个基于图谱的语音智能助手 2. 体验 Nebula-Kind-Operator 一键部署 K8s + Nebula Operator 阅读全文 ","date":"2021-08-26","objectID":"/cources/:0:0","series":null,"tags":null,"title":"上手课程","uri":"/cources/#"},{"categories":null,"content":" 掘金春招活动 那些我希望过去的自己可以知道的事实 阅读全文 DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB 阅读全文 Data on K8s Community 2021 GraphDB on Kubesphere 阅读全文 K8s Community Day 2021 Openfunction + GraphDB 阅读全文 COScon 2021 我的开源之路 阅读全文 PyCon China 2021 图数据库解谜与 Python 的图库应用实践 阅读全文 K8s 上的图数据库 Nebula K8s Operator 的实现解析，图数据库应用在 K8s + OpenFunction 上的落地演示 阅读全文 nMeetup: Nebula 应用上手实操 从头实操 Nebula 的部署，股权穿透，图算法运算，语音智能助手。 阅读全文 How to Train your Dragon 如何成为开源开发者（布道师）。 阅读全文 ","date":"2021-08-26","objectID":"/talk/:0:0","series":null,"tags":null,"title":"我的演讲","uri":"/talk/#"},{"categories":["Nebula Graph"],"content":"Import LiveJournal Dataset into Nebula Graph and Run Nebula Algorithm 导入 Livejournal 数据集到 Nebula 并运行 Nebula Algorithm 图算法","date":"2021-08-24","objectID":"/nebula-livejournal/","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/"},{"categories":["Nebula Graph"],"content":" 一个导入 Livejournal 数据集到 Nebula Graph 图数据库，并执行 Nebula Algorithm 图算法的过程分享。 Related GitHub Repo: https://github.com/wey-gu/nebula-LiveJournal ","date":"2021-08-24","objectID":"/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#"},{"categories":["Nebula Graph"],"content":"nebula-LiveJournalLiveJournal Dataset is a Social Network Dataset in one file with two columns(FromNodeId, ToNodeId). $ head soc-LiveJournal1.txt # Directed graph (each unordered pair of nodes is saved once): soc-LiveJournal1.txt # Directed LiveJournal friednship social network # Nodes: 4847571 Edges: 68993773 # FromNodeId ToNodeId 0 1 0 2 0 3 0 4 0 5 0 6 It could be accessed in https://snap.stanford.edu/data/soc-LiveJournal1.html. Dataset statistics Nodes 4847571 Edges 68993773 Nodes in largest WCC 4843953 (0.999) Edges in largest WCC 68983820 (1.000) Nodes in largest SCC 3828682 (0.790) Edges in largest SCC 65825429 (0.954) Average clustering coefficient 0.2742 Number of triangles 285730264 Fraction of closed triangles 0.04266 Diameter (longest shortest path) 16 90-percentile effective diameter 6.5 ","date":"2021-08-24","objectID":"/nebula-livejournal/:0:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#nebula-livejournal"},{"categories":["Nebula Graph"],"content":"1 Dataset Download and Preprocessing","date":"2021-08-24","objectID":"/nebula-livejournal/:1:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#dataset-download-and-preprocessing"},{"categories":["Nebula Graph"],"content":"1.1 DownloadIt is accesissiable from the official web page: $ cd nebula-livejournal/data $ wget https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz Comments in data file should be removed to make the data import tool happy. ","date":"2021-08-24","objectID":"/nebula-livejournal/:1:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#download"},{"categories":["Nebula Graph"],"content":"1.2 Preprocessing $ gzip -d soc-LiveJournal1.txt.gz $ sed -i '1,4d' soc-LiveJournal1.txt ","date":"2021-08-24","objectID":"/nebula-livejournal/:1:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#preprocessing"},{"categories":["Nebula Graph"],"content":"2 Import dataset to Nebula Graph","date":"2021-08-24","objectID":"/nebula-livejournal/:2:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#import-dataset-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"2.1 With Nebula ImporterNebula-Importer is a Golang Headless import tool for Nebula Graph. You may need to edit the config file under nebula-importer/importer.yaml on Nebula Graph’s address and credential。 Then, Nebula-Importer could be called in Docker as follow: $ cd nebula-livejournal $ docker run --rm -ti \\ --network=nebula-net \\ -v nebula-importer/importer.yaml:/root/importer.yaml \\ -v data/:/root \\ vesoft/nebula-importer:v2 \\ --config /root/importer.yaml Or if you have the binary nebula-importer locally: $ cd data $ \u003cpath_to_nebula-importer_binary\u003e --config ../nebula-importer/importer.yaml ","date":"2021-08-24","objectID":"/nebula-livejournal/:2:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#with-nebula-importer"},{"categories":["Nebula Graph"],"content":"2.2 With Nebula ExchangeNebula-Exchange is a Spark Application to enable batch and streaming data import from multiple data sources to Nebula Graph. To be done. (You can refer to https://siwei.io/nebula-exchange-sst-2.x/) ","date":"2021-08-24","objectID":"/nebula-livejournal/:2:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#with-nebula-exchange"},{"categories":["Nebula Graph"],"content":"3 Run Algorithms with Nebula GraphNebula-Algorithm is a Spark/GraphX Application to run Graph Algorithms with data consumed from files or a Nebula Graph Cluster. Supported Algorithms for now: Name Use Case PageRank page ranking, important node digging Louvain community digging, hierarchical clustering KCore community detection, financial risk control LabelPropagation community detection, consultation propagation, advertising recommendation ConnectedComponent community detection, isolated island detection StronglyConnectedComponent community detection ShortestPath path plan, network plan TriangleCount network structure analysis BetweennessCentrality important node digging, node influence calculation DegreeStatic graph structure analysis ","date":"2021-08-24","objectID":"/nebula-livejournal/:3:0","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#run-algorithms-with-nebula-graph"},{"categories":["Nebula Graph"],"content":"3.1 Ad-hoc Spark Env setupHere I assume the Nebula Graph was bootstraped with Nebula-Up, thus nebula is running in a Docker Network named nebula-docker-compose_nebula-net. Then let’s start a single server spark: docker run --name spark-master --network nebula-docker-compose_nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ -v nebula-algorithm/:/root \\ bde2020/spark-master:2.4.5-hadoop2.7 Thus we could make spark application submt inside this container: docker exec -it spark-master bash cd /root/ # download Nebula-Algorithm Jar Packagem, 2.0.0 for example, for other versions, refer to nebula-algorithm github repo and documentations. wget https://repo1.maven.org/maven2/com/vesoft/nebula-algorithm/2.0.0/nebula-algorithm-2.0.0.jar ","date":"2021-08-24","objectID":"/nebula-livejournal/:3:1","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#ad-hoc-spark-env-setup"},{"categories":["Nebula Graph"],"content":"3.2 Run AlgorithmsThere are many altorithms supported by Nebula-Algorithm, here some of their configuration files were put under nebula-algorithm as an example. Before using them, please first edit and change Nebula Graph Cluster Addresses and credentials. vim nebula-altorithm/algo-pagerank.conf Then we could enter the spark container and call corresponding algorithms as follow. Please adjust your --driver-memeory accordingly, i.e. pagerank altorithm: /spark/bin/spark-submit --master \"local\" --conf spark.rpc.askTimeout=6000s \\ --class com.vesoft.nebula.algorithm.Main \\ --driver-memory 16g nebula-algorithm-2.0.0.jar \\ -p pagerank.conf After the algorithm finished, the output will be under the path insdie the container defined in conf file: write:{ resultPath:/output/ } 题图版权：@sigmund ","date":"2021-08-24","objectID":"/nebula-livejournal/:3:2","series":null,"tags":["Nebula Graph","LiveJournal","Nebula Algorithm","PageRank","Graph Algorithm"],"title":"Nebula LiveJournal，上手 LiveJournal 数据集导入 Nebula Graph 与图算法执行","uri":"/nebula-livejournal/#run-algorithms"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/"},{"categories":["Nebula Graph"],"content":"这篇文章带大家以最小方式，快速趟一下 Nebula Exchange 中 SST 写入方式的步骤。 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:0:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#"},{"categories":["Nebula Graph"],"content":"1 什么是 Nebula Exchange ?之前我在 Nebula Data Import Options 之中介绍过，Nebula Exchange 是一个 Nebula Graph 社区开源的 Spark Applicaiton，它专门用来支持批量或者流式地把数据导入 Nebula Graph Database 之中。 Nebula Exchange 支持多种多样的数据源（从 Apache Parquet, ORC, JSON, CSV, HBase, Hive MaxCompute 到 Neo4j, MySQL, ClickHouse, 再有 Kafka, Pulsar，更多的数据源也在不断增加之中）。 如上图所示，在 Exchange 内部，从除了不同 Reader 可以读取不同数据源之外，在数据经过 Processor 处理之后通过 Writer写入（sink） Nebula Graph 图数据库的时候，除了走正常的 ServerBaseWriter 的写入流程之外，它还可以绕过整个写入流程，利用 Spark 的计算能力并行生成底层 RocksDB 的 SST 文件，从而实现超高性能的数据导入，这个 SST 文件导入的场景就是本文带大家上手熟悉的部分。 详细信息请参阅：Nebula Graph 手册:什么是 Nebula Exchange Nebula Graph 官方博客也有更多 Nebula Exchange 的实践文章 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:1:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#什么是-nebula-exchange-"},{"categories":["Nebula Graph"],"content":"2 步骤概观 实验环境 配置 Exchange 生成 SST 文件 写入 SST 文件到 Nebula Graph ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:2:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#步骤概观"},{"categories":["Nebula Graph"],"content":"3 实验环境准备为了最小化使用 Nebula Exchange 的 SST 功能，我们需要： 搭建一个 Nebula Graph 集群，创建导入数据的 Schema，我们选择使用 Docker-Compose 方式、利用 Nebula-Up 快速部署，并简单修改其网络，以方便同样容器化的 Exchange 程序对其访问。 搭建容器化的 Spark 运行环境 搭建容器化的 HDFS ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:3:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#实验环境准备"},{"categories":["Nebula Graph"],"content":"3.1 1. 搭建 Nebula Graph 集群借助于 Nebula-Up 我们可以在 Linux 环境下一键部署一套 Nebula Graph 集群： curl -fsSL nebula-up.siwei.io/install.sh | bash 待部署成功之后，我们需要对环境做一些修改，这里我做的修改其实就是两点： 只保留一个 metaD 服务 起用 Docker 的外部网络 详细修改的部分参考附录一 应用 docker-compose 的修改： cd ~/.nebula-up/nebula-docker-compose vim docker-compose.yaml # 参考附录一 docker network create nebula-net # 需要创建外部网络 docker-compose up -d --remove-orphans 之后，我们来创建要测试的图空间，并创建图的 Schema，为此，我们可以利用 nebula-console ，同样，Nebula-Up 里自带了容器化的 nebula-console。 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 创建图空间（我们起名字叫 sst ），以及 schema createspacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));:sleep20usesstcreatetagplayer(namestring,ageint); 示例输出 (root@nebula)[(none)]\u003ecreatespacesst(partition_num=5,replica_factor=1,vid_type=fixed_string(32));Executionsucceeded(timespent1468/1918us)(root@nebula)[(none)]\u003e:sleep20(root@nebula)[(none)]\u003eusesstExecutionsucceeded(timespent1253/1566us)Wed,18Aug202108:18:13UTC(root@nebula)[sst]\u003ecreatetagplayer(namestring,ageint);Executionsucceeded(timespent1312/1735us)Wed,18Aug202108:18:23UTC ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:3:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#1-搭建-nebula-graph-集群"},{"categories":["Nebula Graph"],"content":"3.2 搭建容器化的 Spark 环境利用 big data europe 做的工作，这个过程非常容易。 值得注意的是： 现在的 Nebula Exchange 对 Spark 的版本有要求，在现在的 2021 年 8 月，我是用了 spark-2.4.5-hadoop-2.7 的版本。 为了方便，我让 Spark 运行在 Nebula Graph 相同的机器上，并且指定了运行在同一个 Docker 网络下 docker run --name spark-master --network nebula-net \\ -h spark-master -e ENABLE_INIT_DAEMON=false -d \\ bde2020/spark-master:2.4.5-hadoop2.7 然后，我们就可以进入到环境中了： docker exec -it spark-master bash 进到 Spark 容器中之后，可以像这样安装 maven: export MAVEN_VERSION=3.5.4 export MAVEN_HOME=/usr/lib/mvn export PATH=$MAVEN_HOME/bin:$PATH wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026\u0026 \\ mv apache-maven-$MAVEN_VERSION /usr/lib/mvn 还可以这样在容器里下载 nebula-exchange 的 jar 包： cd ~ wget https://repo1.maven.org/maven2/com/vesoft/nebula-exchange/2.1.0/nebula-exchange-2.1.0.jar ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:3:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#搭建容器化的-spark-环境"},{"categories":["Nebula Graph"],"content":"3.3 搭建容器化的 HDFS同样借助 big-data-euroupe 的工作，这非常简单，不过我们要做一点修改，让它的 docker-compose.yml 文件里使用 nebula-net 这个之前创建的 Docker 网络。 详细修改的部分参考附录二 git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop vim docker-compose.yml docker-compose up -d ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:3:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#搭建容器化的-hdfs"},{"categories":["Nebula Graph"],"content":"4 配置Exchange这个配置主要填入的信息就是 Nebula Graph 集群本身和将要写入数据的 Space Name，以及数据源相关的配置（这里我们用 csv 作为例子），最后再配置输出（sink）为 sst Nebula Graph GraphD 地址 MetaD 地址 credential Space Name 数据源 source: csv path fields etc. ink: sst 详细的配置参考附录二 注意，这里 metaD 的地址可以这样获取，可以看到 0.0.0.0:49377-\u003e9559 表示 49377 是外部的地址。 $ docker ps | grep meta 887740c15750 vesoft/nebula-metad:v2.0.0 \"./bin/nebula-metad …\" 6 hours ago Up 6 hours (healthy) 9560/tcp, 0.0.0.0:49377-\u003e9559/tcp, :::49377-\u003e9559/tcp, 0.0.0.0:49376-\u003e19559/tcp, :::49376-\u003e19559/tcp, 0.0.0.0:49375-\u003e19560/tcp, :::49375-\u003e19560/tcp nebula-docker-compose_metad0_1 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:4:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#配置exchange"},{"categories":["Nebula Graph"],"content":"5 生成SST文件","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:5:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#生成sst文件"},{"categories":["Nebula Graph"],"content":"5.1 准备源文件、配置文件 docker cp exchange-sst.conf spark-master:/root/ docker cp player.csv spark-master:/root/ 其中 player.csv 的例子： 1100,Tim Duncan,42 1101,Tony Parker,36 1102,LaMarcus Aldridge,33 1103,Rudy Gay,32 1104,Marco Belinelli,32 1105,Danny Green,31 1106,Kyle Anderson,25 1107,Aron Baynes,32 1108,Boris Diaw,36 1109,Tiago Splitter,34 1110,Cory Joseph,27 1111,David West,38 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:5:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#准备源文件配置文件"},{"categories":["Nebula Graph"],"content":"5.2 执行 exchange 程序进入 spark-master 容器，提交执行 exchange 应用。 docker exec -it spark-master bash cd /root/ /spark/bin/spark-submit --master local \\ --class com.vesoft.nebula.exchange.Exchange nebula-exchange-2.1.0.jar\\ -c exchange-sst.conf 检查执行结果： spark-submit 输出： 21/08/17 03:37:43 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 1093 ms on localhost (executor driver) (32/32) 21/08/17 03:37:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 21/08/17 03:37:43 INFO DAGScheduler: ResultStage 2 (foreachPartition at VerticesProcessor.scala:179) finished in 22.336 s 21/08/17 03:37:43 INFO DAGScheduler: Job 1 finished: foreachPartition at VerticesProcessor.scala:179, took 22.500639 s 21/08/17 03:37:43 INFO Exchange$: SST-Import: failure.player: 0 21/08/17 03:37:43 WARN Exchange$: Edge is not defined 21/08/17 03:37:43 INFO SparkUI: Stopped Spark web UI at http://spark-master:4040 21/08/17 03:37:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped! 验证 HDFS 上生成的 SST 文件： docker exec -it namenode /bin/bash root@2db58903fb53:/# hdfs dfs -ls /sst Found 10 items drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/1 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/10 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/2 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/3 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/4 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/5 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/6 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/7 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/8 drwxr-xr-x - root supergroup 0 2021-08-17 03:37 /sst/9 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:5:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#执行-exchange-程序"},{"categories":["Nebula Graph"],"content":"6 写入SST到NebulaGraph这里的操作实际上都是参考文档：SST 导入，得来。其中就是从 console 之中执行了两步操作： Download Ingest 其中 Download 实际上是触发 Nebula Graph 从服务端发起 HDFS Client 的 download，获取 HDFS 上的 SST 文件，然后放到 storageD 能访问的本地路径下，这里，需要我们在服务端部署 HDFS 的依赖。因为我们是最小实践，我就偷懒手动做了这个 Download 的操作。 ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:6:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#写入sst到nebulagraph"},{"categories":["Nebula Graph"],"content":"6.1 手动下载这里边手动下载我们就要知道 Nebula Graph 服务端下载的路径，实际上是 /data/storage/nebula/\u003cspace_id\u003e/download/，这里的 Space ID 需要手动获取一下： 这个例子里，我们的 Space Name 是 sst，而 Space ID 是 49。 (root@nebula)[sst]\u003eDESCspacesst+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |ID|Name|PartitionNumber|ReplicaFactor|Charset|Collate|VidType|AtomicEdge|Group|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ |49|\"sst\"|10|1|\"utf8\"|\"utf8_bin\"|\"FIXED_STRING(32)\"|\"false\"|\"default\"|+----+-------+------------------+----------------+---------+------------+--------------------+-------------+-----------+ 于是，下边的操作就是手动把 SST 文件从 HDFS 之中 get 下来，再拷贝到 storageD 之中。 docker exec -it namenode /bin/bash $ hdfs dfs -get /sst /sst exit docker cp namenode:/sst . docker exec -it nebula-docker-compose_storaged0_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged1_1 mkdir -p /data/storage/nebula/49/download/ docker exec -it nebula-docker-compose_storaged2_1 mkdir -p /data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged0_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged1_1:/data/storage/nebula/49/download/ docker cp sst nebula-docker-compose_storaged2_1:/data/storage/nebula/49/download/ ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:6:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#手动下载"},{"categories":["Nebula Graph"],"content":"6.2 SST 文件导入 进入 Nebula-Console 所在的容器 ~/.nebula-up/console.sh / # 在 console 容器里发起链接到图数据库，其中 192.168.x.y 是我所在的 Linux VM 的第一个网卡地址，请换成您的 / # nebula-console -addr 192.168.x.y -port 9669 -user root -p password [INFO] connection pool is initialized successfully Welcome to Nebula Graph! 执行 INGEST 开始让 StorageD 读取 SST 文件 (root@nebula) [(none)]\u003e use sst (root@nebula) [sst]\u003e INGEST; 我们可以用如下方法实时查看 Nebula Graph 服务端的日志 tail -f ~/.nebula-up/nebula-docker-compose/logs/*/* 成功的 INGEST 日志： I0817 08:03:28.611877 169 EventListner.h:96] Ingest external SST file: column family default, the external file path /data/storage/nebula/49/download/8/8-6.sst, the internal file path /data/storage/nebula/49/data/000023.sst, the properties of the table: # data blocks=1; # entries=1; # deletions=0; # merge operands=0; # range deletions=0; raw key size=48; raw average key size=48.000000; raw value size=40; raw average value size=40.000000; data block size=75; index block size (user-key? 0, delta-value? 0)=66; filter block size=0; (estimated) table size=141; filter policy name=N/A; prefix extractor name=nullptr; column family ID=N/A; column family name=N/A; comparator name=leveldb.BytewiseComparator; merge operator name=nullptr; property collectors names=[]; SST file compression algo=Snappy; SST file compression options=window_bits=-14; level=32767; strategy=0; max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0; ; creation time=0; time stamp of earliest key=0; file creation time=0; E0817 08:03:28.611912 169 StorageHttpIngestHandler.cpp:63] SSTFile ingest successfully 题图版权：Pietro Jeng ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:6:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#sst-文件导入"},{"categories":["Nebula Graph"],"content":"7 附录","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:7:0","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#附录"},{"categories":["Nebula Graph"],"content":"7.1 附录一docker-compose.yaml diff --git a/docker-compose.yaml b/docker-compose.yaml index 48854de..cfeaedb 100644 --- a/docker-compose.yaml +++ b/docker-compose.yaml @@ -6,11 +6,13 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=metad0 - --ws_ip=metad0 - --port=9559 - --ws_http_port=19559 + - --ws_storage_http_port=19779 - --data_path=/data/meta - --log_dir=/logs - --v=0 @@ -34,81 +36,14 @@ services: cap_add: - SYS_PTRACE - metad1: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad1 - - --ws_ip=metad1 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad1:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta1:/data/meta - - ./logs/meta1:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - - metad2: - image: vesoft/nebula-metad:v2.0.0 - environment: - USER: root - TZ: \"${TZ}\" - command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 - - --local_ip=metad2 - - --ws_ip=metad2 - - --port=9559 - - --ws_http_port=19559 - - --data_path=/data/meta - - --log_dir=/logs - - --v=0 - - --minloglevel=0 - healthcheck: - test: [\"CMD\", \"curl\", \"-sf\", \"http://metad2:19559/status\"] - interval: 30s - timeout: 10s - retries: 3 - start_period: 20s - ports: - - 9559 - - 19559 - - 19560 - volumes: - - ./data/meta2:/data/meta - - ./logs/meta2:/logs - networks: - - nebula-net - restart: on-failure - cap_add: - - SYS_PTRACE - storaged0: image: vesoft/nebula-storaged:v2.0.0 environment: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged0 - --ws_ip=storaged0 - --port=9779 @@ -119,8 +54,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged0:19779/status\"] interval: 30s @@ -146,7 +81,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged1 - --ws_ip=storaged1 - --port=9779 @@ -157,8 +92,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged1:19779/status\"] interval: 30s @@ -184,7 +119,7 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --local_ip=storaged2 - --ws_ip=storaged2 - --port=9779 @@ -195,8 +130,8 @@ services: - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://storaged2:19779/status\"] interval: 30s @@ -222,17 +157,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd:19669/status\"] interval: 30s @@ -257,17 +194,19 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=metad0:9559 - --port=9669 - --ws_ip=graphd1 - --ws_http_port=19669 + - --ws_meta_http_port=19559 - --log_dir=/logs - --v=0 - --minloglevel=0 depends_on: - metad0 - - metad1 - - metad2 healthcheck: test: [\"CMD\", \"curl\", \"-sf\", \"http://graphd1:19669/status\"] interval: 30s @@ -292,17 +231,21 @@ services: USER: root TZ: \"${TZ}\" command: - - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559 + - --meta_server_addrs=met","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:7:1","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#附录一"},{"categories":["Nebula Graph"],"content":"7.2 附录二https://github.com/big-data-europe/docker-hadoop 的 docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index ed40dc6..66ff1f4 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -14,6 +14,8 @@ services: - CLUSTER_NAME=test env_file: - ./hadoop.env + networks: + - nebula-net datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 @@ -25,6 +27,8 @@ services: SERVICE_PRECONDITION: \"namenode:9870\" env_file: - ./hadoop.env + networks: + - nebula-net resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 @@ -34,6 +38,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\" env_file: - ./hadoop.env + networks: + - nebula-net nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 @@ -43,6 +49,8 @@ services: SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\" env_file: - ./hadoop.env + networks: + - nebula-net historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 @@ -54,8 +62,14 @@ services: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env + networks: + - nebula-net volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: + +networks: + nebula-net: + external: true ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:7:2","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#附录二"},{"categories":["Nebula Graph"],"content":"7.3 附录三nebula-exchange-sst.conf { # Spark relation config spark: { app: { name: Nebula Exchange 2.1 } master:local driver: { cores: 1 maxResultSize: 1G } executor: { memory:1G } cores:{ max: 16 } } # Nebula Graph relation config nebula: { address:{ graph:[\"192.168.8.128:9669\"] meta:[\"192.168.8.128:49377\"] } user: root pswd: nebula space: sst # parameters for SST import, not required path:{ local:\"/tmp\" remote:\"/sst\" hdfs.namenode: \"hdfs://192.168.8.128:9000\" } # nebula client connection parameters connection { # socket connect \u0026 execute timeout, unit: millisecond timeout: 30000 } error: { # max number of failures, if the number of failures is bigger than max, then exit the application. max: 32 # failed import job will be recorded in output path output: /tmp/errors } # use google's RateLimiter to limit the requests send to NebulaGraph rate: { # the stable throughput of RateLimiter limit: 1024 # Acquires a permit from RateLimiter, unit: MILLISECONDS # if it can't be obtained within the specified timeout, then give up the request. timeout: 1000 } } # Processing tags # There are tag config examples for different dataSources. tags: [ # HDFS csv # Import mode is sst, just change type.sink to client if you want to use client import mode. { name: player type: { source: csv sink: sst } path: \"file:///root/player.csv\" # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields fields: [_c1, _c2] nebula.fields: [name, age] vertex: { field:_c0 } separator: \",\" header: false batch: 256 partition: 32 } ] } ","date":"2021-08-18","objectID":"/nebula-exchange-sst-2.x/:7:3","series":null,"tags":["Nebula Graph","Nebula Exchange","SST"],"title":"Nebula Exchange SST 2.x Hands-on Guide","uri":"/nebula-exchange-sst-2.x/#附录三"},{"categories":["sketches"],"content":"介绍 Nebula Graph 的云原生 K8s Operator 部署","date":"2021-08-06","objectID":"/sketches/nebula-operator-explained/","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula operator Explained","uri":"/sketches/nebula-operator-explained/"},{"categories":["sketches"],"content":" Nebula Graph operator explained 这个手记里，我介绍了 Nebula Graph 的 K8s Operator: Intro 00:00 Nebula K8s Operator Explained 0:25 How do we use Nebula Operator? 02:23 What is the difference between the Operator based Nebula Graph Cluster and the binary-based one? 03:50 How about the Performance impact when it comes to K8s-Operator deployment? 04:55 What is the easiest way to try out the nebula operator? 06:04 Outra 07:30 ref: https://github.com/vesoft-inc/nebula-operator ","date":"2021-08-06","objectID":"/sketches/nebula-operator-explained/:0:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula operator Explained","uri":"/sketches/nebula-operator-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-08-06","objectID":"/sketches/nebula-operator-explained/:1:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula operator Explained","uri":"/sketches/nebula-operator-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-08-06","objectID":"/sketches/nebula-operator-explained/:2:0","series":null,"tags":["Nebula Graph","K8s"],"title":"Nebula operator Explained","uri":"/sketches/nebula-operator-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Config Explained","date":"2021-07-26","objectID":"/sketches/nebula-config-explained/","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/sketches/nebula-config-explained/"},{"categories":["sketches"],"content":" Nebula Graph config explained 这个手记帮助我们理解 Nebula Graph 的配置相关的知识： 介绍 Nebula Graph 三种配置方式，它们的优先级、范围、生效条件 0:16 介绍 Nebula Graph 在 Docker-Compose/Swarm 部署情况下配置的方式 03:01 介绍 Nebula Graph 在 K8s Operator 部署情况下配置的方式 03:55 我们是否应该用 Local-Config？（剧透：应该） 05:03 ","date":"2021-07-26","objectID":"/sketches/nebula-config-explained/:0:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/sketches/nebula-config-explained/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-26","objectID":"/sketches/nebula-config-explained/:1:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/sketches/nebula-config-explained/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-26","objectID":"/sketches/nebula-config-explained/:2:0","series":null,"tags":["Nebula Graph","config"],"title":"Nebula Config Explained","uri":"/sketches/nebula-config-explained/#youtube"},{"categories":["sketches"],"content":"Nebula Index Demystified","date":"2021-07-13","objectID":"/sketches/nebula-index-demystified/","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Graph Index Demystified","uri":"/sketches/nebula-index-demystified/"},{"categories":["sketches"],"content":" Nebula Graph 原生索引解谜，帮助大家深入了解 Nebula Graph Index 原生索引，到底 Nebula Graph 的原生索引是做什么用的？为什么 Nebula Graph 索引对性能有一些影响？带有索引的写入过程是什么样的？ Index Demystified 0:33 When should we use index? 06:37 Index v.s. Fulltext Index 07:12 Index Performance Impact 08:03 ","date":"2021-07-13","objectID":"/sketches/nebula-index-demystified/:0:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Graph Index Demystified","uri":"/sketches/nebula-index-demystified/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-07-13","objectID":"/sketches/nebula-index-demystified/:1:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Graph Index Demystified","uri":"/sketches/nebula-index-demystified/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-07-13","objectID":"/sketches/nebula-index-demystified/:2:0","series":null,"tags":["Nebula Graph","index"],"title":"Nebula Graph Index Demystified","uri":"/sketches/nebula-index-demystified/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Deployment Options","date":"2021-06-25","objectID":"/sketches/nebula-deployment-options/","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/sketches/nebula-deployment-options/"},{"categories":["sketches"],"content":" Nebula Graph 有很多种分发、部署方式，我们应该如何选择它们呢？ ","date":"2021-06-25","objectID":"/sketches/nebula-deployment-options/:0:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/sketches/nebula-deployment-options/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-06-25","objectID":"/sketches/nebula-deployment-options/:1:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/sketches/nebula-deployment-options/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-06-25","objectID":"/sketches/nebula-deployment-options/:2:0","series":null,"tags":["Nebula Graph","deployment"],"title":"Nebula Graph Deployment Options","uri":"/sketches/nebula-deployment-options/#youtube"},{"categories":["sketches"],"content":"Nebula Graph Data Import Options","date":"2021-06-15","objectID":"/sketches/nebula-data-import-options/","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/sketches/nebula-data-import-options/"},{"categories":["sketches"],"content":" Nebula Graph 提供了好多种数据导入的工具，我们应该如何选择呢？这一张图帮助大家了解 Nebula Graph 所有的数据导入选项。 ","date":"2021-06-15","objectID":"/sketches/nebula-data-import-options/:0:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/sketches/nebula-data-import-options/#"},{"categories":["sketches"],"content":"1 Bilibili ","date":"2021-06-15","objectID":"/sketches/nebula-data-import-options/:1:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/sketches/nebula-data-import-options/#bilibili"},{"categories":["sketches"],"content":"2 Youtube ","date":"2021-06-15","objectID":"/sketches/nebula-data-import-options/:2:0","series":null,"tags":["Nebula Graph","data import"],"title":"Nebula Graph Data Import Options","uri":"/sketches/nebula-data-import-options/#youtube"},{"categories":["Nebula Graph"],"content":"无需依赖，一键安装尝鲜基于 Nebula Operator 的 K8s Nebula Graph Cluster","date":"2021-06-09","objectID":"/nebula-operator-kind/","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind，一键单机玩转 Nebula K8s 集群","uri":"/nebula-operator-kind/"},{"categories":["Nebula Graph"],"content":" Nebula-Kind，无需依赖，一键安装尝鲜基于 Nebula Operator 的 K8s Nebula Graph Cluster。 注： KIND 是一个 K8s 的 SIG，代表 K8s in Docker。 ","date":"2021-06-09","objectID":"/nebula-operator-kind/:0:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind，一键单机玩转 Nebula K8s 集群","uri":"/nebula-operator-kind/#"},{"categories":["Nebula Graph"],"content":"1 Nebula-Operator-Kind 是什么Nebula Graph 作为云原生的分布式开源图数据库，有开源的 K8s Operator 供大家在 K8s 上方便的通过 CRD 去维护、部署 Nebula Graph 集群。 对于手头没有方便的 K8s 环境的同学，如果想尝鲜、学习 Nebula Graph 的 K8s Operator 的话，可能需要耗费一些精力才能搭起来一整套的控制平面的依赖。 作为一个懒人，我做利用 K8s in Docker(KIND)，和之前做 Nebula-Up 的 shell 脚本架子，快速的搞了一个一键安装工具：Nebula-Operator-Kind 它能直接帮我们： 安装 Docker 安装 K8s(KIND) 安装 PV Provider 安装 Nebula-Operator 以及依赖 安装 Nebula-Console 配置 nodePort 用以一键直连 Nebula 集群 安装 kubectl 用来体验 Nebula-Operator 的 CRD 配置 ","date":"2021-06-09","objectID":"/nebula-operator-kind/:1:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind，一键单机玩转 Nebula K8s 集群","uri":"/nebula-operator-kind/#nebula-operator-kind-是什么"},{"categories":["Nebula Graph"],"content":"2 如何使用安装： curl -sL nebula-kind.siwei.io/install.sh | bash 成功之后： 用 ~/.nebula-kind/bin/console 一行连接集群： ~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000 ","date":"2021-06-09","objectID":"/nebula-operator-kind/:2:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind，一键单机玩转 Nebula K8s 集群","uri":"/nebula-operator-kind/#如何使用"},{"categories":["Nebula Graph"],"content":"3 详细信息Repo 的地址是：https://github.com/wey-gu/nebula-operator-kind ，里边有更多的信息，欢迎大家试用、反馈、PR 哈！ Updated Sept. 2021 如果在 KubeSphere All-in-one 环境安装： curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash 如果在 Minikube、其他 K8s 之中安装 curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash 题图版权：Maik Hankemann ","date":"2021-06-09","objectID":"/nebula-operator-kind/:3:0","series":null,"tags":["Nebula Graph","Nebula Operator","K8s"],"title":"Nebula Operator Kind，一键单机玩转 Nebula K8s 集群","uri":"/nebula-operator-kind/#详细信息"},{"categories":null,"content":"您好，我是古思为。我是一个在上海的软件工程师，我在 vesoft 担任 Nebula Graph（开源的分布式图数据库）的社区开发者布道师。 我的工作是通过围绕 Nebula Graph Database 创作内容，构建工具来改善开发者的学习、开发、社区参与体验。 我在开源社区开放的工作，并（花了职业生涯中的前些年意识到）热爱用自己的思想和学到的技术帮助到别人，我认为这是一种的荣幸和宝贵的机遇。 1 ","date":"2021-06-04","objectID":"/about/:0:0","series":null,"tags":null,"title":"","uri":"/about/#您好我是古思为"},{"categories":null,"content":"1 最近的开源项目 Nebula-Corp-Rel-Graph Nebula-Corp-Rel-Graph，基于图数据库的股权穿透系统 阅读全文 Nebula-Siwi Nebula-Siwi，基于图数据库的智能问答助手 阅读全文 Nebula-Holdshare Nebula-Holdshare，图数据库应用示例：股权关系穿透 阅读全文 Nebula-KIND Nebula-Kind，无需依赖，一键安装尝鲜基于 Nebula Operator 的 K8s Nebula Graph Cluster。注： KIND 是一个 K8s 的 SIG，代表 K8s in Docker。 阅读全文 Nebula-Up Nebula-up，一键拉起一个 Nebula 测试环境，支持 mscOS，Windows 10，CentOS 和 Ubuntu。 阅读全文 VSCode-nGQL VSCode-nGQL，Nebula Graph 的 VS Code 插件，ngql 语法高亮。 阅读全文 IPython-nGQL Nebula Graph 的 Jupyter Notebook 和 IPython 插件，方便在 Notebook 之中嵌入 nGQL 的 query 和 结果的调试。 阅读全文 nebula-insights 本文介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。 阅读全文 ","date":"2021-06-04","objectID":"/about/:1:0","series":null,"tags":null,"title":"","uri":"/about/#最近的开源项目"},{"categories":null,"content":"2 我的手绘 Nebula Operator Explained Nebula Graph K8s Operator 介绍 阅读全文 Nebula Config Explained Nebula Graph 配置详解 阅读全文 Nebula Index Demystified Nebula Graph 原生索引解谜 阅读全文 Nebula Data Import Options Nebula Graph 提供了好多种数据导入的工具，我们应该如何选择呢？ 阅读全文 Nebula Deployment Options Nebula Graph 有很多种分发、部署方式，我们应该如何选择它们呢？ 阅读全文 Nebula Intro 新手玩转 Nebula Graph 系列开篇。 阅读全文 ","date":"2021-06-04","objectID":"/about/:2:0","series":null,"tags":null,"title":"","uri":"/about/#我的手绘"},{"categories":null,"content":"3 我的上手课程 股权穿透教程 上手实战股权用图数据库关系穿透 阅读全文 Siwi 上手实战从 0 制作一个基于图谱的语音智能助手 阅读全文 ","date":"2021-06-04","objectID":"/about/:3:0","series":null,"tags":null,"title":"","uri":"/about/#我的上手课程"},{"categories":null,"content":"4 我的演讲 掘金春招活动 那些我希望过去的自己可以知道的事实 阅读全文 DoK Talks #116 Nebula Graph: Open Source Distributed GraphDB 阅读全文 Data on K8s Community 2021 GraphDB on Kubesphere 阅读全文 K8s Community Day 2021 Openfunction + GraphDB 阅读全文 COScon 2021 我的开源之路 阅读全文 PyCon China 2021 图数据库解谜与 Python 的图库应用实践 阅读全文 K8s 上的图数据库 Nebula K8s Operator 的实现解析，图数据库应用在 K8s + OpenFunction 上的落地演示 阅读全文 nMeetup: Nebula 应用上手实操 从头实操 Nebula 的部署，股权穿透，图算法运算，语音智能助手。 阅读全文 How to Train your Dragon 如何成为开源开发者（布道师）。 阅读全文 ","date":"2021-06-04","objectID":"/about/:4:0","series":null,"tags":null,"title":"","uri":"/about/#我的演讲"},{"categories":null,"content":"5 过往的经历我曾在爱立信工作了近十年：2011 年到 2021 年。 我是云计算产品 Cloud Execution Envrioment (CEE) 2 研发团队的系统经理 3，我大部分的工作是通过设计开发 20 多个 CEE 6.6.2 和 CEE 10 的功能与改进，涵盖计算、存储、网络、生命周期管理和安全等领域，来帮助爱立信 IaaS 产品与解决方案不断进化。 同时，我也负责 CEE 产品在中国区的布道（面向外部与内部）。 我曾在 note.siwei.info 记录一些想法和笔记，从2021年春天开始，我会把想法记录下来留在 siwei.io ","date":"2021-06-04","objectID":"/about/:5:0","series":null,"tags":null,"title":"","uri":"/about/#过往的经历"},{"categories":null,"content":"6 联系我您最好通过 twitter，邮箱 wey.gu@vesoft.com 找到我。 如果必须，您也可以通过微信找到我，微信 ID 如下，请添加注明您的来意。 echo c2l2dmVpCg== | base64 -d 我和 Ahmet Alp Balkan 的 这个推文 感同身受： Working in open source (and getting paid for it) is a privilege. It’s a career boost, makes you lots of friends across the industry, and gives you a public brand. I am one of the “lucky few” \u0026 thankful to Microsoft and Google who let me work on OSS nearly all my career. — ahmetb (@ahmetb) February 19, 2021  ↩︎ Ericsson’s Telco. Infrastructure as a Service product offerring: Cloud Execution Environment ↩︎ System Manager, PDU Cloud: 工作描述 ↩︎ ","date":"2021-06-04","objectID":"/about/:6:0","series":null,"tags":null,"title":"","uri":"/about/#联系我"},{"categories":["Nebula Graph"],"content":"本文作者分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。","date":"2021-05-26","objectID":"/nebula-chia/","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/"},{"categories":["Nebula Graph"],"content":" 这篇文章里，我全网首次地分析了 Chia Network 的全链数据，并做了将全链数据导入图数据库：Nebula Graph 之中的尝试，从而可视化地探索了 Chia 图中数据之间的关联关系。 我把涉及的代码开源在了这里：https://github.com/wey-gu/nebula-chia ","date":"2021-05-26","objectID":"/nebula-chia/:0:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#"},{"categories":["Nebula Graph"],"content":"1 Chia 是什么?Chia Network 是由 BitTorrent 的作者 Bram Cohen 的团队在 2017 年创建的区块链项目。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#chia-是什么"},{"categories":["Nebula Graph"],"content":"1.1 为什么再搞一个区块链?Chia 用了全新的中本聪共识算法，这个算法通过不允许并行计算，让挖矿（Proof of Work）所需算力和能耗降到非常低，这使得超大组织、玩家没法像在其他的区块链项目那样有算力的绝对优势，也一定程度上规避了能源的浪费。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#为什么再搞一个区块链"},{"categories":["Nebula Graph"],"content":"1.2 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#如何连接chia"},{"categories":["Nebula Graph"],"content":"1.2 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#安装"},{"categories":["Nebula Graph"],"content":"1.2 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#运行"},{"categories":["Nebula Graph"],"content":"1.2 如何连接Chia?我们可以通过 Chia Network 的客户端来访问它，这个客户端是 Electron + Python 的程序，天然跨平台，既有 GUI 又有 CLI 的方式。 1.2.1 安装只需要按照官方的 Guide 来下载安装就好， https://github.com/Chia-Network/chia-blockchain/wiki/INSTALL，我在 M1 Mac 下安装的时候脚本出了点小问题，大概是因为拉取二进制 wheel 文件网络出问题走到了编译 wheel的逻辑，而那里是依赖 cargo的，如果大家遇到了这个问题，可以提前手动安装一下 rust，或者 cherry-pick 我的这个 PR 。 1.2.2 运行 按照官方 guide，比如 macOS 来说，最后一步执行npm run electron \u0026 就是运行它的GUI客户端。 如果大家像我一样喜欢 CLI，直接在执行完 . ./activate 之后就可以 chia --help了哈☺，里边有只启动部分服务的方式（相比 GUI 启动所有来说)。 在运行之后，如果你的网络不是多层 NAT 的那种，理论上您可以连到 mainnet 并且自动和主链同步数据了，如果您是第二次运行，连接主链，一开始可能有一阵子同步的block 数是不变的，也没有 peer 连过来，不必惊慌，等一下就好了。 Tips: 第一次连到 Chia Network 的同学们，客户端会自动生成一个钱包，及的保存那一串词，它们就是你的私钥哦。 万一，如果真的连不上的话，可能需要在路由上配置，UPnP，防火墙要允许 8444。 1.2.3 访问 Chia 的数据Chia 的客户端把数据存在了几个 SQLite 数据库里，它们的路径是我们安装客户端的用户的家目录：~/.chia/mainnet 下边就是运行起来 Chia 之后生成的主要的两个数据库的二进制文件： ~/.chia/mainnet/db ❯ ll -h total 4350416 -rw-r--r-- 1 weyl staff 2.0G May 6 12:06 blockchain_v1_mainnet.sqlite -rw-r--r-- 1 weyl staff 64K May 6 11:17 blockchain_v1_mainnet.sqlite-shm -rw-r--r-- 1 weyl staff 20M May 6 12:10 blockchain_v1_mainnet.sqlite-wal -rw-r--r-- 1 weyl staff 1.8M May 6 11:46 peer_table_node.sqlite -rw-r--r-- 1 weyl staff 32K May 5 17:30 peer_table_node.sqlite-shm -rw-r--r-- 1 weyl staff 5.4M May 6 11:46 peer_table_node.sqlite-wal ~/.chia/mainnet/wallet/db ❯ ll -h total 3055848 -rw-r--r-- 1 weyl staff 1.4G May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite -rw-r--r-- 1 weyl staff 32K May 7 15:24 blockchain_wallet_v1_mainnet_903996200.sqlite-shm -rw-r--r-- 1 weyl staff 4.0M May 7 15:32 blockchain_wallet_v1_mainnet_903996200.sqlite-wal -rw-r--r-- 1 weyl staff 16K May 7 15:24 wallet_peers.sqlite 然后，我们可以先用 SQLite Browser，一个 SQlite 数据库（文件）的浏览器来看看它。 SQlite 浏览器的官网是 https://sqlitebrowser.org/ 。在下载，安装之后，我们可以通过点击 Open Database/打开数据库 选择浏览上边列出来的两个 .sqlite 扩展名的数据库文件。 打开数据库之后，我们可以选择第一个标签 Database Schema 来看看表的结构。 我们还能像类似于 Excel 一样去看表的数据，还可以 Filter/过滤、Sort/排序任意列。 下一部分，我们来简单看看表里的数据。 Tips: 这里边，~/.chia/mainnet/wallet 和裸目录 ~/.chia/mainnet 下边的 db 里分别都有表文件，他们的信息是有重复的，大家可以分别打开看看哦，即使是相同的表的名字，比如 block_record 内里的信息也略有差别，如果大家知道为什么有这样的差别，欢迎浏览告诉大家哈，可能要仔细研究一下客户端、钱包等代码才行，幸运的是，它们相对比较好阅读，是 Python 写的： https://github.com/Chia-Network/chia-blockchain 。 ","date":"2021-05-26","objectID":"/nebula-chia/:1:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#访问-chia-的数据"},{"categories":["Nebula Graph"],"content":"2 分析 Chia 的数据如果大家仔细看了上边表结构定义的截图，就能注意到一些表的主要信息是嵌套二进制 KV Byte，所以只从 SQLite 并不能看到所有 Chia 的数据，所以我们需要（用一个编程语言来）读取表里的 Byte。 幸运的是，这件事儿因为 Chia 是开源的，而且是 Python 的代码，使得我们可以直接交互式的做。 我花了一点点时间在 Chia 客户端代码里找到了需要的封装类，借助它，可以比较方便的分析 Chia 客户端在本地的全链数据。 如果您不感兴趣细节，可以直接看我分析的结论。 结论之后，我也给大家演示一下是怎么读取它们的。 ","date":"2021-05-26","objectID":"/nebula-chia/:2:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#分析-chia-的数据"},{"categories":["Nebula Graph"],"content":"2.1 TL;DR, 结论我们可以从表中读取到区块链记录（Block Record ），Chia 币记录（Coin Record）。 从区块记录中，我们可以看到关键的涉及交易的信息： 关联的 Coin ，关联的 Puzzle（地址），Coin 的值(Amount) 从币记录中，我们可以看到关键的涉及区块的信息： 生成这个 Coin 所在区块链里的索引高度（Confirmed Index） 如果这个记录是花费 Coin 的，花费它的索引高度（Spent Index） ┌──────────────────────┐ ┌────────────────────────────────────────┐ │ │ │ │ │ Coin Record │ │ Block Record │ │ │ │ │ │ Coin Name │ │ Height ◄────────────────────────────┼─┐ │ │ │ │ │ ┌─┼───► Puzzle │ │ Header │ │ │ │ │ │ │ │ ├─┼───► Coin Parent │ │ Prev Header │ │ │ │ │ │ │ │ ├─┼───► Amount │ │ Block Body │ │ │ │ │ │ farmer_puzzle_hash │ │ │ │ Time Stamp │ │ fees │ │ │ │ │ │ pool_puzzle_hash │ └─────┼─┼─┬─ Confirmed Index │ │ prev_transaction_block_hash │ │ │ │ │ │ prev_transaction_block_height │ │ │ └─ Spent Index │ │ transactions_info ───────────────┼───────┘ │ │ │ ┌─── is_transaction_block │ │ Coinbase │ │ │ sub_epoch_summary ────────────────┼───────┐ │ │ │ │ │ │ └─ ────────────────────┘ │ │ is Peak │ │ │ └──is Block │ │ ┌─────────────────────┐ │ │ │ │ │ └────────────────────────────────────────┘ └─┼─► Sub Epoch Segment │ │ │ └─────────────────────┘ ","date":"2021-05-26","objectID":"/nebula-chia/:2:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#tldr-结论"},{"categories":["Nebula Graph"],"content":"2.2 准备因为安装客户端之后，我们本地实际上已经有了相关的 Python 环境和依赖，只需要在里边跑起来就好。 # 注意，我们要 cd 到之前安装客户端时候克隆的仓库。 cd chia-blockchain # source activate 脚本来切换到仓库安装时候创建的 Python 虚拟环境，并进到 IPython 里。 source venv/bin/activate \u0026\u0026 pip install ipython \u0026\u0026 ipython 然后试着导入客户端里边带有的 Python 的 Chia 的封装类试试看。 In [1]: import sqlite3 ...: from chia.consensus.block_record import BlockRecord # 导入成功，没有报错 In [2]: !pwd # 我的安装克隆目录 /Users/weyl/chia-blockchain 恭喜你做好了准备，我们看看 Block Record 里都有什么。 ","date":"2021-05-26","objectID":"/nebula-chia/:2:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#准备"},{"categories":["Nebula Graph"],"content":"2.3 Chia 链的数据2.3.1 区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 注意，这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#\u003c---- sub_epoch_summaryblob,is_peaktinyint,is_blocktinyint) 2.3.2 Chia 币记录类似的，我们可以获取一个 Coin 的记录，这里边，从表的定义可以看到，唯一二进制（不能直接从数据库查询中被人读懂）的字段就是是币值，不存在嵌套的结构，所以也并不需要封装的类才能看清楚里边的信息。 CREATETABLEcoin_record(coin_nametextPRIMARYKEY,confirmed_indexbigint,spent_indexbigint,spentint,coinbaseint,puzzle_hashtext,coin_parenttext,amountblob,timestampbigint) 这里值得注意的信息主要是 spent_index 和 confirmed_index。 from chia","date":"2021-05-26","objectID":"/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#chia-链的数据"},{"categories":["Nebula Graph"],"content":"2.3 Chia 链的数据2.3.1 区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 注意，这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#","date":"2021-05-26","objectID":"/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#区块记录"},{"categories":["Nebula Graph"],"content":"2.3 Chia 链的数据2.3.1 区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 注意，这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#","date":"2021-05-26","objectID":"/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#chia-币记录"},{"categories":["Nebula Graph"],"content":"2.3 Chia 链的数据2.3.1 区块记录在上一步的 IPython 窗口下。 # 注意，这里的路径的前缀是我们自己的家目录，不同操作系统，不同的用户都会有所不同。 chia_db_path = \"/Users/weyl/.chia/mainnet/db/blockchain_v1_mainnet.sqlite\" cur = con.cursor() # 这里我们取第 201645 高的区块 rows = list(cur.execute('SELECT * FROM block_records WHERE height = 201645')) # 这里 0 表示 SELECT 结果的第一行，3 表示在 BlockRecord 这个表里边，Block 的二进制 BLOB 是第四列，参考本章底部的表定义部分 block_records_201645 = BlockRecord.from_bytes(rows[0][3]) dir(block_records_201645) # 可以查看一些属性 is_transaction_block，timestamp，reward_claims_incorporated In [174]: block_records_201645.is_transaction_block Out[174]: True In [182]: from datetime import datetime In [183]: datetime.fromtimestamp(block_records_201645.timestamp) Out[183]: datetime.datetime(2021, 4, 29, 10, 8, 1) In [190]: block_records_201645.reward_claims_incorporated[0].to_json_dict() Out[190]: {'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6', 'amount': 1750000000000} # 可以快速 print 看大部分信息 print(block_records_201645) block_records_201645 的打印结果如下。 注意，这里我截断了一些数据 {'challenge_block_info_hash': '0x4a562f1ffa7a06fe76b1df74dbdd6bdcfbf63139a6f1fc3291c606d7c976abf6', 'challenge_vdf_output': {'data': '0x0200a6b38d6b58d17129d71737088772561f22a44ef302fe45a70a763b878f998abfe35946df720bcb5d78e214b667bce801d597b46c867928c4b8926c342375a961f36cd63ec698bc25e5ce48c45d9a2074eded0e42d24dd1b50a59e699f671f0900100'}, 'deficit': 16, 'farmer_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'fees': 0, 'finished_challenge_slot_hashes': ['0x2b1a7b4859a8d3597b8e6cbe3b27ab97212be8b19e6867f2a4d0eef26c36340f'], 'finished_infused_challenge_slot_hashes': ['0xd0185a6493b10d84e696c6fc55ec1920e8a96791a604dedfe77635da460f354d'], 'finished_reward_slot_hashes': ['0xe2bcbf560471131a7fb87ffe3f9ddde03166a9b0092a50f1ed1599715857c365'], 'header_hash': '0x2791729e1c914f9c3908a0ad895b5846c86fc4e207cc463820123e9a299c39f3', 'height': 201645, 'infused_challenge_vdf_output': None, 'overflow': True, 'pool_puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba', 'prev_hash': '0xf126ecf64473beb9ae5b84137788100feb9d731c604877c0744cdc6025f4cbeb', 'prev_transaction_block_hash': '0x7103fe2f1aa96998f9ccf6fc98561b64c5f7a98cf942335c4c927fb2eaa9325a', 'prev_transaction_block_height': 201643, 'required_iters': 95752, 'reward_claims_incorporated': [{'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313ab', 'puzzle_hash': '0xbbb014f41d88c74b78501b36e4863d3382bfda9ae3c4d30a1b6df72b962502b6'}, {'amount': 1750000000000, 'parent_coin_info': '0xccd5bb71183532bff220ba46c268991a000000000000000000000000000313aa', 'puzzle_hash': '0x4bc6435b409bcbabe53870dae0f03755f6aabb4594c5915ec983acf12a5d1fba'}, {'amount': 250000000000, 'parent_coin_info': '0x3ff07eb358e8255a65c30a2dce0e5fbb000000000000000000000000000313a8', 'puzzle_hash': '0xcf178071f6aa6cb1c92f00943424bcc8cb774449bd60058fc08e9894f49a1ca4'}], 'reward_infusion_new_challenge': '0x660886f4ab030c07755f53362ae4253dfa93ea853cbc321218f58f159c75adaa', 'signage_point_index': 63, 'sub_epoch_summary_included': None, 'sub_slot_iters': 99614720, 'timestamp': 1619662081, 'total_iters': 660123219464, 'weight': 4121254} 另外，我们取的这个表的定义如下。 CREATETABLEblock_records(header_hashtextPRIMARYKEY,prev_hashtext,heightbigint,blockblob,#","date":"2021-05-26","objectID":"/nebula-chia/:2:3","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#puzzles-address地址"},{"categories":["Nebula Graph"],"content":"3 如何探索 Chia 链随着我们之前分析的信息，自然地，我们可以把 Chia 区块链中的信息取出来，用图（Graph）来表示，这里的图并不是（Graphic）图形、图画的意思，是数学、图论中的图。 在图的语境下，最主要的两个元素就是顶点（Vertex）和边（Edge）。 顶点表示一个实体，而边表示实体之间的某种关系，这种关系可以是对等的（无方向的）也可以是有方向的。 这里我们可以把这里的信息抽象映射到如图的图模型里： Block 顶点 Coin 顶点 Puzzle 顶点 spends 边（Block 到 Coin） confirms 边 （Block 到 Coin） belongs_to 边（Coin 到 Puzzle） 这里，我们应用的图是一种叫做属性图的形式，除了点和边的关系之外。这两种实体（点、边）还有其他信息只和它们的一个实例相关，所以再定义为顶点、边就不是很适合，这些信息就作为点、边的属性（preperty）存在。 这种为了处理实体之间关联、涉及实体、关联的属性信息的，也就是\"属性图\"的存储信息的方式在计算机领域越来越流行，甚至有专门为此结构而原生开发的数据库——图数据库（Graph Database）。 这里，我们用的就是一个叫做 Nebula Graph 的图数据库，它是一个现代的、为超大规模分部署架构设计的、原生存储、查询、计算图数据的项目，更棒的是，它是产生于社区的开源产品。 Tips: 安装 Nebula Graph 一般来说，面向超大规模数据的分布式系统，天然的都是不容易轻量部署的，大家如果第一次使用的话可以试试我写的一个叫做 nebula-up 的小工具，可以一行指令部署一个用来试用、学习的 Nebula Graph 集群，地址在这里： https://github.com/wey-gu/nebula-up/ 。 ","date":"2021-05-26","objectID":"/nebula-chia/:3:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#如何探索-chia-链"},{"categories":["Nebula Graph"],"content":"3.1 Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#nebula-graph-导入-chia-数据到图数据库"},{"categories":["Nebula Graph"],"content":"3.1 Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#数据转换"},{"categories":["Nebula Graph"],"content":"3.1 Nebula Graph 导入 Chia 数据到图数据库我们分两步走，第一步这把 Chia Network 数据转换成 CSV 文件，第二步使用 Nebula 的 Nebula-Importer 把数据导入 Nebula Graph。 3.1.1 数据转换这部分的代码我开源在这里了: https://github.com/wey-gu/nebula-chia 使用它只需要在 Chia Network 的 python venv 下安装它: python3 -m pip install nebula-chia 然后调用 ChaiBatchConvertor 就可以在当前目录下生成两个 CSV 文件。 这里边有一些可以配置的参数，具体可以参考代码 nebulachia/convertor.py from nebulachia.convertor import ChiaBatchConvertor c = ChaiBatchConvertor( block_record_limit=0, coin_record_limit=0, write_batch_size=10000) c.convert_block_record() c.convert_coin_record() 生成的文件： $ ls -lth -rw-r--r-- 1 weyl staff 173M May 19 13:01 coin_record.csv -rw-r--r-- 1 weyl staff 77M May 19 12:59 block_record.csv 这里边字段的含义和类型，可以参考代码中 block_record_row 和 coin_record_row 的 __doc__ In [466]: print(c.coin_record_row.__doc__) Parse row and return a CSV block coin row list. CREATE TABLE coin_record( coin_name text PRIMARY KEY, confirmed_index bigint, spent_index bigint, spent int, coinbase int, puzzle_hash text, coin_parent text, amount blob, timestamp bigint) Coin Record CSV Head: 0 1(int) 2(int) 3(bool) coin_name|confirmed_index|spent_index|spent| 4(bool) 5 6 7(int) coinbase|puzzle_hash|coin_parent|amount| 8(timestamp) timestamp| 9 10 confirmed_hash|spent_hash In [467]: print(c.block_record_row.__doc__) Parse row and return a CSV block record row list. CREATE TABLE block_records( header_hash text PRIMARY KEY, prev_hash text, height bigint, block blob, sub_epoch_summary blob, is_peak tinyint, is_block tinyint) Block Record CSV Head: 0 1 2(int) 3(bool) 4(bool) header_hash|prev_hash|height|is_peak|is_block| 5(int) deficit| 6 challenge_block_info_hash| 7 farmer_puzzle_hash| 8(int) fees| 9 prev_transaction_block_hash| 10 prev_transaction_block_height| 11 12(int) required_iters|signage_point_index| 13(timestamp) timestamp 3.1.2 数据导入有了 CSV 文件，我们可以借助 Nebula-Importer 导入数据到图数据库中。 这里，我们写好了 nebula-importer 的配置文件，其中包涵了如下信息: 在 Nebula Graph 中创建需要的数据模型 Schema，这和我们前边做的图映射的信息是等价的 描述 CSV 文件之中的 Column 的数据到图模型（点，边，点或边的属性）映射关系 # 这里，我的 csv 文件和 配置文件都放在 /home/wei.gu/chia 之下 # 我使用 docker-compose 默认配置部署的 Nebula Graph, # 它创建了叫 nebula-docker-compose_nebula-net 的 docker 网络 docker run --rm -ti \\ --network=nebula-docker-compose_nebula-net \\ -v /home/wei.gu/chia/nebula-chia.yaml:/root/nebula-chia.yaml \\ -v /home/wei.gu/chia:/root \\ vesoft/nebula-importer:v2 \\ --config /root/nebula-chia.yaml 这里我展示一个导入的结果示例，我在单机部署的 Nebula Graph 里导入了我一两周之前取的全量 Chia Network 数据的结果。 ... 2021/05/19 09:55:09 [INFO] reader.go:180: Total lines of file(/root/coin_record.csv) is: 547557, error lines: 0 2021/05/19 09:55:09 [INFO] statsmgr.go:61: Done(/root/coin_record.csv): Time(4385.88s), Finished(4512927), Failed(0), Latency AVG(1305us), Batches Req AVG(2015us), Rows AVG(1028.42/s) 2021/05/19 09:55:10 --- END OF NEBULA IMPORTER --- ","date":"2021-05-26","objectID":"/nebula-chia/:3:1","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#数据导入"},{"categories":["Nebula Graph"],"content":"3.2 探索 Chia 的图数据3.2.1 用图数据库的 Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 用 Nebula Studio 可视化探索Nebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#探索-chia-的图数据"},{"categories":["Nebula Graph"],"content":"3.2 探索 Chia 的图数据3.2.1 用图数据库的 Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 用 Nebula Studio 可视化探索Nebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#用图数据库的-queries"},{"categories":["Nebula Graph"],"content":"3.2 探索 Chia 的图数据3.2.1 用图数据库的 Queries导入 Chia 链的网络到 Nebula Graph 之后，我们可以在里边快速查询数据之间的关联。 比如这个查询表示从区块 524aa2013781ff4cd9d2b5dc... 查起，经过三种边 farmer_puzzle, spends, confirms 双向遍历的结果。 GO5STEPSFROM\\\"524aa2013781ff4cd9d2b5dce40647b670d722e2de25bd2e2b79970a8ec435ee\"\\OVERfarmer_puzzle,spends,confirmsBIDIRECT...Got419437rows(timespent735120/1170946us)Wed,19May202110:11:28UTC 再比如，计算一个 Puzzle 地址上的余额（所有 coin 的总价值）比如这个puzzle bbe39134ccc32c08fdeff... GO1STEPFROM\"bbe39134ccc32c08fdeff4d2c19d1d1f4f7e48cdaf79d37397bc3136ce9b3cb7\"\\OVERbelongs_toREVERSELY\\YIELDCASE$$.coin.is_spent\\WHENtrueTHEN$$.coin.amount\\WHENfalseTHEN-$$.coin.amount\\ENDASAmount|YIELDsum($-.Amount) 3.2.2 用 Nebula Studio 可视化探索Nebula Graph 为我们提供了图形化界面，有了它，我们可以用更符合人脑的方式地查看 Chia Network 中的数据。 比如，我们还是回到上边的那个区块，从这里查询。 我们就获得了这个 block 类型的一个点/ vertex。我们可以从他开始进一步探索，先鼠标单击这个点，在拓展条件里把方向选择双向，默认的边类型是所有的边类型，这样我们就可以把所有 步数内相关联的数据一下子全都找出来。 选择好拓展条件之后，点击拓展就可以。 这里，我们选择了步数为 1，点击拓展（或者双击要拓展的点），之后，我们可以快速双击其他的点继续拓展，这是我鼠标点了几次之后看到的样子： 我们接下来再试试拓展的步数为 2，点击拓展（或者双击要拓展的点），看起来找到了有意思的信息。 我们看到了一个有很多边的黑色的点。 通过查看这个点和我们开始查看的 block 之间的边，我们知道这个点正是 farm 这个 block 的地址，这个地址下边有非常多的 coin。 这只是一个开始，有了这个导入到 Nebula Graph 图数据的基础，我们可以做很多有意思的分析和洞察，大家可以自己试试看，得到更有意思的结果分享给其他同学。 ","date":"2021-05-26","objectID":"/nebula-chia/:3:2","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#用-nebula-studio-可视化探索"},{"categories":["Nebula Graph"],"content":"4 总结这篇文章里，在我们简单介绍了 Chia Network 之后，我们首次的带大家一起从安装一个 Chia 终端，到分析终端同步到本地的 Chia 全网数据，借助于 Chia 终端开源的 Python 代码库，我们分析了全网数据里的重要信息。 之后，我们开源了一个小工具 Nebula-Chia，有了它，就可以把 Chia 的全网数据转换成 CSV 格式，这样，就可以借助 nebula-importer 把所有的数据导入到一个先进的图数据库（Nebula Graph）中。 Nebula Graph 的项目地址是 https://github.com/vesoft-inc/nebula-graph Nebula-Chia 我也开源在 https://github.com/wey-gu/nebula-chia 在图数据库中，我们展示了做基本 Query 的例子和借助图数据库自带的可视化工具，我们可以轻易地获取 Chia 全网数据之间关联关系，有了这个作为基础，这些数据中洞察的潜力和可以尝试的有意思事情可以比较直观和高效地进一步探索了！ 是不是很酷？ ","date":"2021-05-26","objectID":"/nebula-chia/:4:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#总结"},{"categories":["Nebula Graph"],"content":"5 引用 https://www.chia.net/faq/ https://chialisp.com/docs/ https://www.chiaexplorer.com/chia-coins https://docs.google.com/document/d/1tmRIb7lgi4QfKkNaxuKOBHRmwbVlGL4f7EsBDr_5xZE https://github.com/sipa/bech32/tree/master/ref/python https://github.com/Chia-Network/chia-blockchain/blob/main/README.md https://www.chia.net/assets/ChiaGreenPaper.pdf https://docs.nebula-graph.com.cn 题图版权：Icons8 Team ","date":"2021-05-26","objectID":"/nebula-chia/:5:0","series":null,"tags":["Nebula Graph","Chia Network"],"title":"用图数据库可视化探索 Chia Network 区块链数据","uri":"/nebula-chia/#引用"},{"categories":null,"content":" Nebula-Corp-Rel-Graph Nebula-Corp-Rel-Graph，基于图数据库的股权穿透系统 阅读全文 Nebula-Siwi Nebula-Siwi，基于图数据库的智能问答助手 阅读全文 Nebula-Holdshare Nebula-Holdshare，图数据库应用示例：股权关系穿透 阅读全文 Nebula-KIND Nebula-Kind，无需依赖，一键安装尝鲜基于 Nebula Operator 的 K8s Nebula Graph Cluster。注： KIND 是一个 K8s 的 SIG，代表 K8s in Docker。 阅读全文 Nebula-Up Nebula-up，一键拉起一个 Nebula 测试环境，支持 mscOS，Windows 10，CentOS 和 Ubuntu。 阅读全文 VSCode-nGQL VSCode-nGQL，Nebula Graph 的 VS Code 插件，ngql 语法高亮。 阅读全文 Nebula-Chia Nebula-Chia，将 Chia Network 全链导入 Nebula Graph 中的探索分享和转换工具（一并开源出来）。 阅读全文 IPython-nGQL Nebula Graph 的 Jupyter Notebook 和 IPython 插件，方便在 Notebook 之中嵌入 nGQL 的 query 和 结果的调试。 阅读全文 nebula-insights 本文介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。 阅读全文 ","date":"2021-05-26","objectID":"/projects/:0:0","series":null,"tags":null,"title":"个人项目","uri":"/projects/#"},{"categories":null,"content":" Nebula Operator Explained Nebula Graph K8s Operator 介绍 阅读全文 Nebula Config Explained Nebula Graph 配置详解 阅读全文 Nebula Index Demystified Nebula Graph 原生索引解谜 阅读全文 Nebula Data Import Options Nebula Graph 提供了好多种数据导入的工具，我们应该如何选择呢？ 阅读全文 Nebula Deployment Options Nebula Graph 有很多种分发、部署方式，我们应该如何选择它们呢？ 阅读全文 Nebula Intro 新手玩转 Nebula Graph 系列开篇。 阅读全文 ","date":"2021-05-26","objectID":"/sketch-notes/:0:0","series":null,"tags":null,"title":"手绘笔记","uri":"/sketch-notes/#"},{"categories":["sketches"],"content":"Nebula Graph Core Arch.","date":"2021-05-25","objectID":"/sketches/nebula-core-arch/","series":null,"tags":["Nebula Graph","Core"],"title":"Nebula Core Arch","uri":"/sketches/nebula-core-arch/"},{"categories":["sketches"],"content":"1 Bilibili 上 下 ","date":"2021-05-25","objectID":"/sketches/nebula-core-arch/:1:0","series":null,"tags":["Nebula Graph","Core"],"title":"Nebula Core Arch","uri":"/sketches/nebula-core-arch/#bilibili"},{"categories":["sketches"],"content":"2 Youtube 上 下 ","date":"2021-05-25","objectID":"/sketches/nebula-core-arch/:2:0","series":null,"tags":["Nebula Graph","Core"],"title":"Nebula Core Arch","uri":"/sketches/nebula-core-arch/#youtube"},{"categories":["Nebula Graph"],"content":"VSCode-ngql 是 Nebula Graph 的 VS Code 之中对 nGQL 语法高亮的插件。","date":"2021-05-05","objectID":"/vscode-ngql/","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/"},{"categories":["Nebula Graph"],"content":" VSCode-ngql 是 Nebula Graph 的 VS Code 之中对 nGQL 语法高亮的插件。 您可以从 这里 直接下载安装试用。 ","date":"2021-05-05","objectID":"/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#"},{"categories":["Nebula Graph"],"content":"VS Code nGQL Syntax Highlight","date":"2021-05-05","objectID":"/vscode-ngql/:0:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#vs-code-ngql-syntax-highlight"},{"categories":["Nebula Graph"],"content":"1 DownloadSearch ngql from the market or click here. ","date":"2021-05-05","objectID":"/vscode-ngql/:1:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#download"},{"categories":["Nebula Graph"],"content":"2 Features Highlighting all Keywords, Functions of a given .ngql file ","date":"2021-05-05","objectID":"/vscode-ngql/:2:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#features"},{"categories":["Nebula Graph"],"content":"3 Release Notes","date":"2021-05-05","objectID":"/vscode-ngql/:3:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#release-notes"},{"categories":["Nebula Graph"],"content":"3.1 0.0.1Initial release, only .ngql Syntax is supported. ","date":"2021-05-05","objectID":"/vscode-ngql/:3:1","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#001"},{"categories":["Nebula Graph"],"content":"3.2 0.0.2Lower supported vscode version till ^1.50.1 ","date":"2021-05-05","objectID":"/vscode-ngql/:3:2","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#002"},{"categories":["Nebula Graph"],"content":"4 Reference https://docs.nebula-graph.com.cn/ https://github.com/vesoft-inc/nebula-graph/blob/master/src/parser/scanner.lex ","date":"2021-05-05","objectID":"/vscode-ngql/:4:0","series":null,"tags":["Nebula Graph","vscode","ngql"],"title":"VSCode-ngql，Nebula Graph 的 VSCode 语法高亮插件。","uri":"/vscode-ngql/#reference"},{"categories":["Big Data","Cloud"],"content":"本文介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。","date":"2021-05-03","objectID":"/nebula-insights/","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/"},{"categories":["Big Data","Cloud"],"content":" 这是我首发在 Datawhale 的文章，介绍我们如何用公有云 Serverless 技术：Google Cloud Scheduler，Google Cloud Functions 和 BigQuery 搭建数据管道，收集探索开源社区洞察。并将全部代码开源在 GitHub。 引子 我们想要收集一些帮助 Nebula Graph 社区运营的 metrics，希望能从不同来源的数据自动化周期性收集、处理、并方便地展现出来做数据驱动分析的基础设施。 Nebula Graph 是一个现代的开源分布式图数据库(Graph Database)，欢迎同学们从: 官网: https://nebula-graph.com.cn Bilibili: https://space.bilibili.com/472621355 GitHub:https://github.com/vesoft-inc/nebula-graph 了解我们哈。 ","date":"2021-05-03","objectID":"/nebula-insights/:0:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#"},{"categories":["Big Data","Cloud"],"content":"1 需求 方便增加新的数据 数据收集无需人为触发（自动、周期性） 每天数据量不超过1000条 数据可以生成 dashboard，也可以支持统计分期 query 高可用，数据安全 低预算，尽可能不需要运维人力 ","date":"2021-05-03","objectID":"/nebula-insights/:1:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#需求"},{"categories":["Big Data","Cloud"],"content":"1.1 需求分析我们需要搭建一个系统能实现 一个能周期性触发获取数据的事件的服务: scheduler 一个触发之后，把数据 ETL 到数据库中的服务: ETL worker 一个数据仓库 一个能够把数据库作为源，允许用户 query，展示数据的界面: Data-UI 这个需求的特点是虽然数据量很小、但是要求服务高可用、安全。因为这种情况下自建服务器还需要保证HA和数据安全会一定会消耗昂贵运维人力，所以我们应该尽量避免在自己维护的服务器中搭建 scheduler, 和数据库。 最终，我们选择了尽量使用公有云的 aaS 的方案: ┌──────────────────────────┐ │ │ │ Google Cloud Scheduler │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────┐ │ │ │ ┌────────────▼─────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ├───┤ └─────────────────────┘ │ │ │ └────────────┬─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌─────────▼─────────┐ │ │ │ │ │ │ │ │ │ Google BigQuery │ │ └─────────────────────────┘ │ │ ├───────────► ... └─────────▲─────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌──────────┴───────────┐ │ │ │ │ └──────────────────┘ │ Google Data Studio │ │ ┌──┐ │ │ ┌──┐ │ │ ┌──┐ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──┴──┴─┴──┴─┴──┴──────┘ 因为我个人比较熟悉 Google Cloud Platform(GCP)的原因，加上GCP在大数据处理上比较领先，再加上Google提供的 free tier额度非常大方，以至于在我们这个数据量下，所有workload都会是免费的。 这个方案最后选择了全栈 Google Cloud，然而，这实际上只是一个参考，同学们完全可以在其他公有云提供商那里找到对应的服务。 这里我简单介绍一下， Google Cloud Scheduler是自解释的，不用多介绍了。 而 Google Cloud Functions是GCP的无服务器(serverless)的 Function as a Service服务，它的好处是我们可以把无状态的 event-driven 的 workload 代码放上去，它是按需付费（pay as you go)的，类似的服务还有 Google Cloud Run，后者的区别在于我们提供的是一个docker/container（这使得能支持的运行环境可以使任何能跑在容器里的东西），而 Cloud Functions是把我们的代码文件放上去。他们的效果是类似的，因为我准备用Python来做 ETL的东西，Clouf Functions已经支持了，我就直接选择它了。 在scheduler里边，我定义了每一天它发一个 pub/sub（类似于kafka，这里google可以保证至少发成功一次）消息给 Cloud Functions，然后 Cloud Functions会去做 ETL的工作。 这里，实际上我的设计里这个触发的函数调会把数据从API那里获取下来，在内存里处理好之后，存储到在对象存储里为 JSON 文件，然后再调用 Google BigQuery 的 API让 BigQuery直接从对对象存储里拉取 JSON 文件，导入记录到相应的表之中。 Google BigQuery 作为GCP 特别有竞争力的一个产品，是它数据仓库，BigQuery 可以无限扩容，支持海量数据导入，支持 SQL-like 的 query，还自带ML算法，通过SQL就能调用这些算法。它可以和很多GCP以及第三方的组件可以集成起来。 Google Data Studio 是GCP的数据 Insights产品，如果大家用过 Google Analytics 应该已经用过它了。 ","date":"2021-05-03","objectID":"/nebula-insights/:1:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#需求分析"},{"categories":["Big Data","Cloud"],"content":"1.2 数据的获取，API我们第一阶段想要收集的数据来源是 GitHub 上，社区项目的统计数据、Docker Hub上，社区镜像的拉取计数，之后，会增加更多维度的数据。 Github API, ref: https://pygithub.readthedocs.io 这里我们利用了一个Github API的一个 Python 封装，下边是在 IDLE/iPython/Jupyter 里尝试的例子 # 实例化一个client g = Github(login_or_token=token, timeout=60, retry=Retry( total=10, status_forcelist=(500, 502, 504), backoff_factor=0.3)) # 配置好要获取的repo的信息 org_str = \"vesoft-inc\" org = g.get_organization(org_str) repos = org.get_repos() # 这里repos是一个迭代器，方便看到里边的东西，我们把它 list 一下可以看到所有的repo: list(repos) [Repository(full_name=\"vesoft-inc/nebula\"), Repository(full_name=\"vesoft-inc/nebula-docs\"), Repository(full_name=\"vesoft-inc/nebula-dev-docker\"), Repository(full_name=\"vesoft-inc/github-statistics\"), Repository(full_name=\"vesoft-inc/nebula-docker-compose\"), Repository(full_name=\"vesoft-inc/nebula-go\"), Repository(full_name=\"vesoft-inc/nebula-java\"), Repository(full_name=\"vesoft-inc/nebula-python\"), Repository(full_name=\"vesoft-inc/nebula-importer\"), Repository(full_name=\"vesoft-inc/nebula-third-party\"), Repository(full_name=\"vesoft-inc/nebula-storage\"), Repository(full_name=\"vesoft-inc/nebula-graph\"), Repository(full_name=\"vesoft-inc/nebula-common\"), Repository(full_name=\"vesoft-inc/nebula-stats-exporter\"), Repository(full_name=\"vesoft-inc/nebula-web-docker\"), Repository(full_name=\"vesoft-inc/nebula-bench\"), Repository(full_name=\"vesoft-inc/nebula-console\"), Repository(full_name=\"vesoft-inc/nebula-docs-cn\"), Repository(full_name=\"vesoft-inc/nebula-chaos\"), Repository(full_name=\"vesoft-inc/nebula-clients\"), Repository(full_name=\"vesoft-inc/nebula-spark-utils\"), Repository(full_name=\"vesoft-inc/nebula-node\"), Repository(full_name=\"vesoft-inc/nebula-rust\"), Repository(full_name=\"vesoft-inc/nebula-cpp\"), Repository(full_name=\"vesoft-inc/nebula-http-gateway\"), Repository(full_name=\"vesoft-inc/nebula-flink-connector\"), Repository(full_name=\"vesoft-inc/nebula-community\"), Repository(full_name=\"vesoft-inc/nebula-br\"), Repository(full_name=\"vesoft-inc/.github\")] # repo0 是 vesoft-inc/nebula 这个repo，我们可以通过 get_clones_traffic，get_views_traffic 来获取过去十几天的 clone，view 统计 In [16]: repo0.get_clones_traffic() Out[16]: {'count': 362, 'uniques': 150, 'clones': [Clones(uniques=5, timestamp=2021-04-06 00:00:00, count=16), Clones(uniques=8, timestamp=2021-04-07 00:00:00, count=23), Clones(uniques=13, timestamp=2021-04-08 00:00:00, count=30), Clones(uniques=33, timestamp=2021-04-09 00:00:00, count=45), Clones(uniques=2, timestamp=2021-04-10 00:00:00, count=13), Clones(uniques=6, timestamp=2021-04-11 00:00:00, count=19), Clones(uniques=15, timestamp=2021-04-12 00:00:00, count=28), Clones(uniques=40, timestamp=2021-04-13 00:00:00, count=54), Clones(uniques=9, timestamp=2021-04-14 00:00:00, count=21), Clones(uniques=10, timestamp=2021-04-15 00:00:00, count=34), Clones(uniques=10, timestamp=2021-04-16 00:00:00, count=23), Clones(uniques=5, timestamp=2021-04-17 00:00:00, count=17), Clones(uniques=2, timestamp=2021-04-18 00:00:00, count=13), Clones(uniques=9, timestamp=2021-04-19 00:00:00, count=23), Clones(uniques=3, timestamp=2021-04-20 00:00:00, count=3)]} In [17]: repo0.get_views_traffic() Out[17]: {'count': 6019, 'uniques': 1134, 'views': [View(uniques=52, timestamp=2021-04-06 00:00:00, count=169), View(uniques=143, timestamp=2021-04-07 00:00:00, count=569), View(uniques=152, timestamp=2021-04-08 00:00:00, count=635), View(uniques=134, timestamp=2021-04-09 00:00:00, count=648), View(uniques=81, timestamp=2021-04-10 00:00:00, count=318), View(uniques=42, timestamp=2021-04-11 00:00:00, count=197), View(uniques=127, timestamp=2021-04-12 00:00:00, count=515), View(uniques=149, timestamp=2021-04-13 00:00:00, count=580), View(uniques=134, timestamp=2021-04-14 00:00:00, count=762), View(uniques=141, timestamp=2021-04-15 00:00:00, count=385), View(uniques=113, timestamp=2021-04-16 00:00:00, count=284), View(uniques=48, timestamp=2021-04-17 00:00:00, count=168), View(uniques=35, timestamp=2021-04-18 00:00:00, count=135), View(uniques=124, timestamp=2021-04-19 00:00:00, cou","date":"2021-05-03","objectID":"/nebula-insights/:1:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#数据的获取api"},{"categories":["Big Data","Cloud"],"content":"2 实现","date":"2021-05-03","objectID":"/nebula-insights/:2:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#实现"},{"categories":["Big Data","Cloud"],"content":"2.1 计划任务调度 with Cloud Scheduler前边提到，Scheduler --\u003e Functions 中间是通过消息队列实现的可靠事件触发，我们需要在 Google Cloud Pub/Sub里创建一个订阅消息，后边我们会把这个订阅消息从 Scheduler 定期发送，并且在 Function创建的时候定义为触发条件。 $ gcloud pubsub topics create nebula-insights-cron-topic $ gcloud pubsub subscriptions create cron-sub --topic nebula-insights-cron-topic 任务的创建非常直接，在 Scheduler Web Console 上直接图形化操作就可以了，记得要选择触发 Pub/Sub 消息为 cron-sub，消息主题为 nebula-insights-cron-topic ","date":"2021-05-03","objectID":"/nebula-insights/:2:1","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#计划任务调度-with-cloud-scheduler"},{"categories":["Big Data","Cloud"],"content":"2.2 ETL Worker with Python + Google Functions当 Scheduler 每天定时发送消息之后，接收方就是我们要定义的 Google Functions了，它的定义如图 第一步，选择它的触发类型为 Pub/Sub，同时要定义消息的主题和名字。 第二步就是把代码放进去: ┌─────────────────────┐ │ │ ┌──────────────────────────┐ ┌───────────► GitHub API Server │ │ │ │ │ │ │ Google Cloud Functions ◄───► └─────────────────────┘ │ │ │ └────────────▲─────────────┘ │ ┌─────────────────────────┐ │ │ │ │ │ ├───────────► Docker Hub API Server │ ┌────────────▼────────────┐ │ │ │ │ │ │ │ │ │ Google Cloud Storage │ │ └─────────────────────────┘ │ │ ... └────────────┬────────────┘ │ ┌──────────────────┐ │ │ │ │ │ └───────────► Aliyun OSS API │ ┌─────────▼─────────┐ │ │ │ │ └──────────────────┘ │ Google BigQuery │ │ │ └───────────────────┘ 这部分的逻辑就是通过前边分析了的API取得信息，然后组装成需要的格式存到 Cloud Storage(对象存储），然后再导入到 BigQuery（数仓）之中，全部代码在GitHub上: https://github.com/wey-gu/nebula-insights/blob/main/functions/data-fetching-0/main.py 另外，可以参考这个官方教程 https://cloud.google.com/scheduler/docs/tut-pub-sub ","date":"2021-05-03","objectID":"/nebula-insights/:2:2","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#etl-worker-with-python--google-functions"},{"categories":["Big Data","Cloud"],"content":"2.3 数仓表结构定义数仓的表结构比较直接，schema的图贴在下边了，值得注意的是，BigQuery支持嵌套的表结构（而不像一般关系型数据库那样需要把这样的逻辑结构用辅助表来表示），在我们这个场景下非常方便，比如release表中的 assets的三个嵌套字段。 更详细的信息可以参考GitHub上的介绍和代码: https://github.com/wey-gu/nebula-insights#data-etl-bigquery-and-gcs ","date":"2021-05-03","objectID":"/nebula-insights/:2:3","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#数仓表结构定义"},{"categories":["Big Data","Cloud"],"content":"2.4 数据可视化到这里，我们就可以自动在BigQuery里存有每天收集的不同来源的统计数据啦，有了它，我们可以借助 Data Studio 来生成各式各样的可视化表示。 参考 https://cloud.google.com/bigquery/docs/visualize-data-studio ","date":"2021-05-03","objectID":"/nebula-insights/:2:4","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#数据可视化"},{"categories":["Big Data","Cloud"],"content":"3 总结这样，我们实际上不需要任何认为维护的成本和投入，就搭建了一整个数据的流水线，并且只需要按照数据用量付费，在我们的数据量下，及时考虑未来增加数十个新的量度的收集，我们依然没有达到需要付费的用量，是不是很Cool？ 因为数据同时存在于对象存储与数仓里，我们可以方便随时把数据导入任意其他平台上。 BigQuery还有一些非常常用的，自带的机器学习的功能，只需要写一个SQL-Like的query就能触发然后获得预测结果，如果我们用到这些功能的话也会回到 datawhale 为同学们继续分享哈。 第一次做数据工程方面的分享，如果有错误的地方欢迎大家不吝指出哈~~ 谢谢！ ","date":"2021-05-03","objectID":"/nebula-insights/:3:0","series":null,"tags":["Nebula Graph","serverless","Cloud","FaaS"],"title":"Nebula-Insights，我们基于 Serverless 架构的数据管道方案与代码分享。","uri":"/nebula-insights/#总结"},{"categories":["Nebula Graph"],"content":"Nebula-up，一键拉起一个 Nebula 测试环境，包括 Nebula BR、Exchange、Algorithm、Dashboard、Studio","date":"2021-04-26","objectID":"/nebula-up/","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/"},{"categories":["Nebula Graph"],"content":" Update: the All-in-one mode is introduced! Check here and try it! Nebula-Up is PoC utility to enable developer to bootstrap an nebula-graph cluster with nebula-graph-studio(Web UI) + nebula-graph-console(Command UI) ready out of box in an oneliner run. All required packages will handled with nebula-up as well, including Docker on Linux(Ubuntu/CentOS), Docker Desktop on macOS(including both Intel and M1 chip based), and Docker Desktop Windows. Also, it’s optimized to leverage China Repo Mirrors(docker, brew, gitee, etc…) in case needed enable a smooth deployment for both Mainland China users and others. macOS and Linux with Shell: curl -fsSL nebula-up.siwei.io/install.sh | bash Note: you could specify the version of Nebula Graph like: curl -fsSL nebula-up.siwei.io/install.sh | bash -s -- v2.6 ","date":"2021-04-26","objectID":"/nebula-up/:0:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#"},{"categories":["Nebula Graph"],"content":"1 All-in-one modeWith all-in-one mode, you could play with many Nebula Tools in one command, too: Supported tools: Nebula Dashboard Nebula Graph Studio Nebula Graph Console Nebula BR(backup \u0026 restore) Nebula Graph Spark utils Nebula Graph Spark Connector/PySpark Nebula Graph Algorithm Nebula Graph Exchange Nebula Graph Importer Nebula Graph Fulltext Search Nebula Bench ","date":"2021-04-26","objectID":"/nebula-up/:1:0","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#all-in-one-mode"},{"categories":["Nebula Graph"],"content":"1.1 Install all in one # Install Nebula Core with all-in-one mode curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash ","date":"2021-04-26","objectID":"/nebula-up/:1:1","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#install-all-in-one"},{"categories":["Nebula Graph"],"content":"1.2 Install Nebula Core and One of the coponent: # Install Core with Backup and Restore with MinIO curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 br # Install Core with Spark Connector, Nebula Algorithm, Nebula Exchange curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 spark # Install Core with Dashboard curl -fsSL nebula-up.siwei.io/all-in-one.sh | bash -s -- v3 dashboard ","date":"2021-04-26","objectID":"/nebula-up/:1:2","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#install-nebula-core-and-one-of-the-coponent"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree \u003e=0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be \u003chost-ip\u003e:9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. \u003e\u003e\u003e df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() \u003e\u003e\u003e df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#how-to-play-with-all-in-one-mode"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#console-and-basketballplayer-dataset-loading"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#monitor-the-whole-cluster-with-nebula-dashboard"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#access-nebula-graph-studio"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#query-data-with-nebula-spark-connector-in-pyspark-shell"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#run-nebula-exchange"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#run-nebula-graph-algorithm"},{"categories":["Nebula Graph"],"content":"1.3 How to play with all-in-one mode:1.3.1 Console and Basketballplayer Dataset LoadingThen you could call Nebula Console like: # Connect to nebula with console ~/.nebula-up/console.sh # Execute queryies like ~/.nebula-up/console.sh -e \"SHOW HOSTS\" # Load the sample dataset ~/.nebula-up/load-basketballplayer-dataset.sh # Make a Graph Query the sample dataset ~/.nebula-up/console.sh -e 'USE basketballplayer; FIND ALL PATH FROM \"player100\" TO \"team204\" OVER * WHERE follow.degree is EMPTY or follow.degree =0 YIELD path AS p;' 1.3.2 Monitor the whole cluster with Nebula DashboardVisit http://127.0.0.1:7003 with user: root, password: nebula. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the dashboard here: http://nebula-demo.siwei.io:7003 . 1.3.3 Access Nebula Graph StudioVisit http://127.0.0.1:7001 with user: root, password: nebula, host: graphd:9669(for non-all-in-one case, this should be :9669). Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the studio here: http://nebula-demo.siwei.io:7001 . 1.3.4 Query Data with Nebula Spark Connector in PySpark ShellOr play in PySpark like: ~/.nebula-up/nebula-pyspark.sh # call Nebula Spark Connector Reader df = spark.read.format( \"com.vesoft.nebula.connector.NebulaDataSource\").option( \"type\", \"vertex\").option( \"spaceName\", \"basketballplayer\").option( \"label\", \"player\").option( \"returnCols\", \"name,age\").option( \"metaAddress\", \"metad0:9559\").option( \"partitionNumber\", 1).load() # show the dataframe with limit 2 df.show(n=2) The output may look like: ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Python version 2.7.16 (default, Jan 14 2020 07:22:06) SparkSession available as 'spark'. df = spark.read.format( ... \"com.vesoft.nebula.connector.NebulaDataSource\").option( ... \"type\", \"vertex\").option( ... \"spaceName\", \"basketballplayer\").option( ... \"label\", \"player\").option( ... \"returnCols\", \"name,age\").option( ... \"metaAddress\", \"metad0:9559\").option( ... \"partitionNumber\", 1).load() df.show(n=2) +---------+--------------+---+ |_vertexId| name|age| +---------+--------------+---+ |player105| Danny Green| 31| |player109|Tiago Splitter| 34| +---------+--------------+---+ only showing top 2 rows 1.3.5 Run Nebula ExchangeOr run an example Nebula Exchange job to import data from CSV file: ~/.nebula-up/nebula-exchange-example.sh You could check the example configuration file in ~/.nebula-up/nebula-up/spark/exchange.conf 1.3.6 Run Nebula Graph Algorithm Reference: https://github.com/wey-gu/nebula-livejournal Load LiveJournal dataset with Nebula Importer: ~/.nebula-up/load-LiveJournal-dataset.sh Run Nebula Algorithm like: ~/.nebula-up/nebula-algo-pagerank-example.sh 1.3.7 Try Backup and Restore with MinIO as Storage # Create a full backup to MinIO(Be sure to run load-basketballplayer-dataset.sh before doing so) ~/.nebula-up/nebula-br-backup-full.sh # Show all backups ~/.nebula-up/nebula-br-show.sh # Restore to a backup named BACKUP_2022_05_08_11_38_08 ~/.nebula-up/nebula-br-restore-full.sh BACKUP_2022_05_08_11_38_08 Note, you could also browser files in MinIO with from http://127.0.0.1:9001 with user: minioadmin, password: minioadmin. Note, thanks to the sponsorship of Microsoft, we have a demo site of Nebula-UP on Azure: you could visit the MinIO site here: http://nebula-demo.siwei.io:9001 . Windows with PowerShell(Working In Progress): iwr nebula-up.siwei.io/install.ps1 -useb | iex TBD: Finished Windows(Docker Desktop instead of the WSL 1\u00262 in initial phase) part, leveraging chocolatey package manager as homebrew was used in macOS Fully optimized for CN users, for now, git/apt/yum repo were not optimised, newly installed docker repo, brew repo were automatically optimised for CN internet access Packaging similar content into homebrew/chocolatey? CI/UT ","date":"2021-04-26","objectID":"/nebula-up/:1:3","series":null,"tags":["Nebula Graph","nebula-up"],"title":"Nebula-Up，一键拉起一个 Nebula 测试环境","uri":"/nebula-up/#try-backup-and-restore-with-minio-as-storage"},{"categories":["Nebula Graph"],"content":"Nebula Graph 的 Jupyter Notebook 和 IPython 插件，方便在 Notebook 之中嵌入 nGQL 的 query 和 结果的调试。","date":"2021-03-07","objectID":"/ipython-ngql/","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/"},{"categories":["Nebula Graph"],"content":" Nebula Graph 的 Jupyter Notebook 和 IPython 插件，方便在 Notebook 之中嵌入 nGQL 的 query 和 结果的调试。 ipython-ngql is a python package to extend the ability to connect Nebula Graph from your Jupyter Notebook or iPython. It’s easier for data scientists to create, debug and share reusable and all-in-one Jupyter Notebooks with Nebula Graph interaction embedded. ipython-ngql is inspired by ipython-sql created by Catherine Devlin ","date":"2021-03-07","objectID":"/ipython-ngql/:0:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#"},{"categories":["Nebula Graph"],"content":"1 Get Started","date":"2021-03-07","objectID":"/ipython-ngql/:1:0","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#get-started"},{"categories":["Nebula Graph"],"content":"1.1 Installationipython-ngql could be installed either via pip or from this git repo itself. Install via pip pip install ipython-ngql Install inside the repo git clone git@github.com:wey-gu/ipython-ngql.git cd ipython-ngql python setup.py install ","date":"2021-03-07","objectID":"/ipython-ngql/:1:1","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#installation"},{"categories":["Nebula Graph"],"content":"1.2 Load it in Jupyter Notebook or iPython %load_ext ngql ","date":"2021-03-07","objectID":"/ipython-ngql/:1:2","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#load-it-in-jupyter-notebook-or-ipython"},{"categories":["Nebula Graph"],"content":"1.3 Connect to Nebula GraphArguments as below are needed to connect a Nebula Graph DB instance: Argument Description --address or -addr IP address of the Nebula Graph Instance --port or -P Port number of the Nebula Graph Instance --user or -u User name --password or -p Password Below is an exmple on connecting to 127.0.0.1:9669 with username: “user” and password: “password”. %ngql --address 127.0.0.1 --port 9669 --user user --password password ","date":"2021-03-07","objectID":"/ipython-ngql/:1:3","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#connect-to-nebula-graph"},{"categories":["Nebula Graph"],"content":"1.4 Make QueriesNow two kind of iPtython Magics are supported: Option 1: The one line stype with %ngql: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id; Option 2: The multiple lines stype with %%ngql %%ngql USE pokemon_club; SHOW TAGS; SHOW HOSTS; There will be other options in future, i.e. from a .ngql file. ","date":"2021-03-07","objectID":"/ipython-ngql/:1:4","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#make-queries"},{"categories":["Nebula Graph"],"content":"1.5 Query String with Variablesipython-ngql supports taking variables from the local namespace, with the help of Jinja2 template framework, it’s supported to have queries like the below example. The actual query string should be GO FROM \"Sue\" OVER owns_pokemon ..., and \"{{ trainer }}\" was renderred as \"Sue\" by consuming the local variable trainer: In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey ","date":"2021-03-07","objectID":"/ipython-ngql/:1:5","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#query-string-with-variables"},{"categories":["Nebula Graph"],"content":"1.6 Configure \u003ccode\u003engql_result_style\u003c/code\u003eBy default, ipython-ngql will use pandas dataframe as output style to enable more human readable output, while it’s supported to use the raw thrift data format comes from the nebula2-python itself. This can be done ad-hoc with below one line: %config IPythonNGQL.ngql_result_style=\"raw\" After above line being executed, the output will be like: ResultSet(ExecutionResponse( error_code=0, latency_in_us=2844, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) The result are always stored in variable _ in Jupyter Notebook, thus, to tweak the result, just refer a new var to it like: In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/ipython-ngql/:1:6","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#configure-ngql_result_style"},{"categories":["Nebula Graph"],"content":"1.7 Get HelpDon’t remember anything or even relying on the cheatsheet here, oen takeaway for you: the help! In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" ","date":"2021-03-07","objectID":"/ipython-ngql/:1:7","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#get-help"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ \u003e How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" \u003e How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True \u003e How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- \u003e Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password \u003e Use Space %ngql USE nba \u003e Query %ngql SHOW TAGS; \u003e Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql \u003e Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#examples"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#jupyter-notebook"},{"categories":["Nebula Graph"],"content":"1.8 Examples1.8.1 Jupyter NotebookPlease refer here:https://github.com/wey-gu/ipython-ngql/blob/main/examples/get_started.ipynb 1.8.2 iPython venv ❯ ipython In [1]: %load_ext ngql In [2]: %ngql --address 127.0.0.1 --port 9669 --user user --password password Connection Pool Created Out[2]: Name 0 pokemon_club In [3]: %ngql GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name Out[3]: Trainer_Name 0 Tom 1 Jerry 2 Sue 3 Tom 4 Wey In [4]: %%ngql ...: SHOW TAGS; ...: SHOW HOSTS; ...: ...: Out[4]: Host Port Status Leader count Leader distribution Partition distribution 0 storaged0 9779.0 ONLINE 0 No valid partition No valid partition 1 storaged1 9779.0 ONLINE 1 pokemon_club:1 pokemon_club:1 2 storaged2 9779.0 ONLINE 0 No valid partition No valid partition 3 Total NaN None 1 pokemon_club:1 pokemon_club:1 In [5]: trainer = \"Sue\" In [6]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: Out[6]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [7]: %ngql help Supported Configurations: ------------------------ How to config ngql_result_style in \"raw\", \"pandas\" %config IPythonNGQL.ngql_result_style=\"raw\" %config IPythonNGQL.ngql_result_style=\"pandas\" How to config ngql_verbose in True, False %config IPythonNGQL.ngql_verbose=True How to config max_connection_pool_size %config IPythonNGQL.max_connection_pool_size=10 Quick Start: ----------- Connect to Neubla Graph %ngql --address 127.0.0.1 --port 9669 --user user --password password Use Space %ngql USE nba Query %ngql SHOW TAGS; Multile Queries %%ngql SHOW TAGS; SHOW HOSTS; Reload ngql Magic %reload_ext ngql Variables in query, we are using Jinja2 here name = \"nba\" %ngql USE \"{{ name }}\" In [8]: trainer = \"Sue\" In [9]: %%ngql ...: GO FROM \"{{ trainer }}\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[9]: Trainer_Name 0 Jerry 1 Sue 2 Tom 3 Wey In [10]: %config IPythonNGQL.ngql_result_style=\"raw\" In [11]: %%ngql USE pokemon_club; ...: GO FROM \"Tom\" OVER owns_pokemon YIELD owns_pokemon._dst as pokemon_id ...: | GO FROM $-.pokemon_id OVER owns_pokemon REVERSELY YIELD owns_pokemon._dst AS Trainer_Name; ...: ...: Out[11]: ResultSet(ExecutionResponse( error_code=0, latency_in_us=3270, data=DataSet( column_names=[b'Trainer_Name'], rows=[Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Jerry')]), Row( values=[Value( sVal=b'Sue')]), Row( values=[Value( sVal=b'Tom')]), Row( values=[Value( sVal=b'Wey')])]), space_name=b'pokemon_club')) In [12]: r = _ In [13]: r.column_values(key='Trainer_Name')[0]._value.value Out[13]: b'Tom' ","date":"2021-03-07","objectID":"/ipython-ngql/:1:8","series":null,"tags":["Nebula Graph","iPython","Jupyter"],"title":"IPython-nGQL, Nebula Graph 的 Jupyter 插件","uri":"/ipython-ngql/#ipython"}]