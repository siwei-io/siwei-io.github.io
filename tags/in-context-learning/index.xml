<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>In-context Learning - 标签 - siwei.io</title>
        <link>https://siwei.io/tags/in-context-learning/</link>
        <description>In-context Learning - 标签 - siwei.io</description>
        <generator>Hugo -- gohugo.io</generator><managingEditor>weyl.gu@gmail.com (Wey Gu)</managingEditor>
            <webMaster>weyl.gu@gmail.com (Wey Gu)</webMaster><lastBuildDate>Tue, 15 Aug 2023 11:10:34 &#43;0800</lastBuildDate><atom:link href="https://siwei.io/tags/in-context-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Graph RAG: 知识图谱结合 LLM 的检索增强</title>
    <link>https://siwei.io/graph-rag/</link>
    <pubDate>Tue, 15 Aug 2023 11:10:34 &#43;0800</pubDate><author>
        <name>Wey Gu</name>
    </author><guid>https://siwei.io/graph-rag/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/graph-rag/featured-image.webp" referrerpolicy="no-referrer">
            </div><blockquote>
<p>本文为大家揭示我们优先提出的 Graph RAG 方法，这种结合知识图谱、图数据库作为大模型结合私有知识系统最新的技术栈，作为之前的图上下文学习、text2cypher 文章的第三篇文章。</p>
</blockquote>
<p>本文为大家揭示我们优先提出的 Graph RAG 方法，这种结合知识图谱、图数据库作为大模型结合私有知识系统最新的技术栈，作为之前的图上下文学习、text2cypher 文章的第三篇文章。</p>
<h2 id="graph-rag" class="headerLink">
    <a href="#graph-rag" class="header-mark"></a>1 Graph RAG</h2><p>在<a href="https://siwei.io/en/graph-enabled-llama-index/" target="_blank" rel="noopener noreferrer">第一篇关于上下文学习的博客中</a>我们介绍过， RAG（Retrieval Argumented Generation）这种基于特定任务/问题的文档检索范式中，我们通常先收集必要的上下文，然后利用具有认知能力的机器学习模型进行上下文学习（in-context learning），来合成任务的答案。</p>
<p>借助 LLM 这个只需要“说话”就可以灵活处理复杂问题的感知层，只需要两步，就能搭建一个基于私有知识的智能应用：</p>
<ul>
<li>利用各种搜索方式（比如 Embedding 与向量数据库）从给定的文档中检索相关知识。</li>
<li>利用 LLM 理解并智能地合成答案。</li>
</ul>
<p>而这篇博客中，我们结合最新的探索进展和思考，尝试把 Graph RAG 和其他方法进行比较，说得更透一点。并且，我们决定开始用 Graph RAG 这个叫法来描述它。</p>
<blockquote>
<p>实际上，<a href="https://siwei.io/talks/graph-rag-with-jerry/" target="_blank" rel="noopener noreferrer">Graph RAG</a>，是最先又我<a href="https://www.youtube.com/watch?v=bPoNCkjDmco" target="_blank" rel="noopener noreferrer">和 Jerry Liu 的直播研讨会讨论</a>和<a href="https://twitter.com/wey_gu/status/1673362774930628608" target="_blank" rel="noopener noreferrer">相关的讨论的 Twitter Thread</a>中提到的，差不多的内容我在 <a href="https://www.bilibili.com/video/BV1Pp4y157nt" target="_blank" rel="noopener noreferrer">NebulaGraph 社区直播</a> 中也用中文介绍过。</p>
</blockquote>
<h2 id="在-rag-中知识图谱的价值" class="headerLink">
    <a href="#%e5%9c%a8-rag-%e4%b8%ad%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e7%9a%84%e4%bb%b7%e5%80%bc" class="header-mark"></a>2 在 RAG 中知识图谱的价值</h2><p>这部分内容我们在第一篇文章中阐述过，比如一个查询：“告诉我所有关于苹果和乔布斯的事”，基于乔布斯自传这本书进行问答，而这个问题涉及到的上下文分布在自传这本书的 30 页（分块）的时候，传统的“分割数据，Embedding 再向量搜索”方法在多个文档块里用 top-k 去搜索的方法很难得到这种分散，细粒的完整信息。而且，这种方法还很容易遗漏互相关联的文档块，从而导致信息检索不完整。</p>
<p>除此之外，在之后一次技术会议中，我有幸和 leadscloud.com 的徐旭讨论之后（他们因为有知识图谱的技术背景，也做了和我们类似的探索和尝试！），让我意识到知识图谱可以减少基于嵌入的语义搜索所导致的不准确性。徐旭给出的一个有趣的例子是“保温大棚”与“保温杯”，尽管在语义上两者是存在相关性的，但在大多数场景下，这种通用语义（Embedding）下的相关性常常是我们不希望产生的，进而作为错误的上下文而引入“幻觉”。</p>
<p>这时候，保有领域知识的知识图谱则是非常直接可以缓解、消除这种幻觉的手段。</p>
<h2 id="用-nebulagraph-实现-graph-rag" class="headerLink">
    <a href="#%e7%94%a8-nebulagraph-%e5%ae%9e%e7%8e%b0-graph-rag" class="header-mark"></a>3 用 NebulaGraph 实现 Graph RAG</h2><p>一个简单的 Graph RAG 可以如下去简单实现：</p>
<ol>
<li>使用LLM(或其他)模型从问题中提取关键实体。</li>
<li>根据这些实体检索子图，深入到一定的深度（例如，2）。</li>
<li>利用获得的上下文利用LLM产生答案。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 伪代码</span>

<span class="k">def</span> <span class="nf">_get_key_entities</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="kc">None</span> <span class="p">,</span><span class="n">with_llm</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">_expand_synonyms</span><span class="p">(</span><span class="n">entities</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_retrieve_subgraph_context</span><span class="p">(</span><span class="n">entities</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">nebulagraph_store</span><span class="o">.</span><span class="n">get_relations</span><span class="p">(</span><span class="n">entities</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_synthesize_answer</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">llm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">PROMPT_SYNTHESIZE_AND_REFINE</span><span class="p">,</span> <span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simple_graph_rag</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">nebulagraph_store</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
    <span class="n">entities</span> <span class="o">=</span> <span class="n">_get_key_entities</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
    <span class="n">graph_rag_context</span> <span class="o">=</span> <span class="n">_retrieve_subgraph_context</span><span class="p">(</span><span class="n">entities</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_synthesize_answer</span><span class="p">(</span>
        <span class="n">query_str</span><span class="p">,</span> <span class="n">graph_rag_context</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>然而，有了像 Llama Index 这样方便的 LLM 编排工具，开发者可以专注于 LLM 的编排逻辑和 pipeline 设计，而不用亲自处理很多细节的抽象与实现。</p>
<p>所以，用 Llama Index，我们可以轻松搭建 Graph RAG，甚至整合更复杂的 RAG 逻辑，比如 <a href="https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html" target="_blank" rel="noopener noreferrer">Graph+Vector RAG</a>。</p>
<p><figure><img
        
        loading="lazy"
        src="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870"
        srcset="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870, https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870 1.5x, https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870 2x"
        sizes="auto"
        alt="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870"
        title="https://github.com/siwei-io/talks/assets/1651790/f783b592-7a8f-4eab-bd61-cf0837e83870" ></figure></p>
<p>在 Llama Index 中，我们有两种方法实现 Graph RAG：</p>
<ul>
<li><code>KnowledgeGraphIndex</code> 用来从任何私有数据只是从零构建知识图谱（基于 LLM 或者其他语言模型），然后 4 行代码进行 Graph RAG。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph_store = NebulaGraphStore(
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)
storage_context = StorageContext.from_defaults(graph_store=graph_store)

# Build KG
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    storage_context=storage_context,
    max_triplets_per_chunk=10,
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)

kg_query_engine = kg_index.as_query_engine()
</code></pre></td></tr></table>
</div>
</div><ul>
<li><code>KnowledgeGraphRAGQueryEngine</code> 则可以在任何已经存在的知识图谱上进行 Graph RAG，不过我还没有完成这个 <a href="https://github.com/jerryjliu/llama_index/pull/7204" target="_blank" rel="noopener noreferrer">PR</a>。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph_store = NebulaGraphStore(
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)
storage_context = StorageContext.from_defaults(graph_store=graph_store)

graph_rag_query_engine = KnowledgeGraphRAGQueryEngine(
    storage_context=storage_context,
)
</code></pre></td></tr></table>
</div>
</div><p>最后，我做了<a href="https://www.siwei.io/demos/graph-rag/" target="_blank" rel="noopener noreferrer">一个 streamlit 的 demo</a>来比较 Graph RAG 与 Vector RAG，从中我们可以看到 Graph RAG 并没有取代 Embedding、向量搜索的方法，而是增强了/补充了它的不足。</p>
<iframe width="800" height="857" src="https://github.com/siwei-io/talks/assets/1651790/102d00bc-6146-4856-a81f-f953c7254b29"> </iframe>
<h2 id="text2cypher" class="headerLink">
    <a href="#text2cypher" class="header-mark"></a>4 text2cypher</h2><p>基于图谱的 LLM 的另一种有趣方法是text2cypher。这种方法不依赖于实体的子图检索，而是将任务/问题翻译成一个面向答案的特定图查询，和我们常说的 text2sql 方法本质是一样的。</p>
<h3 id="在-nebulagraph-上进行-text2cypher" class="headerLink">
    <a href="#%e5%9c%a8-nebulagraph-%e4%b8%8a%e8%bf%9b%e8%a1%8c-text2cypher" class="header-mark"></a>4.1 在 NebulaGraph 上进行 text2cypher</h3><p>在之前的文章中我们已经介绍过，得益于 LLM，实现 text2cypher 比传统的 ML 方法更为简单和便宜。</p>
<p>比如，<a href="https://python.langchain.com/docs/use_cases/more/graph/graph_nebula_qa" target="_blank" rel="noopener noreferrer">LangChain: NebulaGraphQAChain</a> 和 <a href="https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html" target="_blank" rel="noopener noreferrer">Llama Index: KnowledgeGraphQueryEngine</a> 让我们 3 行代码就能跑起来 text2cypher。</p>
<h3 id="比较-text2cypher-和-subgraph-rag" class="headerLink">
    <a href="#%e6%af%94%e8%be%83-text2cypher-%e5%92%8c-subgraph-rag" class="header-mark"></a>4.2 比较 text2cypher 和 (Sub)Graph RAG</h3><p>这两种方法主要在其检索机制上有所不同。text2cypher 根据 KG 的 Schema 和给定的任务生成图形模式查询，而SubGraph RAG获取相关的子图以提供上下文。</p>
<p>两者都有其优点，为了大家更直观理解他们的特点，我做了这个 demo 视频：</p>
<p>我们可以看到两者的图查询模式在可视化下是有非常清晰的差异的。</p>
<iframe width="800" height="857" src="https://github.com/siwei-io/talks/assets/1651790/05d01e53-d819-4f43-9bf1-75549f7f2be9"> </iframe>
<h3 id="结合text2cypher的graph-rag" class="headerLink">
    <a href="#%e7%bb%93%e5%90%88text2cypher%e7%9a%84graph-rag" class="header-mark"></a>4.3 结合text2cypher的Graph RAG</h3><p>然而，两者并没有绝对的好与坏，不同场景下，它们各有优劣。</p>
<p>在现实世界中，我们可能并不总是知道哪种方法更有效（好帮助区分应该用哪一种），因此，我倾向于考虑同时利用两者，这样获取的两种检索结果作为上下文，一起来生成最终答案的效果可能是最好的。</p>
<p>具体的实现方法在<a href="https://github.com/jerryjliu/llama_index/pull/7204" target="_blank" rel="noopener noreferrer">这个 PR</a>中已经可以做到了，只需要设置<code>with_text2cypher=True</code>，Graph RAG 就会包含text2cypher 上下文，敬请期待它的合并。</p>
<h2 id="结论" class="headerLink">
    <a href="#%e7%bb%93%e8%ae%ba" class="header-mark"></a>5 结论</h2><p>通过将知识图谱、图存储集成到 LLM 技术栈中，Graph RAG 把 RAG 的上下文学习推向了一个新的高度。它能在 LLM 应用中，通过利用现有（或新建）的知识图谱，提取细粒度、精确调整、领域特定且互联的知识。</p>
<p>请继续关注图谱和LLM领域的更深入的探索和进一步的发展。</p>
<blockquote>
<p>题图 prompt： A vast open book serves as the backdrop, with intricately interwoven nodes and lines forming a Graph on its pages. At the center of this graph, there&rsquo;s a glowing brain symbolizing the Knowledge Graph. Rays of light emanate from the brain, reaching every corner of the graph, mirroring neural connections linking diverse information. On the right side of the illustration, a robotic arm with a pen is swiftly writing, representing the input and output of the AI large language model.</p>
</blockquote>]]></description>
</item><item>
    <title>图谱驱动的大语言模型 Llama Index</title>
    <link>https://siwei.io/graph-enabled-llama-index/</link>
    <pubDate>Thu, 01 Jun 2023 14:52:53 &#43;0800</pubDate><author>
        <name>Wey Gu</name>
    </author><guid>https://siwei.io/graph-enabled-llama-index/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/graph-enabled-llama-index/featured-image.webp" referrerpolicy="no-referrer">
            </div><blockquote>
<p>如何利用图谱构建更好的 In-context Learning 大语言模型应用。</p>
</blockquote>
<p><a href="https://www.siwei.io/en/graph-enabled-llama-index/" target="_blank" rel="noopener noreferrer">English version</a></p>
<blockquote>
<p>注：本文是我最初以英文撰写的，然后麻烦 ChatGPT 帮我翻译成了英文，翻译的 prompt 是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">In this thread, you are a Chinese Tech blogger to help translate my blog in markdown from English into Chinese, the blog style is clear, fun yet professional. I will paste chapters in markdown to you and you will send back the translated and polished version.
</code></pre></td></tr></table>
</div>
</div></blockquote>
<h2 id="llm-应用的范式" class="headerLink">
    <a href="#llm-%e5%ba%94%e7%94%a8%e7%9a%84%e8%8c%83%e5%bc%8f" class="header-mark"></a>1 LLM 应用的范式</h2><p>作为认知智能的一大突破，LLM 已经改变了许多行业，以一种我们没有预料到的方式进行自动化、加速和启用。每天都会看到新的 LLN 应用被创建出来，我们仍然在探索如何利用这种魔力的新方法和用例。</p>
<p>将 LLM 引入流程的最典型模式之一是要求 LLM 根据专有的/特定领域的知识理解事物。目前，我们可以向 LLM 添加两种范式以获取这些知识：微调——fine-tune和<a href="https://en.wikipedia.org/wiki/In-context_learning_%28natural_language_processing%29" target="_blank" rel="noopener noreferrer">上下文学习</a>—— in-context learning。</p>
<p>微调是指对 LLM 模型进行附加训练，以增加额外的知识；而上下文学习是在查询提示中添加一些额外的知识。我们目前观察到，<a href="https://arxiv.org/abs/2305.16938" target="_blank" rel="noopener noreferrer">由于其简单性，上下文学习比微调更受欢迎</a>。</p>
<p>在本博客中，我将分享我们在上下文学习方法方面所做的工作。</p>
<h2 id="llama-index数据与-llm-之间的接口" class="headerLink">
    <a href="#llama-index%e6%95%b0%e6%8d%ae%e4%b8%8e-llm-%e4%b9%8b%e9%97%b4%e7%9a%84%e6%8e%a5%e5%8f%a3" class="header-mark"></a>2 Llama Index：数据与 LLM 之间的接口</h2><h3 id="上下文学习" class="headerLink">
    <a href="#%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0" class="header-mark"></a>2.1 上下文学习</h3><p>上下文学习的基本思想是使用现有的 LLM（未更新）来处理特定知识数据集的特殊任务。</p>
<p>例如，要构建一个可以回答关于某个人的任何问题，甚至扮演一个人的数字化化身的应用程序，我们可以将上下文学习应用于一本自传书籍和 LLM。在实践中，应用程序将使用用户的问题和从书中&quot;搜索&quot;到的一些信息构建提示，然后查询 LLM 来获取答案。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">┌───────┐         ┌─────────────────┐         ┌─────────┐
│       │         │ Docs/Knowledge  │         │         │
│       │         └─────────────────┘         │         │
│ User  │─────────────────────────────────────▶   LLM   │
│       │                                     │         │
│       │                                     │         │
└───────┘                                     └─────────┘
</code></pre></td></tr></table>
</div>
</div><p>在这种搜索方法中，实现从文档/知识（上述示例中的那本书）中获取与特定任务相关信息的最有效方式之一是利用嵌入（Embedding）。</p>
<h3 id="嵌入embedding" class="headerLink">
    <a href="#%e5%b5%8c%e5%85%a5embedding" class="header-mark"></a>2.2 嵌入（Embedding）</h3><p>嵌入通常指的是将现实世界的事物映射到多维空间中的向量的方法。例如，我们可以将图像映射到一个（64 x 64）维度的空间中，如果映射足够好，两个图像之间的距离可以反映它们的相似性。</p>
<p>嵌入的另一个例子是 word2vec 算法，它将每个单词都映射到一个向量中。例如，如果嵌入足够好，我们可以对它们进行加法和减法操作，可能会得到以下结果：</p>
<ul>
<li><code>vec(apple) + vec(pie) =~ vec(&quot;apple apie&quot;)</code></li>
</ul>
<p>或者向量测量值 <code>vec(apple) + vec(pie) - vec(&quot;apple apie&quot;)</code> 趋近于0：</p>
<ul>
<li><code>|vec(apple) + vec(pie) - vec(&quot;apple apie&quot;)| =~ 0</code></li>
</ul>
<p>类似地，&ldquo;pear&rdquo; 应该比 &ldquo;dinosaur&rdquo; 更接近 &ldquo;apple&rdquo;：</p>
<ul>
<li><code>|vec(apple) - vec(pear)| &lt; |vec(apple) - vec(dinosaur)|</code></li>
</ul>
<p>有了这个基础，理论上我们可以搜索与给定问题更相关的书籍片段。基本过程如下：</p>
<ul>
<li>将书籍分割为小片段，为每个片段创建嵌入并存储它们</li>
<li>当有一个问题时，计算问题的嵌入</li>
<li>通过计算距离找到与书籍片段最相似的前 K 个嵌入</li>
<li>使用问题和书籍片段构建提示</li>
<li>使用提示查询 LLM</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">                  ┌────┬────┬────┬────┐                  
                  │ 1  │ 2  │ 3  │ 4  │                  
                  ├────┴────┴────┴────┤                  
                  │  Docs/Knowledge   │                  
┌───────┐         │        ...        │       ┌─────────┐
│       │         ├────┬────┬────┬────┤       │         │
│       │         │ 95 │ 96 │    │    │       │         │
│       │         └────┴────┴────┴────┘       │         │
│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │
│       │                                     │         │
│       │                                     │         │
└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘
    │          ┌──────────────────────────┐        ▲     
    └────────┼▶│  Tell me ....., please   │├───────┘     
               └──────────────────────────┘              
             │ ┌────┐ ┌────┐               │             
               │ 3  │ │ 96 │                             
             │ └────┘ └────┘               │             
              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ 
</code></pre></td></tr></table>
</div>
</div><h3 id="llama-index" class="headerLink">
    <a href="#llama-index" class="header-mark"></a>2.3 Llama Index</h3><p>Llama Index 是一个开源工具包，它能帮助我们以最佳实践去做 in-context learning：</p>
<ul>
<li>它提供了各种数据加载器，以统一格式序列化文档/知识，例如 PDF、维基百科页面、Notion、Twitter 等等，这样我们可以无需自行处理预处理、将数据分割为片段等操作。</li>
<li>它还可以帮助我们创建嵌入（以及其他形式的索引），并以一行代码的方式存储嵌入（在内存中或<a href="https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb" target="_blank" rel="noopener noreferrer">向量数据库</a>中）。</li>
<li>它内置了提示和其他工程实现，因此我们无需从头开始创建和研究，例如，<a href="https://twitter.com/jerryjliu0/status/1663213212932902913" target="_blank" rel="noopener noreferrer">用4行代码在现有数据上创建一个聊天机器人</a>。</li>
</ul>
<h2 id="文档分割和嵌入的问题" class="headerLink">
    <a href="#%e6%96%87%e6%a1%a3%e5%88%86%e5%89%b2%e5%92%8c%e5%b5%8c%e5%85%a5%e7%9a%84%e9%97%ae%e9%a2%98" class="header-mark"></a>3 文档分割和嵌入的问题</h2><p>嵌入和向量搜索在许多情况下效果良好，但在某些情况下仍存在挑战，其中之一是可能丢失全局上下文/跨节点上下文。</p>
<p>想象一下，当查询&quot;请告诉我关于作者和 foo 的事情&quot;，在这本书中，假设编号为 1、3、6、19~25、30~44 和 96~99 的分段都涉及到 foo 这个主题。则在这种情况下，简单地搜索与书籍片段相关的前 k 个嵌入可能效果不尽人意，因为这时候只考虑与之最相关的几个片段（比如 k = 3），从而丢失了许多上下文信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">┌────┬────┬────┬────┐
│ 1  │ 2  │ 3  │ 4  │
├────┴────┴────┴────┤
│  Docs/Knowledge   │
│        ...        │
├────┬────┬────┬────┤
│ 95 │ 96 │    │    │
└────┴────┴────┴────┘
</code></pre></td></tr></table>
</div>
</div><p>而解决、缓解这个问题的方法，在 Llama Index 工具的语境下，就是创建<a href="https://gpt-index.readthedocs.io/en/latest/how_to/index_structs/composability.html" target="_blank" rel="noopener noreferrer">组合索引</a>和<a href="https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html" target="_blank" rel="noopener noreferrer">综合索引</a>。</p>
<p>其中，向量存储（VectorStore）只是其中的一部分。除此之外，我们可以定义一个摘要索引和/或树形索引等，以<a href="https://gpt-index.readthedocs.io/en/latest/guides/tutorials/unified_query.html" target="_blank" rel="noopener noreferrer">将不同类型的问题路由到不同的索引</a>，从而避免在需要全局上下文时丧失它。</p>
<p>然而，借助知识图谱，我们可以采取更有意思的方法：</p>
<h2 id="知识图谱" class="headerLink">
    <a href="#%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1" class="header-mark"></a>4 知识图谱</h2><p>知识图谱这个术语最初由<a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/" target="_blank" rel="noopener noreferrer">谷歌在2012年5月提出</a>，作为其增强搜索结果和向用户提供更多上下文信息的努力的一部分。知识图谱旨在理解实体之间的关系，并直接提供查询的答案，而不仅仅返回相关网页的列表。</p>
<p>知识图谱是一种以图形格式组织和连接信息的方式，其中节点表示实体，边表示实体之间的关系。图形结构允许高效地存储、检索和分析数据。</p>
<p>它的结构如下图所示：</p>
<iframe src="harry_potter_graph.html" style="height:500px;width:800px" title="Graph"></iframe>
<p>那么知识图谱到底能怎么帮到我们呢？</p>
<h2 id="嵌入和知识图谱的结合" class="headerLink">
    <a href="#%e5%b5%8c%e5%85%a5%e5%92%8c%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e7%9a%84%e7%bb%93%e5%90%88" class="header-mark"></a>5 嵌入和知识图谱的结合</h2><p>这里的基本思想是，作为信息的精炼格式，知识图谱可以以比我们对原始数据/文档进行的分割更小的粒度进行查询/搜索。因此，通过不替换大块的数据，而是将两者结合起来，我们可以更好地搜索需要全局/跨节点上下文的查询。</p>
<p>请看下面的图示，假设问题是关于 <code>x</code> 的，所有数据片段中有20个与它高度相关。现在，除了获取主要上下文的前3个文档片段（比如编号为 1、2 和 96 的文档片段），我们还从知识图谱中对 <code>x</code> 进行两次跳转查询，那么完整的上下文将包括：</p>
<ul>
<li>问题：&ldquo;Tell me things about the author and x&rdquo;</li>
<li>来自文档片段编号 1、2 和 96 的原始文档，在 Llama Index 中，它们被称为节点 1、节点 2 和节点 96。</li>
<li>包含 &ldquo;x&rdquo; 的知识图谱中的 10 个三元组，通过对 <code>x</code> 进行两层深度的图遍历得到：
<ul>
<li>x -&gt; y（来自节点 1）</li>
<li>x -&gt; a（来自节点 2）</li>
<li>x -&gt; m（来自<strong>节点 4</strong>）</li>
<li>x &lt;- b-&gt; c（来自<strong>节点 95</strong>）</li>
<li>x -&gt; d（来自节点 96）</li>
<li>n -&gt; x（来自<strong>节点 98</strong>）</li>
<li>x &lt;- z &lt;- i（来自<strong>节点 1 和节点 3</strong>）</li>
<li>x &lt;- z &lt;- b（来自<strong>节点 1 和节点 95</strong>）</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐
│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │
│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │
│ `▲&#39;       `─&#39;    │  `─&#39;       `─&#39;   │            `─&#39;   │  `─&#39;       `─&#39;   │
│  │     1         │        2         │        3    │    │        4         │
│ .─.              │                  │            .▼.   │                  │
│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │
│ `◀────┐          │                  │            `─&#39;  ││                  │
├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤
│       │                      Docs/Knowledge           │                   │
│       │                            ...                │                   │
│       │                                               │                   │
├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤
│  .─.  └──────.   │  .─.             │                 ││  .─.             │
│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │
│  `─&#39;       `─&#39;   │  `─&#39;             │                  │  `─&#39;             │
│        95   │    │   │    96        │                  │   │    98        │
│            .▼.   │  .▼.             │                  │   ▼              │
│           ( c )  │ ( d )            │                  │  .─.             │
│            `─&#39;   │  `─&#39;             │                  │ ( x )            │
└──────────────────┴──────────────────┴──────────────────┴──`─&#39;─────────────┘
</code></pre></td></tr></table>
</div>
</div><p>显然，那些（可能很宝贵的）涉及到主题 <code>x</code> 的精炼信息来自于其他节点以及跨节点的信息，都因为我们引入知识图谱的步骤，而能够被包含在 prompt 中，用于进行上下文学习，从而克服了前边提到的问题。</p>
<h2 id="llama-index-中的知识图谱进展" class="headerLink">
    <a href="#llama-index-%e4%b8%ad%e7%9a%84%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e8%bf%9b%e5%b1%95" class="header-mark"></a>6 Llama Index 中的知识图谱进展</h2><p>最初，<a href="https://github.com/jerryjliu/llama_index/pull/433" target="_blank" rel="noopener noreferrer">William F.H.</a>将知识图谱的抽象概念引入了 Llama Index，其中知识图谱中的三元组与关键词相关联，并存储在内存中的文档中，随后<a href="https://github.com/jerryjliu/llama_index/pull/487" target="_blank" rel="noopener noreferrer">Logan Markewich</a>还增加了每个三元组的嵌入。</p>
<p>最近的几周中，我一直在与社区合作，致力于<a href="https://github.com/jerryjliu/llama_index/pull/2581" target="_blank" rel="noopener noreferrer">将 &ldquo;GraphStore&rdquo; 存储上下文引入 Llama Index</a>，从而引入了知识图谱的外部存储。首个实现是使用我自从 2021 年以来一直在开发的开源分布式图数据库 NebulaGraph。</p>
<p>在实现过程中，还引入了遍历图的多个跳数选项以及在前 k 个节点中收集更多关键实体的选项（用于在知识图谱中搜索以获得更多全局上下文），我们仍在对这些变更进行完善。</p>
<p>引入 GraphStore 后，还可以从现有的知识图谱中进行上下文学习，并与其他索引结合使用，这也非常有前景，因为知识图谱被认为具有比其他结构化数据更高的信息密度。</p>
<p>在接下来的几周里，我将在本博客中更新有关 Llama Index 中的知识图谱相关工作的内容，然后在 <a href="https://github.com/jerryjliu/llama_index/pull/2581" target="_blank" rel="noopener noreferrer">PR</a> 合并后，分享端到端的演示项目和教程。请继续关注！</p>]]></description>
</item></channel>
</rss>
