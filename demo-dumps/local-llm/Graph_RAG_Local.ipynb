{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10785f8-54e4-4323-8d6c-244da27d5ad0",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "### 1.Start ChatGLM2-6B as an API service\n",
    "\n",
    "Referring to https://python.langchain.com/docs/integrations/llms/chatglm\n",
    "\n",
    "Assuming it's running on `127.0.0.1:8000`\n",
    "\n",
    "### 2.Prepare Embedding Model\n",
    "\n",
    "#### 2.1 Run mode\n",
    "- Option 0: run remotely\n",
    "  - Referring to https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/self-hosted\n",
    "- Option 1: run locally\n",
    "  - We'll go in this way here!\n",
    "\n",
    "#### 2.2 Embedding Model\n",
    "\n",
    "Let's use [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) as the embedding model.\n",
    "\n",
    "### 3. Prepare NebulaGraph Cluster\n",
    "\n",
    "Install with oneliner:\n",
    "\n",
    "```bash\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "```\n",
    "\n",
    "Install the required packages and load nGQL Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877b3d27-9186-492d-aca1-7664890a0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Pool Created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinese_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo_basketballplayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demo_football_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demo_shareholding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guardians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>operator_biz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>operator_biz_cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>science_2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name\n",
       "0             chinese_kg\n",
       "1  demo_basketballplayer\n",
       "2     demo_football_2022\n",
       "3      demo_shareholding\n",
       "4              guardians\n",
       "5           operator_biz\n",
       "6        operator_biz_cn\n",
       "7           science_2023\n",
       "8                   yelp"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install sentence_transformers langchain llama-index ipython-ngql nebula3-python==3.4.0\n",
    "\n",
    "# assume NebulaGraph is running locally from 127.0.0.1:9669\n",
    "\n",
    "%load_ext ngql\n",
    "%ngql --address graphd --port 9669 --user root --password nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d6d52-412b-47d5-93af-57b192af04b9",
   "metadata": {},
   "source": [
    "## title <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9022fb-6915-428e-8db8-33f2f7de3a34",
   "metadata": {},
   "source": [
    "## Get LLM and Embedding Ready\n",
    "\n",
    "### 1. Local LLM, ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d718ac-8b5f-4246-8ef7-96c0d527a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = \"http://127.0.0.1:8000\" # LLM API\n",
    "embedding_model = \"shibing624/text2vec-base-chinese\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) # logging.DEBUG for more verbose output\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "# LLM\n",
    "from langchain.llms import ChatGLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "llm = ChatGLM(\n",
    "    endpoint_url=endpoint_url,\n",
    "    max_token=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=1,\n",
    "    model_kwargs={\n",
    "        \"sample_model_args\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "llm.with_history = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdc3aa-0fb3-4427-ab92-58ec5316a6b5",
   "metadata": {},
   "source": [
    "### 2. Local Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec801a71-4df9-4c8f-8761-01b849f77630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: shibing624/text2vec-base-chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp5dvs9kbx\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp5dvs9kbx/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# Embedding option 0 run with runhouse\n",
    "\n",
    "# from langchain.embeddings import SelfHostedHuggingFaceEmbeddings\n",
    "# embedding_llm = SelfHostedHuggingFaceEmbeddings(model_id=embedding_model)\n",
    "\n",
    "# Embedding option 1 run locally\n",
    "\n",
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(\n",
    "      model_name=embedding_model,\n",
    "      model_kwargs={'device': EMBEDDING_DEVICE},\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f8f1b-f937-47c3-85f8-5e289b2b7466",
   "metadata": {},
   "source": [
    "### 3. LlamaIndex with LLM and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c417c4-b4e0-4473-bbdd-0d71106b8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Index ServiceContext\n",
    "\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# Set global service context\n",
    "\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c52918-b5a8-45ce-bd7c-30021757c38e",
   "metadata": {},
   "source": [
    "## Indexing for both KG Index and Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be913db5-c5de-4614-8e87-68f9c29030b0",
   "metadata": {},
   "source": [
    "### 1. KG Build\n",
    "\n",
    "#### 1.1 Graph Space Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2110028d-2d34-474b-bcc9-8c8709b52828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS chinese_kg(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e3b9a-0197-46bd-a40f-5724f695f26f",
   "metadata": {},
   "source": [
    "#### 1.2 Graph Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860e08b5-95a7-494e-bbc3-98a4ab0b60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql \n",
    "USE chinese_kg;\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199fafb-6e2e-4bb3-91d7-2cb799a0c4a7",
   "metadata": {},
   "source": [
    "Let's create an Index for `entity.name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e9256e-d87e-4279-8059-28c8972568d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe64b8-1ac4-49bc-b52b-d0f447d65be5",
   "metadata": {},
   "source": [
    "#### 1.3 Llama Index GraphStore and Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc15cf3e-3888-4a24-aa4e-60e3efa8c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\"\n",
    "os.environ['NEBULA_ADDRESS'] = \"graphd:9669\"\n",
    "\n",
    "space_name = \"chinese_kg\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308c7f4-c3dd-4c7f-89b8-58d779e0e53e",
   "metadata": {},
   "source": [
    "#### 1.4 Download Data to be indexed\n",
    "\n",
    "Let's download data from [HowToCook](https://github.com/Anduin2017/HowToCook)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded7e5c7-719b-46a4-acef-3d8a93d46e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    # Download the file\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_github_folder_files(user, repo, path, branch='master', extension='.md'):\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{path}?ref={branch}\"\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "    files = response.json()\n",
    "    _documents = []\n",
    "    # Iterate over each file in the repository folder\n",
    "    for file in files:\n",
    "        if file['type'] == 'file' and file['name'].endswith(extension):\n",
    "            # Create local directories if necessary\n",
    "            local_path = os.path.join('downloaded_files', file['path'])\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            # Download  the file\n",
    "            download_file(file['download_url'], local_path)\n",
    "            print(f'Downloaded {local_path}/{file[\"name\"]}')\n",
    "        elif file['type'] == 'dir':\n",
    "            # Recursively download files in the subdirectory\n",
    "            download_github_folder_files(user, repo, file['path'], branch, extension)\n",
    "\n",
    "# Replace with your URL\n",
    "# https://github.com/Anduin2017/HowToCook\n",
    "user = 'Anduin2017'\n",
    "repo = 'HowToCook'\n",
    "path = ''\n",
    "branch = 'master'\n",
    "#download_github_folder_files(user, repo, path, branch=branch, extension='.md')\n",
    "\n",
    "# Eval variables in docs, only for NebulaGraph Docs\n",
    "# !find './downloaded_files' -type f -exec sed -i 's/{{nebula.name}}/NebulaGraph/g' {} +\n",
    "\n",
    "# rename files into txt\n",
    "!find ./downloaded_files -type f -name \"*.md\" -exec bash -c 'mv \"$0\" \"${0%.md}.txt\"' {} \\;\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./downloaded_files\", recursive=True, exclude_hidden=True\n",
    ")\n",
    "documents = loader.load_data()\n",
    "\n",
    "# Now you have a list of documents loaded from all the markdown files in the specified GitHub folder\n",
    "print(f'Loaded {len(documents)} documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bac72-6906-4788-ab35-c1a9a277e166",
   "metadata": {},
   "source": [
    "> Let's check some of the Data Chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55120538-0146-4d0b-89f4-5f98e7ca9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 白灼虾的做法\n",
      "\n",
      "白灼虾非常适合程序员在沿海地区做，类似于清蒸鱼：简单容错、有营养、有满足感，甚至很好看。\n",
      "\n",
      "## 必备原料和工具\n",
      "\n",
      "- 活虾\n",
      "- 洋葱\n",
      "- 姜\n",
      "- 蒜\n",
      "- 葱\n",
      "- 食用油\n",
      "- 酱油\n",
      "- 料酒\n",
      "- 芝麻\n",
      "- 蚝油\n",
      "- 香醋\n",
      "\n",
      "## 计算\n",
      "\n",
      "每次制作前需要确定计划做几份。一份正好够 1 个人食用\n",
      "\n",
      "总量：\n",
      "\n",
      "- 虾 250g * 份数（建议 1-2 人份）\n",
      "- 葱 一根\n",
      "- 姜 一块\n",
      "- 洋葱 一头\n",
      "- 蒜 5-8 瓣\n",
      "- 食用油 10-15ml\n",
      "- 料酒 20 ml\n",
      "- 酱油 10-15ml\n",
      "- 芝麻 一把\n",
      "- 香醋 10 ml\n",
      "- 蚝油 10 ml\n",
      "\n",
      "## 操作\n",
      "\n",
      "- 洋葱切小块，姜切片，平铺平底锅。\n",
      "- 活虾冲洗一下（去除虾线、剪刀减掉虾腿虾须子都是可选操作），控水，铺在平底锅的洋葱、姜片之上。\n",
      "- 锅内倒入料酒，盖上锅盖，中火 1 分钟，小火 5 分钟，关火 5 分钟。\n",
      "- 和上一步并行操作，制作蘸料：\n",
      "  - 葱切成葱花、蒜切碎、倒入酱油、芝麻、香醋，搅拌之。\n",
      "  - 油烧热，淋入蘸料。\n",
      "- 虾出锅，用干净的盘子装好。\n",
      "\n",
      "![白灼虾](./白灼虾.webp)\n",
      "\n",
      "## 附加内容\n",
      "\n",
      "- 技术细节：\n",
      "  - 开始不能大火、防止糊底。\n",
      "  - 如果锅盖有通气口、时间要相应调节一下（考虑增加 30 秒中火）。\n",
      "  - 蘸料其实也是可选的、也可以是纯的醋，大自然馈赠的鲜虾在没有水带走冲淡鲜甜的情况下口感味道都非常棒的。\n",
      "\n",
      "如果您遵循本指南的制作流程而发现有问题或可以改进的流程，请提出 Issue 或 Pull request 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(documents[14].text) # It's contributed by me! via https://github.com/Anduin2017/HowToCook/pull/219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e9a169-f274-4a4c-8c9d-d9ec36b7163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdownloaded_files\u001b[00m\n",
      "|-- CODE_OF_CONDUCT.txt\n",
      "|-- CONTRIBUTING.txt\n",
      "|-- README.txt\n",
      "`-- \u001b[01;34mdishes\u001b[00m\n",
      "    |-- \u001b[01;34maquatic\u001b[00m\n",
      "    |-- \u001b[01;34mbreakfast\u001b[00m\n",
      "    |-- \u001b[01;34mcondiment\u001b[00m\n",
      "    |-- \u001b[01;34mdessert\u001b[00m\n",
      "    |-- \u001b[01;34mdrink\u001b[00m\n",
      "    `-- \u001b[01;34mmeat_dish\u001b[00m\n",
      "\n",
      "7 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree downloaded_files -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0da59-1361-4619-aec9-2d2247c2b0b7",
   "metadata": {},
   "source": [
    "#### 1.5 Triplets extraction and Knowledge Graph build\n",
    "\n",
    "First, we define specific Prompt Template to enable ChatGLM2 to extract knowledge, the default one will work only for OpenAI Models.\n",
    "\n",
    "> TBD, need to bring BERT/NER with local model here to improve this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b11a59c-4888-4d45-b65c-e9bdf7c2b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "根据给定的文本，通过一步一步总结，理解，最终输出抽取至多 {max_knowledge_triplets} 行 (主语, 谓语, 宾语) 格式的三元组用作构建问答知识图谱，忽略文本中的停止符号。\n",
    "<注意> 保证三元组只有主谓宾三部分，不要把罗列的知识放在一行中，而应该拆为多行知识。\n",
    "<注意> 如果文本是大段代码或者命令行、罗列的步骤，先总结出知识再抽取有意义的知识。\n",
    "<注意> 返回格式为每一行用括号包裹、逗号隔开，没有序号。\n",
    "<注意> 仔细检查，只抽取有意义的知识，没有的时候返回空。\n",
    "<注意> 谓语要翻译成中文。\n",
    "<注意> 要注意三元组主语的选择，要明确，准确。\n",
    "\n",
    "下面是几个例子:\n",
    "---------------------\n",
    "文本: 狗是人类最早驯化的动物，大约在一万四千年前，人类就开始驯化狼，最终演化成了我们现在看到的各种犬种。狗属于哺乳动物，其视觉、听觉和嗅觉都非常灵敏。它们是社会性的动物，通常在群体中生活。狗的寿命一般在10到15年之间，但也有一些犬种可以活到20年以上。它们的食物主要是肉类，但是也能吃一些蔬菜和谷物。\n",
    "主谓宾三元组:\n",
    "(狗, 是, 人类最早驯化的动物)\n",
    "(狗, 属于, 哺乳动物)\n",
    "(狗, 寿命为, 10到15年之间)\n",
    "----\n",
    "# 本例中只抽取不超过 5 行三元组，且将列表信息综合处理。\n",
    "文本: Docker 的安装\n",
    "Docker 是一个开源的商业产品，有两个版本：社区版（Community Edition，缩写为 CE）和企业版（Enterprise Edition，缩写为 EE）。企业版包含了一些收费服务，个人开发者一般用不到。下面的介绍都针对社区版。\n",
    "\n",
    "Docker CE 的安装请参考官方文档，并且支持：\n",
    "\n",
    "- Mac\n",
    "- Windows\n",
    "- Ubuntu\n",
    "- Debian\n",
    "- CentOS\n",
    "- Fedora\n",
    "\n",
    "对于其他 Linux 发行版\n",
    "安装完成后，运行下面的命令，验证是否安装成功。\n",
    "\n",
    "\n",
    "$ docker version\n",
    "# 或者\n",
    "$ docker info\n",
    "Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入 Docker 用户组（官方文档）。\n",
    "\n",
    "\n",
    "$ sudo usermod -aG docker $USER\n",
    "Docker 是服务器----客户端架构。命令行运行docker命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（官方文档）。\n",
    "\n",
    "\n",
    "# service 命令的用法\n",
    "$ sudo service docker start\n",
    "\n",
    "# systemctl 命令的用法\n",
    "$ sudo systemctl start docker\n",
    "主谓宾三元组:\n",
    "(Docker, 是, 开源的商业产品)\n",
    "(Docker CE, 支持, Mac、Windows 和 Linux)\n",
    "(Docker, 包含, 一些收费服务)\n",
    "(Docker, 需要, 用户具有sudo权限)\n",
    "(Docker, 是, 服务器-客户端架构)\n",
    "---------------------\n",
    "\n",
    "下面请根据之前的要求和例子，开始知识抽取任务！\n",
    "---------------------\n",
    "文本: {text}\n",
    "\n",
    "\n",
    "主谓宾三元组:\n",
    "\"\"\"\n",
    "\n",
    "KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    KG_TRIPLET_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")\n",
    "\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n",
    "    \"根据下列要求完成任务，不要忘记 <注意> 的要求\\n\"\n",
    "    \"根据给定的文本，抽取不超过 {max_keywords} 个实体名词关键词，\"\n",
    "    \"这些关键词是作为适合在知识图谱中进行查询的实体。忽略文本中的停止符号。\\n\"\n",
    "    \"<注意> 如果有英文，给出多种合理的大小写情况的关键词，比如关键词 Baseball park，\"\n",
    "    \"可能要给出 'KEYWORDS: Baseball park, Baseball Park'\\n\"\n",
    "    \"<注意> 不要超出 {max_keywords} 个关键词\\n\"\n",
    "    \"<注意> 只返回要求的 KEYWORDS: 开头，然后英文逗号隔开的格式，不带序号、换行。\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"现在返回其中可能得关键词，以这样的格式 --> 'KEYWORDS: keyword1, keyword2, keyword3'\\n\"\n",
    ")\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE = Prompt(\n",
    "    QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n",
    "    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f7daa4-7c27-4193-a33b-1bed3b9d7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "# cleanup NebulaGraph\n",
    "# %ngql clear space chinese_kg\n",
    "# or\n",
    "# graph_store.query(\"clear space chinese_kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c889f-5d43-49a7-a67e-378ad9e560cb",
   "metadata": {},
   "source": [
    "> This will be run only for the first time, afterwards, we will load from persist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf489ef8-d4d9-4399-9c0e-dc6eeb11cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 556\n",
      "-rw-rw-r-- 1 w w 458456 Jul 28 13:03 docstore.json\n",
      "-rw-rw-r-- 1 w w 102959 Jul 28 13:03 index_store.json\n",
      "-rw-rw-r-- 1 w w     51 Jul 28 13:03 vector_store.json\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "##clear graphdatabase\n",
    "##%ngql clear space chinese_kg\n",
    "\n",
    "graph_store.query(\"SHOW HOSTS\")\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    query_keyword_extract_template=QUERY_KEYWORD_EXTRACT_TEMPLATE,\n",
    "    max_knowledge_sequence=15,\n",
    ")\n",
    "\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "!ls -l storage_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c4ffd-4127-4695-b680-46e3b9ae8790",
   "metadata": {},
   "source": [
    "> This could be done after re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d5d1f9-9552-40c3-9e9e-78314241fd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context_graph,\n",
    "    max_triplets_per_chunk=5,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    query_keyword_extract_template=QUERY_KEYWORD_EXTRACT_TEMPLATE,\n",
    "    max_knowledge_sequence=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516620b-72ef-440a-a50f-4c88f088bcf5",
   "metadata": {},
   "source": [
    "### 2. Vector Embedding and Indexing\n",
    "\n",
    "We will not leverage external VectorDB in this demo, but it's easy to switch to any Vector DB with Llama Index.\n",
    "\n",
    "We'll store and search embedding in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "159f890f-56c3-43f2-803e-f8655f0a0ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.98it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.73it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.68it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.49it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.02it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.61it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.45it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.09it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.29it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.65it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.53it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.20it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.70it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 54.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbf447-09fe-4459-b168-3f5faf2c3d8e",
   "metadata": {},
   "source": [
    "## QA on the knowledge base \n",
    "\n",
    "### 1. Create Graph and Vector RAG query engines\n",
    "\n",
    "- vector_query_engine, VectorDB RAG\n",
    "- kg_keyword_query_engine, NebulaGraph RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2816d6a5-654a-4807-a15a-c1fa7a2360f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    max_keywords_per_query=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86088f7-7acf-47be-bdd8-eb1b923aef8e",
   "metadata": {},
   "source": [
    "### 2. Query on Vector RAG vs Graph RAG\n",
    "\n",
    "首先是传统的 **Vector Search RAG** 的结果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "254b01d1-2f0b-4310-9c00-c0623fd1a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>提拉米苏的制作步骤如下：\n",
       "\n",
       "1. 准备材料和工具：马斯卡彭芝士，手指饼干，放凉浓缩咖啡，无菌鸡蛋，白砂糖，可可粉，朗姆酒，一个装成品的容器。\n",
       "\n",
       "2. 计算：将马斯卡彭芝士450克、手指饼干1包、放凉浓缩咖啡350ml、无菌鸡蛋4个、白砂糖50g、可可粉10g、朗姆酒35ml准备好。\n",
       "\n",
       "3. 操作：\n",
       "\n",
       "- 分离蛋黄蛋清。\n",
       "- 称量40g白砂糖，加入蛋白碗中，加10g白砂糖湿性打发。\n",
       "- 称量60g白砂糖，加入蛋黄中，分三次加入马斯卡彭芝士，搅拌至均匀。\n",
       "- 蛋黄中加入朗姆酒，搅拌均匀。\n",
       "- 将打发好的蛋白分三次加入蛋黄芝士液中。\n",
       "- 手指饼干两面浸湿咖啡液，平铺入容器。\n",
       "- 两层芝士液两层饼干交替放入容器（这一步按照大家意愿及容器高度酌情处理）。\n",
       "- 放入冰箱冷藏四个小时（心急的小伙伴可以提早拿出来）。\n",
       "- 取出后在表面筛上可可粉，即可享用啦。\n",
       "\n",
       "4. 百香果橙子特调：\n",
       "\n",
       "- 茉莉绿茶版本：将380毫升开水倒入茉莉绿茶茶叶中，加入橙子1个（约200克，拳头大小），称量3~6克茉莉绿茶茶叶，搅拌均匀。\n",
       "- 苏打气泡水版本：将380毫升苏打气泡水中加入橙子1个（约200克，拳头大小），加入冰块160克以上，搅拌均匀。\n",
       "\n",
       "根据以上步骤，您就可以尝试制作出美味的提拉米苏和百香果橙子特调。祝您成功！</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"提拉米苏怎么做？\")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42579ffb-14e8-4662-a85b-cecb0cadfe04",
   "metadata": {},
   "source": [
    "然后是 **Graph RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a473735d-53cb-4ae7-8b3f-206046be3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: 提拉米苏怎么做\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['怎么做', '提拉米苏']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge sequence in 2-depth in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]` that may be related to the task.\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 包含, 12克\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 需要, 白砂糖\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 属于, 可密封容器\n",
      "提拉米苏 提拉米苏, 需要, 可可粉, 需要, 可可粉\n",
      "提拉米苏 提拉米苏, 是, 甜点\n",
      "提拉米苏 提拉米苏, 是, 意大利传统甜品\n",
      "提拉米苏 提拉米苏, 操作, 盛有蛋白的碗中加白砂糖湿性打发\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 加入, 白砂糖\n",
      "提拉米苏 提拉米苏, 需要, 无菌鸡蛋\n",
      "提拉米苏 提拉米苏, 是, 甜点, 是, 甜点\n",
      "提拉米苏 提拉米苏, 需要, 放凉浓缩咖啡\n",
      "提拉米苏 提拉米苏, 需要, 朗姆酒\n",
      "提拉米苏 提拉米苏, 需要, 手指饼干\n",
      "提拉米苏 提拉米苏, 需要, 可可粉, 包含, 可可粉\n",
      "提拉米苏 提拉米苏, 操作, 分离蛋黄蛋清\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖\n",
      "提拉米苏 提拉米苏, 需要, 马斯卡彭芝士\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 和, 白砂糖\n",
      "提拉米苏 提拉米苏, 操作, 两层芝士液两层饼干交替放入容器\n",
      "提拉米苏 提拉米苏, 需要, 白砂糖, 用于, 水\n",
      "提拉米苏 提拉米苏, 操作, 将打发好的蛋白分三次加入蛋黄芝士液中\n",
      "提拉米苏 提拉米苏, 需要, 可可粉\n",
      "提拉米苏 提拉米苏, 操作, 放入冰箱冷藏四个小时\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>提拉米苏是一道来自意大利的传统甜点,制作步骤比较复杂,但是大致步骤如下:\n",
       "\n",
       "材料:\n",
       "\n",
       "- 12克白砂糖\n",
       "- 可可粉\n",
       "- 需要放凉的浓缩咖啡\n",
       "- 朗姆酒\n",
       "- 无菌鸡蛋\n",
       "\n",
       "步骤:\n",
       "\n",
       "1. 把可拉米苏饼干放入一个大碗中,加入12克白砂糖,用打蛋器或者勺子把白砂糖压碎,让糖粉充分湿润,然后加入200毫升淡色奶油,继续用打蛋器或者勺子搅打,直到糖和奶油充分混合,碗中的混合物变得光滑。\n",
       "\n",
       "2. 把融化的白巧克力加入碗中的混合物中,继续用打蛋器或者勺子搅拌,直到白巧克力完全融化并和奶油混合均匀。\n",
       "\n",
       "3. 把软化的鸡蛋黄加入碗中的混合物中,继续用打蛋器或者勺子搅拌,直到鸡蛋黄完全融入混合物中,形成一个均匀的混合物。\n",
       "\n",
       "4. 把融化的马斯卡彭芝士加入碗中的混合物中,继续用打蛋器或者勺子搅拌,直到马斯卡彭芝士完全融化并和混合物混合均匀。\n",
       "\n",
       "5. 把咖啡倒入杯子中,放凉四个小时。\n",
       "\n",
       "6. 拿出提拉米苏,将其取出,切成薄片,即可享用。\n",
       "\n",
       "请注意,这只是一个大致的步骤,实际制作过程中还需要根据个人口味和习惯进行调整。</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph = kg_keyword_query_engine.query(\"提拉米苏怎么做\")\n",
    "display(Markdown(f\"<b>{response_graph}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bf56e-da3e-4990-8db5-3509d5ece2f3",
   "metadata": {},
   "source": [
    "🔔 可以注意到，在 Retrieval 阶段，Graph RAG 搜集到的知识如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49071132-8019-4bf8-ae51-bf1723f59cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'提拉米苏': ['提拉米苏, 需要, 白砂糖, 包含, 12克',\n",
       "  '提拉米苏, 需要, 白砂糖, 需要, 白砂糖',\n",
       "  '提拉米苏, 需要, 白砂糖, 属于, 可密封容器',\n",
       "  '提拉米苏, 需要, 可可粉, 需要, 可可粉',\n",
       "  '提拉米苏, 是, 甜点',\n",
       "  '提拉米苏, 是, 意大利传统甜品',\n",
       "  '提拉米苏, 操作, 盛有蛋白的碗中加白砂糖湿性打发',\n",
       "  '提拉米苏, 需要, 白砂糖, 加入, 白砂糖',\n",
       "  '提拉米苏, 需要, 无菌鸡蛋',\n",
       "  '提拉米苏, 是, 甜点, 是, 甜点',\n",
       "  '提拉米苏, 需要, 放凉浓缩咖啡',\n",
       "  '提拉米苏, 需要, 朗姆酒',\n",
       "  '提拉米苏, 需要, 手指饼干',\n",
       "  '提拉米苏, 需要, 可可粉, 包含, 可可粉',\n",
       "  '提拉米苏, 操作, 分离蛋黄蛋清',\n",
       "  '提拉米苏, 需要, 白砂糖',\n",
       "  '提拉米苏, 需要, 马斯卡彭芝士',\n",
       "  '提拉米苏, 需要, 白砂糖, 和, 白砂糖',\n",
       "  '提拉米苏, 操作, 两层芝士液两层饼干交替放入容器',\n",
       "  '提拉米苏, 需要, 白砂糖, 用于, 水',\n",
       "  '提拉米苏, 操作, 将打发好的蛋白分三次加入蛋黄芝士液中',\n",
       "  '提拉米苏, 需要, 可可粉',\n",
       "  '提拉米苏, 操作, 放入冰箱冷藏四个小时']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(response_graph.metadata.values())[0]['kg_rel_map']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53445e1-c460-465e-82e6-6d47ec9ed80f",
   "metadata": {},
   "source": [
    "### 3. Hallucination results in Vector RAG\n",
    "\n",
    "Let's ask something that it shouldn't know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b17c6203-75bf-4e59-a747-7bd22d73c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>翡翠白玉汤是一道色香味俱佳的菜肴，通常使用新鲜的翡翠和白玉来制作。以下是翡翠白玉汤的简要做法：\n",
       "\n",
       "所需材料：\n",
       "1. 翡翠：200 克\n",
       "2. 白玉：200 克\n",
       "3. 鸡胸肉：250 克\n",
       "4. 姜片：适量\n",
       "5. 葱段：适量\n",
       "6. 料酒：适量\n",
       "7. 盐：适量\n",
       "8. 清水：适量\n",
       "\n",
       "步骤：\n",
       "\n",
       "1.将翡翠、白玉切成薄片，鸡胸肉切成小丁。\n",
       "2.热锅凉油，加入姜片、葱段，煸炒出香味后加入鸡胸肉丁煸炒至变色。\n",
       "3.加入适量的料酒，煮至香味挥发，然后加入适量的清水，放入翡翠、白玉片，加入适量的盐，煮至食材熟透，捞出备用。\n",
       "4.最后，将翡翠、白玉片和煮好的鸡胸肉倒入另一个锅中，加入适量的鸡清汤，煮至汤汁浓稠即可。\n",
       "5.将调好的翡翠白玉汤盛入碗中，撒上一些香菜或者葱花作为装饰即可。\n",
       "\n",
       "翡翠白玉汤的做法简单，口感鲜美，适合搭配米饭或者直接作为小菜。</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"翡翠白玉汤怎么做？\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2201da-aea0-458d-a67c-4f68f14a2f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>黯然销魂饭是一道著名的粤菜，主要材料包括大鲍鱼、白切鸡、瑶柱、 花菇、草菇、海鲜等。制作过程需要严格控制火候和时间，以保证米香浓郁、口感细腻。下面是黯然销魂饭的制作步骤：\n",
       "\n",
       "材料：\n",
       "\n",
       "- 大鲍鱼 200 克\n",
       "- 白切鸡 200 克\n",
       "- 瑶柱 200 克\n",
       "- 花菇 200 克\n",
       "- 草菇 200 克\n",
       "- 海鲜 200 克\n",
       "- 大米 2 杯\n",
       "- 因为这个菜要煮很久，所以需要准备足够的材料\n",
       "\n",
       "步骤：\n",
       "\n",
       "1. 瑶柱、花菇、草菇、海鲜洗净，切成小块，备用。\n",
       "\n",
       "2. 大鲍鱼和白切鸡切成小块，备用。\n",
       "\n",
       "3. 大米洗干净，备用。\n",
       "\n",
       "4. 锅中加入足够的水，将大米放入锅中，大火煮开后转小火慢慢煮烂。\n",
       "\n",
       "5. 加入瑶柱、花菇、草菇、海鲜，继续煮5-10分钟，直到所有材料煮烂。\n",
       "\n",
       "6. 加入大鲍鱼和白切鸡，用筷子轻轻搅拌，煮5-10分钟，直到鲍鱼和鸡肉熟透。\n",
       "\n",
       "7. 最后加入适量的盐和胡椒粉，即可享用。\n",
       "\n",
       "注意事项：\n",
       "\n",
       "1. 煮的时候要一直开着小火，以免煮过头。\n",
       "\n",
       "2. 加入海鲜和瑶柱等海鲜食材，煮的时间要稍微长一些，以确保海鲜熟透。\n",
       "\n",
       "3. 大米煮好后，煮的时候一定要用中小火，以免大米煮烂糊锅。\n",
       "\n",
       "4. 最后加入适量的盐和胡椒粉，可根据口味调整。</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"黯然销魂饭怎么做？\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a5aac-f2af-4a18-b23a-73385f6572fc",
   "metadata": {},
   "source": [
    "可以看到 Vector 搜索的 Chunk 明明是不相干的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "482ef13b-af99-427a-88cc-61a69ca5e3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 糖醋汁的做法\n",
      "\n",
      "糖醋汁通常情况下由清水、白糖、白醋等制成，有些人喜欢放一些番茄酱来增添不一样的酸甜味或放一些淀粉来增加菜肴汤汁的粘性和浓度，糖醋汁可用于糖醋鱼、糖醋里脊、糖醋排骨等菜品的制作\n",
      "\n",
      "可依据糖醋汁配制的经典比例 1：2：3：4：5 来调制糖醋汁\n",
      "\n",
      "## 必备原料和工具\n",
      "\n",
      "- 清水\n",
      "- 白糖\n",
      "- 白醋/米醋\n",
      "- 料酒\n",
      "- 生抽\n",
      "\n",
      "## 计算\n",
      "\n",
      "- 清水(50ml)\n",
      "- 生抽(40ml)\n",
      "- 白糖(30g)\n",
      "- 白醋(20ml)\n",
      "- 料酒(10ml)\n",
      "\n",
      "## 操作\n",
      "\n",
      "- 按照比例将各调料在小碗中搅拌均匀\n",
      "- 按不同菜肴的方式处理完毕后，将配制好的糖醋汁倒入锅中\n",
      "- 根据各菜肴的不同，烹制 5-10 分钟\n",
      "- 大火收汁，可增加菜的浓度、香味和光泽\n",
      "\n",
      "## 附加内容\n",
      "\n",
      "如果您遵循本指南的制作流程而发现有问题或可以改进的流程，请提出 Issue 或 Pull request 。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(vector_query_engine.retrieve(\"翡翠白玉汤怎么做？\")[0].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39bbe5b8-9d9b-4b17-9fab-982d4cda4c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调至大火收汁，汤汁剩余 1/3 时，关火盛至小盆中。\n",
      "* 注：将锅中的汤汁均匀淋到鱼头上，盛盘时可以将锅中煮的香菜放入小盆底部，这样能让成品菜好看又好吃。\n",
      "* 将香菜放至已经盛出的鱼头上，把切好的美人椒圈放在香菜之上。\n",
      "* 色香味俱全的红烧鱼头出炉！\n",
      "\n",
      "## 附加内容\n",
      "\n",
      "如果您遵循本指南的制作流程而发现有问题或可以改进的流程，请提出 Issue 或 Pull request 。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(vector_query_engine.retrieve(\"黯然销魂饭怎么做？\")[0].node.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6511620-443b-434d-b48e-7f25970c9186",
   "metadata": {},
   "source": [
    "💡 但是在 KG Query 中，这个幻觉不会出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2563b284-b8fd-4089-8600-1b3e58858856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: 黯然销魂饭怎么做？\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['怎么做?', '黯然销魂饭']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = kg_keyword_query_engine.query(\"黯然销魂饭怎么做？\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ae42-c170-4486-8c3c-2b226c0c2455",
   "metadata": {},
   "source": [
    "### 4. Hallucination mitigation with VectorSearch and Knowledge Graph RAG\n",
    "\n",
    "这里我们直接看结果，黯然销魂饭这个食谱里不存在的菜品被排除掉了。\n",
    "\n",
    "> 👇 注意，这里的 graph_vector_rag_query_engine 我在之后的部分定义，执行的时候需要先执行后边的 cell 才能执行这个 Query。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05879e35-cd36-40a8-8344-249b49b4c91d",
   "metadata": {},
   "source": [
    "#### 4.1 Hallucination Mitigation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83b4f6f9-4b6d-489d-bc9b-a052c85531e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: 黯然销魂饭怎么做？\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['关键字: 黯然销魂', '饭', '怎么做', '黯然销魂', '关键字']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:debug hallucination chunk detected, will be removed.\n",
      " Chunk: # 微波葱姜黑鳕鱼的做法\n",
      "\n",
      "这道菜改编自西雅图 Veil 餐厅主厨 Johnny Zhu 的母亲 Margaret Lu 的菜谱。卢女士原菜谱是使用罗非鱼来做这道菜，Johnny 改为鳕鱼，但也可以用大比目鱼鱼排，或者海鲈鱼、鳟鱼等。每种鱼的密度有差别，烹饪时间要做微调。\n",
      "\n",
      "## 必备原料和工具\n",
      "\n",
      "原料：\n",
      "\n",
      "- 黑鳕鱼，带皮\n",
      "\n",
      "调味料：\n",
      "\n",
      "- 青葱\n",
      "- 姜\n",
      "- 料酒\n",
      "- 酱油\n",
      "- 芝麻油\n",
      "- 花生油\n",
      "\n",
      "工具：\n",
      "\n",
      "- 密封袋\n",
      "\n",
      "## 计算\n",
      "\n",
      "每 2 份：\n",
      "\n",
      "- 黑鳕鱼，带皮，2 片，450g（本菜谱主角，所有调料可根据鳕鱼的实际重量进行比例调整）\n",
      "- 青葱，葱白，25g。\n",
      "- 青葱，葱绿，10g。\n",
      "- 姜，13g。\n",
      "- 料酒，5mL。\n",
      "- 酱油，25mL。\n",
      "- 芝麻油，2mL。\n",
      "- 花生油，50mL。\n",
      "\n",
      "## 操作\n",
      "\n",
      "- 鱼片分别放入密封袋，鱼皮向下放在盘子中。\n",
      "- 取葱白切丝 25g，姜去皮后切丝，10g，混合在一起后分成两半，分别放在袋内鱼片上。\n",
      "- 每个袋子倒入 2.5mL 料酒。\n",
      "- 封好密封袋，放入微波炉中，中火（800 瓦）微波至*不透明且容易散开*时（约 3.5-5 分钟），从袋中取出鱼片。\n",
      "- 去除青葱和姜。\n",
      "- 取酱油 25mL，芝麻油 2mL，混合均匀后平均淋在两片鱼片上。\n",
      "- 取葱绿切细丝 10g，姜去皮后切丝 3g，混合后分成两份撒在鱼片上。\n",
      "-\n",
      "INFO:__main__:debug hallucination chunk detected, will be removed.\n",
      " Chunk: 调至大火收汁，汤汁剩余 1/3 时，关火盛至小盆中。\n",
      "* 注：将锅中的汤汁均匀淋到鱼头上，盛盘时可以将锅中煮的香菜放入小盆底部，这样能让成品菜好看又好吃。\n",
      "* 将香菜放至已经盛出的鱼头上，把切好的美人椒圈放在香菜之上。\n",
      "* 色香味俱全的红烧鱼头出炉！\n",
      "\n",
      "## 附加内容\n",
      "\n",
      "如果您遵循本指南的制作流程而发现有问题或可以改进的流程，请提出 Issue 或 Pull request 。\n"
     ]
    }
   ],
   "source": [
    "response = graph_vector_rag_query_engine.query(\"黯然销魂饭怎么做？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487221-ba94-4e32-a618-33bf974d6f41",
   "metadata": {},
   "source": [
    "> 💡 答案是 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2fdbfe6-c857-48a8-846b-3a938b5ecf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbd1a061-a639-4aa6-a4f9-8bf328a52e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector index 搜索得到的 chunk 被判断为幻觉而删掉了\n",
    "len(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600c163-d301-42f0-be12-37565d5e96b8",
   "metadata": {},
   "source": [
    "#### 4.2 Impl. of the KG crosscheck query engine\n",
    "\n",
    "This is the impl. of the cross if empty query engine, when one of the retrievers got empty results, the other retriever will have information being fact-checked by LLM, if it's actually not related, we remove them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb11600f-41a4-462c-85f2-4dda4a0e4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "UNION = \"union\"\n",
    "KG_FIRST = \"kg_first\"\n",
    "CROSS_IF_EMPTY = \"cross_if_empty\"\n",
    "\n",
    "CROSS_CHECK_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a fact checker, now I will put a piece of context and a question, and you will check step by step on whether it's actually related or not, responding only \"Yes\" or \"No\".\n",
    "\n",
    "For example, the context that's only partially related but actually there are details that could tell it should be uncorrelated, respond No\n",
    "Do not add explanations, apologies, or any other things than Yes or No\n",
    "\n",
    "Example:\n",
    "In this example, although 保温杯 is related to 保温 in some sense, the question is not about 杯, thus from reasonable justification, it's NOT related.\n",
    "\n",
    "context:\n",
    "---\n",
    "保温杯是冬天外出必备良品\n",
    "---\n",
    "question:\n",
    "---\n",
    "保温大棚是什么？\n",
    "---\n",
    "related:\n",
    "No\n",
    "\n",
    "Now check this with reasonable justification!\n",
    "\n",
    "context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "question:\n",
    "---\n",
    "{question_str}\n",
    "---\n",
    "related:\n",
    "\"\"\"\n",
    "\n",
    "CROSS_CHECK_PROMPT_TEMPLATE_CHATGLM = \"\"\"\n",
    "\n",
    "你是一个事实核查员，现在我会提供一段背景和一个问题，然后你将逐步检查它们是否相关，并只回答\"Yes\"，表示大概率是相关的、或\"No\"。\n",
    "\n",
    "例如，在这个例子中，虽然保温杯从某种意义上与保温有关，但问题并不涉及杯子，因此从这个不合理性得知它们实际上不相关。\n",
    "\n",
    "context:\n",
    "---\n",
    "保温杯是冬天外出必备良品\n",
    "---\n",
    "question:\n",
    "---\n",
    "保温大棚是什么？\n",
    "---\n",
    "related:\n",
    "No\n",
    "\n",
    "现在开始仔细检查，通过合理性验证判断是否真正相关\n",
    "\n",
    "context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "question:\n",
    "---\n",
    "{query_str}\n",
    "---\n",
    "related:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class KGVectorCrosscheckRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that performs both Vector search and Knowledge Graph search, and cross-checks to mitigate hallucination\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = UNION,\n",
    "        cross_check_propmpt_template: str = CROSS_CHECK_PROMPT_TEMPLATE_CHATGLM,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (UNION, KG_FIRST, CROSS_IF_EMPTY):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        self._service_context = service_context or ServiceContext.from_defaults()\n",
    "        self._cross_check_prompt_template = cross_check_propmpt_template\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "        if self._mode == KG_FIRST and len(kg_nodes) > 0:\n",
    "            # bypass KG retrieval\n",
    "            vector_nodes = []\n",
    "        else:\n",
    "            vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        # no matter UNION, KG_FIRST or CROSS_IF_EMPTY, we need to union them first\n",
    "        retrieve_ids = vector_ids.union(kg_ids)\n",
    "        # In case CROSS_IF_EMPTY and one of the retrieval got nothing, do fact-check\n",
    "        # to avoid hallucinations\n",
    "        one_of_the_retrieval_failed = len(retrieve_ids) > 0 and (\n",
    "            len(vector_ids) == 0 or len(kg_ids) == 0)\n",
    "        if self._mode == CROSS_IF_EMPTY and one_of_the_retrieval_failed:\n",
    "            retrieve_ids_copy = retrieve_ids.copy()\n",
    "            for node_id in retrieve_ids_copy:\n",
    "                node = combined_dict[node_id]\n",
    "                response = self._service_context.llm_predictor.predict(\n",
    "                    self._cross_check_prompt_template,\n",
    "                    context=node.node.get_content(),\n",
    "                    query_str=query_bundle,\n",
    "                )\n",
    "\n",
    "                if \"yes\" not in str(response).lower():\n",
    "                    logger.info(f\"debug hallucination chunk detected, will be removed.\\n Chunk: {combined_dict[node_id].node.get_text()}\")\n",
    "                    retrieve_ids.remove(node_id)\n",
    "\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# option a\n",
    "# create raw retrievers from index\n",
    "# vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "# kg_retriever = KGTableRetriever(\n",
    "#     index=kg_index,\n",
    "#     retriever_mode=\"keyword\",\n",
    "#     include_text=False, \n",
    "#     max_keywords_per_query=3,\n",
    "# )\n",
    "\n",
    "# option b from query engine\n",
    "vector_retriever = vector_query_engine._retriever\n",
    "kg_retriever = kg_keyword_query_engine._retriever\n",
    "\n",
    "combined_retriever = KGVectorCrosscheckRetriever(vector_retriever, kg_retriever, mode=\"cross_if_empty\")\n",
    "\n",
    "# create a response synthesizer\n",
    "# response_synthesizer = get_response_synthesizer(\n",
    "#     service_context=service_context,\n",
    "# )\n",
    "response_synthesizer = vector_query_engine._response_synthesizer or kg_keyword_query_engine._response_synthesizer\n",
    "\n",
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=combined_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cb16f-6d0e-494e-80b2-4eeff6570362",
   "metadata": {},
   "source": [
    "### 5. Compare Graph and Vector RAG Result\n",
    "\n",
    "> *experimental*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7bf65de-7d07-4668-82c6-727ae74d73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_graph_and_vector(q):\n",
    "    graph_store.query(\"show hosts\")\n",
    "    response_graph_rag = vector_query_engine.query(q)\n",
    "    \n",
    "    response_vector_rag = kg_keyword_query_engine.query(q)\n",
    "    \n",
    "    display(\n",
    "        Markdown(\n",
    "            llm(f\"\"\"\n",
    "比较两个关于 \"{q}\" 的问答结果。\n",
    "1. 最终将结果输出为 markdown，评估结果差异的部分输出为 markdown 表格；\n",
    "2. 表格第一列是方法，一共两行，第一行为 “基于 Graph_RAG”，第二行为“基于 Vector_DB”\n",
    "2. 表格第二列中，每一行列出要比较的结果，第三列把结果拆解成结果的主要要点；\n",
    "3. 第三列到最后一列中，要在不同列中分析比较问题与答案的匹配程度、答案的完整度、正确性，评估问答结果的质量；\n",
    "\n",
    "问答结果分别如下：\n",
    "---\n",
    "基于 Graph_RAG 的结果: {response_graph_rag}\n",
    "---\n",
    "基于 Vector_DB 的结果: {response_vector_rag}\n",
    "---\n",
    "\"\"\"\n",
    "               )\n",
    "        )\n",
    "    )\n",
    "    return response_graph_rag, response_vector_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c52bbbc9-08bd-41c7-8125-b21d6f213a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: 白灼虾怎么做？\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['keyword', '虾', '怎么做', '白灼', 'keyword(s): 白灼']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge sequence in 2-depth in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]` that may be related to the task.\n",
      "虾 虾, 摆放整齐, 用于炸虾\n",
      "白灼 白灼, 做法, 白灼, 富含, 蛋白质\n",
      "白灼 白灼, 做法, 白灼, 是, 程序员在沿海地区做\n",
      "白灼 白灼, 做法, 白灼, 需要, 水产\n",
      "白灼 白灼, 做法, 白灼, 需要, 活虾\n",
      "白灼 白灼, 做法, 白灼\n",
      "白灼 白灼, 做法, 白灼, 适宜, 1-2 人份\n",
      "白灼 白灼, 做法, 白灼, 是, 水产\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| 方法 | 基于 Graph_RAG | 基于 Vector_DB |\n",
       "| --- | --- | --- |\n",
       "| 必备原料和工具 | 活虾、洋葱、姜、蒜、葱、食用油、酱油、料酒、芝麻、蚝油、香醋 | 活虾、葱、姜、蒜、食用油、酱油、料酒、芝麻、香醋、蚝油 |\n",
       "| 计算 | 总量：虾 250g * 份数（建议 1-2 人份）<br>葱一根、姜一块、洋葱一头、蒜 5-8 瓣、食用油 10-15ml、料酒 20 ml、酱油 10-15ml、芝麻一把、香醋 10 ml、蚝油 10 ml | 总量：虾 250g * 份数（建议 1-2 人份）<br>葱一根、蒜切碎、倒入酱油、芝麻、香醋，搅拌之<br>油烧热，淋入蘸料 |\n",
       "| 操作 | 1. 洋葱切小块，姜切片，平铺平底锅<br>2. 活虾冲洗一下（去除虾线、剪刀减掉虾腿虾须子都是可选操作），控水，铺在平底锅的洋葱、姜片之上<br>3. 锅内倒入料酒，盖上锅盖，中火 1 分钟，小火 5 分钟，关火 5 分钟<br>4. 和上一步并行操作，制作蘸料：<br>- 葱切成葱花、蒜切碎、倒入酱油、芝麻、香醋，搅拌之<br>- 油烧热，淋入蘸料 |\n",
       "| 附加内容 | - 技术细节：<br>开始不能大火、防止糊底。<br>如果锅盖有通气口、时间要相应调节一下（考虑增加 30 秒中火）。 | - 选择新鲜的大虾，最好是活虾，虾体需要清洗干净。<br>- 在虾体上洒上少量盐，抓匀后放置一段时间，让虾体入味。<br>- 把虾体放进沸水中，大火煮开后转小火，盖上锅盖煮约2-3分钟，直到虾体变红。<br>- 捞出虾体，沥水后加入适量的生抽、料酒、白糖、姜丝和葱花，拌匀后即可上桌享用。 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"白灼虾怎么做？\"\n",
    "r = compare_graph_and_vector(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
