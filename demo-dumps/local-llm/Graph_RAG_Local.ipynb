{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10785f8-54e4-4323-8d6c-244da27d5ad0",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "### 1.Start ChatGLM2-6B as an API service\n",
    "\n",
    "Referring to https://python.langchain.com/docs/integrations/llms/chatglm\n",
    "\n",
    "Assuming it's running on `127.0.0.1:8000`\n",
    "\n",
    "### 2.Prepare Embedding Model\n",
    "\n",
    "#### 2.1 Run mode\n",
    "- Option 0: run remotely\n",
    "  - Referring to https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/self-hosted\n",
    "- Option 1: run locally\n",
    "  - We'll go in this way here!\n",
    "\n",
    "#### 2.2 Embedding Model\n",
    "\n",
    "Let's use [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese) as the embedding model.\n",
    "\n",
    "### 3. Prepare NebulaGraph Cluster\n",
    "\n",
    "Install with oneliner:\n",
    "\n",
    "```bash\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "```\n",
    "\n",
    "Install the required packages and load nGQL Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877b3d27-9186-492d-aca1-7664890a0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Pool Created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinese_kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo_basketballplayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demo_football_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demo_shareholding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guardians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>operator_biz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>operator_biz_cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>science_2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name\n",
       "0             chinese_kg\n",
       "1  demo_basketballplayer\n",
       "2     demo_football_2022\n",
       "3      demo_shareholding\n",
       "4              guardians\n",
       "5           operator_biz\n",
       "6        operator_biz_cn\n",
       "7           science_2023\n",
       "8                   yelp"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install sentence_transformers langchain llama-index ipython-ngql nebula3-python==3.4.0\n",
    "\n",
    "# assume NebulaGraph is running locally from 127.0.0.1:9669\n",
    "\n",
    "%load_ext ngql\n",
    "%ngql --address graphd --port 9669 --user root --password nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d6d52-412b-47d5-93af-57b192af04b9",
   "metadata": {},
   "source": [
    "## title <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9022fb-6915-428e-8db8-33f2f7de3a34",
   "metadata": {},
   "source": [
    "## Get LLM and Embedding Ready\n",
    "\n",
    "### 1. Local LLM, ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d718ac-8b5f-4246-8ef7-96c0d527a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = \"http://127.0.0.1:8000\" # LLM API\n",
    "embedding_model = \"shibing624/text2vec-base-chinese\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) # logging.DEBUG for more verbose output\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "# LLM\n",
    "from langchain.llms import ChatGLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "llm = ChatGLM(\n",
    "    endpoint_url=endpoint_url,\n",
    "    max_token=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=1,\n",
    "    model_kwargs={\n",
    "        \"sample_model_args\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "llm.with_history = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdc3aa-0fb3-4427-ab92-58ec5316a6b5",
   "metadata": {},
   "source": [
    "### 2. Local Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec801a71-4df9-4c8f-8761-01b849f77630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: shibing624/text2vec-base-chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp5dvs9kbx\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp5dvs9kbx/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# Embedding option 0 run with runhouse\n",
    "\n",
    "# from langchain.embeddings import SelfHostedHuggingFaceEmbeddings\n",
    "# embedding_llm = SelfHostedHuggingFaceEmbeddings(model_id=embedding_model)\n",
    "\n",
    "# Embedding option 1 run locally\n",
    "\n",
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(\n",
    "      model_name=embedding_model,\n",
    "      model_kwargs={'device': EMBEDDING_DEVICE},\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f8f1b-f937-47c3-85f8-5e289b2b7466",
   "metadata": {},
   "source": [
    "### 3. LlamaIndex with LLM and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c417c4-b4e0-4473-bbdd-0d71106b8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Index ServiceContext\n",
    "\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# Set global service context\n",
    "\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c52918-b5a8-45ce-bd7c-30021757c38e",
   "metadata": {},
   "source": [
    "## Indexing for both KG Index and Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be913db5-c5de-4614-8e87-68f9c29030b0",
   "metadata": {},
   "source": [
    "### 1. KG Build\n",
    "\n",
    "#### 1.1 Graph Space Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2110028d-2d34-474b-bcc9-8c8709b52828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS chinese_kg(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e3b9a-0197-46bd-a40f-5724f695f26f",
   "metadata": {},
   "source": [
    "#### 1.2 Graph Schema Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860e08b5-95a7-494e-bbc3-98a4ab0b60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql \n",
    "USE chinese_kg;\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199fafb-6e2e-4bb3-91d7-2cb799a0c4a7",
   "metadata": {},
   "source": [
    "Let's create an Index for `entity.name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e9256e-d87e-4279-8059-28c8972568d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe64b8-1ac4-49bc-b52b-d0f447d65be5",
   "metadata": {},
   "source": [
    "#### 1.3 Llama Index GraphStore and Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc15cf3e-3888-4a24-aa4e-60e3efa8c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\"\n",
    "os.environ['NEBULA_ADDRESS'] = \"graphd:9669\"\n",
    "\n",
    "space_name = \"chinese_kg\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308c7f4-c3dd-4c7f-89b8-58d779e0e53e",
   "metadata": {},
   "source": [
    "#### 1.4 Download Data to be indexed\n",
    "\n",
    "Let's download data from [HowToCook](https://github.com/Anduin2017/HowToCook)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded7e5c7-719b-46a4-acef-3d8a93d46e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    # Download the file\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_github_folder_files(user, repo, path, branch='master', extension='.md'):\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{path}?ref={branch}\"\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "    files = response.json()\n",
    "    _documents = []\n",
    "    # Iterate over each file in the repository folder\n",
    "    for file in files:\n",
    "        if file['type'] == 'file' and file['name'].endswith(extension):\n",
    "            # Create local directories if necessary\n",
    "            local_path = os.path.join('downloaded_files', file['path'])\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            # Download  the file\n",
    "            download_file(file['download_url'], local_path)\n",
    "            print(f'Downloaded {local_path}/{file[\"name\"]}')\n",
    "        elif file['type'] == 'dir':\n",
    "            # Recursively download files in the subdirectory\n",
    "            download_github_folder_files(user, repo, file['path'], branch, extension)\n",
    "\n",
    "# Replace with your URL\n",
    "# https://github.com/Anduin2017/HowToCook\n",
    "user = 'Anduin2017'\n",
    "repo = 'HowToCook'\n",
    "path = ''\n",
    "branch = 'master'\n",
    "#download_github_folder_files(user, repo, path, branch=branch, extension='.md')\n",
    "\n",
    "# Eval variables in docs, only for NebulaGraph Docs\n",
    "# !find './downloaded_files' -type f -exec sed -i 's/{{nebula.name}}/NebulaGraph/g' {} +\n",
    "\n",
    "# rename files into txt\n",
    "!find ./downloaded_files -type f -name \"*.md\" -exec bash -c 'mv \"$0\" \"${0%.md}.txt\"' {} \\;\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./downloaded_files\", recursive=True, exclude_hidden=True\n",
    ")\n",
    "documents = loader.load_data()\n",
    "\n",
    "# Now you have a list of documents loaded from all the markdown files in the specified GitHub folder\n",
    "print(f'Loaded {len(documents)} documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bac72-6906-4788-ab35-c1a9a277e166",
   "metadata": {},
   "source": [
    "> Let's check some of the Data Chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55120538-0146-4d0b-89f4-5f98e7ca9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ç™½ç¼è™¾çš„åšæ³•\n",
      "\n",
      "ç™½ç¼è™¾éå¸¸é€‚åˆç¨‹åºå‘˜åœ¨æ²¿æµ·åœ°åŒºåšï¼Œç±»ä¼¼äºæ¸…è’¸é±¼ï¼šç®€å•å®¹é”™ã€æœ‰è¥å…»ã€æœ‰æ»¡è¶³æ„Ÿï¼Œç”šè‡³å¾ˆå¥½çœ‹ã€‚\n",
      "\n",
      "## å¿…å¤‡åŸæ–™å’Œå·¥å…·\n",
      "\n",
      "- æ´»è™¾\n",
      "- æ´‹è‘±\n",
      "- å§œ\n",
      "- è’œ\n",
      "- è‘±\n",
      "- é£Ÿç”¨æ²¹\n",
      "- é…±æ²¹\n",
      "- æ–™é…’\n",
      "- èŠéº»\n",
      "- èšæ²¹\n",
      "- é¦™é†‹\n",
      "\n",
      "## è®¡ç®—\n",
      "\n",
      "æ¯æ¬¡åˆ¶ä½œå‰éœ€è¦ç¡®å®šè®¡åˆ’åšå‡ ä»½ã€‚ä¸€ä»½æ­£å¥½å¤Ÿ 1 ä¸ªäººé£Ÿç”¨\n",
      "\n",
      "æ€»é‡ï¼š\n",
      "\n",
      "- è™¾ 250g * ä»½æ•°ï¼ˆå»ºè®® 1-2 äººä»½ï¼‰\n",
      "- è‘± ä¸€æ ¹\n",
      "- å§œ ä¸€å—\n",
      "- æ´‹è‘± ä¸€å¤´\n",
      "- è’œ 5-8 ç“£\n",
      "- é£Ÿç”¨æ²¹ 10-15ml\n",
      "- æ–™é…’ 20 ml\n",
      "- é…±æ²¹ 10-15ml\n",
      "- èŠéº» ä¸€æŠŠ\n",
      "- é¦™é†‹ 10 ml\n",
      "- èšæ²¹ 10 ml\n",
      "\n",
      "## æ“ä½œ\n",
      "\n",
      "- æ´‹è‘±åˆ‡å°å—ï¼Œå§œåˆ‡ç‰‡ï¼Œå¹³é“ºå¹³åº•é”…ã€‚\n",
      "- æ´»è™¾å†²æ´—ä¸€ä¸‹ï¼ˆå»é™¤è™¾çº¿ã€å‰ªåˆ€å‡æ‰è™¾è…¿è™¾é¡»å­éƒ½æ˜¯å¯é€‰æ“ä½œï¼‰ï¼Œæ§æ°´ï¼Œé“ºåœ¨å¹³åº•é”…çš„æ´‹è‘±ã€å§œç‰‡ä¹‹ä¸Šã€‚\n",
      "- é”…å†…å€’å…¥æ–™é…’ï¼Œç›–ä¸Šé”…ç›–ï¼Œä¸­ç« 1 åˆ†é’Ÿï¼Œå°ç« 5 åˆ†é’Ÿï¼Œå…³ç« 5 åˆ†é’Ÿã€‚\n",
      "- å’Œä¸Šä¸€æ­¥å¹¶è¡Œæ“ä½œï¼Œåˆ¶ä½œè˜¸æ–™ï¼š\n",
      "  - è‘±åˆ‡æˆè‘±èŠ±ã€è’œåˆ‡ç¢ã€å€’å…¥é…±æ²¹ã€èŠéº»ã€é¦™é†‹ï¼Œæ…æ‹Œä¹‹ã€‚\n",
      "  - æ²¹çƒ§çƒ­ï¼Œæ·‹å…¥è˜¸æ–™ã€‚\n",
      "- è™¾å‡ºé”…ï¼Œç”¨å¹²å‡€çš„ç›˜å­è£…å¥½ã€‚\n",
      "\n",
      "![ç™½ç¼è™¾](./ç™½ç¼è™¾.webp)\n",
      "\n",
      "## é™„åŠ å†…å®¹\n",
      "\n",
      "- æŠ€æœ¯ç»†èŠ‚ï¼š\n",
      "  - å¼€å§‹ä¸èƒ½å¤§ç«ã€é˜²æ­¢ç³Šåº•ã€‚\n",
      "  - å¦‚æœé”…ç›–æœ‰é€šæ°”å£ã€æ—¶é—´è¦ç›¸åº”è°ƒèŠ‚ä¸€ä¸‹ï¼ˆè€ƒè™‘å¢åŠ  30 ç§’ä¸­ç«ï¼‰ã€‚\n",
      "  - è˜¸æ–™å…¶å®ä¹Ÿæ˜¯å¯é€‰çš„ã€ä¹Ÿå¯ä»¥æ˜¯çº¯çš„é†‹ï¼Œå¤§è‡ªç„¶é¦ˆèµ çš„é²œè™¾åœ¨æ²¡æœ‰æ°´å¸¦èµ°å†²æ·¡é²œç”œçš„æƒ…å†µä¸‹å£æ„Ÿå‘³é“éƒ½éå¸¸æ£’çš„ã€‚\n",
      "\n",
      "å¦‚æœæ‚¨éµå¾ªæœ¬æŒ‡å—çš„åˆ¶ä½œæµç¨‹è€Œå‘ç°æœ‰é—®é¢˜æˆ–å¯ä»¥æ”¹è¿›çš„æµç¨‹ï¼Œè¯·æå‡º Issue æˆ– Pull request ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(documents[14].text) # It's contributed by me! via https://github.com/Anduin2017/HowToCook/pull/219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e9a169-f274-4a4c-8c9d-d9ec36b7163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdownloaded_files\u001b[00m\n",
      "|-- CODE_OF_CONDUCT.txt\n",
      "|-- CONTRIBUTING.txt\n",
      "|-- README.txt\n",
      "`-- \u001b[01;34mdishes\u001b[00m\n",
      "    |-- \u001b[01;34maquatic\u001b[00m\n",
      "    |-- \u001b[01;34mbreakfast\u001b[00m\n",
      "    |-- \u001b[01;34mcondiment\u001b[00m\n",
      "    |-- \u001b[01;34mdessert\u001b[00m\n",
      "    |-- \u001b[01;34mdrink\u001b[00m\n",
      "    `-- \u001b[01;34mmeat_dish\u001b[00m\n",
      "\n",
      "7 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree downloaded_files -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0da59-1361-4619-aec9-2d2247c2b0b7",
   "metadata": {},
   "source": [
    "#### 1.5 Triplets extraction and Knowledge Graph build\n",
    "\n",
    "First, we define specific Prompt Template to enable ChatGLM2 to extract knowledge, the default one will work only for OpenAI Models.\n",
    "\n",
    "> TBD, need to bring BERT/NER with local model here to improve this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b11a59c-4888-4d45-b65c-e9bdf7c2b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "æ ¹æ®ç»™å®šçš„æ–‡æœ¬ï¼Œé€šè¿‡ä¸€æ­¥ä¸€æ­¥æ€»ç»“ï¼Œç†è§£ï¼Œæœ€ç»ˆè¾“å‡ºæŠ½å–è‡³å¤š {max_knowledge_triplets} è¡Œ (ä¸»è¯­, è°“è¯­, å®¾è¯­) æ ¼å¼çš„ä¸‰å…ƒç»„ç”¨ä½œæ„å»ºé—®ç­”çŸ¥è¯†å›¾è°±ï¼Œå¿½ç•¥æ–‡æœ¬ä¸­çš„åœæ­¢ç¬¦å·ã€‚\n",
    "<æ³¨æ„> ä¿è¯ä¸‰å…ƒç»„åªæœ‰ä¸»è°“å®¾ä¸‰éƒ¨åˆ†ï¼Œä¸è¦æŠŠç½—åˆ—çš„çŸ¥è¯†æ”¾åœ¨ä¸€è¡Œä¸­ï¼Œè€Œåº”è¯¥æ‹†ä¸ºå¤šè¡ŒçŸ¥è¯†ã€‚\n",
    "<æ³¨æ„> å¦‚æœæ–‡æœ¬æ˜¯å¤§æ®µä»£ç æˆ–è€…å‘½ä»¤è¡Œã€ç½—åˆ—çš„æ­¥éª¤ï¼Œå…ˆæ€»ç»“å‡ºçŸ¥è¯†å†æŠ½å–æœ‰æ„ä¹‰çš„çŸ¥è¯†ã€‚\n",
    "<æ³¨æ„> è¿”å›æ ¼å¼ä¸ºæ¯ä¸€è¡Œç”¨æ‹¬å·åŒ…è£¹ã€é€—å·éš”å¼€ï¼Œæ²¡æœ‰åºå·ã€‚\n",
    "<æ³¨æ„> ä»”ç»†æ£€æŸ¥ï¼ŒåªæŠ½å–æœ‰æ„ä¹‰çš„çŸ¥è¯†ï¼Œæ²¡æœ‰çš„æ—¶å€™è¿”å›ç©ºã€‚\n",
    "<æ³¨æ„> è°“è¯­è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚\n",
    "<æ³¨æ„> è¦æ³¨æ„ä¸‰å…ƒç»„ä¸»è¯­çš„é€‰æ‹©ï¼Œè¦æ˜ç¡®ï¼Œå‡†ç¡®ã€‚\n",
    "\n",
    "ä¸‹é¢æ˜¯å‡ ä¸ªä¾‹å­:\n",
    "---------------------\n",
    "æ–‡æœ¬: ç‹—æ˜¯äººç±»æœ€æ—©é©¯åŒ–çš„åŠ¨ç‰©ï¼Œå¤§çº¦åœ¨ä¸€ä¸‡å››åƒå¹´å‰ï¼Œäººç±»å°±å¼€å§‹é©¯åŒ–ç‹¼ï¼Œæœ€ç»ˆæ¼”åŒ–æˆäº†æˆ‘ä»¬ç°åœ¨çœ‹åˆ°çš„å„ç§çŠ¬ç§ã€‚ç‹—å±äºå“ºä¹³åŠ¨ç‰©ï¼Œå…¶è§†è§‰ã€å¬è§‰å’Œå—…è§‰éƒ½éå¸¸çµæ•ã€‚å®ƒä»¬æ˜¯ç¤¾ä¼šæ€§çš„åŠ¨ç‰©ï¼Œé€šå¸¸åœ¨ç¾¤ä½“ä¸­ç”Ÿæ´»ã€‚ç‹—çš„å¯¿å‘½ä¸€èˆ¬åœ¨10åˆ°15å¹´ä¹‹é—´ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›çŠ¬ç§å¯ä»¥æ´»åˆ°20å¹´ä»¥ä¸Šã€‚å®ƒä»¬çš„é£Ÿç‰©ä¸»è¦æ˜¯è‚‰ç±»ï¼Œä½†æ˜¯ä¹Ÿèƒ½åƒä¸€äº›è”¬èœå’Œè°·ç‰©ã€‚\n",
    "ä¸»è°“å®¾ä¸‰å…ƒç»„:\n",
    "(ç‹—, æ˜¯, äººç±»æœ€æ—©é©¯åŒ–çš„åŠ¨ç‰©)\n",
    "(ç‹—, å±äº, å“ºä¹³åŠ¨ç‰©)\n",
    "(ç‹—, å¯¿å‘½ä¸º, 10åˆ°15å¹´ä¹‹é—´)\n",
    "----\n",
    "# æœ¬ä¾‹ä¸­åªæŠ½å–ä¸è¶…è¿‡ 5 è¡Œä¸‰å…ƒç»„ï¼Œä¸”å°†åˆ—è¡¨ä¿¡æ¯ç»¼åˆå¤„ç†ã€‚\n",
    "æ–‡æœ¬: Docker çš„å®‰è£…\n",
    "Docker æ˜¯ä¸€ä¸ªå¼€æºçš„å•†ä¸šäº§å“ï¼Œæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šç¤¾åŒºç‰ˆï¼ˆCommunity Editionï¼Œç¼©å†™ä¸º CEï¼‰å’Œä¼ä¸šç‰ˆï¼ˆEnterprise Editionï¼Œç¼©å†™ä¸º EEï¼‰ã€‚ä¼ä¸šç‰ˆåŒ…å«äº†ä¸€äº›æ”¶è´¹æœåŠ¡ï¼Œä¸ªäººå¼€å‘è€…ä¸€èˆ¬ç”¨ä¸åˆ°ã€‚ä¸‹é¢çš„ä»‹ç»éƒ½é’ˆå¯¹ç¤¾åŒºç‰ˆã€‚\n",
    "\n",
    "Docker CE çš„å®‰è£…è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼Œå¹¶ä¸”æ”¯æŒï¼š\n",
    "\n",
    "- Mac\n",
    "- Windows\n",
    "- Ubuntu\n",
    "- Debian\n",
    "- CentOS\n",
    "- Fedora\n",
    "\n",
    "å¯¹äºå…¶ä»– Linux å‘è¡Œç‰ˆ\n",
    "å®‰è£…å®Œæˆåï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼ŒéªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸã€‚\n",
    "\n",
    "\n",
    "$ docker version\n",
    "# æˆ–è€…\n",
    "$ docker info\n",
    "Docker éœ€è¦ç”¨æˆ·å…·æœ‰ sudo æƒé™ï¼Œä¸ºäº†é¿å…æ¯æ¬¡å‘½ä»¤éƒ½è¾“å…¥sudoï¼Œå¯ä»¥æŠŠç”¨æˆ·åŠ å…¥ Docker ç”¨æˆ·ç»„ï¼ˆå®˜æ–¹æ–‡æ¡£ï¼‰ã€‚\n",
    "\n",
    "\n",
    "$ sudo usermod -aG docker $USER\n",
    "Docker æ˜¯æœåŠ¡å™¨----å®¢æˆ·ç«¯æ¶æ„ã€‚å‘½ä»¤è¡Œè¿è¡Œdockerå‘½ä»¤çš„æ—¶å€™ï¼Œéœ€è¦æœ¬æœºæœ‰ Docker æœåŠ¡ã€‚å¦‚æœè¿™é¡¹æœåŠ¡æ²¡æœ‰å¯åŠ¨ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨ï¼ˆå®˜æ–¹æ–‡æ¡£ï¼‰ã€‚\n",
    "\n",
    "\n",
    "# service å‘½ä»¤çš„ç”¨æ³•\n",
    "$ sudo service docker start\n",
    "\n",
    "# systemctl å‘½ä»¤çš„ç”¨æ³•\n",
    "$ sudo systemctl start docker\n",
    "ä¸»è°“å®¾ä¸‰å…ƒç»„:\n",
    "(Docker, æ˜¯, å¼€æºçš„å•†ä¸šäº§å“)\n",
    "(Docker CE, æ”¯æŒ, Macã€Windows å’Œ Linux)\n",
    "(Docker, åŒ…å«, ä¸€äº›æ”¶è´¹æœåŠ¡)\n",
    "(Docker, éœ€è¦, ç”¨æˆ·å…·æœ‰sudoæƒé™)\n",
    "(Docker, æ˜¯, æœåŠ¡å™¨-å®¢æˆ·ç«¯æ¶æ„)\n",
    "---------------------\n",
    "\n",
    "ä¸‹é¢è¯·æ ¹æ®ä¹‹å‰çš„è¦æ±‚å’Œä¾‹å­ï¼Œå¼€å§‹çŸ¥è¯†æŠ½å–ä»»åŠ¡ï¼\n",
    "---------------------\n",
    "æ–‡æœ¬: {text}\n",
    "\n",
    "\n",
    "ä¸»è°“å®¾ä¸‰å…ƒç»„:\n",
    "\"\"\"\n",
    "\n",
    "KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    KG_TRIPLET_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")\n",
    "\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n",
    "    \"æ ¹æ®ä¸‹åˆ—è¦æ±‚å®Œæˆä»»åŠ¡ï¼Œä¸è¦å¿˜è®° <æ³¨æ„> çš„è¦æ±‚\\n\"\n",
    "    \"æ ¹æ®ç»™å®šçš„æ–‡æœ¬ï¼ŒæŠ½å–ä¸è¶…è¿‡ {max_keywords} ä¸ªå®ä½“åè¯å…³é”®è¯ï¼Œ\"\n",
    "    \"è¿™äº›å…³é”®è¯æ˜¯ä½œä¸ºé€‚åˆåœ¨çŸ¥è¯†å›¾è°±ä¸­è¿›è¡ŒæŸ¥è¯¢çš„å®ä½“ã€‚å¿½ç•¥æ–‡æœ¬ä¸­çš„åœæ­¢ç¬¦å·ã€‚\\n\"\n",
    "    \"<æ³¨æ„> å¦‚æœæœ‰è‹±æ–‡ï¼Œç»™å‡ºå¤šç§åˆç†çš„å¤§å°å†™æƒ…å†µçš„å…³é”®è¯ï¼Œæ¯”å¦‚å…³é”®è¯ Baseball parkï¼Œ\"\n",
    "    \"å¯èƒ½è¦ç»™å‡º 'KEYWORDS: Baseball park, Baseball Park'\\n\"\n",
    "    \"<æ³¨æ„> ä¸è¦è¶…å‡º {max_keywords} ä¸ªå…³é”®è¯\\n\"\n",
    "    \"<æ³¨æ„> åªè¿”å›è¦æ±‚çš„ KEYWORDS: å¼€å¤´ï¼Œç„¶åè‹±æ–‡é€—å·éš”å¼€çš„æ ¼å¼ï¼Œä¸å¸¦åºå·ã€æ¢è¡Œã€‚\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"ç°åœ¨è¿”å›å…¶ä¸­å¯èƒ½å¾—å…³é”®è¯ï¼Œä»¥è¿™æ ·çš„æ ¼å¼ --> 'KEYWORDS: keyword1, keyword2, keyword3'\\n\"\n",
    ")\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE = Prompt(\n",
    "    QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n",
    "    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f7daa4-7c27-4193-a33b-1bed3b9d7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "# cleanup NebulaGraph\n",
    "# %ngql clear space chinese_kg\n",
    "# or\n",
    "# graph_store.query(\"clear space chinese_kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c889f-5d43-49a7-a67e-378ad9e560cb",
   "metadata": {},
   "source": [
    "> This will be run only for the first time, afterwards, we will load from persist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf489ef8-d4d9-4399-9c0e-dc6eeb11cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 556\n",
      "-rw-rw-r-- 1 w w 458456 Jul 28 13:03 docstore.json\n",
      "-rw-rw-r-- 1 w w 102959 Jul 28 13:03 index_store.json\n",
      "-rw-rw-r-- 1 w w     51 Jul 28 13:03 vector_store.json\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "##clear graphdatabase\n",
    "##%ngql clear space chinese_kg\n",
    "\n",
    "graph_store.query(\"SHOW HOSTS\")\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    query_keyword_extract_template=QUERY_KEYWORD_EXTRACT_TEMPLATE,\n",
    "    max_knowledge_sequence=15,\n",
    ")\n",
    "\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "!ls -l storage_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c4ffd-4127-4695-b680-46e3b9ae8790",
   "metadata": {},
   "source": [
    "> This could be done after re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d5d1f9-9552-40c3-9e9e-78314241fd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context_graph = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context_graph,\n",
    "    max_triplets_per_chunk=5,\n",
    "    service_context=service_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    query_keyword_extract_template=QUERY_KEYWORD_EXTRACT_TEMPLATE,\n",
    "    max_knowledge_sequence=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516620b-72ef-440a-a50f-4c88f088bcf5",
   "metadata": {},
   "source": [
    "### 2. Vector Embedding and Indexing\n",
    "\n",
    "We will not leverage external VectorDB in this demo, but it's easy to switch to any Vector DB with Llama Index.\n",
    "\n",
    "We'll store and search embedding in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "159f890f-56c3-43f2-803e-f8655f0a0ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.27it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.98it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.73it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.68it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.41it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.49it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.02it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.61it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.45it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.09it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.29it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.65it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.53it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.20it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.70it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 54.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbf447-09fe-4459-b168-3f5faf2c3d8e",
   "metadata": {},
   "source": [
    "## QA on the knowledge base \n",
    "\n",
    "### 1. Create Graph and Vector RAG query engines\n",
    "\n",
    "- vector_query_engine, VectorDB RAG\n",
    "- kg_keyword_query_engine, NebulaGraph RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2816d6a5-654a-4807-a15a-c1fa7a2360f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    max_keywords_per_query=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86088f7-7acf-47be-bdd8-eb1b923aef8e",
   "metadata": {},
   "source": [
    "### 2. Query on Vector RAG vs Graph RAG\n",
    "\n",
    "é¦–å…ˆæ˜¯ä¼ ç»Ÿçš„ **Vector Search RAG** çš„ç»“æœï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "254b01d1-2f0b-4310-9c00-c0623fd1a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 57.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>ææ‹‰ç±³è‹çš„åˆ¶ä½œæ­¥éª¤å¦‚ä¸‹ï¼š\n",
       "\n",
       "1. å‡†å¤‡ææ–™å’Œå·¥å…·ï¼šé©¬æ–¯å¡å½­èŠå£«ï¼Œæ‰‹æŒ‡é¥¼å¹²ï¼Œæ”¾å‡‰æµ“ç¼©å’–å•¡ï¼Œæ— èŒé¸¡è›‹ï¼Œç™½ç ‚ç³–ï¼Œå¯å¯ç²‰ï¼Œæœ—å§†é…’ï¼Œä¸€ä¸ªè£…æˆå“çš„å®¹å™¨ã€‚\n",
       "\n",
       "2. è®¡ç®—ï¼šå°†é©¬æ–¯å¡å½­èŠå£«450å…‹ã€æ‰‹æŒ‡é¥¼å¹²1åŒ…ã€æ”¾å‡‰æµ“ç¼©å’–å•¡350mlã€æ— èŒé¸¡è›‹4ä¸ªã€ç™½ç ‚ç³–50gã€å¯å¯ç²‰10gã€æœ—å§†é…’35mlå‡†å¤‡å¥½ã€‚\n",
       "\n",
       "3. æ“ä½œï¼š\n",
       "\n",
       "- åˆ†ç¦»è›‹é»„è›‹æ¸…ã€‚\n",
       "- ç§°é‡40gç™½ç ‚ç³–ï¼ŒåŠ å…¥è›‹ç™½ç¢—ä¸­ï¼ŒåŠ 10gç™½ç ‚ç³–æ¹¿æ€§æ‰“å‘ã€‚\n",
       "- ç§°é‡60gç™½ç ‚ç³–ï¼ŒåŠ å…¥è›‹é»„ä¸­ï¼Œåˆ†ä¸‰æ¬¡åŠ å…¥é©¬æ–¯å¡å½­èŠå£«ï¼Œæ…æ‹Œè‡³å‡åŒ€ã€‚\n",
       "- è›‹é»„ä¸­åŠ å…¥æœ—å§†é…’ï¼Œæ…æ‹Œå‡åŒ€ã€‚\n",
       "- å°†æ‰“å‘å¥½çš„è›‹ç™½åˆ†ä¸‰æ¬¡åŠ å…¥è›‹é»„èŠå£«æ¶²ä¸­ã€‚\n",
       "- æ‰‹æŒ‡é¥¼å¹²ä¸¤é¢æµ¸æ¹¿å’–å•¡æ¶²ï¼Œå¹³é“ºå…¥å®¹å™¨ã€‚\n",
       "- ä¸¤å±‚èŠå£«æ¶²ä¸¤å±‚é¥¼å¹²äº¤æ›¿æ”¾å…¥å®¹å™¨ï¼ˆè¿™ä¸€æ­¥æŒ‰ç…§å¤§å®¶æ„æ„¿åŠå®¹å™¨é«˜åº¦é…Œæƒ…å¤„ç†ï¼‰ã€‚\n",
       "- æ”¾å…¥å†°ç®±å†·è—å››ä¸ªå°æ—¶ï¼ˆå¿ƒæ€¥çš„å°ä¼™ä¼´å¯ä»¥ææ—©æ‹¿å‡ºæ¥ï¼‰ã€‚\n",
       "- å–å‡ºååœ¨è¡¨é¢ç­›ä¸Šå¯å¯ç²‰ï¼Œå³å¯äº«ç”¨å•¦ã€‚\n",
       "\n",
       "4. ç™¾é¦™æœæ©™å­ç‰¹è°ƒï¼š\n",
       "\n",
       "- èŒ‰è‰ç»¿èŒ¶ç‰ˆæœ¬ï¼šå°†380æ¯«å‡å¼€æ°´å€’å…¥èŒ‰è‰ç»¿èŒ¶èŒ¶å¶ä¸­ï¼ŒåŠ å…¥æ©™å­1ä¸ªï¼ˆçº¦200å…‹ï¼Œæ‹³å¤´å¤§å°ï¼‰ï¼Œç§°é‡3~6å…‹èŒ‰è‰ç»¿èŒ¶èŒ¶å¶ï¼Œæ…æ‹Œå‡åŒ€ã€‚\n",
       "- è‹æ‰“æ°”æ³¡æ°´ç‰ˆæœ¬ï¼šå°†380æ¯«å‡è‹æ‰“æ°”æ³¡æ°´ä¸­åŠ å…¥æ©™å­1ä¸ªï¼ˆçº¦200å…‹ï¼Œæ‹³å¤´å¤§å°ï¼‰ï¼ŒåŠ å…¥å†°å—160å…‹ä»¥ä¸Šï¼Œæ…æ‹Œå‡åŒ€ã€‚\n",
       "\n",
       "æ ¹æ®ä»¥ä¸Šæ­¥éª¤ï¼Œæ‚¨å°±å¯ä»¥å°è¯•åˆ¶ä½œå‡ºç¾å‘³çš„ææ‹‰ç±³è‹å’Œç™¾é¦™æœæ©™å­ç‰¹è°ƒã€‚ç¥æ‚¨æˆåŠŸï¼</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"ææ‹‰ç±³è‹æ€ä¹ˆåšï¼Ÿ\")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42579ffb-14e8-4662-a85b-cecb0cadfe04",
   "metadata": {},
   "source": [
    "ç„¶åæ˜¯ **Graph RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a473735d-53cb-4ae7-8b3f-206046be3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: ææ‹‰ç±³è‹æ€ä¹ˆåš\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['æ€ä¹ˆåš', 'ææ‹‰ç±³è‹']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge sequence in 2-depth in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]` that may be related to the task.\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, åŒ…å«, 12å…‹\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, éœ€è¦, ç™½ç ‚ç³–\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, å±äº, å¯å¯†å°å®¹å™¨\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰, éœ€è¦, å¯å¯ç²‰\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ˜¯, ç”œç‚¹\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ˜¯, æ„å¤§åˆ©ä¼ ç»Ÿç”œå“\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ“ä½œ, ç››æœ‰è›‹ç™½çš„ç¢—ä¸­åŠ ç™½ç ‚ç³–æ¹¿æ€§æ‰“å‘\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, åŠ å…¥, ç™½ç ‚ç³–\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, æ— èŒé¸¡è›‹\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ˜¯, ç”œç‚¹, æ˜¯, ç”œç‚¹\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, æ”¾å‡‰æµ“ç¼©å’–å•¡\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, æœ—å§†é…’\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, æ‰‹æŒ‡é¥¼å¹²\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰, åŒ…å«, å¯å¯ç²‰\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ“ä½œ, åˆ†ç¦»è›‹é»„è›‹æ¸…\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, é©¬æ–¯å¡å½­èŠå£«\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, å’Œ, ç™½ç ‚ç³–\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ“ä½œ, ä¸¤å±‚èŠå£«æ¶²ä¸¤å±‚é¥¼å¹²äº¤æ›¿æ”¾å…¥å®¹å™¨\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, ç”¨äº, æ°´\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ“ä½œ, å°†æ‰“å‘å¥½çš„è›‹ç™½åˆ†ä¸‰æ¬¡åŠ å…¥è›‹é»„èŠå£«æ¶²ä¸­\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰\n",
      "ææ‹‰ç±³è‹ ææ‹‰ç±³è‹, æ“ä½œ, æ”¾å…¥å†°ç®±å†·è—å››ä¸ªå°æ—¶\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>ææ‹‰ç±³è‹æ˜¯ä¸€é“æ¥è‡ªæ„å¤§åˆ©çš„ä¼ ç»Ÿç”œç‚¹,åˆ¶ä½œæ­¥éª¤æ¯”è¾ƒå¤æ‚,ä½†æ˜¯å¤§è‡´æ­¥éª¤å¦‚ä¸‹:\n",
       "\n",
       "ææ–™:\n",
       "\n",
       "- 12å…‹ç™½ç ‚ç³–\n",
       "- å¯å¯ç²‰\n",
       "- éœ€è¦æ”¾å‡‰çš„æµ“ç¼©å’–å•¡\n",
       "- æœ—å§†é…’\n",
       "- æ— èŒé¸¡è›‹\n",
       "\n",
       "æ­¥éª¤:\n",
       "\n",
       "1. æŠŠå¯æ‹‰ç±³è‹é¥¼å¹²æ”¾å…¥ä¸€ä¸ªå¤§ç¢—ä¸­,åŠ å…¥12å…‹ç™½ç ‚ç³–,ç”¨æ‰“è›‹å™¨æˆ–è€…å‹ºå­æŠŠç™½ç ‚ç³–å‹ç¢,è®©ç³–ç²‰å……åˆ†æ¹¿æ¶¦,ç„¶ååŠ å…¥200æ¯«å‡æ·¡è‰²å¥¶æ²¹,ç»§ç»­ç”¨æ‰“è›‹å™¨æˆ–è€…å‹ºå­æ…æ‰“,ç›´åˆ°ç³–å’Œå¥¶æ²¹å……åˆ†æ··åˆ,ç¢—ä¸­çš„æ··åˆç‰©å˜å¾—å…‰æ»‘ã€‚\n",
       "\n",
       "2. æŠŠèåŒ–çš„ç™½å·§å…‹åŠ›åŠ å…¥ç¢—ä¸­çš„æ··åˆç‰©ä¸­,ç»§ç»­ç”¨æ‰“è›‹å™¨æˆ–è€…å‹ºå­æ…æ‹Œ,ç›´åˆ°ç™½å·§å…‹åŠ›å®Œå…¨èåŒ–å¹¶å’Œå¥¶æ²¹æ··åˆå‡åŒ€ã€‚\n",
       "\n",
       "3. æŠŠè½¯åŒ–çš„é¸¡è›‹é»„åŠ å…¥ç¢—ä¸­çš„æ··åˆç‰©ä¸­,ç»§ç»­ç”¨æ‰“è›‹å™¨æˆ–è€…å‹ºå­æ…æ‹Œ,ç›´åˆ°é¸¡è›‹é»„å®Œå…¨èå…¥æ··åˆç‰©ä¸­,å½¢æˆä¸€ä¸ªå‡åŒ€çš„æ··åˆç‰©ã€‚\n",
       "\n",
       "4. æŠŠèåŒ–çš„é©¬æ–¯å¡å½­èŠå£«åŠ å…¥ç¢—ä¸­çš„æ··åˆç‰©ä¸­,ç»§ç»­ç”¨æ‰“è›‹å™¨æˆ–è€…å‹ºå­æ…æ‹Œ,ç›´åˆ°é©¬æ–¯å¡å½­èŠå£«å®Œå…¨èåŒ–å¹¶å’Œæ··åˆç‰©æ··åˆå‡åŒ€ã€‚\n",
       "\n",
       "5. æŠŠå’–å•¡å€’å…¥æ¯å­ä¸­,æ”¾å‡‰å››ä¸ªå°æ—¶ã€‚\n",
       "\n",
       "6. æ‹¿å‡ºææ‹‰ç±³è‹,å°†å…¶å–å‡º,åˆ‡æˆè–„ç‰‡,å³å¯äº«ç”¨ã€‚\n",
       "\n",
       "è¯·æ³¨æ„,è¿™åªæ˜¯ä¸€ä¸ªå¤§è‡´çš„æ­¥éª¤,å®é™…åˆ¶ä½œè¿‡ç¨‹ä¸­è¿˜éœ€è¦æ ¹æ®ä¸ªäººå£å‘³å’Œä¹ æƒ¯è¿›è¡Œè°ƒæ•´ã€‚</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph = kg_keyword_query_engine.query(\"ææ‹‰ç±³è‹æ€ä¹ˆåš\")\n",
    "display(Markdown(f\"<b>{response_graph}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bf56e-da3e-4990-8db5-3509d5ece2f3",
   "metadata": {},
   "source": [
    "ğŸ”” å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨ Retrieval é˜¶æ®µï¼ŒGraph RAG æœé›†åˆ°çš„çŸ¥è¯†å¦‚ä¸‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49071132-8019-4bf8-ae51-bf1723f59cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ææ‹‰ç±³è‹': ['ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, åŒ…å«, 12å…‹',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, éœ€è¦, ç™½ç ‚ç³–',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, å±äº, å¯å¯†å°å®¹å™¨',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰, éœ€è¦, å¯å¯ç²‰',\n",
       "  'ææ‹‰ç±³è‹, æ˜¯, ç”œç‚¹',\n",
       "  'ææ‹‰ç±³è‹, æ˜¯, æ„å¤§åˆ©ä¼ ç»Ÿç”œå“',\n",
       "  'ææ‹‰ç±³è‹, æ“ä½œ, ç››æœ‰è›‹ç™½çš„ç¢—ä¸­åŠ ç™½ç ‚ç³–æ¹¿æ€§æ‰“å‘',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, åŠ å…¥, ç™½ç ‚ç³–',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, æ— èŒé¸¡è›‹',\n",
       "  'ææ‹‰ç±³è‹, æ˜¯, ç”œç‚¹, æ˜¯, ç”œç‚¹',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, æ”¾å‡‰æµ“ç¼©å’–å•¡',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, æœ—å§†é…’',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, æ‰‹æŒ‡é¥¼å¹²',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰, åŒ…å«, å¯å¯ç²‰',\n",
       "  'ææ‹‰ç±³è‹, æ“ä½œ, åˆ†ç¦»è›‹é»„è›‹æ¸…',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, é©¬æ–¯å¡å½­èŠå£«',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, å’Œ, ç™½ç ‚ç³–',\n",
       "  'ææ‹‰ç±³è‹, æ“ä½œ, ä¸¤å±‚èŠå£«æ¶²ä¸¤å±‚é¥¼å¹²äº¤æ›¿æ”¾å…¥å®¹å™¨',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, ç™½ç ‚ç³–, ç”¨äº, æ°´',\n",
       "  'ææ‹‰ç±³è‹, æ“ä½œ, å°†æ‰“å‘å¥½çš„è›‹ç™½åˆ†ä¸‰æ¬¡åŠ å…¥è›‹é»„èŠå£«æ¶²ä¸­',\n",
       "  'ææ‹‰ç±³è‹, éœ€è¦, å¯å¯ç²‰',\n",
       "  'ææ‹‰ç±³è‹, æ“ä½œ, æ”¾å…¥å†°ç®±å†·è—å››ä¸ªå°æ—¶']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(response_graph.metadata.values())[0]['kg_rel_map']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53445e1-c460-465e-82e6-6d47ec9ed80f",
   "metadata": {},
   "source": [
    "### 3. Hallucination results in Vector RAG\n",
    "\n",
    "Let's ask something that it shouldn't know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b17c6203-75bf-4e59-a747-7bd22d73c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>ç¿¡ç¿ ç™½ç‰æ±¤æ˜¯ä¸€é“è‰²é¦™å‘³ä¿±ä½³çš„èœè‚´ï¼Œé€šå¸¸ä½¿ç”¨æ–°é²œçš„ç¿¡ç¿ å’Œç™½ç‰æ¥åˆ¶ä½œã€‚ä»¥ä¸‹æ˜¯ç¿¡ç¿ ç™½ç‰æ±¤çš„ç®€è¦åšæ³•ï¼š\n",
       "\n",
       "æ‰€éœ€ææ–™ï¼š\n",
       "1. ç¿¡ç¿ ï¼š200 å…‹\n",
       "2. ç™½ç‰ï¼š200 å…‹\n",
       "3. é¸¡èƒ¸è‚‰ï¼š250 å…‹\n",
       "4. å§œç‰‡ï¼šé€‚é‡\n",
       "5. è‘±æ®µï¼šé€‚é‡\n",
       "6. æ–™é…’ï¼šé€‚é‡\n",
       "7. ç›ï¼šé€‚é‡\n",
       "8. æ¸…æ°´ï¼šé€‚é‡\n",
       "\n",
       "æ­¥éª¤ï¼š\n",
       "\n",
       "1.å°†ç¿¡ç¿ ã€ç™½ç‰åˆ‡æˆè–„ç‰‡ï¼Œé¸¡èƒ¸è‚‰åˆ‡æˆå°ä¸ã€‚\n",
       "2.çƒ­é”…å‡‰æ²¹ï¼ŒåŠ å…¥å§œç‰‡ã€è‘±æ®µï¼Œç…¸ç‚’å‡ºé¦™å‘³ååŠ å…¥é¸¡èƒ¸è‚‰ä¸ç…¸ç‚’è‡³å˜è‰²ã€‚\n",
       "3.åŠ å…¥é€‚é‡çš„æ–™é…’ï¼Œç…®è‡³é¦™å‘³æŒ¥å‘ï¼Œç„¶ååŠ å…¥é€‚é‡çš„æ¸…æ°´ï¼Œæ”¾å…¥ç¿¡ç¿ ã€ç™½ç‰ç‰‡ï¼ŒåŠ å…¥é€‚é‡çš„ç›ï¼Œç…®è‡³é£Ÿæç†Ÿé€ï¼Œæå‡ºå¤‡ç”¨ã€‚\n",
       "4.æœ€åï¼Œå°†ç¿¡ç¿ ã€ç™½ç‰ç‰‡å’Œç…®å¥½çš„é¸¡èƒ¸è‚‰å€’å…¥å¦ä¸€ä¸ªé”…ä¸­ï¼ŒåŠ å…¥é€‚é‡çš„é¸¡æ¸…æ±¤ï¼Œç…®è‡³æ±¤æ±æµ“ç¨ å³å¯ã€‚\n",
       "5.å°†è°ƒå¥½çš„ç¿¡ç¿ ç™½ç‰æ±¤ç››å…¥ç¢—ä¸­ï¼Œæ’’ä¸Šä¸€äº›é¦™èœæˆ–è€…è‘±èŠ±ä½œä¸ºè£…é¥°å³å¯ã€‚\n",
       "\n",
       "ç¿¡ç¿ ç™½ç‰æ±¤çš„åšæ³•ç®€å•ï¼Œå£æ„Ÿé²œç¾ï¼Œé€‚åˆæ­é…ç±³é¥­æˆ–è€…ç›´æ¥ä½œä¸ºå°èœã€‚</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"ç¿¡ç¿ ç™½ç‰æ±¤æ€ä¹ˆåšï¼Ÿ\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2201da-aea0-458d-a67c-4f68f14a2f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>é»¯ç„¶é”€é­‚é¥­æ˜¯ä¸€é“è‘—åçš„ç²¤èœï¼Œä¸»è¦ææ–™åŒ…æ‹¬å¤§é²é±¼ã€ç™½åˆ‡é¸¡ã€ç‘¶æŸ±ã€ èŠ±è‡ã€è‰è‡ã€æµ·é²œç­‰ã€‚åˆ¶ä½œè¿‡ç¨‹éœ€è¦ä¸¥æ ¼æ§åˆ¶ç«å€™å’Œæ—¶é—´ï¼Œä»¥ä¿è¯ç±³é¦™æµ“éƒã€å£æ„Ÿç»†è…»ã€‚ä¸‹é¢æ˜¯é»¯ç„¶é”€é­‚é¥­çš„åˆ¶ä½œæ­¥éª¤ï¼š\n",
       "\n",
       "ææ–™ï¼š\n",
       "\n",
       "- å¤§é²é±¼ 200 å…‹\n",
       "- ç™½åˆ‡é¸¡ 200 å…‹\n",
       "- ç‘¶æŸ± 200 å…‹\n",
       "- èŠ±è‡ 200 å…‹\n",
       "- è‰è‡ 200 å…‹\n",
       "- æµ·é²œ 200 å…‹\n",
       "- å¤§ç±³ 2 æ¯\n",
       "- å› ä¸ºè¿™ä¸ªèœè¦ç…®å¾ˆä¹…ï¼Œæ‰€ä»¥éœ€è¦å‡†å¤‡è¶³å¤Ÿçš„ææ–™\n",
       "\n",
       "æ­¥éª¤ï¼š\n",
       "\n",
       "1. ç‘¶æŸ±ã€èŠ±è‡ã€è‰è‡ã€æµ·é²œæ´—å‡€ï¼Œåˆ‡æˆå°å—ï¼Œå¤‡ç”¨ã€‚\n",
       "\n",
       "2. å¤§é²é±¼å’Œç™½åˆ‡é¸¡åˆ‡æˆå°å—ï¼Œå¤‡ç”¨ã€‚\n",
       "\n",
       "3. å¤§ç±³æ´—å¹²å‡€ï¼Œå¤‡ç”¨ã€‚\n",
       "\n",
       "4. é”…ä¸­åŠ å…¥è¶³å¤Ÿçš„æ°´ï¼Œå°†å¤§ç±³æ”¾å…¥é”…ä¸­ï¼Œå¤§ç«ç…®å¼€åè½¬å°ç«æ…¢æ…¢ç…®çƒ‚ã€‚\n",
       "\n",
       "5. åŠ å…¥ç‘¶æŸ±ã€èŠ±è‡ã€è‰è‡ã€æµ·é²œï¼Œç»§ç»­ç…®5-10åˆ†é’Ÿï¼Œç›´åˆ°æ‰€æœ‰ææ–™ç…®çƒ‚ã€‚\n",
       "\n",
       "6. åŠ å…¥å¤§é²é±¼å’Œç™½åˆ‡é¸¡ï¼Œç”¨ç­·å­è½»è½»æ…æ‹Œï¼Œç…®5-10åˆ†é’Ÿï¼Œç›´åˆ°é²é±¼å’Œé¸¡è‚‰ç†Ÿé€ã€‚\n",
       "\n",
       "7. æœ€ååŠ å…¥é€‚é‡çš„ç›å’Œèƒ¡æ¤’ç²‰ï¼Œå³å¯äº«ç”¨ã€‚\n",
       "\n",
       "æ³¨æ„äº‹é¡¹ï¼š\n",
       "\n",
       "1. ç…®çš„æ—¶å€™è¦ä¸€ç›´å¼€ç€å°ç«ï¼Œä»¥å…ç…®è¿‡å¤´ã€‚\n",
       "\n",
       "2. åŠ å…¥æµ·é²œå’Œç‘¶æŸ±ç­‰æµ·é²œé£Ÿæï¼Œç…®çš„æ—¶é—´è¦ç¨å¾®é•¿ä¸€äº›ï¼Œä»¥ç¡®ä¿æµ·é²œç†Ÿé€ã€‚\n",
       "\n",
       "3. å¤§ç±³ç…®å¥½åï¼Œç…®çš„æ—¶å€™ä¸€å®šè¦ç”¨ä¸­å°ç«ï¼Œä»¥å…å¤§ç±³ç…®çƒ‚ç³Šé”…ã€‚\n",
       "\n",
       "4. æœ€ååŠ å…¥é€‚é‡çš„ç›å’Œèƒ¡æ¤’ç²‰ï¼Œå¯æ ¹æ®å£å‘³è°ƒæ•´ã€‚</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a5aac-f2af-4a18-b23a-73385f6572fc",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ° Vector æœç´¢çš„ Chunk æ˜æ˜æ˜¯ä¸ç›¸å¹²çš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "482ef13b-af99-427a-88cc-61a69ca5e3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ç³–é†‹æ±çš„åšæ³•\n",
      "\n",
      "ç³–é†‹æ±é€šå¸¸æƒ…å†µä¸‹ç”±æ¸…æ°´ã€ç™½ç³–ã€ç™½é†‹ç­‰åˆ¶æˆï¼Œæœ‰äº›äººå–œæ¬¢æ”¾ä¸€äº›ç•ªèŒ„é…±æ¥å¢æ·»ä¸ä¸€æ ·çš„é…¸ç”œå‘³æˆ–æ”¾ä¸€äº›æ·€ç²‰æ¥å¢åŠ èœè‚´æ±¤æ±çš„ç²˜æ€§å’Œæµ“åº¦ï¼Œç³–é†‹æ±å¯ç”¨äºç³–é†‹é±¼ã€ç³–é†‹é‡Œè„Šã€ç³–é†‹æ’éª¨ç­‰èœå“çš„åˆ¶ä½œ\n",
      "\n",
      "å¯ä¾æ®ç³–é†‹æ±é…åˆ¶çš„ç»å…¸æ¯”ä¾‹ 1ï¼š2ï¼š3ï¼š4ï¼š5 æ¥è°ƒåˆ¶ç³–é†‹æ±\n",
      "\n",
      "## å¿…å¤‡åŸæ–™å’Œå·¥å…·\n",
      "\n",
      "- æ¸…æ°´\n",
      "- ç™½ç³–\n",
      "- ç™½é†‹/ç±³é†‹\n",
      "- æ–™é…’\n",
      "- ç”ŸæŠ½\n",
      "\n",
      "## è®¡ç®—\n",
      "\n",
      "- æ¸…æ°´(50ml)\n",
      "- ç”ŸæŠ½(40ml)\n",
      "- ç™½ç³–(30g)\n",
      "- ç™½é†‹(20ml)\n",
      "- æ–™é…’(10ml)\n",
      "\n",
      "## æ“ä½œ\n",
      "\n",
      "- æŒ‰ç…§æ¯”ä¾‹å°†å„è°ƒæ–™åœ¨å°ç¢—ä¸­æ…æ‹Œå‡åŒ€\n",
      "- æŒ‰ä¸åŒèœè‚´çš„æ–¹å¼å¤„ç†å®Œæ¯•åï¼Œå°†é…åˆ¶å¥½çš„ç³–é†‹æ±å€’å…¥é”…ä¸­\n",
      "- æ ¹æ®å„èœè‚´çš„ä¸åŒï¼Œçƒ¹åˆ¶ 5-10 åˆ†é’Ÿ\n",
      "- å¤§ç«æ”¶æ±ï¼Œå¯å¢åŠ èœçš„æµ“åº¦ã€é¦™å‘³å’Œå…‰æ³½\n",
      "\n",
      "## é™„åŠ å†…å®¹\n",
      "\n",
      "å¦‚æœæ‚¨éµå¾ªæœ¬æŒ‡å—çš„åˆ¶ä½œæµç¨‹è€Œå‘ç°æœ‰é—®é¢˜æˆ–å¯ä»¥æ”¹è¿›çš„æµç¨‹ï¼Œè¯·æå‡º Issue æˆ– Pull request ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(vector_query_engine.retrieve(\"ç¿¡ç¿ ç™½ç‰æ±¤æ€ä¹ˆåšï¼Ÿ\")[0].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39bbe5b8-9d9b-4b17-9fab-982d4cda4c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è°ƒè‡³å¤§ç«æ”¶æ±ï¼Œæ±¤æ±å‰©ä½™ 1/3 æ—¶ï¼Œå…³ç«ç››è‡³å°ç›†ä¸­ã€‚\n",
      "* æ³¨ï¼šå°†é”…ä¸­çš„æ±¤æ±å‡åŒ€æ·‹åˆ°é±¼å¤´ä¸Šï¼Œç››ç›˜æ—¶å¯ä»¥å°†é”…ä¸­ç…®çš„é¦™èœæ”¾å…¥å°ç›†åº•éƒ¨ï¼Œè¿™æ ·èƒ½è®©æˆå“èœå¥½çœ‹åˆå¥½åƒã€‚\n",
      "* å°†é¦™èœæ”¾è‡³å·²ç»ç››å‡ºçš„é±¼å¤´ä¸Šï¼ŒæŠŠåˆ‡å¥½çš„ç¾äººæ¤’åœˆæ”¾åœ¨é¦™èœä¹‹ä¸Šã€‚\n",
      "* è‰²é¦™å‘³ä¿±å…¨çš„çº¢çƒ§é±¼å¤´å‡ºç‚‰ï¼\n",
      "\n",
      "## é™„åŠ å†…å®¹\n",
      "\n",
      "å¦‚æœæ‚¨éµå¾ªæœ¬æŒ‡å—çš„åˆ¶ä½œæµç¨‹è€Œå‘ç°æœ‰é—®é¢˜æˆ–å¯ä»¥æ”¹è¿›çš„æµç¨‹ï¼Œè¯·æå‡º Issue æˆ– Pull request ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(vector_query_engine.retrieve(\"é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\")[0].node.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6511620-443b-434d-b48e-7f25970c9186",
   "metadata": {},
   "source": [
    "ğŸ’¡ ä½†æ˜¯åœ¨ KG Query ä¸­ï¼Œè¿™ä¸ªå¹»è§‰ä¸ä¼šå‡ºç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2563b284-b8fd-4089-8600-1b3e58858856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['æ€ä¹ˆåš?', 'é»¯ç„¶é”€é­‚é¥­']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = kg_keyword_query_engine.query(\"é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ae42-c170-4486-8c3c-2b226c0c2455",
   "metadata": {},
   "source": [
    "### 4. Hallucination mitigation with VectorSearch and Knowledge Graph RAG\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬ç›´æ¥çœ‹ç»“æœï¼Œé»¯ç„¶é”€é­‚é¥­è¿™ä¸ªé£Ÿè°±é‡Œä¸å­˜åœ¨çš„èœå“è¢«æ’é™¤æ‰äº†ã€‚\n",
    "\n",
    "> ğŸ‘‡ æ³¨æ„ï¼Œè¿™é‡Œçš„ graph_vector_rag_query_engine æˆ‘åœ¨ä¹‹åçš„éƒ¨åˆ†å®šä¹‰ï¼Œæ‰§è¡Œçš„æ—¶å€™éœ€è¦å…ˆæ‰§è¡Œåè¾¹çš„ cell æ‰èƒ½æ‰§è¡Œè¿™ä¸ª Queryã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05879e35-cd36-40a8-8344-249b49b4c91d",
   "metadata": {},
   "source": [
    "#### 4.1 Hallucination Mitigation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83b4f6f9-4b6d-489d-bc9b-a052c85531e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['å…³é”®å­—: é»¯ç„¶é”€é­‚', 'é¥­', 'æ€ä¹ˆåš', 'é»¯ç„¶é”€é­‚', 'å…³é”®å­—']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:debug hallucination chunk detected, will be removed.\n",
      " Chunk: # å¾®æ³¢è‘±å§œé»‘é³•é±¼çš„åšæ³•\n",
      "\n",
      "è¿™é“èœæ”¹ç¼–è‡ªè¥¿é›…å›¾ Veil é¤å…ä¸»å¨ Johnny Zhu çš„æ¯äº² Margaret Lu çš„èœè°±ã€‚å¢å¥³å£«åŸèœè°±æ˜¯ä½¿ç”¨ç½—éé±¼æ¥åšè¿™é“èœï¼ŒJohnny æ”¹ä¸ºé³•é±¼ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨å¤§æ¯”ç›®é±¼é±¼æ’ï¼Œæˆ–è€…æµ·é²ˆé±¼ã€é³Ÿé±¼ç­‰ã€‚æ¯ç§é±¼çš„å¯†åº¦æœ‰å·®åˆ«ï¼Œçƒ¹é¥ªæ—¶é—´è¦åšå¾®è°ƒã€‚\n",
      "\n",
      "## å¿…å¤‡åŸæ–™å’Œå·¥å…·\n",
      "\n",
      "åŸæ–™ï¼š\n",
      "\n",
      "- é»‘é³•é±¼ï¼Œå¸¦çš®\n",
      "\n",
      "è°ƒå‘³æ–™ï¼š\n",
      "\n",
      "- é’è‘±\n",
      "- å§œ\n",
      "- æ–™é…’\n",
      "- é…±æ²¹\n",
      "- èŠéº»æ²¹\n",
      "- èŠ±ç”Ÿæ²¹\n",
      "\n",
      "å·¥å…·ï¼š\n",
      "\n",
      "- å¯†å°è¢‹\n",
      "\n",
      "## è®¡ç®—\n",
      "\n",
      "æ¯ 2 ä»½ï¼š\n",
      "\n",
      "- é»‘é³•é±¼ï¼Œå¸¦çš®ï¼Œ2 ç‰‡ï¼Œ450gï¼ˆæœ¬èœè°±ä¸»è§’ï¼Œæ‰€æœ‰è°ƒæ–™å¯æ ¹æ®é³•é±¼çš„å®é™…é‡é‡è¿›è¡Œæ¯”ä¾‹è°ƒæ•´ï¼‰\n",
      "- é’è‘±ï¼Œè‘±ç™½ï¼Œ25gã€‚\n",
      "- é’è‘±ï¼Œè‘±ç»¿ï¼Œ10gã€‚\n",
      "- å§œï¼Œ13gã€‚\n",
      "- æ–™é…’ï¼Œ5mLã€‚\n",
      "- é…±æ²¹ï¼Œ25mLã€‚\n",
      "- èŠéº»æ²¹ï¼Œ2mLã€‚\n",
      "- èŠ±ç”Ÿæ²¹ï¼Œ50mLã€‚\n",
      "\n",
      "## æ“ä½œ\n",
      "\n",
      "- é±¼ç‰‡åˆ†åˆ«æ”¾å…¥å¯†å°è¢‹ï¼Œé±¼çš®å‘ä¸‹æ”¾åœ¨ç›˜å­ä¸­ã€‚\n",
      "- å–è‘±ç™½åˆ‡ä¸ 25gï¼Œå§œå»çš®ååˆ‡ä¸ï¼Œ10gï¼Œæ··åˆåœ¨ä¸€èµ·ååˆ†æˆä¸¤åŠï¼Œåˆ†åˆ«æ”¾åœ¨è¢‹å†…é±¼ç‰‡ä¸Šã€‚\n",
      "- æ¯ä¸ªè¢‹å­å€’å…¥ 2.5mL æ–™é…’ã€‚\n",
      "- å°å¥½å¯†å°è¢‹ï¼Œæ”¾å…¥å¾®æ³¢ç‚‰ä¸­ï¼Œä¸­ç«ï¼ˆ800 ç“¦ï¼‰å¾®æ³¢è‡³*ä¸é€æ˜ä¸”å®¹æ˜“æ•£å¼€*æ—¶ï¼ˆçº¦ 3.5-5 åˆ†é’Ÿï¼‰ï¼Œä»è¢‹ä¸­å–å‡ºé±¼ç‰‡ã€‚\n",
      "- å»é™¤é’è‘±å’Œå§œã€‚\n",
      "- å–é…±æ²¹ 25mLï¼ŒèŠéº»æ²¹ 2mLï¼Œæ··åˆå‡åŒ€åå¹³å‡æ·‹åœ¨ä¸¤ç‰‡é±¼ç‰‡ä¸Šã€‚\n",
      "- å–è‘±ç»¿åˆ‡ç»†ä¸ 10gï¼Œå§œå»çš®ååˆ‡ä¸ 3gï¼Œæ··åˆååˆ†æˆä¸¤ä»½æ’’åœ¨é±¼ç‰‡ä¸Šã€‚\n",
      "-\n",
      "INFO:__main__:debug hallucination chunk detected, will be removed.\n",
      " Chunk: è°ƒè‡³å¤§ç«æ”¶æ±ï¼Œæ±¤æ±å‰©ä½™ 1/3 æ—¶ï¼Œå…³ç«ç››è‡³å°ç›†ä¸­ã€‚\n",
      "* æ³¨ï¼šå°†é”…ä¸­çš„æ±¤æ±å‡åŒ€æ·‹åˆ°é±¼å¤´ä¸Šï¼Œç››ç›˜æ—¶å¯ä»¥å°†é”…ä¸­ç…®çš„é¦™èœæ”¾å…¥å°ç›†åº•éƒ¨ï¼Œè¿™æ ·èƒ½è®©æˆå“èœå¥½çœ‹åˆå¥½åƒã€‚\n",
      "* å°†é¦™èœæ”¾è‡³å·²ç»ç››å‡ºçš„é±¼å¤´ä¸Šï¼ŒæŠŠåˆ‡å¥½çš„ç¾äººæ¤’åœˆæ”¾åœ¨é¦™èœä¹‹ä¸Šã€‚\n",
      "* è‰²é¦™å‘³ä¿±å…¨çš„çº¢çƒ§é±¼å¤´å‡ºç‚‰ï¼\n",
      "\n",
      "## é™„åŠ å†…å®¹\n",
      "\n",
      "å¦‚æœæ‚¨éµå¾ªæœ¬æŒ‡å—çš„åˆ¶ä½œæµç¨‹è€Œå‘ç°æœ‰é—®é¢˜æˆ–å¯ä»¥æ”¹è¿›çš„æµç¨‹ï¼Œè¯·æå‡º Issue æˆ– Pull request ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = graph_vector_rag_query_engine.query(\"é»¯ç„¶é”€é­‚é¥­æ€ä¹ˆåšï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487221-ba94-4e32-a618-33bf974d6f41",
   "metadata": {},
   "source": [
    "> ğŸ’¡ ç­”æ¡ˆæ˜¯ None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2fdbfe6-c857-48a8-846b-3a938b5ecf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbd1a061-a639-4aa6-a4f9-8bf328a52e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector index æœç´¢å¾—åˆ°çš„ chunk è¢«åˆ¤æ–­ä¸ºå¹»è§‰è€Œåˆ æ‰äº†\n",
    "len(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600c163-d301-42f0-be12-37565d5e96b8",
   "metadata": {},
   "source": [
    "#### 4.2 Impl. of the KG crosscheck query engine\n",
    "\n",
    "This is the impl. of the cross if empty query engine, when one of the retrievers got empty results, the other retriever will have information being fact-checked by LLM, if it's actually not related, we remove them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb11600f-41a4-462c-85f2-4dda4a0e4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "UNION = \"union\"\n",
    "KG_FIRST = \"kg_first\"\n",
    "CROSS_IF_EMPTY = \"cross_if_empty\"\n",
    "\n",
    "CROSS_CHECK_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a fact checker, now I will put a piece of context and a question, and you will check step by step on whether it's actually related or not, responding only \"Yes\" or \"No\".\n",
    "\n",
    "For example, the context that's only partially related but actually there are details that could tell it should be uncorrelated, respond No\n",
    "Do not add explanations, apologies, or any other things than Yes or No\n",
    "\n",
    "Example:\n",
    "In this example, although ä¿æ¸©æ¯ is related to ä¿æ¸© in some sense, the question is not about æ¯, thus from reasonable justification, it's NOT related.\n",
    "\n",
    "context:\n",
    "---\n",
    "ä¿æ¸©æ¯æ˜¯å†¬å¤©å¤–å‡ºå¿…å¤‡è‰¯å“\n",
    "---\n",
    "question:\n",
    "---\n",
    "ä¿æ¸©å¤§æ£šæ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "---\n",
    "related:\n",
    "No\n",
    "\n",
    "Now check this with reasonable justification!\n",
    "\n",
    "context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "question:\n",
    "---\n",
    "{question_str}\n",
    "---\n",
    "related:\n",
    "\"\"\"\n",
    "\n",
    "CROSS_CHECK_PROMPT_TEMPLATE_CHATGLM = \"\"\"\n",
    "\n",
    "ä½ æ˜¯ä¸€ä¸ªäº‹å®æ ¸æŸ¥å‘˜ï¼Œç°åœ¨æˆ‘ä¼šæä¾›ä¸€æ®µèƒŒæ™¯å’Œä¸€ä¸ªé—®é¢˜ï¼Œç„¶åä½ å°†é€æ­¥æ£€æŸ¥å®ƒä»¬æ˜¯å¦ç›¸å…³ï¼Œå¹¶åªå›ç­”\"Yes\"ï¼Œè¡¨ç¤ºå¤§æ¦‚ç‡æ˜¯ç›¸å…³çš„ã€æˆ–\"No\"ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè™½ç„¶ä¿æ¸©æ¯ä»æŸç§æ„ä¹‰ä¸Šä¸ä¿æ¸©æœ‰å…³ï¼Œä½†é—®é¢˜å¹¶ä¸æ¶‰åŠæ¯å­ï¼Œå› æ­¤ä»è¿™ä¸ªä¸åˆç†æ€§å¾—çŸ¥å®ƒä»¬å®é™…ä¸Šä¸ç›¸å…³ã€‚\n",
    "\n",
    "context:\n",
    "---\n",
    "ä¿æ¸©æ¯æ˜¯å†¬å¤©å¤–å‡ºå¿…å¤‡è‰¯å“\n",
    "---\n",
    "question:\n",
    "---\n",
    "ä¿æ¸©å¤§æ£šæ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "---\n",
    "related:\n",
    "No\n",
    "\n",
    "ç°åœ¨å¼€å§‹ä»”ç»†æ£€æŸ¥ï¼Œé€šè¿‡åˆç†æ€§éªŒè¯åˆ¤æ–­æ˜¯å¦çœŸæ­£ç›¸å…³\n",
    "\n",
    "context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "question:\n",
    "---\n",
    "{query_str}\n",
    "---\n",
    "related:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class KGVectorCrosscheckRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that performs both Vector search and Knowledge Graph search, and cross-checks to mitigate hallucination\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = UNION,\n",
    "        cross_check_propmpt_template: str = CROSS_CHECK_PROMPT_TEMPLATE_CHATGLM,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (UNION, KG_FIRST, CROSS_IF_EMPTY):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        self._service_context = service_context or ServiceContext.from_defaults()\n",
    "        self._cross_check_prompt_template = cross_check_propmpt_template\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "        if self._mode == KG_FIRST and len(kg_nodes) > 0:\n",
    "            # bypass KG retrieval\n",
    "            vector_nodes = []\n",
    "        else:\n",
    "            vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        # no matter UNION, KG_FIRST or CROSS_IF_EMPTY, we need to union them first\n",
    "        retrieve_ids = vector_ids.union(kg_ids)\n",
    "        # In case CROSS_IF_EMPTY and one of the retrieval got nothing, do fact-check\n",
    "        # to avoid hallucinations\n",
    "        one_of_the_retrieval_failed = len(retrieve_ids) > 0 and (\n",
    "            len(vector_ids) == 0 or len(kg_ids) == 0)\n",
    "        if self._mode == CROSS_IF_EMPTY and one_of_the_retrieval_failed:\n",
    "            retrieve_ids_copy = retrieve_ids.copy()\n",
    "            for node_id in retrieve_ids_copy:\n",
    "                node = combined_dict[node_id]\n",
    "                response = self._service_context.llm_predictor.predict(\n",
    "                    self._cross_check_prompt_template,\n",
    "                    context=node.node.get_content(),\n",
    "                    query_str=query_bundle,\n",
    "                )\n",
    "\n",
    "                if \"yes\" not in str(response).lower():\n",
    "                    logger.info(f\"debug hallucination chunk detected, will be removed.\\n Chunk: {combined_dict[node_id].node.get_text()}\")\n",
    "                    retrieve_ids.remove(node_id)\n",
    "\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# option a\n",
    "# create raw retrievers from index\n",
    "# vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "# kg_retriever = KGTableRetriever(\n",
    "#     index=kg_index,\n",
    "#     retriever_mode=\"keyword\",\n",
    "#     include_text=False, \n",
    "#     max_keywords_per_query=3,\n",
    "# )\n",
    "\n",
    "# option b from query engine\n",
    "vector_retriever = vector_query_engine._retriever\n",
    "kg_retriever = kg_keyword_query_engine._retriever\n",
    "\n",
    "combined_retriever = KGVectorCrosscheckRetriever(vector_retriever, kg_retriever, mode=\"cross_if_empty\")\n",
    "\n",
    "# create a response synthesizer\n",
    "# response_synthesizer = get_response_synthesizer(\n",
    "#     service_context=service_context,\n",
    "# )\n",
    "response_synthesizer = vector_query_engine._response_synthesizer or kg_keyword_query_engine._response_synthesizer\n",
    "\n",
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=combined_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cb16f-6d0e-494e-80b2-4eeff6570362",
   "metadata": {},
   "source": [
    "### 5. Compare Graph and Vector RAG Result\n",
    "\n",
    "> *experimental*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7bf65de-7d07-4668-82c6-727ae74d73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_graph_and_vector(q):\n",
    "    graph_store.query(\"show hosts\")\n",
    "    response_graph_rag = vector_query_engine.query(q)\n",
    "    \n",
    "    response_vector_rag = kg_keyword_query_engine.query(q)\n",
    "    \n",
    "    display(\n",
    "        Markdown(\n",
    "            llm(f\"\"\"\n",
    "æ¯”è¾ƒä¸¤ä¸ªå…³äº \"{q}\" çš„é—®ç­”ç»“æœã€‚\n",
    "1. æœ€ç»ˆå°†ç»“æœè¾“å‡ºä¸º markdownï¼Œè¯„ä¼°ç»“æœå·®å¼‚çš„éƒ¨åˆ†è¾“å‡ºä¸º markdown è¡¨æ ¼ï¼›\n",
    "2. è¡¨æ ¼ç¬¬ä¸€åˆ—æ˜¯æ–¹æ³•ï¼Œä¸€å…±ä¸¤è¡Œï¼Œç¬¬ä¸€è¡Œä¸º â€œåŸºäº Graph_RAGâ€ï¼Œç¬¬äºŒè¡Œä¸ºâ€œåŸºäº Vector_DBâ€\n",
    "2. è¡¨æ ¼ç¬¬äºŒåˆ—ä¸­ï¼Œæ¯ä¸€è¡Œåˆ—å‡ºè¦æ¯”è¾ƒçš„ç»“æœï¼Œç¬¬ä¸‰åˆ—æŠŠç»“æœæ‹†è§£æˆç»“æœçš„ä¸»è¦è¦ç‚¹ï¼›\n",
    "3. ç¬¬ä¸‰åˆ—åˆ°æœ€åä¸€åˆ—ä¸­ï¼Œè¦åœ¨ä¸åŒåˆ—ä¸­åˆ†ææ¯”è¾ƒé—®é¢˜ä¸ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦ã€ç­”æ¡ˆçš„å®Œæ•´åº¦ã€æ­£ç¡®æ€§ï¼Œè¯„ä¼°é—®ç­”ç»“æœçš„è´¨é‡ï¼›\n",
    "\n",
    "é—®ç­”ç»“æœåˆ†åˆ«å¦‚ä¸‹ï¼š\n",
    "---\n",
    "åŸºäº Graph_RAG çš„ç»“æœ: {response_graph_rag}\n",
    "---\n",
    "åŸºäº Vector_DB çš„ç»“æœ: {response_vector_rag}\n",
    "---\n",
    "\"\"\"\n",
    "               )\n",
    "        )\n",
    "    )\n",
    "    return response_graph_rag, response_vector_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c52bbbc9-08bd-41c7-8125-b21d6f213a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: ç™½ç¼è™¾æ€ä¹ˆåšï¼Ÿ\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['keyword', 'è™¾', 'æ€ä¹ˆåš', 'ç™½ç¼', 'keyword(s): ç™½ç¼']\n",
      "ERROR:llama_index.indices.knowledge_graph.retriever:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge sequence in 2-depth in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]` that may be related to the task.\n",
      "è™¾ è™¾, æ‘†æ”¾æ•´é½, ç”¨äºç‚¸è™¾\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, å¯Œå«, è›‹ç™½è´¨\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, æ˜¯, ç¨‹åºå‘˜åœ¨æ²¿æµ·åœ°åŒºåš\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, éœ€è¦, æ°´äº§\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, éœ€è¦, æ´»è™¾\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, é€‚å®œ, 1-2 äººä»½\n",
      "ç™½ç¼ ç™½ç¼, åšæ³•, ç™½ç¼, æ˜¯, æ°´äº§\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| æ–¹æ³• | åŸºäº Graph_RAG | åŸºäº Vector_DB |\n",
       "| --- | --- | --- |\n",
       "| å¿…å¤‡åŸæ–™å’Œå·¥å…· | æ´»è™¾ã€æ´‹è‘±ã€å§œã€è’œã€è‘±ã€é£Ÿç”¨æ²¹ã€é…±æ²¹ã€æ–™é…’ã€èŠéº»ã€èšæ²¹ã€é¦™é†‹ | æ´»è™¾ã€è‘±ã€å§œã€è’œã€é£Ÿç”¨æ²¹ã€é…±æ²¹ã€æ–™é…’ã€èŠéº»ã€é¦™é†‹ã€èšæ²¹ |\n",
       "| è®¡ç®— | æ€»é‡ï¼šè™¾ 250g * ä»½æ•°ï¼ˆå»ºè®® 1-2 äººä»½ï¼‰<br>è‘±ä¸€æ ¹ã€å§œä¸€å—ã€æ´‹è‘±ä¸€å¤´ã€è’œ 5-8 ç“£ã€é£Ÿç”¨æ²¹ 10-15mlã€æ–™é…’ 20 mlã€é…±æ²¹ 10-15mlã€èŠéº»ä¸€æŠŠã€é¦™é†‹ 10 mlã€èšæ²¹ 10 ml | æ€»é‡ï¼šè™¾ 250g * ä»½æ•°ï¼ˆå»ºè®® 1-2 äººä»½ï¼‰<br>è‘±ä¸€æ ¹ã€è’œåˆ‡ç¢ã€å€’å…¥é…±æ²¹ã€èŠéº»ã€é¦™é†‹ï¼Œæ…æ‹Œä¹‹<br>æ²¹çƒ§çƒ­ï¼Œæ·‹å…¥è˜¸æ–™ |\n",
       "| æ“ä½œ | 1. æ´‹è‘±åˆ‡å°å—ï¼Œå§œåˆ‡ç‰‡ï¼Œå¹³é“ºå¹³åº•é”…<br>2. æ´»è™¾å†²æ´—ä¸€ä¸‹ï¼ˆå»é™¤è™¾çº¿ã€å‰ªåˆ€å‡æ‰è™¾è…¿è™¾é¡»å­éƒ½æ˜¯å¯é€‰æ“ä½œï¼‰ï¼Œæ§æ°´ï¼Œé“ºåœ¨å¹³åº•é”…çš„æ´‹è‘±ã€å§œç‰‡ä¹‹ä¸Š<br>3. é”…å†…å€’å…¥æ–™é…’ï¼Œç›–ä¸Šé”…ç›–ï¼Œä¸­ç« 1 åˆ†é’Ÿï¼Œå°ç« 5 åˆ†é’Ÿï¼Œå…³ç« 5 åˆ†é’Ÿ<br>4. å’Œä¸Šä¸€æ­¥å¹¶è¡Œæ“ä½œï¼Œåˆ¶ä½œè˜¸æ–™ï¼š<br>- è‘±åˆ‡æˆè‘±èŠ±ã€è’œåˆ‡ç¢ã€å€’å…¥é…±æ²¹ã€èŠéº»ã€é¦™é†‹ï¼Œæ…æ‹Œä¹‹<br>- æ²¹çƒ§çƒ­ï¼Œæ·‹å…¥è˜¸æ–™ |\n",
       "| é™„åŠ å†…å®¹ | - æŠ€æœ¯ç»†èŠ‚ï¼š<br>å¼€å§‹ä¸èƒ½å¤§ç«ã€é˜²æ­¢ç³Šåº•ã€‚<br>å¦‚æœé”…ç›–æœ‰é€šæ°”å£ã€æ—¶é—´è¦ç›¸åº”è°ƒèŠ‚ä¸€ä¸‹ï¼ˆè€ƒè™‘å¢åŠ  30 ç§’ä¸­ç«ï¼‰ã€‚ | - é€‰æ‹©æ–°é²œçš„å¤§è™¾ï¼Œæœ€å¥½æ˜¯æ´»è™¾ï¼Œè™¾ä½“éœ€è¦æ¸…æ´—å¹²å‡€ã€‚<br>- åœ¨è™¾ä½“ä¸Šæ´’ä¸Šå°‘é‡ç›ï¼ŒæŠ“åŒ€åæ”¾ç½®ä¸€æ®µæ—¶é—´ï¼Œè®©è™¾ä½“å…¥å‘³ã€‚<br>- æŠŠè™¾ä½“æ”¾è¿›æ²¸æ°´ä¸­ï¼Œå¤§ç«ç…®å¼€åè½¬å°ç«ï¼Œç›–ä¸Šé”…ç›–ç…®çº¦2-3åˆ†é’Ÿï¼Œç›´åˆ°è™¾ä½“å˜çº¢ã€‚<br>- æå‡ºè™¾ä½“ï¼Œæ²¥æ°´ååŠ å…¥é€‚é‡çš„ç”ŸæŠ½ã€æ–™é…’ã€ç™½ç³–ã€å§œä¸å’Œè‘±èŠ±ï¼Œæ‹ŒåŒ€åå³å¯ä¸Šæ¡Œäº«ç”¨ã€‚ |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"ç™½ç¼è™¾æ€ä¹ˆåšï¼Ÿ\"\n",
    "r = compare_graph_and_vector(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
